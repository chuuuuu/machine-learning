{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw1_emsembler_average.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wS_4-77xHk44",
        "g0pdrhQAO41L",
        "aQikz3IPiyPf",
        "nfrVxqJanGpE",
        "9tmCwXgpot3t"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chuuuuu/machine_learning_2021/blob/main/homework/hw01/hw1_emsembler_average.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz0_QVkxCrX3"
      },
      "source": [
        "# **Homework 1: COVID-19 Cases Prediction (Regression)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMj55YDKG6ch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "170676b2-8880-4142-cd5f-aa3f13ec9db6"
      },
      "source": [
        "from os import path, makedirs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "WORKSPACE = 'drive/MyDrive/ColabNotebooks/HW1'\n",
        "\n",
        "if not path.exists(f'{WORKSPACE}/dataset'):\n",
        "  makedirs(f'{WORKSPACE}/dataset')\n",
        "  !gdown --id '19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF' --output '{WORKSPACE}/dataset/covid.train.csv'\n",
        "  !gdown --id '1CE240jLm2npU-tdz81-oVKEF3T2yfT1O' --output '{WORKSPACE}/dataset/covid.test.csv'\n",
        "\n",
        "DATA_PATH = f'{WORKSPACE}/dataset'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF-QTQLkv5h8"
      },
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# set a random seed for reproducibility\n",
        "myseed = 42069\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)\n",
        "\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "config = {\n",
        "    # 'INPUT_DIM': 14,\n",
        "    'INPUT_DIM': 90,\n",
        "    'TRAIN_PATH': f'{DATA_PATH}/covid.train.csv',\n",
        "    'TEST_PATH': f'{DATA_PATH}/covid.test.csv',\n",
        "    'MODEL_PATH': 'models/model.pth',\n",
        "    'PRED_PATH': 'pred.csv',\n",
        "\n",
        "    'EPOCH_NUM': 30000,\n",
        "    'BATCH_SIZE': 4096,\n",
        "    'VAL_RATIO': 0.1,\n",
        "    'OPTIMIZER': 'Adam',\n",
        "    'OPTIM_PARAMS': {\n",
        "        'lr': 5e-2,\n",
        "        # 'weight_decay': 1e-4,\n",
        "    },\n",
        "    # 'DECAY_RATE': 0.999,\n",
        "    'DECAY_RATE': 1,\n",
        "    'MIN_LR': 1e-4,\n",
        "    'EARLY_STOP': 300,\n",
        "    'MODEL_NUM': 3,\n",
        "    'STATE': 1,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaRGyPYGvvV5"
      },
      "source": [
        "class Drawer():\n",
        "    def plot_learning_curve(self, loss_record, title=''):\n",
        "        ''' Plot learning curve of your DNN (train & dev loss) '''\n",
        "        total_steps = len(loss_record['train'])\n",
        "        x_1 = range(total_steps)\n",
        "        x_2 = x_1[::len(loss_record['train']) // len(loss_record['val'])]\n",
        "        figure(figsize=(6, 4))\n",
        "        plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
        "        plt.plot(x_2, loss_record['val'], c='tab:cyan', label='val')\n",
        "        plt.ylim(0.0, 5.)\n",
        "        plt.xlabel('Training steps')\n",
        "        plt.ylabel('MSE loss')\n",
        "        plt.title('Learning curve of {}'.format(title))\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_pred(self, dv_set, model, device, lim=35., preds=None, targets=None):\n",
        "        ''' Plot prediction of your DNN '''\n",
        "        if preds is None or targets is None:\n",
        "            model.eval()\n",
        "            preds, targets = [], []\n",
        "            for x, y in dv_set:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                with torch.no_grad():\n",
        "                    pred = model(x)\n",
        "                    preds.append(pred.detach().cpu())\n",
        "                    targets.append(y.detach().cpu())\n",
        "            preds = torch.cat(preds, dim=0).numpy()\n",
        "            targets = torch.cat(targets, dim=0).numpy()\n",
        "\n",
        "        figure(figsize=(5, 5))\n",
        "        plt.scatter(targets, preds, c='r', alpha=0.5)\n",
        "        plt.plot([-0.2, lim], [-0.2, lim], c='b')\n",
        "        plt.xlim(-0.2, lim)\n",
        "        plt.ylim(-0.2, lim)\n",
        "        plt.xlabel('ground truth value')\n",
        "        plt.ylabel('predicted value')\n",
        "        plt.title('Ground Truth v.s. Prediction')\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uazKiUCwD9U"
      },
      "source": [
        "from sklearn.feature_selection import f_regression, SelectKBest, mutual_info_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DataManager():\n",
        "    def __init__(self):\n",
        "        print('init data manager...')\n",
        "        TRAIN_PATH = config['TRAIN_PATH']\n",
        "        INPUT_DIM = config['INPUT_DIM']\n",
        "\n",
        "        with open(TRAIN_PATH, 'r') as f:\n",
        "            self.data = list(csv.reader(f))\n",
        "            self.data = np.array(self.data[1:])[:, 1:].astype(np.float32)\n",
        "\n",
        "        self.X = self.data[:, :-1]\n",
        "        self.y = self.data[:, -1]\n",
        "\n",
        "        selector = SelectKBest(f_regression, k=INPUT_DIM)\n",
        "        selector.fit(self.X, self.y)\n",
        "        self.cols = selector.get_support(indices=True)\n",
        "\n",
        "        self.X = self.X[:, self.cols]\n",
        "\n",
        "    def get_train_data(self):\n",
        "        print('getting train data...')\n",
        "        VAL_RATIO = config['VAL_RATIO']\n",
        "        STATE = config['STATE']\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(self.X, self.y, test_size=VAL_RATIO, random_state=STATE)\n",
        "        config['STATE'] += 1\n",
        "\n",
        "        return X_train, X_val, y_train, y_val\n",
        "    \n",
        "    def get_test_data(self):\n",
        "        print('getting test data...')\n",
        "        TEST_PATH = config['TEST_PATH']\n",
        "        with open(TEST_PATH, 'r') as f:\n",
        "            data = list(csv.reader(f))\n",
        "            data = np.array(data[1:])[:, 1:].astype(np.float32)\n",
        "        X_test = data[:, self.cols]\n",
        "        return X_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3N7YzC72Ykp"
      },
      "source": [
        "class CovidDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        print('init dataset...')\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        if y is None:\n",
        "            self.y = None\n",
        "        else:\n",
        "            self.y = torch.from_numpy(y).float()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is None:\n",
        "            return self.X[idx]\n",
        "\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGJ7r5sQ4bTU"
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        print('init neural net...')\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        INPUT_DIM = config['INPUT_DIM']\n",
        "\n",
        "        # for input_dim == 14\n",
        "        # 12: 0.844\n",
        "        # 16: 0.860\n",
        "        # 14: 0.855\n",
        "        self.net = nn.Sequential(\n",
        "            # nn.BatchNorm1d(INPUT_DIM),\n",
        "            # nn.Linear(INPUT_DIM, 16),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Linear(16, 1),\n",
        "            nn.BatchNorm1d(INPUT_DIM),\n",
        "            nn.Linear(INPUT_DIM, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "        )\n",
        "\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "    def get_loss(self, y_pred, y):\n",
        "        return self.criterion(y_pred, y)\n",
        "\n",
        "    def summary(self):\n",
        "        INPUT_DIM = config['INPUT_DIM']\n",
        "        summary(self, (INPUT_DIM, ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxNvo1sv5Qyb"
      },
      "source": [
        "class Trainer():\n",
        "    def __init__(self):\n",
        "        print('init trainer')\n",
        "        self.set_device()\n",
        "        self.set_model()\n",
        "        self.set_data_loader()\n",
        "        self.set_drawer()\n",
        "        self.set_optim()\n",
        "\n",
        "        self.loss_record = {'train': [], 'val': []}\n",
        "\n",
        "    def set_drawer(self):\n",
        "        self.drawer = Drawer()\n",
        "\n",
        "    def draw_learning_curve(self):\n",
        "        self.drawer.plot_learning_curve(self.loss_record, title='deep model')\n",
        "\n",
        "    def draw_val_results(self):\n",
        "        MODEL_PATH = config['MODEL_PATH']\n",
        "        del self.model\n",
        "        self.set_model()\n",
        "\n",
        "        ckpt = torch.load(MODEL_PATH, map_location='cpu')\n",
        "        self.model.load_state_dict(ckpt)\n",
        "        self.drawer.plot_pred(self.val_loader, self.model, self.device)\n",
        "\n",
        "    def pred_y_test(self):\n",
        "        print('predicting...')\n",
        "        BATCH_SIZE = config['BATCH_SIZE']\n",
        "        PRED_PATH = config['PRED_PATH']\n",
        "\n",
        "        X_test = self.dataManager.get_test_data()\n",
        "        test_loader = DataLoader(X_test, BATCH_SIZE, False, drop_last=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "        self.model.eval()\n",
        "        y_preds = []\n",
        "        for x in test_loader:\n",
        "            x = x.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                y_pred = self.model(x)\n",
        "                y_preds.append(y_pred.detach().cpu())\n",
        "        \n",
        "        y_preds = torch.cat(y_preds, dim=0).numpy()\n",
        "        return y_preds\n",
        "\n",
        "    def set_device(self):\n",
        "        ''' Get device (if GPU is available, use GPU) '''\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def set_data_loader(self):\n",
        "        BATCH_SIZE = config['BATCH_SIZE']\n",
        "        self.dataManager = DataManager()\n",
        "        X_train, X_val, y_train, y_val = self.dataManager.get_train_data()   \n",
        "\n",
        "        train_set = CovidDataset(X_train, y_train)\n",
        "        self.train_loader = DataLoader(train_set, BATCH_SIZE, True, drop_last=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "        val_set = CovidDataset(X_val, y_val)\n",
        "        self.val_loader = DataLoader(val_set, BATCH_SIZE, False, drop_last=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "    def set_model(self):\n",
        "        self.model = NeuralNet().to(self.device)\n",
        "\n",
        "    def set_optim(self):\n",
        "        OPTIMIZER = config['OPTIMIZER']\n",
        "        OPTIM_PARAMS = config['OPTIM_PARAMS']\n",
        "\n",
        "        self.optimizer = getattr(torch.optim, OPTIMIZER)(self.model.parameters(), **OPTIM_PARAMS)\n",
        "\n",
        "    def train(self):\n",
        "        EPOCH_NUM = config['EPOCH_NUM']\n",
        "        MODEL_PATH = config['MODEL_PATH']\n",
        "        EARLY_STOP = config['EARLY_STOP']\n",
        "\n",
        "        min_val_loss = float('inf')\n",
        "        early_stop_count = 0\n",
        "        for epoch in range(EPOCH_NUM):\n",
        "            self.model.train()\n",
        "            for x, y in self.train_loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                x, y = x.to(self.device), y.to(self.device)\n",
        "                y_pred = self.model(x)\n",
        "                loss = self.model.get_loss(y_pred, y)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            self.update_lr()\n",
        "            val_loss = self.get_loss(self.val_loader)\n",
        "            train_loss = self.get_loss(self.train_loader)\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'epoch: {epoch+1}, train_loss: {train_loss}, val_loss: {val_loss}')\n",
        "\n",
        "            if val_loss < min_val_loss:\n",
        "                min_val_loss = val_loss\n",
        "                print(f'Saving model, epoch: {epoch+1}, train_loss: {train_loss}, val_loss: {val_loss}')\n",
        "                torch.save(self.model.state_dict(), MODEL_PATH)\n",
        "                early_stop_cnt = 0\n",
        "\n",
        "            else:\n",
        "                early_stop_cnt += 1\n",
        "\n",
        "            self.loss_record['val'].append(val_loss)\n",
        "            self.loss_record['train'].append(train_loss)\n",
        "\n",
        "            if early_stop_cnt > EARLY_STOP:\n",
        "                break\n",
        "\n",
        "        # print(f'Saving model, epoch: {epoch+1}, train_loss: {train_loss}, val_loss: {val_loss}')\n",
        "        # torch.save(self.model.state_dict(), MODEL_PATH)\n",
        "        ckpt = torch.load(MODEL_PATH, map_location='cpu')  # Load your best model\n",
        "        self.model.load_state_dict(ckpt)\n",
        "\n",
        "        print(f'finished training after {epoch+1} epochs')\n",
        "        self.model.summary()\n",
        "\n",
        "    \n",
        "    def update_lr(self):\n",
        "        DECAY_RATE = config['DECAY_RATE']\n",
        "        MIN_LR = config['MIN_LR']\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = param_group['lr'] * DECAY_RATE\n",
        "            param_group['lr'] = max(MIN_LR, param_group['lr'])\n",
        "\n",
        "    def get_loss(self, loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(self.device), y.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                y_pred = self.model(x)\n",
        "                loss = self.model.get_loss(y_pred, y)\n",
        "            \n",
        "            total_loss += loss.detach().cpu().item() * len(x)\n",
        "        total_loss /= len(loader.dataset)\n",
        "\n",
        "        return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikKiqeHNUTUK"
      },
      "source": [
        "class Emssembler():\n",
        "    def __init__(self):\n",
        "        MODEL_NUM = config['MODEL_NUM']\n",
        "        self.trainers = []\n",
        "        for i in range(MODEL_NUM):\n",
        "            self.trainers.append(Trainer())\n",
        "\n",
        "    def train(self):\n",
        "        MODEL_NUM = config['MODEL_NUM']\n",
        "        for trainer in self.trainers:\n",
        "            trainer.train()\n",
        "\n",
        "    def pred(self):\n",
        "        MODEL_NUM = config['MODEL_NUM']\n",
        "        PRED_PATH = config['PRED_PATH']\n",
        "        \n",
        "        y_preds = None\n",
        "        for trainer in self.trainers:\n",
        "            if y_preds is None:\n",
        "                y_preds = trainer.pred_y_test()\n",
        "            else:\n",
        "                y_preds += trainer.pred_y_test()\n",
        "\n",
        "        y_preds /= MODEL_NUM\n",
        "\n",
        "        with open(PRED_PATH, 'w') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['id', 'tested_positive'])\n",
        "            for i, p in enumerate(y_preds):\n",
        "                writer.writerow([i, p])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMBMUOu0ng0Y",
        "outputId": "0376f210-20e4-4712-bebb-357fdf546d05"
      },
      "source": [
        "# trainer = Trainer()\n",
        "# trainer.train()\n",
        "# trainer.draw_learning_curve()\n",
        "# trainer.draw_val_results()\n",
        "# trainer.pred_y_test()\n",
        "emssembler = Emssembler()\n",
        "emssembler.train()\n",
        "emssembler.pred()\n",
        "print(f'config: {config}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init trainer\n",
            "init neural net...\n",
            "init data manager...\n",
            "getting train data...\n",
            "init dataset...\n",
            "init dataset...\n",
            "init trainer\n",
            "init neural net...\n",
            "init data manager...\n",
            "getting train data...\n",
            "init dataset...\n",
            "init dataset...\n",
            "init trainer\n",
            "init neural net...\n",
            "init data manager...\n",
            "getting train data...\n",
            "init dataset...\n",
            "init dataset...\n",
            "epoch: 1, train_loss: 212.09690856933594, val_loss: 209.4459991455078\n",
            "Saving model, epoch: 1, train_loss: 212.09690856933594, val_loss: 209.4459991455078\n",
            "Saving model, epoch: 2, train_loss: 159.7740936279297, val_loss: 158.03463745117188\n",
            "Saving model, epoch: 3, train_loss: 85.24370574951172, val_loss: 83.52495574951172\n",
            "Saving model, epoch: 4, train_loss: 33.01015853881836, val_loss: 31.402835845947266\n",
            "Saving model, epoch: 10, train_loss: 20.869855880737305, val_loss: 20.821613311767578\n",
            "Saving model, epoch: 18, train_loss: 20.45854949951172, val_loss: 19.47454071044922\n",
            "Saving model, epoch: 19, train_loss: 15.57339859008789, val_loss: 14.793313026428223\n",
            "Saving model, epoch: 22, train_loss: 13.491826057434082, val_loss: 13.154701232910156\n",
            "Saving model, epoch: 23, train_loss: 11.73496150970459, val_loss: 11.400842666625977\n",
            "Saving model, epoch: 31, train_loss: 9.901591300964355, val_loss: 9.306375503540039\n",
            "Saving model, epoch: 32, train_loss: 7.330155372619629, val_loss: 6.815127849578857\n",
            "Saving model, epoch: 33, train_loss: 6.367077350616455, val_loss: 5.973087310791016\n",
            "Saving model, epoch: 36, train_loss: 5.826581001281738, val_loss: 5.608541011810303\n",
            "Saving model, epoch: 37, train_loss: 5.586677551269531, val_loss: 5.3290181159973145\n",
            "Saving model, epoch: 44, train_loss: 4.861660957336426, val_loss: 4.6165642738342285\n",
            "Saving model, epoch: 45, train_loss: 4.089285850524902, val_loss: 3.958219289779663\n",
            "Saving model, epoch: 46, train_loss: 3.863891363143921, val_loss: 3.8628227710723877\n",
            "Saving model, epoch: 49, train_loss: 3.6082277297973633, val_loss: 3.756134271621704\n",
            "Saving model, epoch: 50, train_loss: 3.4639925956726074, val_loss: 3.575645923614502\n",
            "Saving model, epoch: 56, train_loss: 3.0419158935546875, val_loss: 3.315636157989502\n",
            "Saving model, epoch: 57, train_loss: 2.8559634685516357, val_loss: 3.205646514892578\n",
            "Saving model, epoch: 60, train_loss: 2.7729458808898926, val_loss: 3.1996119022369385\n",
            "Saving model, epoch: 61, train_loss: 2.635662317276001, val_loss: 3.025479793548584\n",
            "Saving model, epoch: 62, train_loss: 2.5597286224365234, val_loss: 2.900317668914795\n",
            "Saving model, epoch: 63, train_loss: 2.5827982425689697, val_loss: 2.876939535140991\n",
            "Saving model, epoch: 65, train_loss: 2.590339183807373, val_loss: 2.834719181060791\n",
            "Saving model, epoch: 66, train_loss: 2.4640893936157227, val_loss: 2.7114436626434326\n",
            "Saving model, epoch: 67, train_loss: 2.3267881870269775, val_loss: 2.589777946472168\n",
            "Saving model, epoch: 68, train_loss: 2.2551376819610596, val_loss: 2.5430784225463867\n",
            "Saving model, epoch: 70, train_loss: 2.2171132564544678, val_loss: 2.5317656993865967\n",
            "Saving model, epoch: 71, train_loss: 2.1471776962280273, val_loss: 2.44639253616333\n",
            "Saving model, epoch: 72, train_loss: 2.0731618404388428, val_loss: 2.3430898189544678\n",
            "Saving model, epoch: 73, train_loss: 2.0411407947540283, val_loss: 2.2806286811828613\n",
            "Saving model, epoch: 74, train_loss: 2.0377540588378906, val_loss: 2.253826141357422\n",
            "Saving model, epoch: 75, train_loss: 2.013920545578003, val_loss: 2.21891713142395\n",
            "Saving model, epoch: 76, train_loss: 1.9515693187713623, val_loss: 2.1584408283233643\n",
            "Saving model, epoch: 77, train_loss: 1.8844246864318848, val_loss: 2.1025900840759277\n",
            "Saving model, epoch: 78, train_loss: 1.849348545074463, val_loss: 2.0819194316864014\n",
            "Saving model, epoch: 80, train_loss: 1.8134434223175049, val_loss: 2.0566132068634033\n",
            "Saving model, epoch: 81, train_loss: 1.7684051990509033, val_loss: 1.9974673986434937\n",
            "Saving model, epoch: 82, train_loss: 1.7282174825668335, val_loss: 1.9376835823059082\n",
            "Saving model, epoch: 83, train_loss: 1.7094184160232544, val_loss: 1.9016900062561035\n",
            "Saving model, epoch: 84, train_loss: 1.6962251663208008, val_loss: 1.878778100013733\n",
            "Saving model, epoch: 85, train_loss: 1.6686625480651855, val_loss: 1.8500876426696777\n",
            "Saving model, epoch: 86, train_loss: 1.6315170526504517, val_loss: 1.8189096450805664\n",
            "Saving model, epoch: 87, train_loss: 1.6035847663879395, val_loss: 1.800181269645691\n",
            "Saving model, epoch: 88, train_loss: 1.5884864330291748, val_loss: 1.792776107788086\n",
            "Saving model, epoch: 89, train_loss: 1.5716321468353271, val_loss: 1.7781288623809814\n",
            "Saving model, epoch: 90, train_loss: 1.5458379983901978, val_loss: 1.7483066320419312\n",
            "Saving model, epoch: 91, train_loss: 1.521246075630188, val_loss: 1.7170048952102661\n",
            "Saving model, epoch: 92, train_loss: 1.506344199180603, val_loss: 1.6958820819854736\n",
            "Saving model, epoch: 93, train_loss: 1.493793249130249, val_loss: 1.6805555820465088\n",
            "Saving model, epoch: 94, train_loss: 1.4746723175048828, val_loss: 1.6631370782852173\n",
            "Saving model, epoch: 95, train_loss: 1.4523969888687134, val_loss: 1.646501064300537\n",
            "Saving model, epoch: 96, train_loss: 1.4356755018234253, val_loss: 1.6370196342468262\n",
            "Saving model, epoch: 97, train_loss: 1.4241141080856323, val_loss: 1.6312541961669922\n",
            "Saving model, epoch: 98, train_loss: 1.410269856452942, val_loss: 1.6189005374908447\n",
            "Saving model, epoch: 99, train_loss: 1.3929080963134766, val_loss: 1.598546028137207\n",
            "Saving model, epoch: 100, train_loss: 1.3779023885726929, val_loss: 1.5779311656951904\n",
            "epoch: 101, train_loss: 1.3670337200164795, val_loss: 1.562247395515442\n",
            "Saving model, epoch: 101, train_loss: 1.3670337200164795, val_loss: 1.562247395515442\n",
            "Saving model, epoch: 102, train_loss: 1.3555314540863037, val_loss: 1.548228144645691\n",
            "Saving model, epoch: 103, train_loss: 1.3410747051239014, val_loss: 1.5335713624954224\n",
            "Saving model, epoch: 104, train_loss: 1.3273221254348755, val_loss: 1.5210551023483276\n",
            "Saving model, epoch: 105, train_loss: 1.3168898820877075, val_loss: 1.5108850002288818\n",
            "Saving model, epoch: 106, train_loss: 1.3072192668914795, val_loss: 1.5000592470169067\n",
            "Saving model, epoch: 107, train_loss: 1.2956745624542236, val_loss: 1.48537278175354\n",
            "Saving model, epoch: 108, train_loss: 1.2842093706130981, val_loss: 1.469390630722046\n",
            "Saving model, epoch: 109, train_loss: 1.275100588798523, val_loss: 1.4555256366729736\n",
            "Saving model, epoch: 110, train_loss: 1.2665690183639526, val_loss: 1.4436135292053223\n",
            "Saving model, epoch: 111, train_loss: 1.2568089962005615, val_loss: 1.4321112632751465\n",
            "Saving model, epoch: 112, train_loss: 1.2471089363098145, val_loss: 1.422062635421753\n",
            "Saving model, epoch: 113, train_loss: 1.2389799356460571, val_loss: 1.4141219854354858\n",
            "Saving model, epoch: 114, train_loss: 1.2313733100891113, val_loss: 1.406221628189087\n",
            "Saving model, epoch: 115, train_loss: 1.2230043411254883, val_loss: 1.3968098163604736\n",
            "Saving model, epoch: 116, train_loss: 1.2148631811141968, val_loss: 1.3873053789138794\n",
            "Saving model, epoch: 117, train_loss: 1.2079123258590698, val_loss: 1.3794924020767212\n",
            "Saving model, epoch: 118, train_loss: 1.2012194395065308, val_loss: 1.3731480836868286\n",
            "Saving model, epoch: 119, train_loss: 1.1939693689346313, val_loss: 1.3671754598617554\n",
            "Saving model, epoch: 120, train_loss: 1.1868624687194824, val_loss: 1.362176775932312\n",
            "Saving model, epoch: 121, train_loss: 1.1805691719055176, val_loss: 1.3579500913619995\n",
            "Saving model, epoch: 122, train_loss: 1.1744167804718018, val_loss: 1.353008508682251\n",
            "Saving model, epoch: 123, train_loss: 1.1679670810699463, val_loss: 1.346897840499878\n",
            "Saving model, epoch: 124, train_loss: 1.161872386932373, val_loss: 1.3406122922897339\n",
            "Saving model, epoch: 125, train_loss: 1.1563435792922974, val_loss: 1.334967851638794\n",
            "Saving model, epoch: 126, train_loss: 1.1508103609085083, val_loss: 1.329910159111023\n",
            "Saving model, epoch: 127, train_loss: 1.145054817199707, val_loss: 1.325382947921753\n",
            "Saving model, epoch: 128, train_loss: 1.1396666765213013, val_loss: 1.3213671445846558\n",
            "Saving model, epoch: 129, train_loss: 1.13470458984375, val_loss: 1.3174450397491455\n",
            "Saving model, epoch: 130, train_loss: 1.1297792196273804, val_loss: 1.312882661819458\n",
            "Saving model, epoch: 131, train_loss: 1.1249287128448486, val_loss: 1.3077640533447266\n",
            "Saving model, epoch: 132, train_loss: 1.1204208135604858, val_loss: 1.3026536703109741\n",
            "Saving model, epoch: 133, train_loss: 1.1160708665847778, val_loss: 1.297708511352539\n",
            "Saving model, epoch: 134, train_loss: 1.111678957939148, val_loss: 1.2928149700164795\n",
            "Saving model, epoch: 135, train_loss: 1.1073763370513916, val_loss: 1.2880769968032837\n",
            "Saving model, epoch: 136, train_loss: 1.1033307313919067, val_loss: 1.2834886312484741\n",
            "Saving model, epoch: 137, train_loss: 1.0993974208831787, val_loss: 1.2788020372390747\n",
            "Saving model, epoch: 138, train_loss: 1.0954633951187134, val_loss: 1.273893117904663\n",
            "Saving model, epoch: 139, train_loss: 1.0916553735733032, val_loss: 1.2691069841384888\n",
            "Saving model, epoch: 140, train_loss: 1.0879452228546143, val_loss: 1.264655590057373\n",
            "Saving model, epoch: 141, train_loss: 1.0842019319534302, val_loss: 1.2604297399520874\n",
            "Saving model, epoch: 142, train_loss: 1.080471158027649, val_loss: 1.2563481330871582\n",
            "Saving model, epoch: 143, train_loss: 1.0768674612045288, val_loss: 1.2524036169052124\n",
            "Saving model, epoch: 144, train_loss: 1.073363184928894, val_loss: 1.2484872341156006\n",
            "Saving model, epoch: 145, train_loss: 1.0699007511138916, val_loss: 1.2445517778396606\n",
            "Saving model, epoch: 146, train_loss: 1.0664818286895752, val_loss: 1.2406684160232544\n",
            "Saving model, epoch: 147, train_loss: 1.0631393194198608, val_loss: 1.2368650436401367\n",
            "Saving model, epoch: 148, train_loss: 1.059771180152893, val_loss: 1.2331724166870117\n",
            "Saving model, epoch: 149, train_loss: 1.0563910007476807, val_loss: 1.2295204401016235\n",
            "Saving model, epoch: 150, train_loss: 1.053116798400879, val_loss: 1.2259283065795898\n",
            "Saving model, epoch: 151, train_loss: 1.0499296188354492, val_loss: 1.2222509384155273\n",
            "Saving model, epoch: 152, train_loss: 1.046730637550354, val_loss: 1.218517780303955\n",
            "Saving model, epoch: 153, train_loss: 1.0435734987258911, val_loss: 1.214774250984192\n",
            "Saving model, epoch: 154, train_loss: 1.040474534034729, val_loss: 1.2111856937408447\n",
            "Saving model, epoch: 155, train_loss: 1.0373812913894653, val_loss: 1.2076537609100342\n",
            "Saving model, epoch: 156, train_loss: 1.034266471862793, val_loss: 1.204241394996643\n",
            "Saving model, epoch: 157, train_loss: 1.031165361404419, val_loss: 1.2009598016738892\n",
            "Saving model, epoch: 158, train_loss: 1.0281107425689697, val_loss: 1.197681188583374\n",
            "Saving model, epoch: 159, train_loss: 1.025094985961914, val_loss: 1.1945407390594482\n",
            "Saving model, epoch: 160, train_loss: 1.022149920463562, val_loss: 1.19167160987854\n",
            "Saving model, epoch: 161, train_loss: 1.019268274307251, val_loss: 1.1889779567718506\n",
            "Saving model, epoch: 162, train_loss: 1.0164295434951782, val_loss: 1.1860952377319336\n",
            "Saving model, epoch: 163, train_loss: 1.013624906539917, val_loss: 1.1832544803619385\n",
            "Saving model, epoch: 164, train_loss: 1.0108845233917236, val_loss: 1.1804709434509277\n",
            "Saving model, epoch: 165, train_loss: 1.0082288980484009, val_loss: 1.1776660680770874\n",
            "Saving model, epoch: 166, train_loss: 1.005623459815979, val_loss: 1.1748017072677612\n",
            "Saving model, epoch: 167, train_loss: 1.0030317306518555, val_loss: 1.1720083951950073\n",
            "Saving model, epoch: 168, train_loss: 1.0004669427871704, val_loss: 1.1692074537277222\n",
            "Saving model, epoch: 169, train_loss: 0.9979308247566223, val_loss: 1.166427731513977\n",
            "Saving model, epoch: 170, train_loss: 0.9954152703285217, val_loss: 1.1636425256729126\n",
            "Saving model, epoch: 171, train_loss: 0.9929608106613159, val_loss: 1.160875678062439\n",
            "Saving model, epoch: 172, train_loss: 0.9905576705932617, val_loss: 1.158139705657959\n",
            "Saving model, epoch: 173, train_loss: 0.988177478313446, val_loss: 1.155397653579712\n",
            "Saving model, epoch: 174, train_loss: 0.9858378171920776, val_loss: 1.1526317596435547\n",
            "Saving model, epoch: 175, train_loss: 0.9835003018379211, val_loss: 1.1498546600341797\n",
            "Saving model, epoch: 176, train_loss: 0.9811666011810303, val_loss: 1.1471471786499023\n",
            "Saving model, epoch: 177, train_loss: 0.9788568019866943, val_loss: 1.1446634531021118\n",
            "Saving model, epoch: 178, train_loss: 0.9765502214431763, val_loss: 1.1423413753509521\n",
            "Saving model, epoch: 179, train_loss: 0.974341094493866, val_loss: 1.1400526762008667\n",
            "Saving model, epoch: 180, train_loss: 0.9721946716308594, val_loss: 1.1375105381011963\n",
            "Saving model, epoch: 181, train_loss: 0.9700546860694885, val_loss: 1.134839415550232\n",
            "Saving model, epoch: 182, train_loss: 0.9679387807846069, val_loss: 1.132076382637024\n",
            "Saving model, epoch: 183, train_loss: 0.965843141078949, val_loss: 1.1291700601577759\n",
            "Saving model, epoch: 184, train_loss: 0.9637647271156311, val_loss: 1.1261955499649048\n",
            "Saving model, epoch: 185, train_loss: 0.961711585521698, val_loss: 1.1232701539993286\n",
            "Saving model, epoch: 186, train_loss: 0.9597104787826538, val_loss: 1.1204878091812134\n",
            "Saving model, epoch: 187, train_loss: 0.9577323198318481, val_loss: 1.117868423461914\n",
            "Saving model, epoch: 188, train_loss: 0.9557766914367676, val_loss: 1.1152631044387817\n",
            "Saving model, epoch: 189, train_loss: 0.953843355178833, val_loss: 1.1127265691757202\n",
            "Saving model, epoch: 190, train_loss: 0.9519506096839905, val_loss: 1.1104300022125244\n",
            "Saving model, epoch: 191, train_loss: 0.9500877857208252, val_loss: 1.108297348022461\n",
            "Saving model, epoch: 192, train_loss: 0.9482528567314148, val_loss: 1.106319546699524\n",
            "Saving model, epoch: 193, train_loss: 0.9464362263679504, val_loss: 1.1044766902923584\n",
            "Saving model, epoch: 194, train_loss: 0.9446325302124023, val_loss: 1.102614164352417\n",
            "Saving model, epoch: 195, train_loss: 0.9428680539131165, val_loss: 1.1007673740386963\n",
            "Saving model, epoch: 196, train_loss: 0.9411291480064392, val_loss: 1.0989753007888794\n",
            "Saving model, epoch: 197, train_loss: 0.939408540725708, val_loss: 1.0971720218658447\n",
            "Saving model, epoch: 198, train_loss: 0.9377204179763794, val_loss: 1.09531831741333\n",
            "Saving model, epoch: 199, train_loss: 0.9360506534576416, val_loss: 1.0933705568313599\n",
            "Saving model, epoch: 200, train_loss: 0.9343993067741394, val_loss: 1.0913846492767334\n",
            "epoch: 201, train_loss: 0.932763397693634, val_loss: 1.0894863605499268\n",
            "Saving model, epoch: 201, train_loss: 0.932763397693634, val_loss: 1.0894863605499268\n",
            "Saving model, epoch: 202, train_loss: 0.931149959564209, val_loss: 1.0876940488815308\n",
            "Saving model, epoch: 203, train_loss: 0.9295384883880615, val_loss: 1.0860145092010498\n",
            "Saving model, epoch: 204, train_loss: 0.9279320240020752, val_loss: 1.0844701528549194\n",
            "Saving model, epoch: 205, train_loss: 0.926308810710907, val_loss: 1.0830693244934082\n",
            "Saving model, epoch: 206, train_loss: 0.9246851205825806, val_loss: 1.081788182258606\n",
            "Saving model, epoch: 207, train_loss: 0.9230780005455017, val_loss: 1.0804651975631714\n",
            "Saving model, epoch: 208, train_loss: 0.921491801738739, val_loss: 1.0790542364120483\n",
            "Saving model, epoch: 209, train_loss: 0.919910728931427, val_loss: 1.0775853395462036\n",
            "Saving model, epoch: 210, train_loss: 0.9183453321456909, val_loss: 1.0761337280273438\n",
            "Saving model, epoch: 211, train_loss: 0.9168111681938171, val_loss: 1.0747214555740356\n",
            "Saving model, epoch: 212, train_loss: 0.9152946472167969, val_loss: 1.0733325481414795\n",
            "Saving model, epoch: 213, train_loss: 0.9137943387031555, val_loss: 1.0719674825668335\n",
            "Saving model, epoch: 214, train_loss: 0.9123319387435913, val_loss: 1.0705978870391846\n",
            "Saving model, epoch: 215, train_loss: 0.9108955264091492, val_loss: 1.0692752599716187\n",
            "Saving model, epoch: 216, train_loss: 0.9094875454902649, val_loss: 1.0680491924285889\n",
            "Saving model, epoch: 217, train_loss: 0.9081028699874878, val_loss: 1.0668675899505615\n",
            "Saving model, epoch: 218, train_loss: 0.9067364931106567, val_loss: 1.0656015872955322\n",
            "Saving model, epoch: 219, train_loss: 0.905390202999115, val_loss: 1.0643532276153564\n",
            "Saving model, epoch: 220, train_loss: 0.9040724039077759, val_loss: 1.063045620918274\n",
            "Saving model, epoch: 221, train_loss: 0.9027854204177856, val_loss: 1.0616410970687866\n",
            "Saving model, epoch: 222, train_loss: 0.9015092253684998, val_loss: 1.060341238975525\n",
            "Saving model, epoch: 223, train_loss: 0.900248646736145, val_loss: 1.0591883659362793\n",
            "Saving model, epoch: 224, train_loss: 0.8990018367767334, val_loss: 1.0580021142959595\n",
            "Saving model, epoch: 225, train_loss: 0.897777795791626, val_loss: 1.0567820072174072\n",
            "Saving model, epoch: 226, train_loss: 0.8965749144554138, val_loss: 1.055568814277649\n",
            "Saving model, epoch: 227, train_loss: 0.8953935503959656, val_loss: 1.0543034076690674\n",
            "Saving model, epoch: 228, train_loss: 0.8942248225212097, val_loss: 1.0530598163604736\n",
            "Saving model, epoch: 229, train_loss: 0.8930680155754089, val_loss: 1.051651954650879\n",
            "Saving model, epoch: 230, train_loss: 0.8919208645820618, val_loss: 1.050187349319458\n",
            "Saving model, epoch: 231, train_loss: 0.8907811045646667, val_loss: 1.0487133264541626\n",
            "Saving model, epoch: 232, train_loss: 0.8896592855453491, val_loss: 1.04739511013031\n",
            "Saving model, epoch: 233, train_loss: 0.8885495066642761, val_loss: 1.0462945699691772\n",
            "Saving model, epoch: 234, train_loss: 0.8874527812004089, val_loss: 1.0451703071594238\n",
            "Saving model, epoch: 235, train_loss: 0.8863664865493774, val_loss: 1.0440762042999268\n",
            "Saving model, epoch: 236, train_loss: 0.8852872848510742, val_loss: 1.04307222366333\n",
            "Saving model, epoch: 237, train_loss: 0.8842303156852722, val_loss: 1.0421653985977173\n",
            "Saving model, epoch: 238, train_loss: 0.8831766247749329, val_loss: 1.0413261651992798\n",
            "Saving model, epoch: 239, train_loss: 0.8821422457695007, val_loss: 1.0404984951019287\n",
            "Saving model, epoch: 240, train_loss: 0.8811181783676147, val_loss: 1.039537787437439\n",
            "Saving model, epoch: 241, train_loss: 0.8801190853118896, val_loss: 1.038568377494812\n",
            "Saving model, epoch: 242, train_loss: 0.8791264891624451, val_loss: 1.0375288724899292\n",
            "Saving model, epoch: 243, train_loss: 0.8781411647796631, val_loss: 1.0364128351211548\n",
            "Saving model, epoch: 244, train_loss: 0.8771663308143616, val_loss: 1.0352261066436768\n",
            "Saving model, epoch: 245, train_loss: 0.8761973977088928, val_loss: 1.0341213941574097\n",
            "Saving model, epoch: 246, train_loss: 0.8752323985099792, val_loss: 1.0330299139022827\n",
            "Saving model, epoch: 247, train_loss: 0.8742753863334656, val_loss: 1.03193199634552\n",
            "Saving model, epoch: 248, train_loss: 0.8733307719230652, val_loss: 1.0308136940002441\n",
            "Saving model, epoch: 249, train_loss: 0.8724095821380615, val_loss: 1.0296086072921753\n",
            "Saving model, epoch: 250, train_loss: 0.8715056777000427, val_loss: 1.0283350944519043\n",
            "Saving model, epoch: 251, train_loss: 0.8706139922142029, val_loss: 1.0272184610366821\n",
            "Saving model, epoch: 252, train_loss: 0.8697267770767212, val_loss: 1.026318073272705\n",
            "Saving model, epoch: 253, train_loss: 0.868842601776123, val_loss: 1.0255147218704224\n",
            "Saving model, epoch: 254, train_loss: 0.8679571151733398, val_loss: 1.024667501449585\n",
            "Saving model, epoch: 255, train_loss: 0.8670914769172668, val_loss: 1.0237953662872314\n",
            "Saving model, epoch: 256, train_loss: 0.8662336468696594, val_loss: 1.0229135751724243\n",
            "Saving model, epoch: 257, train_loss: 0.8653886914253235, val_loss: 1.0219552516937256\n",
            "Saving model, epoch: 258, train_loss: 0.8645457625389099, val_loss: 1.0207058191299438\n",
            "Saving model, epoch: 259, train_loss: 0.8637006282806396, val_loss: 1.0193586349487305\n",
            "Saving model, epoch: 260, train_loss: 0.8628713488578796, val_loss: 1.0181291103363037\n",
            "Saving model, epoch: 261, train_loss: 0.8620523810386658, val_loss: 1.017187476158142\n",
            "Saving model, epoch: 262, train_loss: 0.8612439036369324, val_loss: 1.0164577960968018\n",
            "Saving model, epoch: 263, train_loss: 0.8604384660720825, val_loss: 1.015712022781372\n",
            "Saving model, epoch: 264, train_loss: 0.8596239686012268, val_loss: 1.014997959136963\n",
            "Saving model, epoch: 265, train_loss: 0.8588162064552307, val_loss: 1.0143085718154907\n",
            "Saving model, epoch: 266, train_loss: 0.8580156564712524, val_loss: 1.0135443210601807\n",
            "Saving model, epoch: 267, train_loss: 0.8572267889976501, val_loss: 1.0126453638076782\n",
            "Saving model, epoch: 268, train_loss: 0.8564483523368835, val_loss: 1.0117651224136353\n",
            "Saving model, epoch: 269, train_loss: 0.855663537979126, val_loss: 1.0109012126922607\n",
            "Saving model, epoch: 270, train_loss: 0.8548799753189087, val_loss: 1.0100725889205933\n",
            "Saving model, epoch: 271, train_loss: 0.8541025519371033, val_loss: 1.0093400478363037\n",
            "Saving model, epoch: 272, train_loss: 0.8533326983451843, val_loss: 1.0085381269454956\n",
            "Saving model, epoch: 273, train_loss: 0.8525713086128235, val_loss: 1.0077961683273315\n",
            "Saving model, epoch: 274, train_loss: 0.8518043160438538, val_loss: 1.0069993734359741\n",
            "Saving model, epoch: 275, train_loss: 0.8510496020317078, val_loss: 1.0062310695648193\n",
            "Saving model, epoch: 276, train_loss: 0.8502965569496155, val_loss: 1.0055336952209473\n",
            "Saving model, epoch: 277, train_loss: 0.8495418429374695, val_loss: 1.005060076713562\n",
            "Saving model, epoch: 278, train_loss: 0.8487963676452637, val_loss: 1.004774570465088\n",
            "Saving model, epoch: 279, train_loss: 0.8480589389801025, val_loss: 1.004276156425476\n",
            "Saving model, epoch: 280, train_loss: 0.8473188877105713, val_loss: 1.0035206079483032\n",
            "Saving model, epoch: 281, train_loss: 0.8465946912765503, val_loss: 1.0026209354400635\n",
            "Saving model, epoch: 282, train_loss: 0.8458764553070068, val_loss: 1.0018060207366943\n",
            "Saving model, epoch: 283, train_loss: 0.8451583981513977, val_loss: 1.000970482826233\n",
            "Saving model, epoch: 284, train_loss: 0.8444504737854004, val_loss: 1.000274896621704\n",
            "Saving model, epoch: 285, train_loss: 0.8437437415122986, val_loss: 0.9996291399002075\n",
            "Saving model, epoch: 286, train_loss: 0.8430289030075073, val_loss: 0.9990376830101013\n",
            "Saving model, epoch: 287, train_loss: 0.8423210382461548, val_loss: 0.9985056519508362\n",
            "Saving model, epoch: 288, train_loss: 0.8416364192962646, val_loss: 0.9980371594429016\n",
            "Saving model, epoch: 289, train_loss: 0.8409522175788879, val_loss: 0.9974434971809387\n",
            "Saving model, epoch: 290, train_loss: 0.840277373790741, val_loss: 0.9967617392539978\n",
            "Saving model, epoch: 291, train_loss: 0.8396109342575073, val_loss: 0.9959238767623901\n",
            "Saving model, epoch: 292, train_loss: 0.8389444351196289, val_loss: 0.995193362236023\n",
            "Saving model, epoch: 293, train_loss: 0.8382925391197205, val_loss: 0.9946094751358032\n",
            "Saving model, epoch: 294, train_loss: 0.8376478552818298, val_loss: 0.9941397905349731\n",
            "Saving model, epoch: 295, train_loss: 0.8370071053504944, val_loss: 0.9935386180877686\n",
            "Saving model, epoch: 296, train_loss: 0.8363726139068604, val_loss: 0.9927031397819519\n",
            "Saving model, epoch: 297, train_loss: 0.8357433080673218, val_loss: 0.9919533729553223\n",
            "Saving model, epoch: 298, train_loss: 0.8351309299468994, val_loss: 0.9914814829826355\n",
            "Saving model, epoch: 299, train_loss: 0.8345212936401367, val_loss: 0.9910992383956909\n",
            "Saving model, epoch: 300, train_loss: 0.8339147567749023, val_loss: 0.9905713200569153\n",
            "epoch: 301, train_loss: 0.8333299160003662, val_loss: 0.9899734854698181\n",
            "Saving model, epoch: 301, train_loss: 0.8333299160003662, val_loss: 0.9899734854698181\n",
            "Saving model, epoch: 302, train_loss: 0.8327459096908569, val_loss: 0.989355206489563\n",
            "Saving model, epoch: 303, train_loss: 0.832165002822876, val_loss: 0.988653302192688\n",
            "Saving model, epoch: 304, train_loss: 0.8315859436988831, val_loss: 0.9876930117607117\n",
            "Saving model, epoch: 305, train_loss: 0.8310254216194153, val_loss: 0.9869052171707153\n",
            "Saving model, epoch: 306, train_loss: 0.8304548263549805, val_loss: 0.9863491058349609\n",
            "Saving model, epoch: 307, train_loss: 0.8298898339271545, val_loss: 0.9859224557876587\n",
            "Saving model, epoch: 308, train_loss: 0.829328179359436, val_loss: 0.9855397343635559\n",
            "Saving model, epoch: 309, train_loss: 0.828790009021759, val_loss: 0.9850286245346069\n",
            "Saving model, epoch: 310, train_loss: 0.8282235860824585, val_loss: 0.9843294620513916\n",
            "Saving model, epoch: 311, train_loss: 0.8276761770248413, val_loss: 0.9835523366928101\n",
            "Saving model, epoch: 312, train_loss: 0.8271300792694092, val_loss: 0.9830086827278137\n",
            "Saving model, epoch: 313, train_loss: 0.8265718221664429, val_loss: 0.9825092554092407\n",
            "Saving model, epoch: 314, train_loss: 0.8260377645492554, val_loss: 0.9819493889808655\n",
            "Saving model, epoch: 315, train_loss: 0.8254997134208679, val_loss: 0.9813138246536255\n",
            "Saving model, epoch: 316, train_loss: 0.8249787092208862, val_loss: 0.9807808995246887\n",
            "Saving model, epoch: 317, train_loss: 0.8244572877883911, val_loss: 0.9801846742630005\n",
            "Saving model, epoch: 318, train_loss: 0.8239327073097229, val_loss: 0.9794911742210388\n",
            "Saving model, epoch: 319, train_loss: 0.8234232664108276, val_loss: 0.9790418744087219\n",
            "Saving model, epoch: 320, train_loss: 0.8229158520698547, val_loss: 0.9787527322769165\n",
            "Saving model, epoch: 321, train_loss: 0.822407603263855, val_loss: 0.9784669876098633\n",
            "Saving model, epoch: 322, train_loss: 0.8219016194343567, val_loss: 0.9780953526496887\n",
            "Saving model, epoch: 323, train_loss: 0.821405291557312, val_loss: 0.9776718616485596\n",
            "Saving model, epoch: 324, train_loss: 0.8208981156349182, val_loss: 0.9773020148277283\n",
            "Saving model, epoch: 325, train_loss: 0.820389986038208, val_loss: 0.9769137501716614\n",
            "Saving model, epoch: 326, train_loss: 0.8198965787887573, val_loss: 0.976603627204895\n",
            "Saving model, epoch: 327, train_loss: 0.8193972706794739, val_loss: 0.9761975407600403\n",
            "Saving model, epoch: 328, train_loss: 0.8188938498497009, val_loss: 0.9755759835243225\n",
            "Saving model, epoch: 329, train_loss: 0.8183947205543518, val_loss: 0.9749058485031128\n",
            "Saving model, epoch: 330, train_loss: 0.8178954124450684, val_loss: 0.9742879271507263\n",
            "Saving model, epoch: 331, train_loss: 0.8174124360084534, val_loss: 0.9739331007003784\n",
            "Saving model, epoch: 332, train_loss: 0.81693035364151, val_loss: 0.9737392663955688\n",
            "Saving model, epoch: 333, train_loss: 0.8164348006248474, val_loss: 0.9735543727874756\n",
            "Saving model, epoch: 334, train_loss: 0.8159520030021667, val_loss: 0.9733789563179016\n",
            "Saving model, epoch: 335, train_loss: 0.8154876232147217, val_loss: 0.9729354381561279\n",
            "Saving model, epoch: 336, train_loss: 0.8150039315223694, val_loss: 0.9723784327507019\n",
            "Saving model, epoch: 337, train_loss: 0.8145201206207275, val_loss: 0.9718378186225891\n",
            "Saving model, epoch: 338, train_loss: 0.8140541315078735, val_loss: 0.9714679718017578\n",
            "Saving model, epoch: 339, train_loss: 0.8135944604873657, val_loss: 0.9710152745246887\n",
            "Saving model, epoch: 340, train_loss: 0.8131269812583923, val_loss: 0.9701583385467529\n",
            "Saving model, epoch: 341, train_loss: 0.8126690983772278, val_loss: 0.9696736335754395\n",
            "Saving model, epoch: 342, train_loss: 0.8122118711471558, val_loss: 0.9694086909294128\n",
            "Saving model, epoch: 343, train_loss: 0.8117498159408569, val_loss: 0.9689823389053345\n",
            "Saving model, epoch: 344, train_loss: 0.8112954497337341, val_loss: 0.9686647653579712\n",
            "Saving model, epoch: 345, train_loss: 0.8108474016189575, val_loss: 0.9686188697814941\n",
            "Saving model, epoch: 347, train_loss: 0.8099536299705505, val_loss: 0.9685989618301392\n",
            "Saving model, epoch: 348, train_loss: 0.8095002174377441, val_loss: 0.9680963158607483\n",
            "Saving model, epoch: 349, train_loss: 0.8090519905090332, val_loss: 0.9677935242652893\n",
            "Saving model, epoch: 350, train_loss: 0.8086117506027222, val_loss: 0.9677906036376953\n",
            "Saving model, epoch: 351, train_loss: 0.8081761002540588, val_loss: 0.967681884765625\n",
            "Saving model, epoch: 352, train_loss: 0.8077359199523926, val_loss: 0.9674474596977234\n",
            "Saving model, epoch: 353, train_loss: 0.8072937726974487, val_loss: 0.9666683673858643\n",
            "Saving model, epoch: 354, train_loss: 0.8068622946739197, val_loss: 0.9660544991493225\n",
            "Saving model, epoch: 355, train_loss: 0.8064293265342712, val_loss: 0.9658625721931458\n",
            "Saving model, epoch: 356, train_loss: 0.8059991598129272, val_loss: 0.9657773375511169\n",
            "Saving model, epoch: 358, train_loss: 0.8051503896713257, val_loss: 0.9654572606086731\n",
            "Saving model, epoch: 359, train_loss: 0.8047155141830444, val_loss: 0.9652106165885925\n",
            "Saving model, epoch: 360, train_loss: 0.8042925596237183, val_loss: 0.9649407267570496\n",
            "Saving model, epoch: 361, train_loss: 0.8038787245750427, val_loss: 0.9648428559303284\n",
            "Saving model, epoch: 362, train_loss: 0.8034482002258301, val_loss: 0.9642434120178223\n",
            "Saving model, epoch: 363, train_loss: 0.8030313849449158, val_loss: 0.9640688300132751\n",
            "Saving model, epoch: 364, train_loss: 0.8026075959205627, val_loss: 0.963546872138977\n",
            "Saving model, epoch: 365, train_loss: 0.8021894693374634, val_loss: 0.9630174040794373\n",
            "Saving model, epoch: 366, train_loss: 0.8017827272415161, val_loss: 0.9628476500511169\n",
            "Saving model, epoch: 367, train_loss: 0.8013703227043152, val_loss: 0.9627112150192261\n",
            "Saving model, epoch: 368, train_loss: 0.8009539842605591, val_loss: 0.9619542956352234\n",
            "Saving model, epoch: 369, train_loss: 0.8005467653274536, val_loss: 0.9614499807357788\n",
            "Saving model, epoch: 371, train_loss: 0.7997456192970276, val_loss: 0.9613022208213806\n",
            "Saving model, epoch: 372, train_loss: 0.799339234828949, val_loss: 0.9606826901435852\n",
            "Saving model, epoch: 373, train_loss: 0.7989357709884644, val_loss: 0.9605963826179504\n",
            "Saving model, epoch: 376, train_loss: 0.7977494597434998, val_loss: 0.9603936076164246\n",
            "Saving model, epoch: 377, train_loss: 0.797351062297821, val_loss: 0.9591612219810486\n",
            "Saving model, epoch: 378, train_loss: 0.7969537377357483, val_loss: 0.9589638113975525\n",
            "Saving model, epoch: 381, train_loss: 0.7957862019538879, val_loss: 0.9586946964263916\n",
            "Saving model, epoch: 382, train_loss: 0.7953935861587524, val_loss: 0.9585375785827637\n",
            "Saving model, epoch: 384, train_loss: 0.7946211695671082, val_loss: 0.9581485390663147\n",
            "Saving model, epoch: 385, train_loss: 0.7942225337028503, val_loss: 0.9573960900306702\n",
            "Saving model, epoch: 386, train_loss: 0.7938345670700073, val_loss: 0.9571069478988647\n",
            "Saving model, epoch: 388, train_loss: 0.7930692434310913, val_loss: 0.9569299817085266\n",
            "Saving model, epoch: 389, train_loss: 0.7926753759384155, val_loss: 0.9561654329299927\n",
            "Saving model, epoch: 390, train_loss: 0.7922871112823486, val_loss: 0.9559237957000732\n",
            "Saving model, epoch: 391, train_loss: 0.7919012308120728, val_loss: 0.9559184908866882\n",
            "Saving model, epoch: 392, train_loss: 0.7915109395980835, val_loss: 0.9556757807731628\n",
            "Saving model, epoch: 393, train_loss: 0.7911253571510315, val_loss: 0.9554158449172974\n",
            "Saving model, epoch: 394, train_loss: 0.7907513380050659, val_loss: 0.9550143480300903\n",
            "Saving model, epoch: 395, train_loss: 0.7903725504875183, val_loss: 0.9547693133354187\n",
            "Saving model, epoch: 396, train_loss: 0.7900039553642273, val_loss: 0.9544998407363892\n",
            "Saving model, epoch: 397, train_loss: 0.7896127104759216, val_loss: 0.9543092846870422\n",
            "Saving model, epoch: 398, train_loss: 0.7892394661903381, val_loss: 0.9537461996078491\n",
            "Saving model, epoch: 399, train_loss: 0.7888748645782471, val_loss: 0.9535386562347412\n",
            "epoch: 401, train_loss: 0.7881396412849426, val_loss: 0.9535096287727356\n",
            "Saving model, epoch: 401, train_loss: 0.7881396412849426, val_loss: 0.9535096287727356\n",
            "Saving model, epoch: 402, train_loss: 0.7877580523490906, val_loss: 0.9530094861984253\n",
            "Saving model, epoch: 403, train_loss: 0.7873619794845581, val_loss: 0.9529557824134827\n",
            "Saving model, epoch: 406, train_loss: 0.7862220406532288, val_loss: 0.9522820115089417\n",
            "Saving model, epoch: 407, train_loss: 0.7858602404594421, val_loss: 0.9518308043479919\n",
            "Saving model, epoch: 411, train_loss: 0.7843484282493591, val_loss: 0.9512367248535156\n",
            "Saving model, epoch: 414, train_loss: 0.7832246422767639, val_loss: 0.9510093331336975\n",
            "Saving model, epoch: 415, train_loss: 0.7828642725944519, val_loss: 0.9501584768295288\n",
            "Saving model, epoch: 418, train_loss: 0.7817265391349792, val_loss: 0.9497490525245667\n",
            "Saving model, epoch: 421, train_loss: 0.7805976271629333, val_loss: 0.9497417211532593\n",
            "Saving model, epoch: 422, train_loss: 0.7802183032035828, val_loss: 0.9485862255096436\n",
            "Saving model, epoch: 423, train_loss: 0.7798317074775696, val_loss: 0.9480108022689819\n",
            "Saving model, epoch: 426, train_loss: 0.7786771059036255, val_loss: 0.9476151466369629\n",
            "Saving model, epoch: 427, train_loss: 0.778296172618866, val_loss: 0.9473410844802856\n",
            "Saving model, epoch: 430, train_loss: 0.7771310806274414, val_loss: 0.9468398690223694\n",
            "Saving model, epoch: 431, train_loss: 0.7767658233642578, val_loss: 0.9460972547531128\n",
            "Saving model, epoch: 432, train_loss: 0.7764008641242981, val_loss: 0.9457585215568542\n",
            "Saving model, epoch: 435, train_loss: 0.7752968668937683, val_loss: 0.9454663991928101\n",
            "Saving model, epoch: 438, train_loss: 0.774206817150116, val_loss: 0.9448257684707642\n",
            "Saving model, epoch: 441, train_loss: 0.7731468677520752, val_loss: 0.9448029398918152\n",
            "Saving model, epoch: 442, train_loss: 0.7728071808815002, val_loss: 0.9445003867149353\n",
            "Saving model, epoch: 444, train_loss: 0.772126317024231, val_loss: 0.944404661655426\n",
            "Saving model, epoch: 445, train_loss: 0.7717688083648682, val_loss: 0.9439391493797302\n",
            "Saving model, epoch: 446, train_loss: 0.7714284658432007, val_loss: 0.9434273838996887\n",
            "Saving model, epoch: 450, train_loss: 0.7700153589248657, val_loss: 0.9428802132606506\n",
            "Saving model, epoch: 451, train_loss: 0.769676923751831, val_loss: 0.9426108002662659\n",
            "Saving model, epoch: 454, train_loss: 0.7685867547988892, val_loss: 0.942608654499054\n",
            "Saving model, epoch: 455, train_loss: 0.7682273387908936, val_loss: 0.9422202706336975\n",
            "Saving model, epoch: 457, train_loss: 0.7675188183784485, val_loss: 0.9422066807746887\n",
            "Saving model, epoch: 458, train_loss: 0.7671666741371155, val_loss: 0.9417370557785034\n",
            "Saving model, epoch: 460, train_loss: 0.7664729952812195, val_loss: 0.9415354132652283\n",
            "Saving model, epoch: 461, train_loss: 0.7661370038986206, val_loss: 0.941121518611908\n",
            "Saving model, epoch: 462, train_loss: 0.765816867351532, val_loss: 0.9406802654266357\n",
            "Saving model, epoch: 463, train_loss: 0.7655048370361328, val_loss: 0.940680205821991\n",
            "Saving model, epoch: 469, train_loss: 0.7636464834213257, val_loss: 0.9404982924461365\n",
            "Saving model, epoch: 479, train_loss: 0.7606202363967896, val_loss: 0.9401642680168152\n",
            "Saving model, epoch: 481, train_loss: 0.7600220441818237, val_loss: 0.940127432346344\n",
            "Saving model, epoch: 492, train_loss: 0.7567370533943176, val_loss: 0.9394577145576477\n",
            "Saving model, epoch: 493, train_loss: 0.7564452886581421, val_loss: 0.9384827017784119\n",
            "Saving model, epoch: 498, train_loss: 0.7549049854278564, val_loss: 0.9379176497459412\n",
            "Saving model, epoch: 499, train_loss: 0.7545932531356812, val_loss: 0.9377598762512207\n",
            "Saving model, epoch: 500, train_loss: 0.7542887330055237, val_loss: 0.9374139308929443\n",
            "epoch: 501, train_loss: 0.7539842128753662, val_loss: 0.9375160336494446\n",
            "Saving model, epoch: 503, train_loss: 0.7533935308456421, val_loss: 0.9365956783294678\n",
            "Saving model, epoch: 596, train_loss: 0.7260618805885315, val_loss: 0.9363593459129333\n",
            "epoch: 601, train_loss: 0.7245228290557861, val_loss: 0.9400442242622375\n",
            "epoch: 701, train_loss: 0.694724440574646, val_loss: 0.9379588961601257\n",
            "Saving model, epoch: 720, train_loss: 0.6894007921218872, val_loss: 0.9362754225730896\n",
            "Saving model, epoch: 722, train_loss: 0.6888900995254517, val_loss: 0.9351220726966858\n",
            "Saving model, epoch: 727, train_loss: 0.6873592734336853, val_loss: 0.9329950213432312\n",
            "Saving model, epoch: 730, train_loss: 0.6865291595458984, val_loss: 0.9322658777236938\n",
            "Saving model, epoch: 735, train_loss: 0.6847785115242004, val_loss: 0.9305461645126343\n",
            "Saving model, epoch: 760, train_loss: 0.6772365570068359, val_loss: 0.9299079179763794\n",
            "epoch: 801, train_loss: 0.6680483222007751, val_loss: 0.9439331889152527\n",
            "epoch: 901, train_loss: 0.676776111125946, val_loss: 0.9978094696998596\n",
            "epoch: 1001, train_loss: 0.6413347125053406, val_loss: 0.9463182091712952\n",
            "finished training after 1061 epochs\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "       BatchNorm1d-1                   [-1, 90]             180\n",
            "            Linear-2                   [-1, 16]           1,456\n",
            "              ReLU-3                   [-1, 16]               0\n",
            "            Linear-4                    [-1, 1]              17\n",
            "================================================================\n",
            "Total params: 1,653\n",
            "Trainable params: 1,653\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.01\n",
            "Estimated Total Size (MB): 0.01\n",
            "----------------------------------------------------------------\n",
            "epoch: 1, train_loss: 69.51387786865234, val_loss: 80.83313751220703\n",
            "Saving model, epoch: 1, train_loss: 69.51387786865234, val_loss: 80.83313751220703\n",
            "Saving model, epoch: 2, train_loss: 19.753034591674805, val_loss: 24.47092056274414\n",
            "Saving model, epoch: 21, train_loss: 15.77424144744873, val_loss: 16.078718185424805\n",
            "Saving model, epoch: 22, train_loss: 10.87691879272461, val_loss: 10.110082626342773\n",
            "Saving model, epoch: 23, train_loss: 8.99044132232666, val_loss: 8.333474159240723\n",
            "Saving model, epoch: 32, train_loss: 8.797723770141602, val_loss: 7.941460132598877\n",
            "Saving model, epoch: 33, train_loss: 7.7129998207092285, val_loss: 6.946227073669434\n",
            "Saving model, epoch: 34, train_loss: 6.387348175048828, val_loss: 5.748922824859619\n",
            "Saving model, epoch: 35, train_loss: 5.4967827796936035, val_loss: 5.096739292144775\n",
            "Saving model, epoch: 42, train_loss: 4.558060646057129, val_loss: 5.031863689422607\n",
            "Saving model, epoch: 43, train_loss: 3.9541594982147217, val_loss: 4.1872944831848145\n",
            "Saving model, epoch: 44, train_loss: 3.795206308364868, val_loss: 3.864912509918213\n",
            "Saving model, epoch: 45, train_loss: 3.8491411209106445, val_loss: 3.8286919593811035\n",
            "Saving model, epoch: 46, train_loss: 3.8305723667144775, val_loss: 3.7690722942352295\n",
            "Saving model, epoch: 47, train_loss: 3.685749053955078, val_loss: 3.629080295562744\n",
            "Saving model, epoch: 48, train_loss: 3.564786672592163, val_loss: 3.5701279640197754\n",
            "Saving model, epoch: 53, train_loss: 3.275557518005371, val_loss: 3.3873653411865234\n",
            "Saving model, epoch: 54, train_loss: 3.0982863903045654, val_loss: 3.097579002380371\n",
            "Saving model, epoch: 55, train_loss: 2.9870104789733887, val_loss: 2.9123077392578125\n",
            "Saving model, epoch: 56, train_loss: 2.9031782150268555, val_loss: 2.8112847805023193\n",
            "Saving model, epoch: 57, train_loss: 2.8119313716888428, val_loss: 2.7490522861480713\n",
            "Saving model, epoch: 58, train_loss: 2.735935926437378, val_loss: 2.7273876667022705\n",
            "Saving model, epoch: 64, train_loss: 2.431811809539795, val_loss: 2.5858938694000244\n",
            "Saving model, epoch: 65, train_loss: 2.3737151622772217, val_loss: 2.4746179580688477\n",
            "Saving model, epoch: 66, train_loss: 2.3671939373016357, val_loss: 2.419306516647339\n",
            "Saving model, epoch: 67, train_loss: 2.3559718132019043, val_loss: 2.385223627090454\n",
            "Saving model, epoch: 68, train_loss: 2.3111655712127686, val_loss: 2.352388858795166\n",
            "Saving model, epoch: 69, train_loss: 2.2561655044555664, val_loss: 2.3335683345794678\n",
            "Saving model, epoch: 72, train_loss: 2.180786371231079, val_loss: 2.316929817199707\n",
            "Saving model, epoch: 73, train_loss: 2.1438138484954834, val_loss: 2.258814573287964\n",
            "Saving model, epoch: 74, train_loss: 2.1074025630950928, val_loss: 2.1973989009857178\n",
            "Saving model, epoch: 75, train_loss: 2.081000328063965, val_loss: 2.154834747314453\n",
            "Saving model, epoch: 76, train_loss: 2.0556094646453857, val_loss: 2.127474784851074\n",
            "Saving model, epoch: 77, train_loss: 2.0220072269439697, val_loss: 2.106384754180908\n",
            "Saving model, epoch: 78, train_loss: 1.9882053136825562, val_loss: 2.096407890319824\n",
            "Saving model, epoch: 82, train_loss: 1.9167221784591675, val_loss: 2.067962408065796\n",
            "Saving model, epoch: 83, train_loss: 1.8835917711257935, val_loss: 2.0052998065948486\n",
            "Saving model, epoch: 84, train_loss: 1.859208583831787, val_loss: 1.9485368728637695\n",
            "Saving model, epoch: 85, train_loss: 1.8443599939346313, val_loss: 1.9085078239440918\n",
            "Saving model, epoch: 86, train_loss: 1.8295294046401978, val_loss: 1.879624605178833\n",
            "Saving model, epoch: 87, train_loss: 1.8101747035980225, val_loss: 1.8574589490890503\n",
            "Saving model, epoch: 88, train_loss: 1.7902910709381104, val_loss: 1.8438524007797241\n",
            "Saving model, epoch: 89, train_loss: 1.7731637954711914, val_loss: 1.8358310461044312\n",
            "Saving model, epoch: 90, train_loss: 1.7563042640686035, val_loss: 1.8241536617279053\n",
            "Saving model, epoch: 91, train_loss: 1.7376376390457153, val_loss: 1.8022421598434448\n",
            "Saving model, epoch: 92, train_loss: 1.7199997901916504, val_loss: 1.7742342948913574\n",
            "Saving model, epoch: 93, train_loss: 1.7058720588684082, val_loss: 1.748755931854248\n",
            "Saving model, epoch: 94, train_loss: 1.6925768852233887, val_loss: 1.7284414768218994\n",
            "Saving model, epoch: 95, train_loss: 1.6766040325164795, val_loss: 1.7123550176620483\n",
            "Saving model, epoch: 96, train_loss: 1.6593959331512451, val_loss: 1.700168251991272\n",
            "Saving model, epoch: 97, train_loss: 1.6448310613632202, val_loss: 1.691436767578125\n",
            "Saving model, epoch: 98, train_loss: 1.6330499649047852, val_loss: 1.6819581985473633\n",
            "Saving model, epoch: 99, train_loss: 1.620667576789856, val_loss: 1.6657445430755615\n",
            "Saving model, epoch: 100, train_loss: 1.6063106060028076, val_loss: 1.6420801877975464\n",
            "epoch: 101, train_loss: 1.5920493602752686, val_loss: 1.6165302991867065\n",
            "Saving model, epoch: 101, train_loss: 1.5920493602752686, val_loss: 1.6165302991867065\n",
            "Saving model, epoch: 102, train_loss: 1.5794122219085693, val_loss: 1.5946216583251953\n",
            "Saving model, epoch: 103, train_loss: 1.567421317100525, val_loss: 1.5780142545700073\n",
            "Saving model, epoch: 104, train_loss: 1.5550609827041626, val_loss: 1.5663448572158813\n",
            "Saving model, epoch: 105, train_loss: 1.5430545806884766, val_loss: 1.558462381362915\n",
            "Saving model, epoch: 106, train_loss: 1.5319281816482544, val_loss: 1.5511096715927124\n",
            "Saving model, epoch: 107, train_loss: 1.5206738710403442, val_loss: 1.5406378507614136\n",
            "Saving model, epoch: 108, train_loss: 1.5091017484664917, val_loss: 1.5258091688156128\n",
            "Saving model, epoch: 109, train_loss: 1.4981168508529663, val_loss: 1.508439540863037\n",
            "Saving model, epoch: 110, train_loss: 1.4881868362426758, val_loss: 1.4914147853851318\n",
            "Saving model, epoch: 111, train_loss: 1.4781957864761353, val_loss: 1.4761947393417358\n",
            "Saving model, epoch: 112, train_loss: 1.4675853252410889, val_loss: 1.4630241394042969\n",
            "Saving model, epoch: 113, train_loss: 1.4571739435195923, val_loss: 1.4517978429794312\n",
            "Saving model, epoch: 114, train_loss: 1.4475486278533936, val_loss: 1.4413790702819824\n",
            "Saving model, epoch: 115, train_loss: 1.4384516477584839, val_loss: 1.4297620058059692\n",
            "Saving model, epoch: 116, train_loss: 1.4291729927062988, val_loss: 1.4159936904907227\n",
            "Saving model, epoch: 117, train_loss: 1.4199090003967285, val_loss: 1.401080846786499\n",
            "Saving model, epoch: 118, train_loss: 1.4108628034591675, val_loss: 1.3871830701828003\n",
            "Saving model, epoch: 119, train_loss: 1.4018598794937134, val_loss: 1.3757240772247314\n",
            "Saving model, epoch: 120, train_loss: 1.3929791450500488, val_loss: 1.3669356107711792\n",
            "Saving model, epoch: 121, train_loss: 1.38445246219635, val_loss: 1.3600382804870605\n",
            "Saving model, epoch: 122, train_loss: 1.3762296438217163, val_loss: 1.353101372718811\n",
            "Saving model, epoch: 123, train_loss: 1.3680360317230225, val_loss: 1.3438026905059814\n",
            "Saving model, epoch: 124, train_loss: 1.359796166419983, val_loss: 1.332255482673645\n",
            "Saving model, epoch: 125, train_loss: 1.3518052101135254, val_loss: 1.319796085357666\n",
            "Saving model, epoch: 126, train_loss: 1.3441728353500366, val_loss: 1.3080153465270996\n",
            "Saving model, epoch: 127, train_loss: 1.3366210460662842, val_loss: 1.2979159355163574\n",
            "Saving model, epoch: 128, train_loss: 1.3290507793426514, val_loss: 1.28924560546875\n",
            "Saving model, epoch: 129, train_loss: 1.3217333555221558, val_loss: 1.2814089059829712\n",
            "Saving model, epoch: 130, train_loss: 1.3147187232971191, val_loss: 1.2732518911361694\n",
            "Saving model, epoch: 131, train_loss: 1.3078248500823975, val_loss: 1.2641395330429077\n",
            "Saving model, epoch: 132, train_loss: 1.3010681867599487, val_loss: 1.2544772624969482\n",
            "Saving model, epoch: 133, train_loss: 1.2944769859313965, val_loss: 1.245345115661621\n",
            "Saving model, epoch: 134, train_loss: 1.2880462408065796, val_loss: 1.2376116514205933\n",
            "Saving model, epoch: 135, train_loss: 1.2817083597183228, val_loss: 1.2313607931137085\n",
            "Saving model, epoch: 136, train_loss: 1.2755119800567627, val_loss: 1.2264018058776855\n",
            "Saving model, epoch: 137, train_loss: 1.2695508003234863, val_loss: 1.2215871810913086\n",
            "Saving model, epoch: 138, train_loss: 1.2637020349502563, val_loss: 1.2155754566192627\n",
            "Saving model, epoch: 139, train_loss: 1.2578872442245483, val_loss: 1.208243489265442\n",
            "Saving model, epoch: 140, train_loss: 1.2522073984146118, val_loss: 1.2000237703323364\n",
            "Saving model, epoch: 141, train_loss: 1.2466461658477783, val_loss: 1.1921768188476562\n",
            "Saving model, epoch: 142, train_loss: 1.2411432266235352, val_loss: 1.1854149103164673\n",
            "Saving model, epoch: 143, train_loss: 1.2357064485549927, val_loss: 1.1798808574676514\n",
            "Saving model, epoch: 144, train_loss: 1.2303260564804077, val_loss: 1.1750481128692627\n",
            "Saving model, epoch: 145, train_loss: 1.2249566316604614, val_loss: 1.1702983379364014\n",
            "Saving model, epoch: 146, train_loss: 1.2195686101913452, val_loss: 1.1653592586517334\n",
            "Saving model, epoch: 147, train_loss: 1.2142223119735718, val_loss: 1.1602473258972168\n",
            "Saving model, epoch: 148, train_loss: 1.208878993988037, val_loss: 1.1550334692001343\n",
            "Saving model, epoch: 149, train_loss: 1.2035431861877441, val_loss: 1.1501654386520386\n",
            "Saving model, epoch: 150, train_loss: 1.198279857635498, val_loss: 1.14573335647583\n",
            "Saving model, epoch: 151, train_loss: 1.1930510997772217, val_loss: 1.1414060592651367\n",
            "Saving model, epoch: 152, train_loss: 1.1878894567489624, val_loss: 1.13680100440979\n",
            "Saving model, epoch: 153, train_loss: 1.1826814413070679, val_loss: 1.1316906213760376\n",
            "Saving model, epoch: 154, train_loss: 1.1775784492492676, val_loss: 1.1264671087265015\n",
            "Saving model, epoch: 155, train_loss: 1.1724659204483032, val_loss: 1.120951533317566\n",
            "Saving model, epoch: 156, train_loss: 1.1672968864440918, val_loss: 1.1157433986663818\n",
            "Saving model, epoch: 157, train_loss: 1.162226915359497, val_loss: 1.1112159490585327\n",
            "Saving model, epoch: 158, train_loss: 1.1573175191879272, val_loss: 1.106889247894287\n",
            "Saving model, epoch: 159, train_loss: 1.1524897813796997, val_loss: 1.1026333570480347\n",
            "Saving model, epoch: 160, train_loss: 1.1476101875305176, val_loss: 1.0983223915100098\n",
            "Saving model, epoch: 161, train_loss: 1.1428455114364624, val_loss: 1.093998908996582\n",
            "Saving model, epoch: 162, train_loss: 1.1381219625473022, val_loss: 1.0895335674285889\n",
            "Saving model, epoch: 163, train_loss: 1.1335188150405884, val_loss: 1.0853163003921509\n",
            "Saving model, epoch: 164, train_loss: 1.1289135217666626, val_loss: 1.0817359685897827\n",
            "Saving model, epoch: 165, train_loss: 1.124348521232605, val_loss: 1.078696608543396\n",
            "Saving model, epoch: 166, train_loss: 1.1197398900985718, val_loss: 1.0756579637527466\n",
            "Saving model, epoch: 167, train_loss: 1.1152710914611816, val_loss: 1.0724118947982788\n",
            "Saving model, epoch: 168, train_loss: 1.1109992265701294, val_loss: 1.0685153007507324\n",
            "Saving model, epoch: 169, train_loss: 1.1068364381790161, val_loss: 1.0648529529571533\n",
            "Saving model, epoch: 170, train_loss: 1.102677345275879, val_loss: 1.0613847970962524\n",
            "Saving model, epoch: 171, train_loss: 1.098574161529541, val_loss: 1.0581276416778564\n",
            "Saving model, epoch: 172, train_loss: 1.0945731401443481, val_loss: 1.0550130605697632\n",
            "Saving model, epoch: 173, train_loss: 1.0906591415405273, val_loss: 1.0516690015792847\n",
            "Saving model, epoch: 174, train_loss: 1.0868993997573853, val_loss: 1.0481290817260742\n",
            "Saving model, epoch: 175, train_loss: 1.0832624435424805, val_loss: 1.04451322555542\n",
            "Saving model, epoch: 176, train_loss: 1.079700231552124, val_loss: 1.0405564308166504\n",
            "Saving model, epoch: 177, train_loss: 1.0761932134628296, val_loss: 1.0361709594726562\n",
            "Saving model, epoch: 178, train_loss: 1.0727509260177612, val_loss: 1.0318337678909302\n",
            "Saving model, epoch: 179, train_loss: 1.0693596601486206, val_loss: 1.0277589559555054\n",
            "Saving model, epoch: 180, train_loss: 1.0660349130630493, val_loss: 1.024109959602356\n",
            "Saving model, epoch: 181, train_loss: 1.0627671480178833, val_loss: 1.0210657119750977\n",
            "Saving model, epoch: 182, train_loss: 1.0595359802246094, val_loss: 1.0184009075164795\n",
            "Saving model, epoch: 183, train_loss: 1.056326985359192, val_loss: 1.0158568620681763\n",
            "Saving model, epoch: 184, train_loss: 1.0531376600265503, val_loss: 1.0132064819335938\n",
            "Saving model, epoch: 185, train_loss: 1.0499906539916992, val_loss: 1.0102945566177368\n",
            "Saving model, epoch: 186, train_loss: 1.0468904972076416, val_loss: 1.0074365139007568\n",
            "Saving model, epoch: 187, train_loss: 1.0438005924224854, val_loss: 1.0047909021377563\n",
            "Saving model, epoch: 188, train_loss: 1.0407586097717285, val_loss: 1.002777099609375\n",
            "Saving model, epoch: 189, train_loss: 1.0378060340881348, val_loss: 1.0015848875045776\n",
            "Saving model, epoch: 190, train_loss: 1.0349074602127075, val_loss: 1.0007083415985107\n",
            "Saving model, epoch: 191, train_loss: 1.0320249795913696, val_loss: 0.9996113777160645\n",
            "Saving model, epoch: 192, train_loss: 1.029176115989685, val_loss: 0.9977768063545227\n",
            "Saving model, epoch: 193, train_loss: 1.026342511177063, val_loss: 0.9953548908233643\n",
            "Saving model, epoch: 194, train_loss: 1.023516297340393, val_loss: 0.9929648637771606\n",
            "Saving model, epoch: 195, train_loss: 1.0207321643829346, val_loss: 0.9906414747238159\n",
            "Saving model, epoch: 196, train_loss: 1.0179872512817383, val_loss: 0.9883915185928345\n",
            "Saving model, epoch: 197, train_loss: 1.0153064727783203, val_loss: 0.9863355755805969\n",
            "Saving model, epoch: 198, train_loss: 1.012673258781433, val_loss: 0.9843302369117737\n",
            "Saving model, epoch: 199, train_loss: 1.0100640058517456, val_loss: 0.98214191198349\n",
            "Saving model, epoch: 200, train_loss: 1.0075064897537231, val_loss: 0.9797053337097168\n",
            "epoch: 201, train_loss: 1.00498366355896, val_loss: 0.9773814678192139\n",
            "Saving model, epoch: 201, train_loss: 1.00498366355896, val_loss: 0.9773814678192139\n",
            "Saving model, epoch: 202, train_loss: 1.0025044679641724, val_loss: 0.9753562211990356\n",
            "Saving model, epoch: 203, train_loss: 1.0000383853912354, val_loss: 0.9735655188560486\n",
            "Saving model, epoch: 204, train_loss: 0.9976098537445068, val_loss: 0.9719642996788025\n",
            "Saving model, epoch: 205, train_loss: 0.9951988458633423, val_loss: 0.9704560041427612\n",
            "Saving model, epoch: 206, train_loss: 0.9927883148193359, val_loss: 0.9688563346862793\n",
            "Saving model, epoch: 207, train_loss: 0.990415096282959, val_loss: 0.9671445488929749\n",
            "Saving model, epoch: 208, train_loss: 0.9881067276000977, val_loss: 0.965121328830719\n",
            "Saving model, epoch: 209, train_loss: 0.9858909845352173, val_loss: 0.9629921913146973\n",
            "Saving model, epoch: 210, train_loss: 0.9836919903755188, val_loss: 0.9608511328697205\n",
            "Saving model, epoch: 211, train_loss: 0.9815157651901245, val_loss: 0.9589835405349731\n",
            "Saving model, epoch: 212, train_loss: 0.9793877005577087, val_loss: 0.9573748111724854\n",
            "Saving model, epoch: 213, train_loss: 0.9772739410400391, val_loss: 0.9558275938034058\n",
            "Saving model, epoch: 214, train_loss: 0.9752089977264404, val_loss: 0.9541904926300049\n",
            "Saving model, epoch: 215, train_loss: 0.9731901288032532, val_loss: 0.9527562856674194\n",
            "Saving model, epoch: 216, train_loss: 0.9711493849754333, val_loss: 0.9514940977096558\n",
            "Saving model, epoch: 217, train_loss: 0.9691488742828369, val_loss: 0.9502034187316895\n",
            "Saving model, epoch: 218, train_loss: 0.9671708345413208, val_loss: 0.9487303495407104\n",
            "Saving model, epoch: 219, train_loss: 0.9652294516563416, val_loss: 0.9471792578697205\n",
            "Saving model, epoch: 220, train_loss: 0.9633154273033142, val_loss: 0.9457807540893555\n",
            "Saving model, epoch: 221, train_loss: 0.9614591598510742, val_loss: 0.9446578025817871\n",
            "Saving model, epoch: 222, train_loss: 0.9596129059791565, val_loss: 0.9436963796615601\n",
            "Saving model, epoch: 223, train_loss: 0.957752525806427, val_loss: 0.9426174163818359\n",
            "Saving model, epoch: 224, train_loss: 0.9559298753738403, val_loss: 0.9410135746002197\n",
            "Saving model, epoch: 225, train_loss: 0.9541206359863281, val_loss: 0.9392534494400024\n",
            "Saving model, epoch: 226, train_loss: 0.9522955417633057, val_loss: 0.9378548264503479\n",
            "Saving model, epoch: 227, train_loss: 0.9504788517951965, val_loss: 0.9370813965797424\n",
            "Saving model, epoch: 228, train_loss: 0.9486997127532959, val_loss: 0.9370064735412598\n",
            "Saving model, epoch: 229, train_loss: 0.9469603896141052, val_loss: 0.9366933107376099\n",
            "Saving model, epoch: 230, train_loss: 0.9452298879623413, val_loss: 0.9358568787574768\n",
            "Saving model, epoch: 231, train_loss: 0.9435211420059204, val_loss: 0.9348985552787781\n",
            "Saving model, epoch: 232, train_loss: 0.9418447017669678, val_loss: 0.9340057373046875\n",
            "Saving model, epoch: 233, train_loss: 0.9401949644088745, val_loss: 0.9333956241607666\n",
            "Saving model, epoch: 234, train_loss: 0.938569188117981, val_loss: 0.9331464767456055\n",
            "Saving model, epoch: 235, train_loss: 0.9369412064552307, val_loss: 0.9326088428497314\n",
            "Saving model, epoch: 236, train_loss: 0.9353229999542236, val_loss: 0.9317893385887146\n",
            "Saving model, epoch: 237, train_loss: 0.9337316751480103, val_loss: 0.9314756989479065\n",
            "Saving model, epoch: 245, train_loss: 0.921462893486023, val_loss: 0.9312995076179504\n",
            "Saving model, epoch: 246, train_loss: 0.9199033975601196, val_loss: 0.9305031299591064\n",
            "Saving model, epoch: 247, train_loss: 0.9183531403541565, val_loss: 0.9295174479484558\n",
            "Saving model, epoch: 248, train_loss: 0.9167928695678711, val_loss: 0.9288579821586609\n",
            "Saving model, epoch: 249, train_loss: 0.9152389764785767, val_loss: 0.9285844564437866\n",
            "Saving model, epoch: 250, train_loss: 0.9137954711914062, val_loss: 0.928473711013794\n",
            "Saving model, epoch: 251, train_loss: 0.912339448928833, val_loss: 0.9278677701950073\n",
            "Saving model, epoch: 252, train_loss: 0.9109004139900208, val_loss: 0.9269985556602478\n",
            "Saving model, epoch: 253, train_loss: 0.9094720482826233, val_loss: 0.9265975952148438\n",
            "Saving model, epoch: 294, train_loss: 0.8596816062927246, val_loss: 0.9265380501747131\n",
            "Saving model, epoch: 298, train_loss: 0.8555324673652649, val_loss: 0.9256735444068909\n",
            "Saving model, epoch: 299, train_loss: 0.8545190095901489, val_loss: 0.9254586696624756\n",
            "epoch: 301, train_loss: 0.8525472283363342, val_loss: 0.9254763722419739\n",
            "Saving model, epoch: 302, train_loss: 0.8515785932540894, val_loss: 0.9242513179779053\n",
            "Saving model, epoch: 303, train_loss: 0.8506665229797363, val_loss: 0.923139214515686\n",
            "Saving model, epoch: 304, train_loss: 0.8497692942619324, val_loss: 0.9229222536087036\n",
            "Saving model, epoch: 307, train_loss: 0.8471353054046631, val_loss: 0.9227119088172913\n",
            "Saving model, epoch: 308, train_loss: 0.8462433218955994, val_loss: 0.9216447472572327\n",
            "Saving model, epoch: 309, train_loss: 0.8453590869903564, val_loss: 0.9212910532951355\n",
            "Saving model, epoch: 310, train_loss: 0.8444826006889343, val_loss: 0.9212725162506104\n",
            "Saving model, epoch: 311, train_loss: 0.8436110615730286, val_loss: 0.9209110736846924\n",
            "Saving model, epoch: 312, train_loss: 0.8427552580833435, val_loss: 0.9202523231506348\n",
            "Saving model, epoch: 313, train_loss: 0.8418937921524048, val_loss: 0.9197438955307007\n",
            "Saving model, epoch: 316, train_loss: 0.839454710483551, val_loss: 0.9194960594177246\n",
            "Saving model, epoch: 317, train_loss: 0.8386619687080383, val_loss: 0.9190981984138489\n",
            "Saving model, epoch: 318, train_loss: 0.8378577828407288, val_loss: 0.9189892411231995\n",
            "Saving model, epoch: 323, train_loss: 0.8339594006538391, val_loss: 0.9185960292816162\n",
            "Saving model, epoch: 324, train_loss: 0.8332078456878662, val_loss: 0.9184437990188599\n",
            "Saving model, epoch: 326, train_loss: 0.831727921962738, val_loss: 0.9184158444404602\n",
            "Saving model, epoch: 327, train_loss: 0.8309887647628784, val_loss: 0.918200671672821\n",
            "Saving model, epoch: 328, train_loss: 0.8302425742149353, val_loss: 0.9179555177688599\n",
            "Saving model, epoch: 329, train_loss: 0.8295136094093323, val_loss: 0.9174919724464417\n",
            "Saving model, epoch: 330, train_loss: 0.8288025259971619, val_loss: 0.9170743227005005\n",
            "Saving model, epoch: 331, train_loss: 0.8280891180038452, val_loss: 0.9168457984924316\n",
            "Saving model, epoch: 332, train_loss: 0.8273788690567017, val_loss: 0.9167367219924927\n",
            "Saving model, epoch: 335, train_loss: 0.8252896666526794, val_loss: 0.916735827922821\n",
            "Saving model, epoch: 336, train_loss: 0.8245933055877686, val_loss: 0.9157557487487793\n",
            "Saving model, epoch: 337, train_loss: 0.8238877058029175, val_loss: 0.9146824479103088\n",
            "Saving model, epoch: 338, train_loss: 0.8232287168502808, val_loss: 0.9146168828010559\n",
            "Saving model, epoch: 342, train_loss: 0.8205867409706116, val_loss: 0.9135770201683044\n",
            "Saving model, epoch: 345, train_loss: 0.8186520338058472, val_loss: 0.9128455519676208\n",
            "Saving model, epoch: 346, train_loss: 0.8180229067802429, val_loss: 0.9114136099815369\n",
            "Saving model, epoch: 349, train_loss: 0.8162321448326111, val_loss: 0.9112082123756409\n",
            "Saving model, epoch: 350, train_loss: 0.8156379461288452, val_loss: 0.910733699798584\n",
            "Saving model, epoch: 351, train_loss: 0.8150952458381653, val_loss: 0.910726010799408\n",
            "Saving model, epoch: 354, train_loss: 0.8134318590164185, val_loss: 0.9103662371635437\n",
            "Saving model, epoch: 355, train_loss: 0.8129129409790039, val_loss: 0.9100427031517029\n",
            "epoch: 401, train_loss: 0.7895820736885071, val_loss: 0.9143061637878418\n",
            "epoch: 501, train_loss: 0.7520985007286072, val_loss: 0.9212340712547302\n",
            "epoch: 601, train_loss: 0.7219488024711609, val_loss: 0.9253873229026794\n",
            "finished training after 656 epochs\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "       BatchNorm1d-1                   [-1, 90]             180\n",
            "            Linear-2                   [-1, 16]           1,456\n",
            "              ReLU-3                   [-1, 16]               0\n",
            "            Linear-4                    [-1, 1]              17\n",
            "================================================================\n",
            "Total params: 1,653\n",
            "Trainable params: 1,653\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.01\n",
            "Estimated Total Size (MB): 0.01\n",
            "----------------------------------------------------------------\n",
            "epoch: 1, train_loss: 158.77442932128906, val_loss: 154.67068481445312\n",
            "Saving model, epoch: 1, train_loss: 158.77442932128906, val_loss: 154.67068481445312\n",
            "Saving model, epoch: 2, train_loss: 49.60441589355469, val_loss: 47.67786407470703\n",
            "Saving model, epoch: 3, train_loss: 20.43962287902832, val_loss: 20.00909996032715\n",
            "Saving model, epoch: 22, train_loss: 14.863300323486328, val_loss: 14.545162200927734\n",
            "Saving model, epoch: 23, train_loss: 10.432029724121094, val_loss: 10.274361610412598\n",
            "Saving model, epoch: 30, train_loss: 10.091401100158691, val_loss: 10.224515914916992\n",
            "Saving model, epoch: 31, train_loss: 7.389463424682617, val_loss: 7.51978063583374\n",
            "Saving model, epoch: 32, train_loss: 6.360769271850586, val_loss: 6.4964704513549805\n",
            "Saving model, epoch: 35, train_loss: 5.89409065246582, val_loss: 6.0772552490234375\n",
            "Saving model, epoch: 36, train_loss: 5.484163284301758, val_loss: 5.676136493682861\n",
            "Saving model, epoch: 43, train_loss: 4.862059593200684, val_loss: 5.115196228027344\n",
            "Saving model, epoch: 44, train_loss: 4.35507869720459, val_loss: 4.663463592529297\n",
            "Saving model, epoch: 45, train_loss: 4.282727241516113, val_loss: 4.602131366729736\n",
            "Saving model, epoch: 46, train_loss: 4.248661041259766, val_loss: 4.517766952514648\n",
            "Saving model, epoch: 47, train_loss: 4.006143569946289, val_loss: 4.17822790145874\n",
            "Saving model, epoch: 48, train_loss: 3.697796583175659, val_loss: 3.761702299118042\n",
            "Saving model, epoch: 49, train_loss: 3.606135845184326, val_loss: 3.5862059593200684\n",
            "Saving model, epoch: 54, train_loss: 3.417884349822998, val_loss: 3.5322349071502686\n",
            "Saving model, epoch: 55, train_loss: 3.1489715576171875, val_loss: 3.3764290809631348\n",
            "Saving model, epoch: 60, train_loss: 2.8568098545074463, val_loss: 3.2644097805023193\n",
            "Saving model, epoch: 61, train_loss: 2.836674928665161, val_loss: 3.1891605854034424\n",
            "Saving model, epoch: 63, train_loss: 2.9031288623809814, val_loss: 3.1853184700012207\n",
            "Saving model, epoch: 64, train_loss: 2.8208932876586914, val_loss: 3.1013002395629883\n",
            "Saving model, epoch: 65, train_loss: 2.687854766845703, val_loss: 2.986522674560547\n",
            "Saving model, epoch: 66, train_loss: 2.5900180339813232, val_loss: 2.9228219985961914\n",
            "Saving model, epoch: 70, train_loss: 2.4245731830596924, val_loss: 2.8607521057128906\n",
            "Saving model, epoch: 71, train_loss: 2.373135566711426, val_loss: 2.7972748279571533\n",
            "Saving model, epoch: 72, train_loss: 2.3608503341674805, val_loss: 2.765428304672241\n",
            "Saving model, epoch: 73, train_loss: 2.3545937538146973, val_loss: 2.7395920753479004\n",
            "Saving model, epoch: 74, train_loss: 2.3161792755126953, val_loss: 2.6864407062530518\n",
            "Saving model, epoch: 75, train_loss: 2.2489309310913086, val_loss: 2.6101644039154053\n",
            "Saving model, epoch: 76, train_loss: 2.189605712890625, val_loss: 2.5448269844055176\n",
            "Saving model, epoch: 77, train_loss: 2.160562038421631, val_loss: 2.5105597972869873\n",
            "Saving model, epoch: 78, train_loss: 2.145368814468384, val_loss: 2.4898505210876465\n",
            "Saving model, epoch: 79, train_loss: 2.115539312362671, val_loss: 2.4539804458618164\n",
            "Saving model, epoch: 80, train_loss: 2.0692572593688965, val_loss: 2.4018383026123047\n",
            "Saving model, epoch: 81, train_loss: 2.028956413269043, val_loss: 2.3573877811431885\n",
            "Saving model, epoch: 82, train_loss: 2.007049322128296, val_loss: 2.334580659866333\n",
            "Saving model, epoch: 83, train_loss: 1.9908223152160645, val_loss: 2.321523427963257\n",
            "Saving model, epoch: 84, train_loss: 1.9630309343338013, val_loss: 2.301164388656616\n",
            "Saving model, epoch: 85, train_loss: 1.9250845909118652, val_loss: 2.2726833820343018\n",
            "Saving model, epoch: 86, train_loss: 1.8920295238494873, val_loss: 2.2483389377593994\n",
            "Saving model, epoch: 87, train_loss: 1.8699020147323608, val_loss: 2.2310831546783447\n",
            "Saving model, epoch: 88, train_loss: 1.8495649099349976, val_loss: 2.210314989089966\n",
            "Saving model, epoch: 89, train_loss: 1.8225780725479126, val_loss: 2.1773219108581543\n",
            "Saving model, epoch: 90, train_loss: 1.7932062149047852, val_loss: 2.138801097869873\n",
            "Saving model, epoch: 91, train_loss: 1.7704945802688599, val_loss: 2.1081151962280273\n",
            "Saving model, epoch: 92, train_loss: 1.75371253490448, val_loss: 2.0862324237823486\n",
            "Saving model, epoch: 93, train_loss: 1.7346265316009521, val_loss: 2.066007375717163\n",
            "Saving model, epoch: 94, train_loss: 1.7106192111968994, val_loss: 2.044172763824463\n",
            "Saving model, epoch: 95, train_loss: 1.6875207424163818, val_loss: 2.024670362472534\n",
            "Saving model, epoch: 96, train_loss: 1.670040249824524, val_loss: 2.0097737312316895\n",
            "Saving model, epoch: 97, train_loss: 1.6551426649093628, val_loss: 1.9956074953079224\n",
            "Saving model, epoch: 98, train_loss: 1.637563943862915, val_loss: 1.9770758152008057\n",
            "Saving model, epoch: 99, train_loss: 1.618004322052002, val_loss: 1.954742670059204\n",
            "Saving model, epoch: 100, train_loss: 1.601102590560913, val_loss: 1.9339158535003662\n",
            "epoch: 101, train_loss: 1.587505578994751, val_loss: 1.9160219430923462\n",
            "Saving model, epoch: 101, train_loss: 1.587505578994751, val_loss: 1.9160219430923462\n",
            "Saving model, epoch: 102, train_loss: 1.573744773864746, val_loss: 1.8984519243240356\n",
            "Saving model, epoch: 103, train_loss: 1.558227777481079, val_loss: 1.8803417682647705\n",
            "Saving model, epoch: 104, train_loss: 1.5434240102767944, val_loss: 1.8636043071746826\n",
            "Saving model, epoch: 105, train_loss: 1.531176209449768, val_loss: 1.8499901294708252\n",
            "Saving model, epoch: 106, train_loss: 1.5194475650787354, val_loss: 1.8369656801223755\n",
            "Saving model, epoch: 107, train_loss: 1.5065621137619019, val_loss: 1.8222522735595703\n",
            "Saving model, epoch: 108, train_loss: 1.4937688112258911, val_loss: 1.8064979314804077\n",
            "Saving model, epoch: 109, train_loss: 1.4827947616577148, val_loss: 1.7914955615997314\n",
            "Saving model, epoch: 110, train_loss: 1.4729241132736206, val_loss: 1.7771987915039062\n",
            "Saving model, epoch: 111, train_loss: 1.4623215198516846, val_loss: 1.7619154453277588\n",
            "Saving model, epoch: 112, train_loss: 1.451202392578125, val_loss: 1.746508002281189\n",
            "Saving model, epoch: 113, train_loss: 1.44102144241333, val_loss: 1.7322065830230713\n",
            "Saving model, epoch: 114, train_loss: 1.4318124055862427, val_loss: 1.7190477848052979\n",
            "Saving model, epoch: 115, train_loss: 1.42246413230896, val_loss: 1.7060364484786987\n",
            "Saving model, epoch: 116, train_loss: 1.4127861261367798, val_loss: 1.6931999921798706\n",
            "Saving model, epoch: 117, train_loss: 1.4036471843719482, val_loss: 1.68084716796875\n",
            "Saving model, epoch: 118, train_loss: 1.395218014717102, val_loss: 1.6691817045211792\n",
            "Saving model, epoch: 119, train_loss: 1.386621117591858, val_loss: 1.6582022905349731\n",
            "Saving model, epoch: 120, train_loss: 1.3777331113815308, val_loss: 1.647292971611023\n",
            "Saving model, epoch: 121, train_loss: 1.3692963123321533, val_loss: 1.6367450952529907\n",
            "Saving model, epoch: 122, train_loss: 1.3615535497665405, val_loss: 1.626417875289917\n",
            "Saving model, epoch: 123, train_loss: 1.3538309335708618, val_loss: 1.6155027151107788\n",
            "Saving model, epoch: 124, train_loss: 1.3459655046463013, val_loss: 1.6039106845855713\n",
            "Saving model, epoch: 125, train_loss: 1.3383997678756714, val_loss: 1.5915868282318115\n",
            "Saving model, epoch: 126, train_loss: 1.331230878829956, val_loss: 1.5796740055084229\n",
            "Saving model, epoch: 127, train_loss: 1.3239740133285522, val_loss: 1.568233847618103\n",
            "Saving model, epoch: 128, train_loss: 1.3166913986206055, val_loss: 1.5570800304412842\n",
            "Saving model, epoch: 129, train_loss: 1.3097385168075562, val_loss: 1.5465686321258545\n",
            "Saving model, epoch: 130, train_loss: 1.3031768798828125, val_loss: 1.5365251302719116\n",
            "Saving model, epoch: 131, train_loss: 1.296659231185913, val_loss: 1.526713252067566\n",
            "Saving model, epoch: 132, train_loss: 1.2901531457901, val_loss: 1.5173646211624146\n",
            "Saving model, epoch: 133, train_loss: 1.2837616205215454, val_loss: 1.5088046789169312\n",
            "Saving model, epoch: 134, train_loss: 1.2774994373321533, val_loss: 1.5004342794418335\n",
            "Saving model, epoch: 135, train_loss: 1.2711851596832275, val_loss: 1.4924867153167725\n",
            "Saving model, epoch: 136, train_loss: 1.2649623155593872, val_loss: 1.4849568605422974\n",
            "Saving model, epoch: 137, train_loss: 1.2590359449386597, val_loss: 1.4777641296386719\n",
            "Saving model, epoch: 138, train_loss: 1.253274917602539, val_loss: 1.4708205461502075\n",
            "Saving model, epoch: 139, train_loss: 1.2475985288619995, val_loss: 1.4639177322387695\n",
            "Saving model, epoch: 140, train_loss: 1.2420763969421387, val_loss: 1.4570282697677612\n",
            "Saving model, epoch: 141, train_loss: 1.23673677444458, val_loss: 1.4502036571502686\n",
            "Saving model, epoch: 142, train_loss: 1.2314434051513672, val_loss: 1.4435760974884033\n",
            "Saving model, epoch: 143, train_loss: 1.2262269258499146, val_loss: 1.437148094177246\n",
            "Saving model, epoch: 144, train_loss: 1.2211341857910156, val_loss: 1.4308587312698364\n",
            "Saving model, epoch: 145, train_loss: 1.2161873579025269, val_loss: 1.4247359037399292\n",
            "Saving model, epoch: 146, train_loss: 1.2113802433013916, val_loss: 1.4186571836471558\n",
            "Saving model, epoch: 147, train_loss: 1.2066972255706787, val_loss: 1.4127485752105713\n",
            "Saving model, epoch: 148, train_loss: 1.2021006345748901, val_loss: 1.4068067073822021\n",
            "Saving model, epoch: 149, train_loss: 1.1976170539855957, val_loss: 1.4006701707839966\n",
            "Saving model, epoch: 150, train_loss: 1.1930941343307495, val_loss: 1.3942570686340332\n",
            "Saving model, epoch: 151, train_loss: 1.1885700225830078, val_loss: 1.387515902519226\n",
            "Saving model, epoch: 152, train_loss: 1.1841634511947632, val_loss: 1.3806382417678833\n",
            "Saving model, epoch: 153, train_loss: 1.1798046827316284, val_loss: 1.3736823797225952\n",
            "Saving model, epoch: 154, train_loss: 1.1754616498947144, val_loss: 1.3665622472763062\n",
            "Saving model, epoch: 155, train_loss: 1.171165943145752, val_loss: 1.3595151901245117\n",
            "Saving model, epoch: 156, train_loss: 1.1669719219207764, val_loss: 1.3525333404541016\n",
            "Saving model, epoch: 157, train_loss: 1.16286301612854, val_loss: 1.3456640243530273\n",
            "Saving model, epoch: 158, train_loss: 1.1587696075439453, val_loss: 1.3389499187469482\n",
            "Saving model, epoch: 159, train_loss: 1.1547045707702637, val_loss: 1.332411289215088\n",
            "Saving model, epoch: 160, train_loss: 1.1506781578063965, val_loss: 1.325922966003418\n",
            "Saving model, epoch: 161, train_loss: 1.1466796398162842, val_loss: 1.3194650411605835\n",
            "Saving model, epoch: 162, train_loss: 1.1426527500152588, val_loss: 1.3130282163619995\n",
            "Saving model, epoch: 163, train_loss: 1.1386369466781616, val_loss: 1.3065948486328125\n",
            "Saving model, epoch: 164, train_loss: 1.1347013711929321, val_loss: 1.3003051280975342\n",
            "Saving model, epoch: 165, train_loss: 1.1307529211044312, val_loss: 1.2940702438354492\n",
            "Saving model, epoch: 166, train_loss: 1.126824975013733, val_loss: 1.2879241704940796\n",
            "Saving model, epoch: 167, train_loss: 1.1229196786880493, val_loss: 1.2817305326461792\n",
            "Saving model, epoch: 168, train_loss: 1.1190717220306396, val_loss: 1.2755721807479858\n",
            "Saving model, epoch: 169, train_loss: 1.1152591705322266, val_loss: 1.269700288772583\n",
            "Saving model, epoch: 170, train_loss: 1.1114691495895386, val_loss: 1.2639751434326172\n",
            "Saving model, epoch: 171, train_loss: 1.107690691947937, val_loss: 1.258336067199707\n",
            "Saving model, epoch: 172, train_loss: 1.1039398908615112, val_loss: 1.2529181241989136\n",
            "Saving model, epoch: 173, train_loss: 1.100332498550415, val_loss: 1.2477482557296753\n",
            "Saving model, epoch: 174, train_loss: 1.0967910289764404, val_loss: 1.242287278175354\n",
            "Saving model, epoch: 175, train_loss: 1.0932320356369019, val_loss: 1.2367173433303833\n",
            "Saving model, epoch: 176, train_loss: 1.0896573066711426, val_loss: 1.2316644191741943\n",
            "Saving model, epoch: 177, train_loss: 1.086089849472046, val_loss: 1.2270333766937256\n",
            "Saving model, epoch: 178, train_loss: 1.082552433013916, val_loss: 1.222549319267273\n",
            "Saving model, epoch: 179, train_loss: 1.0790355205535889, val_loss: 1.2183195352554321\n",
            "Saving model, epoch: 180, train_loss: 1.0755692720413208, val_loss: 1.2143665552139282\n",
            "Saving model, epoch: 181, train_loss: 1.0720444917678833, val_loss: 1.2105138301849365\n",
            "Saving model, epoch: 182, train_loss: 1.0684925317764282, val_loss: 1.2066882848739624\n",
            "Saving model, epoch: 183, train_loss: 1.0649150609970093, val_loss: 1.2028523683547974\n",
            "Saving model, epoch: 184, train_loss: 1.0612893104553223, val_loss: 1.198414921760559\n",
            "Saving model, epoch: 185, train_loss: 1.0575913190841675, val_loss: 1.1934093236923218\n",
            "Saving model, epoch: 186, train_loss: 1.053882360458374, val_loss: 1.1881695985794067\n",
            "Saving model, epoch: 187, train_loss: 1.0501927137374878, val_loss: 1.1830213069915771\n",
            "Saving model, epoch: 188, train_loss: 1.0464670658111572, val_loss: 1.1781507730484009\n",
            "Saving model, epoch: 189, train_loss: 1.0428282022476196, val_loss: 1.1733026504516602\n",
            "Saving model, epoch: 190, train_loss: 1.0393608808517456, val_loss: 1.168502926826477\n",
            "Saving model, epoch: 191, train_loss: 1.0360642671585083, val_loss: 1.1640398502349854\n",
            "Saving model, epoch: 192, train_loss: 1.032839059829712, val_loss: 1.1597061157226562\n",
            "Saving model, epoch: 193, train_loss: 1.0296908617019653, val_loss: 1.1554720401763916\n",
            "Saving model, epoch: 194, train_loss: 1.0266309976577759, val_loss: 1.151390790939331\n",
            "Saving model, epoch: 195, train_loss: 1.0237001180648804, val_loss: 1.1475048065185547\n",
            "Saving model, epoch: 196, train_loss: 1.0208450555801392, val_loss: 1.1442331075668335\n",
            "Saving model, epoch: 197, train_loss: 1.018027424812317, val_loss: 1.141447901725769\n",
            "Saving model, epoch: 198, train_loss: 1.01533842086792, val_loss: 1.1388901472091675\n",
            "Saving model, epoch: 199, train_loss: 1.0127594470977783, val_loss: 1.1365031003952026\n",
            "Saving model, epoch: 200, train_loss: 1.0101858377456665, val_loss: 1.1340110301971436\n",
            "epoch: 201, train_loss: 1.007615327835083, val_loss: 1.1314499378204346\n",
            "Saving model, epoch: 201, train_loss: 1.007615327835083, val_loss: 1.1314499378204346\n",
            "Saving model, epoch: 202, train_loss: 1.0050995349884033, val_loss: 1.1287617683410645\n",
            "Saving model, epoch: 203, train_loss: 1.0026741027832031, val_loss: 1.1258653402328491\n",
            "Saving model, epoch: 204, train_loss: 1.0002959966659546, val_loss: 1.1228206157684326\n",
            "Saving model, epoch: 205, train_loss: 0.9979434013366699, val_loss: 1.1202609539031982\n",
            "Saving model, epoch: 206, train_loss: 0.9955825805664062, val_loss: 1.117889642715454\n",
            "Saving model, epoch: 207, train_loss: 0.9932315945625305, val_loss: 1.1156609058380127\n",
            "Saving model, epoch: 208, train_loss: 0.9908958673477173, val_loss: 1.1134892702102661\n",
            "Saving model, epoch: 209, train_loss: 0.9885936975479126, val_loss: 1.1114708185195923\n",
            "Saving model, epoch: 210, train_loss: 0.9863427877426147, val_loss: 1.109358787536621\n",
            "Saving model, epoch: 211, train_loss: 0.9841270446777344, val_loss: 1.107038140296936\n",
            "Saving model, epoch: 212, train_loss: 0.9819768667221069, val_loss: 1.1046251058578491\n",
            "Saving model, epoch: 213, train_loss: 0.9798862338066101, val_loss: 1.1021583080291748\n",
            "Saving model, epoch: 214, train_loss: 0.977809727191925, val_loss: 1.0996711254119873\n",
            "Saving model, epoch: 215, train_loss: 0.9757716059684753, val_loss: 1.0969581604003906\n",
            "Saving model, epoch: 216, train_loss: 0.9737538695335388, val_loss: 1.0941150188446045\n",
            "Saving model, epoch: 217, train_loss: 0.9717257022857666, val_loss: 1.0911797285079956\n",
            "Saving model, epoch: 218, train_loss: 0.9697017669677734, val_loss: 1.0881907939910889\n",
            "Saving model, epoch: 219, train_loss: 0.9677188992500305, val_loss: 1.08527672290802\n",
            "Saving model, epoch: 220, train_loss: 0.9657437801361084, val_loss: 1.08242928981781\n",
            "Saving model, epoch: 221, train_loss: 0.9638035893440247, val_loss: 1.079766035079956\n",
            "Saving model, epoch: 222, train_loss: 0.9618828892707825, val_loss: 1.0772674083709717\n",
            "Saving model, epoch: 223, train_loss: 0.9600188136100769, val_loss: 1.0751088857650757\n",
            "Saving model, epoch: 224, train_loss: 0.958212673664093, val_loss: 1.0727967023849487\n",
            "Saving model, epoch: 225, train_loss: 0.9564525485038757, val_loss: 1.0706576108932495\n",
            "Saving model, epoch: 226, train_loss: 0.9547318816184998, val_loss: 1.0687931776046753\n",
            "Saving model, epoch: 227, train_loss: 0.9530225396156311, val_loss: 1.0672575235366821\n",
            "Saving model, epoch: 228, train_loss: 0.9513380527496338, val_loss: 1.06611168384552\n",
            "Saving model, epoch: 229, train_loss: 0.9496796727180481, val_loss: 1.065287709236145\n",
            "Saving model, epoch: 230, train_loss: 0.9480304718017578, val_loss: 1.0646817684173584\n",
            "Saving model, epoch: 231, train_loss: 0.9464093446731567, val_loss: 1.064306378364563\n",
            "Saving model, epoch: 232, train_loss: 0.944794774055481, val_loss: 1.0640369653701782\n",
            "Saving model, epoch: 233, train_loss: 0.9431785345077515, val_loss: 1.0638387203216553\n",
            "Saving model, epoch: 234, train_loss: 0.9415854811668396, val_loss: 1.0635343790054321\n",
            "Saving model, epoch: 235, train_loss: 0.9400109052658081, val_loss: 1.0631896257400513\n",
            "Saving model, epoch: 236, train_loss: 0.938469409942627, val_loss: 1.062771201133728\n",
            "Saving model, epoch: 237, train_loss: 0.9369484186172485, val_loss: 1.062196969985962\n",
            "Saving model, epoch: 238, train_loss: 0.9354397654533386, val_loss: 1.0613784790039062\n",
            "Saving model, epoch: 239, train_loss: 0.9339703321456909, val_loss: 1.060304880142212\n",
            "Saving model, epoch: 240, train_loss: 0.9325199127197266, val_loss: 1.0590380430221558\n",
            "Saving model, epoch: 241, train_loss: 0.9310843348503113, val_loss: 1.0576671361923218\n",
            "Saving model, epoch: 242, train_loss: 0.9296637773513794, val_loss: 1.0561916828155518\n",
            "Saving model, epoch: 243, train_loss: 0.9282694458961487, val_loss: 1.0547618865966797\n",
            "Saving model, epoch: 244, train_loss: 0.9269139170646667, val_loss: 1.0534296035766602\n",
            "Saving model, epoch: 245, train_loss: 0.9255838990211487, val_loss: 1.0522713661193848\n",
            "Saving model, epoch: 246, train_loss: 0.9242615699768066, val_loss: 1.0511651039123535\n",
            "Saving model, epoch: 247, train_loss: 0.9229391813278198, val_loss: 1.050114393234253\n",
            "Saving model, epoch: 248, train_loss: 0.9216268658638, val_loss: 1.048951506614685\n",
            "Saving model, epoch: 249, train_loss: 0.9203388094902039, val_loss: 1.0477628707885742\n",
            "Saving model, epoch: 250, train_loss: 0.9191154837608337, val_loss: 1.0467077493667603\n",
            "Saving model, epoch: 251, train_loss: 0.9179054498672485, val_loss: 1.045723795890808\n",
            "Saving model, epoch: 252, train_loss: 0.916711151599884, val_loss: 1.0448356866836548\n",
            "Saving model, epoch: 253, train_loss: 0.9155219197273254, val_loss: 1.043969750404358\n",
            "Saving model, epoch: 254, train_loss: 0.9143381118774414, val_loss: 1.0430068969726562\n",
            "Saving model, epoch: 255, train_loss: 0.9131612777709961, val_loss: 1.0418294668197632\n",
            "Saving model, epoch: 256, train_loss: 0.911981999874115, val_loss: 1.0404154062271118\n",
            "Saving model, epoch: 257, train_loss: 0.910814106464386, val_loss: 1.0388391017913818\n",
            "Saving model, epoch: 258, train_loss: 0.9096433520317078, val_loss: 1.0371971130371094\n",
            "Saving model, epoch: 259, train_loss: 0.9084804058074951, val_loss: 1.0355640649795532\n",
            "Saving model, epoch: 260, train_loss: 0.907318115234375, val_loss: 1.0340768098831177\n",
            "Saving model, epoch: 261, train_loss: 0.9061555862426758, val_loss: 1.032669186592102\n",
            "Saving model, epoch: 262, train_loss: 0.9050248861312866, val_loss: 1.0312877893447876\n",
            "Saving model, epoch: 263, train_loss: 0.9039299488067627, val_loss: 1.0299506187438965\n",
            "Saving model, epoch: 264, train_loss: 0.9028552174568176, val_loss: 1.0287891626358032\n",
            "Saving model, epoch: 265, train_loss: 0.9017871618270874, val_loss: 1.0276514291763306\n",
            "Saving model, epoch: 266, train_loss: 0.9007310271263123, val_loss: 1.0265777111053467\n",
            "Saving model, epoch: 267, train_loss: 0.8996816277503967, val_loss: 1.025499939918518\n",
            "Saving model, epoch: 268, train_loss: 0.8986565470695496, val_loss: 1.0244383811950684\n",
            "Saving model, epoch: 269, train_loss: 0.8976377844810486, val_loss: 1.0234274864196777\n",
            "Saving model, epoch: 270, train_loss: 0.8966370820999146, val_loss: 1.0225478410720825\n",
            "Saving model, epoch: 271, train_loss: 0.8956366181373596, val_loss: 1.0215795040130615\n",
            "Saving model, epoch: 272, train_loss: 0.8946435451507568, val_loss: 1.0207794904708862\n",
            "Saving model, epoch: 273, train_loss: 0.8936581611633301, val_loss: 1.0201292037963867\n",
            "Saving model, epoch: 274, train_loss: 0.8926872611045837, val_loss: 1.0196062326431274\n",
            "Saving model, epoch: 275, train_loss: 0.8917256593704224, val_loss: 1.018968105316162\n",
            "Saving model, epoch: 276, train_loss: 0.8907621502876282, val_loss: 1.0182427167892456\n",
            "Saving model, epoch: 277, train_loss: 0.8898008465766907, val_loss: 1.0173693895339966\n",
            "Saving model, epoch: 278, train_loss: 0.8888663053512573, val_loss: 1.0165481567382812\n",
            "Saving model, epoch: 279, train_loss: 0.8879482746124268, val_loss: 1.0157352685928345\n",
            "Saving model, epoch: 280, train_loss: 0.8870449662208557, val_loss: 1.0149314403533936\n",
            "Saving model, epoch: 281, train_loss: 0.8861377239227295, val_loss: 1.0142807960510254\n",
            "Saving model, epoch: 282, train_loss: 0.8852511048316956, val_loss: 1.0136358737945557\n",
            "Saving model, epoch: 283, train_loss: 0.8843715786933899, val_loss: 1.0128602981567383\n",
            "Saving model, epoch: 284, train_loss: 0.8834813237190247, val_loss: 1.0118860006332397\n",
            "Saving model, epoch: 285, train_loss: 0.8825982213020325, val_loss: 1.0107297897338867\n",
            "Saving model, epoch: 286, train_loss: 0.8817266225814819, val_loss: 1.0097140073776245\n",
            "Saving model, epoch: 287, train_loss: 0.8808619976043701, val_loss: 1.00876784324646\n",
            "Saving model, epoch: 288, train_loss: 0.8800156116485596, val_loss: 1.0079443454742432\n",
            "Saving model, epoch: 289, train_loss: 0.8791701793670654, val_loss: 1.007203459739685\n",
            "Saving model, epoch: 290, train_loss: 0.8783313632011414, val_loss: 1.0064527988433838\n",
            "Saving model, epoch: 291, train_loss: 0.8774959444999695, val_loss: 1.005537509918213\n",
            "Saving model, epoch: 292, train_loss: 0.8766772150993347, val_loss: 1.0047322511672974\n",
            "Saving model, epoch: 293, train_loss: 0.8758596181869507, val_loss: 1.0041276216506958\n",
            "Saving model, epoch: 294, train_loss: 0.8750494122505188, val_loss: 1.0035274028778076\n",
            "Saving model, epoch: 295, train_loss: 0.8742514252662659, val_loss: 1.002894639968872\n",
            "Saving model, epoch: 296, train_loss: 0.8734604120254517, val_loss: 1.002441167831421\n",
            "Saving model, epoch: 297, train_loss: 0.8726618885993958, val_loss: 1.0019983053207397\n",
            "Saving model, epoch: 298, train_loss: 0.871856689453125, val_loss: 1.0015735626220703\n",
            "Saving model, epoch: 299, train_loss: 0.8710704445838928, val_loss: 1.0012922286987305\n",
            "Saving model, epoch: 300, train_loss: 0.8702792525291443, val_loss: 1.001165747642517\n",
            "epoch: 301, train_loss: 0.8695143461227417, val_loss: 1.00096595287323\n",
            "Saving model, epoch: 301, train_loss: 0.8695143461227417, val_loss: 1.00096595287323\n",
            "Saving model, epoch: 302, train_loss: 0.8687571287155151, val_loss: 1.0006085634231567\n",
            "Saving model, epoch: 303, train_loss: 0.867996871471405, val_loss: 1.0002005100250244\n",
            "Saving model, epoch: 304, train_loss: 0.8672515749931335, val_loss: 0.9998636841773987\n",
            "Saving model, epoch: 305, train_loss: 0.8665328621864319, val_loss: 0.9995549917221069\n",
            "Saving model, epoch: 306, train_loss: 0.8658249378204346, val_loss: 0.9991995096206665\n",
            "Saving model, epoch: 307, train_loss: 0.8651142120361328, val_loss: 0.9986807107925415\n",
            "Saving model, epoch: 308, train_loss: 0.864416241645813, val_loss: 0.9981521964073181\n",
            "Saving model, epoch: 309, train_loss: 0.8637345433235168, val_loss: 0.99750816822052\n",
            "Saving model, epoch: 310, train_loss: 0.8630529642105103, val_loss: 0.9968217611312866\n",
            "Saving model, epoch: 311, train_loss: 0.8623828291893005, val_loss: 0.9961602091789246\n",
            "Saving model, epoch: 312, train_loss: 0.8617201447486877, val_loss: 0.995498776435852\n",
            "Saving model, epoch: 313, train_loss: 0.8610643148422241, val_loss: 0.9949747323989868\n",
            "Saving model, epoch: 314, train_loss: 0.8604156374931335, val_loss: 0.9945796132087708\n",
            "Saving model, epoch: 315, train_loss: 0.8597719669342041, val_loss: 0.9943750500679016\n",
            "Saving model, epoch: 316, train_loss: 0.859126627445221, val_loss: 0.9942493438720703\n",
            "Saving model, epoch: 317, train_loss: 0.8584878444671631, val_loss: 0.9940994381904602\n",
            "Saving model, epoch: 318, train_loss: 0.8578448295593262, val_loss: 0.9938585162162781\n",
            "Saving model, epoch: 319, train_loss: 0.8572136759757996, val_loss: 0.9936144351959229\n",
            "Saving model, epoch: 320, train_loss: 0.8565848469734192, val_loss: 0.9934184551239014\n",
            "Saving model, epoch: 321, train_loss: 0.8559586405754089, val_loss: 0.9931632280349731\n",
            "Saving model, epoch: 322, train_loss: 0.8553407788276672, val_loss: 0.99286949634552\n",
            "Saving model, epoch: 323, train_loss: 0.8547284007072449, val_loss: 0.9927017688751221\n",
            "Saving model, epoch: 324, train_loss: 0.8541343212127686, val_loss: 0.9924377202987671\n",
            "Saving model, epoch: 325, train_loss: 0.8535474538803101, val_loss: 0.9919813275337219\n",
            "Saving model, epoch: 326, train_loss: 0.8529571890830994, val_loss: 0.991331160068512\n",
            "Saving model, epoch: 327, train_loss: 0.8523787260055542, val_loss: 0.9907547235488892\n",
            "Saving model, epoch: 328, train_loss: 0.8517917990684509, val_loss: 0.9902521967887878\n",
            "Saving model, epoch: 329, train_loss: 0.8512039184570312, val_loss: 0.9898889660835266\n",
            "Saving model, epoch: 330, train_loss: 0.8506208062171936, val_loss: 0.989504337310791\n",
            "Saving model, epoch: 331, train_loss: 0.8500401973724365, val_loss: 0.9891119599342346\n",
            "Saving model, epoch: 332, train_loss: 0.8494599461555481, val_loss: 0.988744854927063\n",
            "Saving model, epoch: 333, train_loss: 0.8488755226135254, val_loss: 0.9884652495384216\n",
            "Saving model, epoch: 334, train_loss: 0.8482998609542847, val_loss: 0.9880332350730896\n",
            "Saving model, epoch: 335, train_loss: 0.8477299809455872, val_loss: 0.9875497221946716\n",
            "Saving model, epoch: 336, train_loss: 0.8471584320068359, val_loss: 0.9873226284980774\n",
            "Saving model, epoch: 337, train_loss: 0.8465790152549744, val_loss: 0.9871822595596313\n",
            "Saving model, epoch: 338, train_loss: 0.8459920883178711, val_loss: 0.9869151711463928\n",
            "Saving model, epoch: 339, train_loss: 0.8454121351242065, val_loss: 0.9864721894264221\n",
            "Saving model, epoch: 340, train_loss: 0.8448328375816345, val_loss: 0.9860484600067139\n",
            "Saving model, epoch: 341, train_loss: 0.8442445993423462, val_loss: 0.9857682585716248\n",
            "Saving model, epoch: 342, train_loss: 0.8436442017555237, val_loss: 0.9856136441230774\n",
            "Saving model, epoch: 343, train_loss: 0.8430466055870056, val_loss: 0.9855285286903381\n",
            "Saving model, epoch: 344, train_loss: 0.8424561619758606, val_loss: 0.9852176904678345\n",
            "Saving model, epoch: 345, train_loss: 0.8418594598770142, val_loss: 0.9846256971359253\n",
            "Saving model, epoch: 346, train_loss: 0.8412646055221558, val_loss: 0.9840154647827148\n",
            "Saving model, epoch: 347, train_loss: 0.8406776189804077, val_loss: 0.9835729598999023\n",
            "Saving model, epoch: 348, train_loss: 0.8400886058807373, val_loss: 0.9832305908203125\n",
            "Saving model, epoch: 349, train_loss: 0.8394892811775208, val_loss: 0.9829021096229553\n",
            "Saving model, epoch: 350, train_loss: 0.8388922214508057, val_loss: 0.9823862314224243\n",
            "Saving model, epoch: 351, train_loss: 0.8383018374443054, val_loss: 0.981804609298706\n",
            "Saving model, epoch: 352, train_loss: 0.8377197980880737, val_loss: 0.9814033508300781\n",
            "Saving model, epoch: 357, train_loss: 0.8348166942596436, val_loss: 0.9813757538795471\n",
            "Saving model, epoch: 358, train_loss: 0.834236204624176, val_loss: 0.9810189604759216\n",
            "Saving model, epoch: 359, train_loss: 0.8336761593818665, val_loss: 0.9809134006500244\n",
            "Saving model, epoch: 360, train_loss: 0.8331213593482971, val_loss: 0.9807404279708862\n",
            "Saving model, epoch: 361, train_loss: 0.8325615525245667, val_loss: 0.9801381230354309\n",
            "Saving model, epoch: 362, train_loss: 0.8319926261901855, val_loss: 0.9796240925788879\n",
            "Saving model, epoch: 363, train_loss: 0.8314232230186462, val_loss: 0.9792778491973877\n",
            "Saving model, epoch: 364, train_loss: 0.8308622241020203, val_loss: 0.9788548946380615\n",
            "Saving model, epoch: 365, train_loss: 0.8302940726280212, val_loss: 0.9786324501037598\n",
            "Saving model, epoch: 366, train_loss: 0.8297174572944641, val_loss: 0.9784632921218872\n",
            "Saving model, epoch: 367, train_loss: 0.8291403651237488, val_loss: 0.9783220887184143\n",
            "Saving model, epoch: 368, train_loss: 0.8285722732543945, val_loss: 0.9780069589614868\n",
            "Saving model, epoch: 369, train_loss: 0.8279906511306763, val_loss: 0.9778530597686768\n",
            "Saving model, epoch: 370, train_loss: 0.8274199962615967, val_loss: 0.9777972102165222\n",
            "Saving model, epoch: 371, train_loss: 0.826865553855896, val_loss: 0.9776614308357239\n",
            "Saving model, epoch: 372, train_loss: 0.8263057470321655, val_loss: 0.977401614189148\n",
            "Saving model, epoch: 373, train_loss: 0.8257502913475037, val_loss: 0.9770079255104065\n",
            "Saving model, epoch: 374, train_loss: 0.8252149820327759, val_loss: 0.9766360521316528\n",
            "Saving model, epoch: 375, train_loss: 0.824665367603302, val_loss: 0.976306676864624\n",
            "Saving model, epoch: 376, train_loss: 0.824116051197052, val_loss: 0.9757987260818481\n",
            "Saving model, epoch: 377, train_loss: 0.8235903382301331, val_loss: 0.9752217531204224\n",
            "Saving model, epoch: 378, train_loss: 0.8230650424957275, val_loss: 0.9745736122131348\n",
            "Saving model, epoch: 379, train_loss: 0.8225558996200562, val_loss: 0.9739472270011902\n",
            "Saving model, epoch: 380, train_loss: 0.8220232725143433, val_loss: 0.9735513925552368\n",
            "Saving model, epoch: 381, train_loss: 0.8215029835700989, val_loss: 0.9732488393783569\n",
            "Saving model, epoch: 382, train_loss: 0.8209913969039917, val_loss: 0.9730194807052612\n",
            "Saving model, epoch: 383, train_loss: 0.8204890489578247, val_loss: 0.9726126194000244\n",
            "Saving model, epoch: 384, train_loss: 0.8199759721755981, val_loss: 0.9721967577934265\n",
            "Saving model, epoch: 385, train_loss: 0.8194493651390076, val_loss: 0.9716045260429382\n",
            "Saving model, epoch: 386, train_loss: 0.818929135799408, val_loss: 0.9711151123046875\n",
            "Saving model, epoch: 387, train_loss: 0.8184133172035217, val_loss: 0.9706533551216125\n",
            "Saving model, epoch: 388, train_loss: 0.8178954720497131, val_loss: 0.9700552225112915\n",
            "Saving model, epoch: 389, train_loss: 0.8173825144767761, val_loss: 0.969311535358429\n",
            "Saving model, epoch: 390, train_loss: 0.8168882727622986, val_loss: 0.9686636328697205\n",
            "Saving model, epoch: 391, train_loss: 0.8163992166519165, val_loss: 0.9682379364967346\n",
            "Saving model, epoch: 392, train_loss: 0.8158891201019287, val_loss: 0.9673755764961243\n",
            "Saving model, epoch: 393, train_loss: 0.8153804540634155, val_loss: 0.9661929607391357\n",
            "Saving model, epoch: 394, train_loss: 0.8148760199546814, val_loss: 0.9654120206832886\n",
            "Saving model, epoch: 395, train_loss: 0.8143788576126099, val_loss: 0.9650837779045105\n",
            "Saving model, epoch: 396, train_loss: 0.8139025568962097, val_loss: 0.964989185333252\n",
            "Saving model, epoch: 397, train_loss: 0.813404381275177, val_loss: 0.9648584127426147\n",
            "Saving model, epoch: 398, train_loss: 0.8129056692123413, val_loss: 0.9644870162010193\n",
            "Saving model, epoch: 399, train_loss: 0.8124085068702698, val_loss: 0.9641603827476501\n",
            "Saving model, epoch: 400, train_loss: 0.8119086027145386, val_loss: 0.9639419913291931\n",
            "epoch: 401, train_loss: 0.8114160299301147, val_loss: 0.9637234210968018\n",
            "Saving model, epoch: 401, train_loss: 0.8114160299301147, val_loss: 0.9637234210968018\n",
            "Saving model, epoch: 402, train_loss: 0.810935378074646, val_loss: 0.9634190201759338\n",
            "Saving model, epoch: 403, train_loss: 0.810455858707428, val_loss: 0.9629678130149841\n",
            "Saving model, epoch: 404, train_loss: 0.8099676966667175, val_loss: 0.9623164534568787\n",
            "Saving model, epoch: 405, train_loss: 0.8095093965530396, val_loss: 0.9614139795303345\n",
            "Saving model, epoch: 406, train_loss: 0.8090327978134155, val_loss: 0.9608188271522522\n",
            "Saving model, epoch: 407, train_loss: 0.8085507750511169, val_loss: 0.9604496359825134\n",
            "Saving model, epoch: 408, train_loss: 0.8080768585205078, val_loss: 0.9602020382881165\n",
            "Saving model, epoch: 409, train_loss: 0.8076218962669373, val_loss: 0.9598587155342102\n",
            "Saving model, epoch: 410, train_loss: 0.8071655631065369, val_loss: 0.9594686627388\n",
            "Saving model, epoch: 411, train_loss: 0.8067087531089783, val_loss: 0.958784282207489\n",
            "Saving model, epoch: 412, train_loss: 0.8062477707862854, val_loss: 0.9580882787704468\n",
            "Saving model, epoch: 413, train_loss: 0.8057924509048462, val_loss: 0.9572842121124268\n",
            "Saving model, epoch: 414, train_loss: 0.8053433895111084, val_loss: 0.9566548466682434\n",
            "Saving model, epoch: 415, train_loss: 0.8048977255821228, val_loss: 0.956321656703949\n",
            "Saving model, epoch: 416, train_loss: 0.8044493794441223, val_loss: 0.956108033657074\n",
            "Saving model, epoch: 418, train_loss: 0.8035838603973389, val_loss: 0.9559911489486694\n",
            "Saving model, epoch: 419, train_loss: 0.8031569719314575, val_loss: 0.9556040167808533\n",
            "Saving model, epoch: 420, train_loss: 0.8027310371398926, val_loss: 0.9549482464790344\n",
            "Saving model, epoch: 421, train_loss: 0.8023123741149902, val_loss: 0.9542638659477234\n",
            "Saving model, epoch: 422, train_loss: 0.8018998503684998, val_loss: 0.9538894891738892\n",
            "Saving model, epoch: 423, train_loss: 0.801464855670929, val_loss: 0.9537943601608276\n",
            "Saving model, epoch: 424, train_loss: 0.8010447025299072, val_loss: 0.9536247849464417\n",
            "Saving model, epoch: 425, train_loss: 0.8006302714347839, val_loss: 0.9532120227813721\n",
            "Saving model, epoch: 426, train_loss: 0.800226628780365, val_loss: 0.95269775390625\n",
            "Saving model, epoch: 427, train_loss: 0.799802303314209, val_loss: 0.9521257877349854\n",
            "Saving model, epoch: 428, train_loss: 0.7993870377540588, val_loss: 0.9516701698303223\n",
            "Saving model, epoch: 429, train_loss: 0.7989937663078308, val_loss: 0.9512656927108765\n",
            "Saving model, epoch: 430, train_loss: 0.7985948324203491, val_loss: 0.9511834979057312\n",
            "Saving model, epoch: 434, train_loss: 0.7969796657562256, val_loss: 0.9508127570152283\n",
            "Saving model, epoch: 435, train_loss: 0.7965668439865112, val_loss: 0.9503799676895142\n",
            "Saving model, epoch: 436, train_loss: 0.7961686253547668, val_loss: 0.9499620199203491\n",
            "Saving model, epoch: 437, train_loss: 0.7957702279090881, val_loss: 0.9495545625686646\n",
            "Saving model, epoch: 438, train_loss: 0.7953583002090454, val_loss: 0.9492277503013611\n",
            "Saving model, epoch: 439, train_loss: 0.7949464321136475, val_loss: 0.9489259719848633\n",
            "Saving model, epoch: 440, train_loss: 0.7945443987846375, val_loss: 0.9487167596817017\n",
            "Saving model, epoch: 441, train_loss: 0.7941433191299438, val_loss: 0.9485916495323181\n",
            "Saving model, epoch: 442, train_loss: 0.7937427759170532, val_loss: 0.9485321044921875\n",
            "Saving model, epoch: 444, train_loss: 0.7929234504699707, val_loss: 0.9484728574752808\n",
            "Saving model, epoch: 445, train_loss: 0.7925339341163635, val_loss: 0.9481291174888611\n",
            "Saving model, epoch: 446, train_loss: 0.7921295762062073, val_loss: 0.9475486278533936\n",
            "Saving model, epoch: 447, train_loss: 0.7917357683181763, val_loss: 0.9469034075737\n",
            "Saving model, epoch: 448, train_loss: 0.7913413643836975, val_loss: 0.946675181388855\n",
            "Saving model, epoch: 454, train_loss: 0.7890204787254333, val_loss: 0.9464617967605591\n",
            "Saving model, epoch: 455, train_loss: 0.7886379957199097, val_loss: 0.9460328221321106\n",
            "Saving model, epoch: 456, train_loss: 0.7882601022720337, val_loss: 0.9456673860549927\n",
            "Saving model, epoch: 457, train_loss: 0.7878780364990234, val_loss: 0.9454115033149719\n",
            "Saving model, epoch: 458, train_loss: 0.7874977588653564, val_loss: 0.9453780651092529\n",
            "Saving model, epoch: 459, train_loss: 0.7871220707893372, val_loss: 0.9452674984931946\n",
            "Saving model, epoch: 460, train_loss: 0.7867512106895447, val_loss: 0.9451868534088135\n",
            "Saving model, epoch: 463, train_loss: 0.7856288552284241, val_loss: 0.9451285600662231\n",
            "Saving model, epoch: 464, train_loss: 0.7852553725242615, val_loss: 0.9448273777961731\n",
            "Saving model, epoch: 465, train_loss: 0.7848753333091736, val_loss: 0.9444626569747925\n",
            "Saving model, epoch: 466, train_loss: 0.7845091223716736, val_loss: 0.9443127512931824\n",
            "Saving model, epoch: 468, train_loss: 0.7837936878204346, val_loss: 0.9441545605659485\n",
            "Saving model, epoch: 469, train_loss: 0.7834097146987915, val_loss: 0.9438984990119934\n",
            "Saving model, epoch: 470, train_loss: 0.7830613851547241, val_loss: 0.9437150359153748\n",
            "Saving model, epoch: 471, train_loss: 0.7827113270759583, val_loss: 0.9437082409858704\n",
            "Saving model, epoch: 475, train_loss: 0.7812511920928955, val_loss: 0.9434013962745667\n",
            "Saving model, epoch: 476, train_loss: 0.7808616757392883, val_loss: 0.943342387676239\n",
            "Saving model, epoch: 482, train_loss: 0.7786231637001038, val_loss: 0.9432024359703064\n",
            "Saving model, epoch: 486, train_loss: 0.7771137952804565, val_loss: 0.9430499076843262\n",
            "Saving model, epoch: 487, train_loss: 0.7767122387886047, val_loss: 0.9425901174545288\n",
            "Saving model, epoch: 488, train_loss: 0.7763464450836182, val_loss: 0.9419797658920288\n",
            "Saving model, epoch: 489, train_loss: 0.7759806513786316, val_loss: 0.9415034651756287\n",
            "Saving model, epoch: 490, train_loss: 0.7756054401397705, val_loss: 0.94111168384552\n",
            "Saving model, epoch: 491, train_loss: 0.7752268314361572, val_loss: 0.9408883452415466\n",
            "Saving model, epoch: 492, train_loss: 0.7748395204544067, val_loss: 0.9408473968505859\n",
            "Saving model, epoch: 493, train_loss: 0.7744682431221008, val_loss: 0.9405686259269714\n",
            "Saving model, epoch: 494, train_loss: 0.7740799188613892, val_loss: 0.940108597278595\n",
            "Saving model, epoch: 495, train_loss: 0.773711085319519, val_loss: 0.9394701719284058\n",
            "Saving model, epoch: 496, train_loss: 0.7733378410339355, val_loss: 0.9386101365089417\n",
            "Saving model, epoch: 497, train_loss: 0.7729759812355042, val_loss: 0.9379053115844727\n",
            "Saving model, epoch: 498, train_loss: 0.7726093530654907, val_loss: 0.9375503063201904\n",
            "Saving model, epoch: 499, train_loss: 0.7722303867340088, val_loss: 0.9372784495353699\n",
            "Saving model, epoch: 500, train_loss: 0.7718634605407715, val_loss: 0.9369515776634216\n",
            "epoch: 501, train_loss: 0.7714848518371582, val_loss: 0.9371352195739746\n",
            "Saving model, epoch: 505, train_loss: 0.7700195908546448, val_loss: 0.9368197321891785\n",
            "Saving model, epoch: 506, train_loss: 0.7696534991264343, val_loss: 0.9363279938697815\n",
            "Saving model, epoch: 507, train_loss: 0.7692829966545105, val_loss: 0.9360774159431458\n",
            "Saving model, epoch: 510, train_loss: 0.7681125402450562, val_loss: 0.9359056949615479\n",
            "Saving model, epoch: 511, train_loss: 0.7677163481712341, val_loss: 0.9355455040931702\n",
            "Saving model, epoch: 512, train_loss: 0.7673379182815552, val_loss: 0.935445249080658\n",
            "Saving model, epoch: 513, train_loss: 0.7669662833213806, val_loss: 0.9352782964706421\n",
            "Saving model, epoch: 514, train_loss: 0.7665697336196899, val_loss: 0.9349844455718994\n",
            "Saving model, epoch: 515, train_loss: 0.7661826014518738, val_loss: 0.9346108436584473\n",
            "Saving model, epoch: 516, train_loss: 0.7657952904701233, val_loss: 0.9344040751457214\n",
            "Saving model, epoch: 519, train_loss: 0.7646099925041199, val_loss: 0.9342613220214844\n",
            "Saving model, epoch: 520, train_loss: 0.7641977071762085, val_loss: 0.9341346621513367\n",
            "Saving model, epoch: 521, train_loss: 0.7637745141983032, val_loss: 0.9340021014213562\n",
            "Saving model, epoch: 522, train_loss: 0.763373076915741, val_loss: 0.9339234828948975\n",
            "Saving model, epoch: 524, train_loss: 0.7625992894172668, val_loss: 0.9337309002876282\n",
            "epoch: 601, train_loss: 0.7377673387527466, val_loss: 0.9399106502532959\n",
            "epoch: 701, train_loss: 0.7108179330825806, val_loss: 0.9455870389938354\n",
            "epoch: 801, train_loss: 0.6898142099380493, val_loss: 0.9520471692085266\n",
            "finished training after 825 epochs\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "       BatchNorm1d-1                   [-1, 90]             180\n",
            "            Linear-2                   [-1, 16]           1,456\n",
            "              ReLU-3                   [-1, 16]               0\n",
            "            Linear-4                    [-1, 1]              17\n",
            "================================================================\n",
            "Total params: 1,653\n",
            "Trainable params: 1,653\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.01\n",
            "Estimated Total Size (MB): 0.01\n",
            "----------------------------------------------------------------\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "config: {'INPUT_DIM': 90, 'TRAIN_PATH': 'gdrive/MyDrive/ColabNotebooks/HW1/dataset/covid.train.csv', 'TEST_PATH': 'gdrive/MyDrive/ColabNotebooks/HW1/dataset/covid.test.csv', 'MODEL_PATH': 'models/model.pth', 'PRED_PATH': 'pred.csv', 'EPOCH_NUM': 30000, 'BATCH_SIZE': 4096, 'VAL_RATIO': 0.1, 'OPTIMIZER': 'Adam', 'OPTIM_PARAMS': {'lr': 0.05}, 'DECAY_RATE': 1, 'MIN_LR': 0.0001, 'EARLY_STOP': 300, 'MODEL_NUM': 3, 'STATE': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}