{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of feature_selection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wS_4-77xHk44",
        "g0pdrhQAO41L",
        "aQikz3IPiyPf",
        "nfrVxqJanGpE",
        "9tmCwXgpot3t"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz0_QVkxCrX3"
      },
      "source": [
        "# **Homework 1: COVID-19 Cases Prediction (Regression)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMj55YDKG6ch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "637b3af9-c3ca-42c1-b9b9-9e8d1af2dd29"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!gdown --id '19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF' --output covid.train.csv\n",
        "!gdown --id '1CE240jLm2npU-tdz81-oVKEF3T2yfT1O' --output covid.test.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF\n",
            "To: /content/covid.train.csv\n",
            "100% 2.00M/2.00M [00:00<00:00, 63.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CE240jLm2npU-tdz81-oVKEF3T2yfT1O\n",
            "To: /content/covid.test.csv\n",
            "100% 651k/651k [00:00<00:00, 95.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF-QTQLkv5h8"
      },
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# set a random seed for reproducibility\n",
        "myseed = 42069\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)\n",
        "\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "config = {\n",
        "    'INPUT_DIM': 14,\n",
        "    'TRAIN_PATH': 'covid.train.csv',\n",
        "    'TEST_PATH': 'covid.test.csv',\n",
        "    'MODEL_PATH': 'models/model.pth',\n",
        "    'PRED_PATH': 'pred.csv',\n",
        "\n",
        "    'EPOCH_NUM': 30000,\n",
        "    'BATCH_SIZE': 4096,\n",
        "    'VAL_RATIO': 0.1,\n",
        "    'OPTIMIZER': 'Adam',\n",
        "    'OPTIM_PARAMS': {\n",
        "        'lr': 5e-2,\n",
        "        # 'weight_decay': 5e-3,\n",
        "    },\n",
        "    'DECAY_RATE': 0.999,\n",
        "    'MIN_LR': 1e-4,\n",
        "    'EARLY_STOP': 300,\n",
        "    'MODEL_NUM': 20,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaRGyPYGvvV5"
      },
      "source": [
        "class Drawer():\n",
        "    def plot_learning_curve(self, loss_record, title=''):\n",
        "        ''' Plot learning curve of your DNN (train & dev loss) '''\n",
        "        total_steps = len(loss_record['train'])\n",
        "        x_1 = range(total_steps)\n",
        "        x_2 = x_1[::len(loss_record['train']) // len(loss_record['val'])]\n",
        "        figure(figsize=(6, 4))\n",
        "        plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
        "        plt.plot(x_2, loss_record['val'], c='tab:cyan', label='val')\n",
        "        plt.ylim(0.0, 5.)\n",
        "        plt.xlabel('Training steps')\n",
        "        plt.ylabel('MSE loss')\n",
        "        plt.title('Learning curve of {}'.format(title))\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_pred(self, dv_set, model, device, lim=35., preds=None, targets=None):\n",
        "        ''' Plot prediction of your DNN '''\n",
        "        if preds is None or targets is None:\n",
        "            model.eval()\n",
        "            preds, targets = [], []\n",
        "            for x, y in dv_set:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                with torch.no_grad():\n",
        "                    pred = model(x)\n",
        "                    preds.append(pred.detach().cpu())\n",
        "                    targets.append(y.detach().cpu())\n",
        "            preds = torch.cat(preds, dim=0).numpy()\n",
        "            targets = torch.cat(targets, dim=0).numpy()\n",
        "\n",
        "        figure(figsize=(5, 5))\n",
        "        plt.scatter(targets, preds, c='r', alpha=0.5)\n",
        "        plt.plot([-0.2, lim], [-0.2, lim], c='b')\n",
        "        plt.xlim(-0.2, lim)\n",
        "        plt.ylim(-0.2, lim)\n",
        "        plt.xlabel('ground truth value')\n",
        "        plt.ylabel('predicted value')\n",
        "        plt.title('Ground Truth v.s. Prediction')\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uazKiUCwD9U"
      },
      "source": [
        "from sklearn.feature_selection import f_regression, SelectKBest, mutual_info_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "class DataManager():\n",
        "    def __init__(self):\n",
        "        print('init data manager...')\n",
        "        TRAIN_PATH = config['TRAIN_PATH']\n",
        "        INPUT_DIM = config['INPUT_DIM']\n",
        "\n",
        "        self.state = 1\n",
        "\n",
        "        with open(TRAIN_PATH, 'r') as f:\n",
        "            self.data = list(csv.reader(f))\n",
        "            self.data = np.array(self.data[1:])[:, 1:].astype(np.float32)\n",
        "\n",
        "        self.X = self.data[:, :-1]\n",
        "        self.y = self.data[:, -1]\n",
        "\n",
        "        selector = SelectKBest(f_regression, k=INPUT_DIM)\n",
        "        selector.fit(self.X, self.y)\n",
        "        self.cols = selector.get_support(indices=True)\n",
        "\n",
        "        self.X = self.X[:, self.cols]\n",
        "\n",
        "    def get_train_data(self):\n",
        "        print('getting train data...')\n",
        "        VAL_RATIO = config['VAL_RATIO']\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(self.X, self.y, test_size=VAL_RATIO, random_state=self.state)\n",
        "        self.state += 1\n",
        "\n",
        "        return X_train, X_val, y_train, y_val\n",
        "    \n",
        "    def get_test_data(self):\n",
        "        print('getting test data...')\n",
        "        TEST_PATH = config['TEST_PATH']\n",
        "        with open(TEST_PATH, 'r') as f:\n",
        "            data = list(csv.reader(f))\n",
        "            data = np.array(data[1:])[:, 1:].astype(np.float32)\n",
        "        X_test = data[:, self.cols]\n",
        "        return X_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3N7YzC72Ykp"
      },
      "source": [
        "class CovidDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        print('init dataset...')\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        if y is None:\n",
        "            self.y = None\n",
        "        else:\n",
        "            self.y = torch.from_numpy(y).float()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is None:\n",
        "            return self.X[idx]\n",
        "\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGJ7r5sQ4bTU"
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        print('init neural net...')\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        INPUT_DIM = config['INPUT_DIM']\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.BatchNorm1d(INPUT_DIM),\n",
        "            nn.Linear(INPUT_DIM, 64),\n",
        "            nn.ReLU(),\n",
        "            # nn.BatchNorm1d(64),\n",
        "            # nn.Linear(128, 128),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Linear(32, 16),\n",
        "            # nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "        )\n",
        "\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "    def get_loss(self, y_pred, y):\n",
        "        return self.criterion(y_pred, y)\n",
        "\n",
        "    def summary(self):\n",
        "        INPUT_DIM = config['INPUT_DIM']\n",
        "        summary(self, (INPUT_DIM, ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxNvo1sv5Qyb"
      },
      "source": [
        "class Trainer():\n",
        "    def __init__(self):\n",
        "        print('init trainer')\n",
        "        self.set_device()\n",
        "        self.set_model()\n",
        "        self.set_data_loader()\n",
        "        self.set_drawer()\n",
        "        self.set_optim()\n",
        "\n",
        "        self.loss_record = {'train': [], 'val': []}\n",
        "\n",
        "    def set_drawer(self):\n",
        "        self.drawer = Drawer()\n",
        "\n",
        "    def draw_learning_curve(self):\n",
        "        self.drawer.plot_learning_curve(self.loss_record, title='deep model')\n",
        "\n",
        "    def draw_val_results(self):\n",
        "        MODEL_PATH = config['MODEL_PATH']\n",
        "        del self.model\n",
        "        self.set_model()\n",
        "\n",
        "        ckpt = torch.load(MODEL_PATH, map_location='cpu')\n",
        "        self.model.load_state_dict(ckpt)\n",
        "        self.drawer.plot_pred(self.val_loader, self.model, self.device)\n",
        "\n",
        "    def pred_y_test(self):\n",
        "        print('predicting...')\n",
        "        BATCH_SIZE = config['BATCH_SIZE']\n",
        "        PRED_PATH = config['PRED_PATH']\n",
        "\n",
        "        X_test = self.dataManager.get_test_data()\n",
        "        test_loader = DataLoader(X_test, BATCH_SIZE, False, drop_last=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "        self.model.eval()\n",
        "        y_preds = []\n",
        "        for x in test_loader:\n",
        "            x = x.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                y_pred = self.model(x)\n",
        "                y_preds.append(y_pred.detach().cpu())\n",
        "        \n",
        "        y_preds = torch.cat(y_preds, dim=0).numpy()\n",
        "        return y_preds\n",
        "\n",
        "    def set_device(self):\n",
        "        ''' Get device (if GPU is available, use GPU) '''\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def set_data_loader(self):\n",
        "        BATCH_SIZE = config['BATCH_SIZE']\n",
        "        self.dataManager = DataManager()\n",
        "        X_train, X_val, y_train, y_val = self.dataManager.get_train_data()   \n",
        "\n",
        "        train_set = CovidDataset(X_train, y_train)\n",
        "        self.train_loader = DataLoader(train_set, BATCH_SIZE, True, drop_last=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "        val_set = CovidDataset(X_val, y_val)\n",
        "        self.val_loader = DataLoader(val_set, BATCH_SIZE, False, drop_last=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "    def set_model(self):\n",
        "        self.model = NeuralNet().to(self.device)\n",
        "\n",
        "    def set_optim(self):\n",
        "        OPTIMIZER = config['OPTIMIZER']\n",
        "        OPTIM_PARAMS = config['OPTIM_PARAMS']\n",
        "\n",
        "        self.optimizer = getattr(torch.optim, OPTIMIZER)(self.model.parameters(), **OPTIM_PARAMS)\n",
        "\n",
        "    def train(self):\n",
        "        EPOCH_NUM = config['EPOCH_NUM']\n",
        "        MODEL_PATH = config['MODEL_PATH']\n",
        "        EARLY_STOP = config['EARLY_STOP']\n",
        "\n",
        "        min_val_loss = float('inf')\n",
        "        early_stop_count = 0\n",
        "        for epoch in range(EPOCH_NUM):\n",
        "            self.model.train()\n",
        "            for x, y in self.train_loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                x, y = x.to(self.device), y.to(self.device)\n",
        "                y_pred = self.model(x)\n",
        "                loss = self.model.get_loss(y_pred, y)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            self.update_lr()\n",
        "            val_loss = self.get_loss(self.val_loader)\n",
        "            train_loss = self.get_loss(self.train_loader)\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'epoch: {epoch+1}, train_loss: {train_loss}, val_loss: {val_loss}')\n",
        "\n",
        "            if val_loss < min_val_loss:\n",
        "                min_val_loss = val_loss\n",
        "                print(f'Saving model, epoch: {epoch+1}, train_loss: {train_loss}, val_loss: {val_loss}')\n",
        "                torch.save(self.model.state_dict(), MODEL_PATH)\n",
        "                early_stop_cnt = 0\n",
        "\n",
        "            else:\n",
        "                early_stop_cnt += 1\n",
        "\n",
        "            self.loss_record['val'].append(val_loss)\n",
        "            self.loss_record['train'].append(train_loss)\n",
        "\n",
        "            if early_stop_cnt > EARLY_STOP:\n",
        "                break\n",
        "\n",
        "        # print(f'Saving model, epoch: {epoch+1}, train_loss: {train_loss}, val_loss: {val_loss}')\n",
        "        # torch.save(self.model.state_dict(), MODEL_PATH)\n",
        "        \n",
        "        print(f'finished training after {epoch+1} epochs')\n",
        "        self.model.summary()\n",
        "\n",
        "    \n",
        "    def update_lr(self):\n",
        "        DECAY_RATE = config['DECAY_RATE']\n",
        "        MIN_LR = config['MIN_LR']\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = param_group['lr'] * DECAY_RATE\n",
        "            param_group['lr'] = max(MIN_LR, param_group['lr'])\n",
        "\n",
        "    def get_loss(self, loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(self.device), y.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                y_pred = self.model(x)\n",
        "                loss = self.model.get_loss(y_pred, y)\n",
        "            \n",
        "            total_loss += loss.detach().cpu().item() * len(x)\n",
        "        total_loss /= len(loader.dataset)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def get_y_pred(self, loader):\n",
        "        self.model.eval()\n",
        "        y_preds = []\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(self.device), y.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                y_pred = self.model(x)\n",
        "                y_preds.append(y_pred.detach().cpu())\n",
        "\n",
        "        y_preds = torch.cat(y_preds, dim=0).numpy()\n",
        "        return y_preds\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikKiqeHNUTUK"
      },
      "source": [
        "class Emssembler(Trainer):\n",
        "    def __init__(self):\n",
        "        MODEL_NUM = config['MODEL_NUM']\n",
        "        self.trainers = []\n",
        "        for i in range(MODEL_NUM):\n",
        "            self.trainers.append(Trainer())\n",
        "        \n",
        "        self.trainModels()\n",
        "        super(Emssembler, self).__init__()\n",
        "\n",
        "    def set_model(self):\n",
        "        self.model = MetaLearner().to(self.device)\n",
        "\n",
        "    def set_data_loader(self):\n",
        "        BATCH_SIZE = config['BATCH_SIZE']\n",
        "        MODEL_NUM = config['MODEL_NUM']\n",
        "        self.dataManager = DataManager()\n",
        "        X_train, X_val, y_train, y_val = self.dataManager.get_train_data()   \n",
        "        train_set = CovidDataset(X_train, y_train)\n",
        "        train_loader = DataLoader(train_set, BATCH_SIZE, False, drop_last=False, num_workers=0, pin_memory=True)\n",
        "        val_set = CovidDataset(X_val, y_val)\n",
        "        val_loader = DataLoader(val_set, BATCH_SIZE, False, drop_last=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "        meta_X_train = None\n",
        "        meta_X_val = None\n",
        "        for i in range(MODEL_NUM):\n",
        "            train_y_pred = self.trainers[i].get_y_pred(train_loader)\n",
        "            train_y_pred = np.reshape(train_y_pred, (train_y_pred.shape[0], 1))\n",
        "            if meta_X_train is None:\n",
        "                meta_X_train = train_y_pred\n",
        "            else:\n",
        "                meta_X_train = np.concatenate((meta_X_train, train_y_pred), axis=1)\n",
        "\n",
        "            val_y_pred = self.trainers[i].get_y_pred(val_loader)\n",
        "            val_y_pred = np.reshape(val_y_pred, (val_y_pred.shape[0], 1))\n",
        "            if meta_X_val is None:\n",
        "                meta_X_val = val_y_pred\n",
        "            else:\n",
        "                meta_X_val = np.concatenate((meta_X_val, val_y_pred), axis=1)\n",
        "\n",
        "        meta_train_set = CovidDataset(meta_X_train, y_train)\n",
        "        meta_val_set = CovidDataset(meta_X_val, y_val)\n",
        "        self.train_loader = DataLoader(meta_train_set, BATCH_SIZE, True, drop_last=False, num_workers=0, pin_memory=True)\n",
        "        self.val_loader = DataLoader(meta_val_set, BATCH_SIZE, False, drop_last=False, num_workers=0, pin_memory=True)\n",
        "        \n",
        "    def trainModels(self):\n",
        "        MODEL_NUM = config['MODEL_NUM']\n",
        "        for trainer in self.trainers:\n",
        "            trainer.train()\n",
        "\n",
        "    def pred_y_test(self):\n",
        "        MODEL_NUM = config['MODEL_NUM']\n",
        "        PRED_PATH = config['PRED_PATH']\n",
        "        BATCH_SIZE = config['BATCH_SIZE']\n",
        "        \n",
        "        # y_preds = None\n",
        "        meta_X_test = None\n",
        "        for trainer in self.trainers:\n",
        "            test_y_pred = trainer.pred_y_test()\n",
        "            test_y_pred = np.reshape(test_y_pred, (test_y_pred.shape[0], 1))\n",
        "            if meta_X_test is None:\n",
        "                meta_X_test = test_y_pred\n",
        "            else:\n",
        "                meta_X_test = np.concatenate((meta_X_test, test_y_pred), axis=1)\n",
        "\n",
        "        test_loader = DataLoader(meta_X_test, BATCH_SIZE, False, drop_last=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "        self.model.eval()\n",
        "        meta_y_preds = []\n",
        "        for x in test_loader:\n",
        "            x = x.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                meta_y_pred = self.model(x)\n",
        "                meta_y_preds.append(meta_y_pred.detach().cpu())\n",
        "        \n",
        "        meta_y_preds = torch.cat(meta_y_preds, dim=0).numpy()\n",
        "\n",
        "        with open(PRED_PATH, 'w') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['id', 'tested_positive'])\n",
        "            for i, p in enumerate(meta_y_preds):\n",
        "                writer.writerow([i, p])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au2n-Y2SbYzN"
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "class MetaLearner(nn.Module):\n",
        "    def __init__(self):\n",
        "        print('init neural net...')\n",
        "        super(MetaLearner, self).__init__()\n",
        "\n",
        "        MODEL_NUM = config['MODEL_NUM']\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(MODEL_NUM, 1),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Linear(16, 1),\n",
        "        )\n",
        "\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "    def get_loss(self, y_pred, y):\n",
        "        return self.criterion(y_pred, y)\n",
        "\n",
        "    def summary(self):\n",
        "        MODEL_NUM = config['MODEL_NUM']\n",
        "        summary(self, (MODEL_NUM, ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMBMUOu0ng0Y",
        "outputId": "411ac56f-e535-45a8-f6cd-2f4fc0d51a84"
      },
      "source": [
        "# trainer = Trainer()\n",
        "# trainer.train()\n",
        "# trainer.draw_learning_curve()\n",
        "# trainer.draw_val_results()\n",
        "# trainer.pred_y_test()\n",
        "emssembler = Emssembler()\n",
        "emssembler.train()\n",
        "emssembler.pred_y_test()\n",
        "print(f'config: {config}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Saving model, epoch: 10908, train_loss: 0.9009036421775818, val_loss: 0.86656254529953\n",
            "Saving model, epoch: 10909, train_loss: 0.9009022116661072, val_loss: 0.8665613532066345\n",
            "Saving model, epoch: 10910, train_loss: 0.9009007811546326, val_loss: 0.8665607571601868\n",
            "Saving model, epoch: 10911, train_loss: 0.9008995294570923, val_loss: 0.8665601015090942\n",
            "Saving model, epoch: 10912, train_loss: 0.9008980989456177, val_loss: 0.8665595054626465\n",
            "Saving model, epoch: 10913, train_loss: 0.9008969068527222, val_loss: 0.8665587306022644\n",
            "Saving model, epoch: 10914, train_loss: 0.9008955955505371, val_loss: 0.8665580749511719\n",
            "Saving model, epoch: 10915, train_loss: 0.900894284248352, val_loss: 0.8665571212768555\n",
            "Saving model, epoch: 10916, train_loss: 0.9008927941322327, val_loss: 0.8665564656257629\n",
            "Saving model, epoch: 10917, train_loss: 0.9008914828300476, val_loss: 0.8665554523468018\n",
            "Saving model, epoch: 10918, train_loss: 0.9008901715278625, val_loss: 0.8665549159049988\n",
            "Saving model, epoch: 10919, train_loss: 0.900888979434967, val_loss: 0.8665540814399719\n",
            "Saving model, epoch: 10920, train_loss: 0.9008875489234924, val_loss: 0.866553544998169\n",
            "Saving model, epoch: 10921, train_loss: 0.9008859395980835, val_loss: 0.8665525913238525\n",
            "Saving model, epoch: 10922, train_loss: 0.9008846282958984, val_loss: 0.8665520548820496\n",
            "Saving model, epoch: 10923, train_loss: 0.9008834362030029, val_loss: 0.8665512204170227\n",
            "Saving model, epoch: 10924, train_loss: 0.9008820056915283, val_loss: 0.8665505051612854\n",
            "Saving model, epoch: 10925, train_loss: 0.9008806943893433, val_loss: 0.866550087928772\n",
            "Saving model, epoch: 10926, train_loss: 0.9008793830871582, val_loss: 0.8665491938591003\n",
            "Saving model, epoch: 10927, train_loss: 0.9008778929710388, val_loss: 0.8665482401847839\n",
            "Saving model, epoch: 10928, train_loss: 0.9008767008781433, val_loss: 0.8665476441383362\n",
            "Saving model, epoch: 10929, train_loss: 0.9008752703666687, val_loss: 0.8665470480918884\n",
            "Saving model, epoch: 10930, train_loss: 0.9008738994598389, val_loss: 0.8665459156036377\n",
            "Saving model, epoch: 10931, train_loss: 0.9008726477622986, val_loss: 0.8665454387664795\n",
            "Saving model, epoch: 10932, train_loss: 0.9008710384368896, val_loss: 0.8665446043014526\n",
            "Saving model, epoch: 10933, train_loss: 0.9008698463439941, val_loss: 0.8665436506271362\n",
            "Saving model, epoch: 10934, train_loss: 0.9008684754371643, val_loss: 0.8665435314178467\n",
            "Saving model, epoch: 10935, train_loss: 0.9008671641349792, val_loss: 0.8665424585342407\n",
            "Saving model, epoch: 10936, train_loss: 0.9008658528327942, val_loss: 0.8665414452552795\n",
            "Saving model, epoch: 10937, train_loss: 0.9008644223213196, val_loss: 0.86654132604599\n",
            "Saving model, epoch: 10938, train_loss: 0.9008630514144897, val_loss: 0.8665401935577393\n",
            "Saving model, epoch: 10939, train_loss: 0.9008616209030151, val_loss: 0.8665394186973572\n",
            "Saving model, epoch: 10940, train_loss: 0.9008603096008301, val_loss: 0.8665385246276855\n",
            "Saving model, epoch: 10941, train_loss: 0.900858998298645, val_loss: 0.8665380477905273\n",
            "Saving model, epoch: 10942, train_loss: 0.9008576273918152, val_loss: 0.86653733253479\n",
            "Saving model, epoch: 10943, train_loss: 0.9008563756942749, val_loss: 0.8665361404418945\n",
            "Saving model, epoch: 10944, train_loss: 0.900854766368866, val_loss: 0.8665356636047363\n",
            "Saving model, epoch: 10945, train_loss: 0.9008533954620361, val_loss: 0.8665345907211304\n",
            "Saving model, epoch: 10946, train_loss: 0.9008522033691406, val_loss: 0.8665341734886169\n",
            "Saving model, epoch: 10947, train_loss: 0.9008506536483765, val_loss: 0.8665334582328796\n",
            "Saving model, epoch: 10948, train_loss: 0.9008493423461914, val_loss: 0.8665326833724976\n",
            "Saving model, epoch: 10949, train_loss: 0.9008479714393616, val_loss: 0.8665319085121155\n",
            "Saving model, epoch: 10950, train_loss: 0.9008466601371765, val_loss: 0.8665311336517334\n",
            "Saving model, epoch: 10951, train_loss: 0.9008452296257019, val_loss: 0.86653071641922\n",
            "Saving model, epoch: 10952, train_loss: 0.9008439183235168, val_loss: 0.8665297031402588\n",
            "Saving model, epoch: 10953, train_loss: 0.900842547416687, val_loss: 0.8665288686752319\n",
            "Saving model, epoch: 10954, train_loss: 0.9008411169052124, val_loss: 0.866528332233429\n",
            "Saving model, epoch: 10955, train_loss: 0.9008399248123169, val_loss: 0.8665275573730469\n",
            "Saving model, epoch: 10956, train_loss: 0.9008384346961975, val_loss: 0.8665265440940857\n",
            "Saving model, epoch: 10957, train_loss: 0.9008370041847229, val_loss: 0.8665259480476379\n",
            "Saving model, epoch: 10958, train_loss: 0.9008359313011169, val_loss: 0.8665252327919006\n",
            "Saving model, epoch: 10959, train_loss: 0.900834321975708, val_loss: 0.8665242791175842\n",
            "Saving model, epoch: 10960, train_loss: 0.900833010673523, val_loss: 0.8665237426757812\n",
            "Saving model, epoch: 10961, train_loss: 0.9008316993713379, val_loss: 0.8665229082107544\n",
            "Saving model, epoch: 10962, train_loss: 0.9008301496505737, val_loss: 0.8665221929550171\n",
            "Saving model, epoch: 10963, train_loss: 0.9008287787437439, val_loss: 0.8665214776992798\n",
            "Saving model, epoch: 10964, train_loss: 0.9008274674415588, val_loss: 0.8665210604667664\n",
            "Saving model, epoch: 10965, train_loss: 0.9008260369300842, val_loss: 0.86652010679245\n",
            "Saving model, epoch: 10966, train_loss: 0.9008246660232544, val_loss: 0.8665189146995544\n",
            "Saving model, epoch: 10967, train_loss: 0.9008234739303589, val_loss: 0.866517961025238\n",
            "Saving model, epoch: 10968, train_loss: 0.9008220434188843, val_loss: 0.8665174841880798\n",
            "Saving model, epoch: 10969, train_loss: 0.9008206129074097, val_loss: 0.8665171265602112\n",
            "Saving model, epoch: 10970, train_loss: 0.9008193016052246, val_loss: 0.8665160536766052\n",
            "Saving model, epoch: 10971, train_loss: 0.9008176326751709, val_loss: 0.8665152192115784\n",
            "Saving model, epoch: 10972, train_loss: 0.9008164405822754, val_loss: 0.8665145635604858\n",
            "Saving model, epoch: 10973, train_loss: 0.9008150100708008, val_loss: 0.8665136098861694\n",
            "Saving model, epoch: 10974, train_loss: 0.9008135795593262, val_loss: 0.8665132522583008\n",
            "Saving model, epoch: 10975, train_loss: 0.9008122086524963, val_loss: 0.8665120601654053\n",
            "Saving model, epoch: 10976, train_loss: 0.9008110165596008, val_loss: 0.8665119409561157\n",
            "Saving model, epoch: 10977, train_loss: 0.9008094668388367, val_loss: 0.8665109872817993\n",
            "Saving model, epoch: 10978, train_loss: 0.9008081555366516, val_loss: 0.8665101528167725\n",
            "Saving model, epoch: 10979, train_loss: 0.9008067846298218, val_loss: 0.8665097951889038\n",
            "Saving model, epoch: 10980, train_loss: 0.9008053541183472, val_loss: 0.866508424282074\n",
            "Saving model, epoch: 10981, train_loss: 0.9008039832115173, val_loss: 0.8665080666542053\n",
            "Saving model, epoch: 10982, train_loss: 0.900802731513977, val_loss: 0.8665072917938232\n",
            "Saving model, epoch: 10983, train_loss: 0.9008011221885681, val_loss: 0.8665062189102173\n",
            "Saving model, epoch: 10984, train_loss: 0.9007999300956726, val_loss: 0.8665059804916382\n",
            "Saving model, epoch: 10985, train_loss: 0.9007983207702637, val_loss: 0.8665049076080322\n",
            "Saving model, epoch: 10986, train_loss: 0.9007970094680786, val_loss: 0.8665043115615845\n",
            "Saving model, epoch: 10987, train_loss: 0.9007956385612488, val_loss: 0.8665035367012024\n",
            "Saving model, epoch: 10988, train_loss: 0.9007943272590637, val_loss: 0.8665024638175964\n",
            "Saving model, epoch: 10989, train_loss: 0.9007928967475891, val_loss: 0.8665014505386353\n",
            "Saving model, epoch: 10990, train_loss: 0.9007915258407593, val_loss: 0.8665012717247009\n",
            "Saving model, epoch: 10991, train_loss: 0.9007900953292847, val_loss: 0.8665002584457397\n",
            "Saving model, epoch: 10992, train_loss: 0.9007886648178101, val_loss: 0.8664993643760681\n",
            "Saving model, epoch: 10994, train_loss: 0.9007858633995056, val_loss: 0.8664978742599487\n",
            "Saving model, epoch: 10995, train_loss: 0.9007844924926758, val_loss: 0.8664970993995667\n",
            "Saving model, epoch: 10996, train_loss: 0.9007830619812012, val_loss: 0.8664963245391846\n",
            "Saving model, epoch: 10997, train_loss: 0.9007818698883057, val_loss: 0.8664959073066711\n",
            "Saving model, epoch: 10998, train_loss: 0.9007802605628967, val_loss: 0.8664947152137756\n",
            "Saving model, epoch: 10999, train_loss: 0.9007789492607117, val_loss: 0.8664939403533936\n",
            "Saving model, epoch: 11000, train_loss: 0.9007774591445923, val_loss: 0.8664933443069458\n",
            "epoch: 11001, train_loss: 0.9007760286331177, val_loss: 0.866492748260498\n",
            "Saving model, epoch: 11001, train_loss: 0.9007760286331177, val_loss: 0.866492748260498\n",
            "Saving model, epoch: 11002, train_loss: 0.9007745385169983, val_loss: 0.8664920330047607\n",
            "Saving model, epoch: 11003, train_loss: 0.900773286819458, val_loss: 0.8664911389350891\n",
            "Saving model, epoch: 11004, train_loss: 0.9007717967033386, val_loss: 0.8664898872375488\n",
            "Saving model, epoch: 11005, train_loss: 0.9007701873779297, val_loss: 0.8664894700050354\n",
            "Saving model, epoch: 11006, train_loss: 0.9007689952850342, val_loss: 0.8664883971214294\n",
            "Saving model, epoch: 11007, train_loss: 0.9007675647735596, val_loss: 0.8664878606796265\n",
            "Saving model, epoch: 11008, train_loss: 0.9007663726806641, val_loss: 0.866487443447113\n",
            "Saving model, epoch: 11009, train_loss: 0.9007647633552551, val_loss: 0.8664863705635071\n",
            "Saving model, epoch: 11010, train_loss: 0.9007634520530701, val_loss: 0.8664856553077698\n",
            "Saving model, epoch: 11011, train_loss: 0.9007619619369507, val_loss: 0.8664849400520325\n",
            "Saving model, epoch: 11012, train_loss: 0.9007607698440552, val_loss: 0.8664839863777161\n",
            "Saving model, epoch: 11013, train_loss: 0.900759220123291, val_loss: 0.8664836883544922\n",
            "Saving model, epoch: 11014, train_loss: 0.9007579684257507, val_loss: 0.8664827942848206\n",
            "Saving model, epoch: 11015, train_loss: 0.9007563591003418, val_loss: 0.8664816617965698\n",
            "Saving model, epoch: 11016, train_loss: 0.9007551074028015, val_loss: 0.866481363773346\n",
            "Saving model, epoch: 11017, train_loss: 0.9007536172866821, val_loss: 0.8664805293083191\n",
            "Saving model, epoch: 11018, train_loss: 0.9007523059844971, val_loss: 0.8664795160293579\n",
            "Saving model, epoch: 11019, train_loss: 0.9007508158683777, val_loss: 0.8664790391921997\n",
            "Saving model, epoch: 11020, train_loss: 0.9007493257522583, val_loss: 0.8664780855178833\n",
            "Saving model, epoch: 11021, train_loss: 0.9007478952407837, val_loss: 0.866477370262146\n",
            "Saving model, epoch: 11022, train_loss: 0.9007464051246643, val_loss: 0.8664767146110535\n",
            "Saving model, epoch: 11023, train_loss: 0.900745153427124, val_loss: 0.866475522518158\n",
            "Saving model, epoch: 11024, train_loss: 0.9007436633110046, val_loss: 0.8664749264717102\n",
            "Saving model, epoch: 11025, train_loss: 0.9007422924041748, val_loss: 0.8664746284484863\n",
            "Saving model, epoch: 11026, train_loss: 0.9007406830787659, val_loss: 0.8664731979370117\n",
            "Saving model, epoch: 11027, train_loss: 0.9007394313812256, val_loss: 0.866472601890564\n",
            "Saving model, epoch: 11028, train_loss: 0.9007380604743958, val_loss: 0.8664720058441162\n",
            "Saving model, epoch: 11029, train_loss: 0.9007366299629211, val_loss: 0.8664712905883789\n",
            "Saving model, epoch: 11030, train_loss: 0.9007352590560913, val_loss: 0.8664700984954834\n",
            "Saving model, epoch: 11031, train_loss: 0.9007337093353271, val_loss: 0.8664697408676147\n",
            "Saving model, epoch: 11032, train_loss: 0.9007323384284973, val_loss: 0.8664687871932983\n",
            "Saving model, epoch: 11033, train_loss: 0.9007307887077332, val_loss: 0.8664677143096924\n",
            "Saving model, epoch: 11034, train_loss: 0.9007294774055481, val_loss: 0.8664671182632446\n",
            "Saving model, epoch: 11035, train_loss: 0.9007281064987183, val_loss: 0.8664660453796387\n",
            "Saving model, epoch: 11036, train_loss: 0.9007267951965332, val_loss: 0.8664654493331909\n",
            "Saving model, epoch: 11037, train_loss: 0.9007253646850586, val_loss: 0.8664650321006775\n",
            "Saving model, epoch: 11038, train_loss: 0.9007237553596497, val_loss: 0.8664644956588745\n",
            "Saving model, epoch: 11039, train_loss: 0.9007225632667542, val_loss: 0.8664629459381104\n",
            "Saving model, epoch: 11040, train_loss: 0.9007208943367004, val_loss: 0.8664626479148865\n",
            "Saving model, epoch: 11041, train_loss: 0.9007196426391602, val_loss: 0.8664618730545044\n",
            "Saving model, epoch: 11042, train_loss: 0.9007181525230408, val_loss: 0.866460919380188\n",
            "Saving model, epoch: 11043, train_loss: 0.9007165431976318, val_loss: 0.8664600253105164\n",
            "Saving model, epoch: 11044, train_loss: 0.9007152318954468, val_loss: 0.8664593696594238\n",
            "Saving model, epoch: 11045, train_loss: 0.9007138609886169, val_loss: 0.8664584159851074\n",
            "Saving model, epoch: 11046, train_loss: 0.9007124304771423, val_loss: 0.8664577603340149\n",
            "Saving model, epoch: 11047, train_loss: 0.900710940361023, val_loss: 0.8664575815200806\n",
            "Saving model, epoch: 11048, train_loss: 0.9007095098495483, val_loss: 0.8664562702178955\n",
            "Saving model, epoch: 11049, train_loss: 0.9007080793380737, val_loss: 0.8664558529853821\n",
            "Saving model, epoch: 11050, train_loss: 0.9007067084312439, val_loss: 0.8664547801017761\n",
            "Saving model, epoch: 11051, train_loss: 0.9007052779197693, val_loss: 0.8664541244506836\n",
            "Saving model, epoch: 11052, train_loss: 0.9007037878036499, val_loss: 0.866453230381012\n",
            "Saving model, epoch: 11053, train_loss: 0.9007023572921753, val_loss: 0.866452693939209\n",
            "Saving model, epoch: 11054, train_loss: 0.9007008671760559, val_loss: 0.8664516806602478\n",
            "Saving model, epoch: 11055, train_loss: 0.9006994366645813, val_loss: 0.8664507269859314\n",
            "Saving model, epoch: 11056, train_loss: 0.9006980657577515, val_loss: 0.8664498329162598\n",
            "Saving model, epoch: 11057, train_loss: 0.9006967544555664, val_loss: 0.8664495348930359\n",
            "Saving model, epoch: 11058, train_loss: 0.900695264339447, val_loss: 0.8664484620094299\n",
            "Saving model, epoch: 11059, train_loss: 0.9006938338279724, val_loss: 0.8664476275444031\n",
            "Saving model, epoch: 11060, train_loss: 0.9006921052932739, val_loss: 0.8664464354515076\n",
            "Saving model, epoch: 11061, train_loss: 0.900691032409668, val_loss: 0.8664460182189941\n",
            "Saving model, epoch: 11062, train_loss: 0.9006895422935486, val_loss: 0.8664457201957703\n",
            "Saving model, epoch: 11063, train_loss: 0.900688111782074, val_loss: 0.8664447069168091\n",
            "Saving model, epoch: 11064, train_loss: 0.900686502456665, val_loss: 0.8664436340332031\n",
            "Saving model, epoch: 11065, train_loss: 0.9006850123405457, val_loss: 0.866442859172821\n",
            "Saving model, epoch: 11066, train_loss: 0.900683581829071, val_loss: 0.8664422035217285\n",
            "Saving model, epoch: 11067, train_loss: 0.9006823897361755, val_loss: 0.8664411306381226\n",
            "Saving model, epoch: 11068, train_loss: 0.9006807804107666, val_loss: 0.8664409518241882\n",
            "Saving model, epoch: 11069, train_loss: 0.900679349899292, val_loss: 0.8664397597312927\n",
            "Saving model, epoch: 11070, train_loss: 0.9006779789924622, val_loss: 0.8664389848709106\n",
            "Saving model, epoch: 11071, train_loss: 0.9006765484809875, val_loss: 0.866438090801239\n",
            "Saving model, epoch: 11072, train_loss: 0.9006749391555786, val_loss: 0.8664378523826599\n",
            "Saving model, epoch: 11073, train_loss: 0.9006735682487488, val_loss: 0.866436779499054\n",
            "Saving model, epoch: 11074, train_loss: 0.9006722569465637, val_loss: 0.8664357662200928\n",
            "Saving model, epoch: 11075, train_loss: 0.9006705284118652, val_loss: 0.8664350509643555\n",
            "Saving model, epoch: 11076, train_loss: 0.9006692171096802, val_loss: 0.8664342761039734\n",
            "Saving model, epoch: 11077, train_loss: 0.9006679058074951, val_loss: 0.866433322429657\n",
            "Saving model, epoch: 11078, train_loss: 0.9006664156913757, val_loss: 0.8664325475692749\n",
            "Saving model, epoch: 11079, train_loss: 0.9006649255752563, val_loss: 0.866432249546051\n",
            "Saving model, epoch: 11080, train_loss: 0.9006634950637817, val_loss: 0.8664311170578003\n",
            "Saving model, epoch: 11081, train_loss: 0.9006620645523071, val_loss: 0.8664302825927734\n",
            "Saving model, epoch: 11082, train_loss: 0.9006604552268982, val_loss: 0.86642986536026\n",
            "Saving model, epoch: 11083, train_loss: 0.9006592631340027, val_loss: 0.866428792476654\n",
            "Saving model, epoch: 11084, train_loss: 0.9006576538085938, val_loss: 0.8664277791976929\n",
            "Saving model, epoch: 11085, train_loss: 0.9006561636924744, val_loss: 0.8664270043373108\n",
            "Saving model, epoch: 11086, train_loss: 0.900654673576355, val_loss: 0.8664258122444153\n",
            "Saving model, epoch: 11087, train_loss: 0.9006533622741699, val_loss: 0.8664253950119019\n",
            "Saving model, epoch: 11088, train_loss: 0.9006518721580505, val_loss: 0.8664249181747437\n",
            "Saving model, epoch: 11089, train_loss: 0.9006503224372864, val_loss: 0.8664243817329407\n",
            "Saving model, epoch: 11090, train_loss: 0.9006490111351013, val_loss: 0.8664228320121765\n",
            "Saving model, epoch: 11091, train_loss: 0.9006475210189819, val_loss: 0.8664222955703735\n",
            "Saving model, epoch: 11092, train_loss: 0.9006460309028625, val_loss: 0.8664215803146362\n",
            "Saving model, epoch: 11093, train_loss: 0.9006446003913879, val_loss: 0.8664208054542542\n",
            "Saving model, epoch: 11094, train_loss: 0.900642991065979, val_loss: 0.8664202690124512\n",
            "Saving model, epoch: 11095, train_loss: 0.9006416201591492, val_loss: 0.8664190769195557\n",
            "Saving model, epoch: 11096, train_loss: 0.9006401896476746, val_loss: 0.8664184808731079\n",
            "Saving model, epoch: 11097, train_loss: 0.9006387591362, val_loss: 0.8664175271987915\n",
            "Saving model, epoch: 11098, train_loss: 0.9006373882293701, val_loss: 0.866416871547699\n",
            "Saving model, epoch: 11099, train_loss: 0.9006357789039612, val_loss: 0.8664158582687378\n",
            "Saving model, epoch: 11100, train_loss: 0.9006341695785522, val_loss: 0.8664153218269348\n",
            "epoch: 11101, train_loss: 0.9006327390670776, val_loss: 0.8664143681526184\n",
            "Saving model, epoch: 11101, train_loss: 0.9006327390670776, val_loss: 0.8664143681526184\n",
            "Saving model, epoch: 11102, train_loss: 0.9006313681602478, val_loss: 0.8664138317108154\n",
            "Saving model, epoch: 11103, train_loss: 0.9006298184394836, val_loss: 0.8664129972457886\n",
            "Saving model, epoch: 11104, train_loss: 0.9006283283233643, val_loss: 0.8664122819900513\n",
            "Saving model, epoch: 11105, train_loss: 0.9006268978118896, val_loss: 0.8664112091064453\n",
            "Saving model, epoch: 11106, train_loss: 0.9006255269050598, val_loss: 0.8664104342460632\n",
            "Saving model, epoch: 11107, train_loss: 0.9006240367889404, val_loss: 0.8664095997810364\n",
            "Saving model, epoch: 11108, train_loss: 0.9006226062774658, val_loss: 0.8664085865020752\n",
            "Saving model, epoch: 11109, train_loss: 0.9006212949752808, val_loss: 0.8664082884788513\n",
            "Saving model, epoch: 11110, train_loss: 0.9006196856498718, val_loss: 0.8664073944091797\n",
            "Saving model, epoch: 11111, train_loss: 0.9006181955337524, val_loss: 0.8664065003395081\n",
            "Saving model, epoch: 11112, train_loss: 0.9006167054176331, val_loss: 0.8664058446884155\n",
            "Saving model, epoch: 11113, train_loss: 0.9006150960922241, val_loss: 0.86640465259552\n",
            "Saving model, epoch: 11114, train_loss: 0.9006137847900391, val_loss: 0.866404116153717\n",
            "Saving model, epoch: 11115, train_loss: 0.9006121754646301, val_loss: 0.8664034605026245\n",
            "Saving model, epoch: 11116, train_loss: 0.9006108641624451, val_loss: 0.8664025664329529\n",
            "Saving model, epoch: 11117, train_loss: 0.9006092548370361, val_loss: 0.8664013147354126\n",
            "Saving model, epoch: 11118, train_loss: 0.9006078243255615, val_loss: 0.8664008378982544\n",
            "Saving model, epoch: 11119, train_loss: 0.9006064534187317, val_loss: 0.8664001226425171\n",
            "Saving model, epoch: 11120, train_loss: 0.9006049036979675, val_loss: 0.8663992285728455\n",
            "Saving model, epoch: 11121, train_loss: 0.9006034135818481, val_loss: 0.86639803647995\n",
            "Saving model, epoch: 11122, train_loss: 0.9006019234657288, val_loss: 0.8663976788520813\n",
            "Saving model, epoch: 11123, train_loss: 0.9006003737449646, val_loss: 0.8663965463638306\n",
            "Saving model, epoch: 11124, train_loss: 0.9005990028381348, val_loss: 0.8663955926895142\n",
            "Saving model, epoch: 11125, train_loss: 0.9005975127220154, val_loss: 0.8663952946662903\n",
            "Saving model, epoch: 11126, train_loss: 0.9005960822105408, val_loss: 0.8663946986198425\n",
            "Saving model, epoch: 11127, train_loss: 0.9005945920944214, val_loss: 0.8663935661315918\n",
            "Saving model, epoch: 11128, train_loss: 0.9005929827690125, val_loss: 0.8663929104804993\n",
            "Saving model, epoch: 11129, train_loss: 0.9005915522575378, val_loss: 0.8663917779922485\n",
            "Saving model, epoch: 11130, train_loss: 0.900590181350708, val_loss: 0.8663907647132874\n",
            "Saving model, epoch: 11131, train_loss: 0.9005885720252991, val_loss: 0.86639004945755\n",
            "Saving model, epoch: 11132, train_loss: 0.9005869626998901, val_loss: 0.8663890957832336\n",
            "Saving model, epoch: 11133, train_loss: 0.9005856513977051, val_loss: 0.8663883209228516\n",
            "Saving model, epoch: 11134, train_loss: 0.90058434009552, val_loss: 0.8663878440856934\n",
            "Saving model, epoch: 11135, train_loss: 0.9005827307701111, val_loss: 0.8663870692253113\n",
            "Saving model, epoch: 11136, train_loss: 0.9005811214447021, val_loss: 0.8663858771324158\n",
            "Saving model, epoch: 11137, train_loss: 0.9005796313285828, val_loss: 0.866385281085968\n",
            "Saving model, epoch: 11138, train_loss: 0.9005780816078186, val_loss: 0.866384744644165\n",
            "Saving model, epoch: 11139, train_loss: 0.9005767107009888, val_loss: 0.866383969783783\n",
            "Saving model, epoch: 11140, train_loss: 0.9005751013755798, val_loss: 0.8663829565048218\n",
            "Saving model, epoch: 11141, train_loss: 0.9005736708641052, val_loss: 0.8663819432258606\n",
            "Saving model, epoch: 11142, train_loss: 0.9005722999572754, val_loss: 0.8663812279701233\n",
            "Saving model, epoch: 11143, train_loss: 0.9005706906318665, val_loss: 0.8663803935050964\n",
            "Saving model, epoch: 11144, train_loss: 0.9005694389343262, val_loss: 0.8663793206214905\n",
            "Saving model, epoch: 11145, train_loss: 0.9005677700042725, val_loss: 0.8663792014122009\n",
            "Saving model, epoch: 11146, train_loss: 0.9005663394927979, val_loss: 0.8663778901100159\n",
            "Saving model, epoch: 11147, train_loss: 0.9005646109580994, val_loss: 0.8663769364356995\n",
            "Saving model, epoch: 11148, train_loss: 0.9005634188652039, val_loss: 0.8663761615753174\n",
            "Saving model, epoch: 11149, train_loss: 0.9005617499351501, val_loss: 0.8663756847381592\n",
            "Saving model, epoch: 11150, train_loss: 0.9005603194236755, val_loss: 0.8663749694824219\n",
            "Saving model, epoch: 11151, train_loss: 0.9005587100982666, val_loss: 0.8663736581802368\n",
            "Saving model, epoch: 11152, train_loss: 0.9005572199821472, val_loss: 0.8663732409477234\n",
            "Saving model, epoch: 11153, train_loss: 0.9005557894706726, val_loss: 0.8663724064826965\n",
            "Saving model, epoch: 11154, train_loss: 0.9005542993545532, val_loss: 0.8663713932037354\n",
            "Saving model, epoch: 11155, train_loss: 0.9005526900291443, val_loss: 0.8663707971572876\n",
            "Saving model, epoch: 11156, train_loss: 0.9005513787269592, val_loss: 0.8663697242736816\n",
            "Saving model, epoch: 11157, train_loss: 0.9005496501922607, val_loss: 0.86636883020401\n",
            "Saving model, epoch: 11158, train_loss: 0.9005481600761414, val_loss: 0.8663680553436279\n",
            "Saving model, epoch: 11159, train_loss: 0.900546669960022, val_loss: 0.8663670420646667\n",
            "Saving model, epoch: 11160, train_loss: 0.9005455374717712, val_loss: 0.8663666248321533\n",
            "Saving model, epoch: 11161, train_loss: 0.900543749332428, val_loss: 0.8663655519485474\n",
            "Saving model, epoch: 11162, train_loss: 0.9005422592163086, val_loss: 0.8663646578788757\n",
            "Saving model, epoch: 11163, train_loss: 0.900540828704834, val_loss: 0.8663639426231384\n",
            "Saving model, epoch: 11164, train_loss: 0.9005393981933594, val_loss: 0.866363525390625\n",
            "Saving model, epoch: 11165, train_loss: 0.9005377292633057, val_loss: 0.8663625121116638\n",
            "Saving model, epoch: 11166, train_loss: 0.900536298751831, val_loss: 0.8663615584373474\n",
            "Saving model, epoch: 11167, train_loss: 0.9005346894264221, val_loss: 0.8663609027862549\n",
            "Saving model, epoch: 11168, train_loss: 0.9005330801010132, val_loss: 0.8663602471351624\n",
            "Saving model, epoch: 11169, train_loss: 0.9005318880081177, val_loss: 0.8663595914840698\n",
            "Saving model, epoch: 11170, train_loss: 0.9005301594734192, val_loss: 0.8663583397865295\n",
            "Saving model, epoch: 11171, train_loss: 0.9005286693572998, val_loss: 0.8663574457168579\n",
            "Saving model, epoch: 11172, train_loss: 0.9005271792411804, val_loss: 0.8663572072982788\n",
            "Saving model, epoch: 11173, train_loss: 0.9005256295204163, val_loss: 0.8663558959960938\n",
            "Saving model, epoch: 11174, train_loss: 0.9005242586135864, val_loss: 0.8663550615310669\n",
            "Saving model, epoch: 11175, train_loss: 0.9005228281021118, val_loss: 0.8663541078567505\n",
            "Saving model, epoch: 11176, train_loss: 0.9005210399627686, val_loss: 0.8663532733917236\n",
            "Saving model, epoch: 11177, train_loss: 0.9005194306373596, val_loss: 0.8663524985313416\n",
            "Saving model, epoch: 11178, train_loss: 0.900518000125885, val_loss: 0.8663520216941833\n",
            "Saving model, epoch: 11179, train_loss: 0.9005166292190552, val_loss: 0.8663511872291565\n",
            "Saving model, epoch: 11180, train_loss: 0.9005150198936462, val_loss: 0.8663500547409058\n",
            "Saving model, epoch: 11181, train_loss: 0.9005135893821716, val_loss: 0.8663491010665894\n",
            "Saving model, epoch: 11182, train_loss: 0.9005119800567627, val_loss: 0.8663485050201416\n",
            "Saving model, epoch: 11183, train_loss: 0.9005103707313538, val_loss: 0.8663477897644043\n",
            "Saving model, epoch: 11184, train_loss: 0.9005089998245239, val_loss: 0.8663466572761536\n",
            "Saving model, epoch: 11185, train_loss: 0.900507390499115, val_loss: 0.8663461208343506\n",
            "Saving model, epoch: 11186, train_loss: 0.900505781173706, val_loss: 0.8663452863693237\n",
            "Saving model, epoch: 11187, train_loss: 0.9005043506622314, val_loss: 0.8663445115089417\n",
            "Saving model, epoch: 11188, train_loss: 0.9005028605461121, val_loss: 0.8663433194160461\n",
            "Saving model, epoch: 11189, train_loss: 0.9005014300346375, val_loss: 0.8663430213928223\n",
            "Saving model, epoch: 11190, train_loss: 0.9004999399185181, val_loss: 0.8663417100906372\n",
            "Saving model, epoch: 11191, train_loss: 0.9004984498023987, val_loss: 0.8663413524627686\n",
            "Saving model, epoch: 11192, train_loss: 0.9004969000816345, val_loss: 0.8663404583930969\n",
            "Saving model, epoch: 11193, train_loss: 0.9004952907562256, val_loss: 0.8663393259048462\n",
            "Saving model, epoch: 11194, train_loss: 0.9004936814308167, val_loss: 0.866338849067688\n",
            "Saving model, epoch: 11195, train_loss: 0.9004920721054077, val_loss: 0.8663373589515686\n",
            "Saving model, epoch: 11196, train_loss: 0.9004907011985779, val_loss: 0.8663367629051208\n",
            "Saving model, epoch: 11197, train_loss: 0.900489091873169, val_loss: 0.8663359880447388\n",
            "Saving model, epoch: 11198, train_loss: 0.9004876613616943, val_loss: 0.8663347959518433\n",
            "Saving model, epoch: 11199, train_loss: 0.9004860520362854, val_loss: 0.8663343191146851\n",
            "Saving model, epoch: 11200, train_loss: 0.900484561920166, val_loss: 0.8663329482078552\n",
            "epoch: 11201, train_loss: 0.9004830718040466, val_loss: 0.8663327097892761\n",
            "Saving model, epoch: 11201, train_loss: 0.9004830718040466, val_loss: 0.8663327097892761\n",
            "Saving model, epoch: 11202, train_loss: 0.9004815220832825, val_loss: 0.8663318157196045\n",
            "Saving model, epoch: 11203, train_loss: 0.9004800319671631, val_loss: 0.8663309216499329\n",
            "Saving model, epoch: 11204, train_loss: 0.9004784226417542, val_loss: 0.8663300275802612\n",
            "Saving model, epoch: 11205, train_loss: 0.9004768133163452, val_loss: 0.8663294315338135\n",
            "Saving model, epoch: 11206, train_loss: 0.9004754424095154, val_loss: 0.8663285970687866\n",
            "Saving model, epoch: 11207, train_loss: 0.9004738330841064, val_loss: 0.866327702999115\n",
            "Saving model, epoch: 11208, train_loss: 0.9004722237586975, val_loss: 0.8663265705108643\n",
            "Saving model, epoch: 11209, train_loss: 0.9004707932472229, val_loss: 0.866325855255127\n",
            "Saving model, epoch: 11210, train_loss: 0.900469183921814, val_loss: 0.8663250803947449\n",
            "Saving model, epoch: 11211, train_loss: 0.9004676938056946, val_loss: 0.8663244843482971\n",
            "Saving model, epoch: 11212, train_loss: 0.9004662036895752, val_loss: 0.8663234114646912\n",
            "Saving model, epoch: 11213, train_loss: 0.9004645943641663, val_loss: 0.8663225769996643\n",
            "Saving model, epoch: 11214, train_loss: 0.9004630446434021, val_loss: 0.8663219809532166\n",
            "Saving model, epoch: 11215, train_loss: 0.9004614353179932, val_loss: 0.8663208484649658\n",
            "Saving model, epoch: 11216, train_loss: 0.9004598259925842, val_loss: 0.8663198351860046\n",
            "Saving model, epoch: 11217, train_loss: 0.9004585146903992, val_loss: 0.866318941116333\n",
            "Saving model, epoch: 11218, train_loss: 0.9004569053649902, val_loss: 0.8663183450698853\n",
            "Saving model, epoch: 11219, train_loss: 0.900455117225647, val_loss: 0.8663175106048584\n",
            "Saving model, epoch: 11220, train_loss: 0.9004539251327515, val_loss: 0.866316556930542\n",
            "Saving model, epoch: 11221, train_loss: 0.9004523158073425, val_loss: 0.8663157820701599\n",
            "Saving model, epoch: 11222, train_loss: 0.9004508256912231, val_loss: 0.8663151264190674\n",
            "Saving model, epoch: 11223, train_loss: 0.900449275970459, val_loss: 0.8663138151168823\n",
            "Saving model, epoch: 11224, train_loss: 0.90044766664505, val_loss: 0.866313099861145\n",
            "Saving model, epoch: 11225, train_loss: 0.9004460573196411, val_loss: 0.8663127422332764\n",
            "Saving model, epoch: 11226, train_loss: 0.9004446864128113, val_loss: 0.8663119077682495\n",
            "Saving model, epoch: 11227, train_loss: 0.9004429578781128, val_loss: 0.866310715675354\n",
            "Saving model, epoch: 11228, train_loss: 0.9004414677619934, val_loss: 0.8663100004196167\n",
            "Saving model, epoch: 11229, train_loss: 0.9004398584365845, val_loss: 0.8663093447685242\n",
            "Saving model, epoch: 11230, train_loss: 0.9004384279251099, val_loss: 0.8663082122802734\n",
            "Saving model, epoch: 11231, train_loss: 0.9004368185997009, val_loss: 0.8663070797920227\n",
            "Saving model, epoch: 11232, train_loss: 0.9004351496696472, val_loss: 0.8663067817687988\n",
            "Saving model, epoch: 11233, train_loss: 0.9004335999488831, val_loss: 0.8663060665130615\n",
            "Saving model, epoch: 11234, train_loss: 0.9004319906234741, val_loss: 0.866304874420166\n",
            "Saving model, epoch: 11235, train_loss: 0.9004303812980652, val_loss: 0.8663040399551392\n",
            "Saving model, epoch: 11236, train_loss: 0.9004290103912354, val_loss: 0.8663033246994019\n",
            "Saving model, epoch: 11237, train_loss: 0.900427520275116, val_loss: 0.8663023710250854\n",
            "Saving model, epoch: 11238, train_loss: 0.900425910949707, val_loss: 0.8663015961647034\n",
            "Saving model, epoch: 11239, train_loss: 0.9004243612289429, val_loss: 0.8663008213043213\n",
            "Saving model, epoch: 11240, train_loss: 0.9004227519035339, val_loss: 0.8662997484207153\n",
            "Saving model, epoch: 11241, train_loss: 0.9004213809967041, val_loss: 0.866299033164978\n",
            "Saving model, epoch: 11242, train_loss: 0.9004195332527161, val_loss: 0.866298258304596\n",
            "Saving model, epoch: 11243, train_loss: 0.9004181623458862, val_loss: 0.86629718542099\n",
            "Saving model, epoch: 11244, train_loss: 0.9004164338111877, val_loss: 0.8662962317466736\n",
            "Saving model, epoch: 11245, train_loss: 0.9004149436950684, val_loss: 0.866295337677002\n",
            "Saving model, epoch: 11246, train_loss: 0.900413453578949, val_loss: 0.8662949204444885\n",
            "Saving model, epoch: 11247, train_loss: 0.9004119038581848, val_loss: 0.8662939667701721\n",
            "Saving model, epoch: 11248, train_loss: 0.9004102945327759, val_loss: 0.8662929534912109\n",
            "Saving model, epoch: 11249, train_loss: 0.9004088044166565, val_loss: 0.8662918210029602\n",
            "Saving model, epoch: 11250, train_loss: 0.900407075881958, val_loss: 0.8662909269332886\n",
            "Saving model, epoch: 11251, train_loss: 0.9004057049751282, val_loss: 0.8662906289100647\n",
            "Saving model, epoch: 11252, train_loss: 0.9004040956497192, val_loss: 0.8662890195846558\n",
            "Saving model, epoch: 11253, train_loss: 0.9004024863243103, val_loss: 0.8662885427474976\n",
            "Saving model, epoch: 11254, train_loss: 0.9004010558128357, val_loss: 0.8662875890731812\n",
            "Saving model, epoch: 11255, train_loss: 0.900399386882782, val_loss: 0.866287112236023\n",
            "Saving model, epoch: 11256, train_loss: 0.9003978371620178, val_loss: 0.8662862181663513\n",
            "Saving model, epoch: 11257, train_loss: 0.9003962278366089, val_loss: 0.8662850856781006\n",
            "Saving model, epoch: 11258, train_loss: 0.9003946185112, val_loss: 0.8662843704223633\n",
            "Saving model, epoch: 11259, train_loss: 0.900393009185791, val_loss: 0.8662838935852051\n",
            "Saving model, epoch: 11260, train_loss: 0.9003915190696716, val_loss: 0.86628258228302\n",
            "Saving model, epoch: 11261, train_loss: 0.9003899097442627, val_loss: 0.8662816286087036\n",
            "Saving model, epoch: 11262, train_loss: 0.9003883004188538, val_loss: 0.8662808537483215\n",
            "Saving model, epoch: 11263, train_loss: 0.9003866910934448, val_loss: 0.8662799596786499\n",
            "Saving model, epoch: 11264, train_loss: 0.9003852009773254, val_loss: 0.8662791848182678\n",
            "Saving model, epoch: 11265, train_loss: 0.9003835916519165, val_loss: 0.866278350353241\n",
            "Saving model, epoch: 11266, train_loss: 0.9003821611404419, val_loss: 0.8662774562835693\n",
            "Saving model, epoch: 11267, train_loss: 0.9003804922103882, val_loss: 0.866276204586029\n",
            "Saving model, epoch: 11268, train_loss: 0.9003788828849792, val_loss: 0.8662754893302917\n",
            "Saving model, epoch: 11269, train_loss: 0.9003772735595703, val_loss: 0.8662750124931335\n",
            "Saving model, epoch: 11270, train_loss: 0.9003755450248718, val_loss: 0.8662744164466858\n",
            "Saving model, epoch: 11271, train_loss: 0.900374174118042, val_loss: 0.8662735223770142\n",
            "Saving model, epoch: 11272, train_loss: 0.9003725647926331, val_loss: 0.8662725687026978\n",
            "Saving model, epoch: 11273, train_loss: 0.9003709554672241, val_loss: 0.8662713170051575\n",
            "Saving model, epoch: 11274, train_loss: 0.9003695249557495, val_loss: 0.8662706017494202\n",
            "Saving model, epoch: 11275, train_loss: 0.900367796421051, val_loss: 0.8662699460983276\n",
            "Saving model, epoch: 11276, train_loss: 0.9003663063049316, val_loss: 0.8662692308425903\n",
            "Saving model, epoch: 11277, train_loss: 0.9003646969795227, val_loss: 0.8662679195404053\n",
            "Saving model, epoch: 11278, train_loss: 0.9003630876541138, val_loss: 0.8662674427032471\n",
            "Saving model, epoch: 11279, train_loss: 0.9003613591194153, val_loss: 0.8662665486335754\n",
            "Saving model, epoch: 11280, train_loss: 0.9003598690032959, val_loss: 0.8662654757499695\n",
            "Saving model, epoch: 11281, train_loss: 0.9003583788871765, val_loss: 0.8662646412849426\n",
            "Saving model, epoch: 11282, train_loss: 0.9003568887710571, val_loss: 0.866263747215271\n",
            "Saving model, epoch: 11283, train_loss: 0.9003552794456482, val_loss: 0.8662630915641785\n",
            "Saving model, epoch: 11284, train_loss: 0.9003534317016602, val_loss: 0.8662620186805725\n",
            "Saving model, epoch: 11285, train_loss: 0.9003518223762512, val_loss: 0.8662613034248352\n",
            "Saving model, epoch: 11286, train_loss: 0.9003504514694214, val_loss: 0.8662606477737427\n",
            "Saving model, epoch: 11287, train_loss: 0.9003486037254333, val_loss: 0.866259753704071\n",
            "Saving model, epoch: 11288, train_loss: 0.9003472924232483, val_loss: 0.8662586808204651\n",
            "Saving model, epoch: 11289, train_loss: 0.9003456234931946, val_loss: 0.8662577867507935\n",
            "Saving model, epoch: 11290, train_loss: 0.9003438949584961, val_loss: 0.8662567138671875\n",
            "Saving model, epoch: 11291, train_loss: 0.9003425240516663, val_loss: 0.8662558794021606\n",
            "Saving model, epoch: 11292, train_loss: 0.9003407955169678, val_loss: 0.8662552237510681\n",
            "Saving model, epoch: 11293, train_loss: 0.9003391861915588, val_loss: 0.8662539720535278\n",
            "Saving model, epoch: 11294, train_loss: 0.9003375768661499, val_loss: 0.8662529587745667\n",
            "Saving model, epoch: 11295, train_loss: 0.900335967540741, val_loss: 0.8662521839141846\n",
            "Saving model, epoch: 11296, train_loss: 0.9003344774246216, val_loss: 0.866251528263092\n",
            "Saving model, epoch: 11297, train_loss: 0.9003327488899231, val_loss: 0.8662508726119995\n",
            "Saving model, epoch: 11298, train_loss: 0.9003311395645142, val_loss: 0.8662499189376831\n",
            "Saving model, epoch: 11299, train_loss: 0.9003295302391052, val_loss: 0.866249144077301\n",
            "Saving model, epoch: 11300, train_loss: 0.9003279209136963, val_loss: 0.866247832775116\n",
            "epoch: 11301, train_loss: 0.9003264307975769, val_loss: 0.8662471175193787\n",
            "Saving model, epoch: 11301, train_loss: 0.9003264307975769, val_loss: 0.8662471175193787\n",
            "Saving model, epoch: 11302, train_loss: 0.9003249406814575, val_loss: 0.8662461638450623\n",
            "Saving model, epoch: 11303, train_loss: 0.9003233313560486, val_loss: 0.8662456274032593\n",
            "Saving model, epoch: 11304, train_loss: 0.9003217220306396, val_loss: 0.8662440180778503\n",
            "Saving model, epoch: 11305, train_loss: 0.9003199934959412, val_loss: 0.8662434816360474\n",
            "Saving model, epoch: 11306, train_loss: 0.9003182649612427, val_loss: 0.8662427663803101\n",
            "Saving model, epoch: 11307, train_loss: 0.9003167748451233, val_loss: 0.8662418127059937\n",
            "Saving model, epoch: 11308, train_loss: 0.9003152847290039, val_loss: 0.8662407398223877\n",
            "Saving model, epoch: 11309, train_loss: 0.9003137350082397, val_loss: 0.8662404417991638\n",
            "Saving model, epoch: 11310, train_loss: 0.9003118276596069, val_loss: 0.8662394285202026\n",
            "Saving model, epoch: 11311, train_loss: 0.900310218334198, val_loss: 0.8662387132644653\n",
            "Saving model, epoch: 11312, train_loss: 0.9003088474273682, val_loss: 0.866237461566925\n",
            "Saving model, epoch: 11313, train_loss: 0.9003072381019592, val_loss: 0.8662365078926086\n",
            "Saving model, epoch: 11314, train_loss: 0.9003055095672607, val_loss: 0.8662357926368713\n",
            "Saving model, epoch: 11315, train_loss: 0.9003039002418518, val_loss: 0.8662346601486206\n",
            "Saving model, epoch: 11316, train_loss: 0.9003022909164429, val_loss: 0.8662338256835938\n",
            "Saving model, epoch: 11317, train_loss: 0.9003008008003235, val_loss: 0.8662334084510803\n",
            "Saving model, epoch: 11318, train_loss: 0.9002991914749146, val_loss: 0.8662318587303162\n",
            "Saving model, epoch: 11319, train_loss: 0.9002974033355713, val_loss: 0.8662310242652893\n",
            "Saving model, epoch: 11320, train_loss: 0.9002959728240967, val_loss: 0.8662302494049072\n",
            "Saving model, epoch: 11321, train_loss: 0.9002942442893982, val_loss: 0.8662292957305908\n",
            "Saving model, epoch: 11322, train_loss: 0.9002925753593445, val_loss: 0.8662287592887878\n",
            "Saving model, epoch: 11323, train_loss: 0.9002909660339355, val_loss: 0.8662281036376953\n",
            "Saving model, epoch: 11324, train_loss: 0.9002893567085266, val_loss: 0.8662273287773132\n",
            "Saving model, epoch: 11325, train_loss: 0.9002878665924072, val_loss: 0.8662261962890625\n",
            "Saving model, epoch: 11326, train_loss: 0.9002861380577087, val_loss: 0.8662254810333252\n",
            "Saving model, epoch: 11327, train_loss: 0.9002845287322998, val_loss: 0.8662242889404297\n",
            "Saving model, epoch: 11328, train_loss: 0.9002829194068909, val_loss: 0.8662236332893372\n",
            "Saving model, epoch: 11329, train_loss: 0.9002813100814819, val_loss: 0.866222620010376\n",
            "Saving model, epoch: 11330, train_loss: 0.900279700756073, val_loss: 0.8662219047546387\n",
            "Saving model, epoch: 11331, train_loss: 0.9002779722213745, val_loss: 0.8662208914756775\n",
            "Saving model, epoch: 11332, train_loss: 0.9002763032913208, val_loss: 0.8662196397781372\n",
            "Saving model, epoch: 11333, train_loss: 0.9002748727798462, val_loss: 0.8662189245223999\n",
            "Saving model, epoch: 11334, train_loss: 0.9002732634544373, val_loss: 0.866218090057373\n",
            "Saving model, epoch: 11335, train_loss: 0.9002715945243835, val_loss: 0.8662174940109253\n",
            "Saving model, epoch: 11336, train_loss: 0.9002698659896851, val_loss: 0.8662165403366089\n",
            "Saving model, epoch: 11337, train_loss: 0.9002684354782104, val_loss: 0.866215169429779\n",
            "Saving model, epoch: 11338, train_loss: 0.9002666473388672, val_loss: 0.8662146329879761\n",
            "Saving model, epoch: 11339, train_loss: 0.9002650380134583, val_loss: 0.8662137985229492\n",
            "Saving model, epoch: 11340, train_loss: 0.9002634286880493, val_loss: 0.8662126064300537\n",
            "Saving model, epoch: 11341, train_loss: 0.9002618193626404, val_loss: 0.8662120699882507\n",
            "Saving model, epoch: 11342, train_loss: 0.9002602100372314, val_loss: 0.8662110567092896\n",
            "Saving model, epoch: 11343, train_loss: 0.9002586007118225, val_loss: 0.8662098050117493\n",
            "Saving model, epoch: 11344, train_loss: 0.900256872177124, val_loss: 0.8662087917327881\n",
            "Saving model, epoch: 11345, train_loss: 0.9002552032470703, val_loss: 0.8662084341049194\n",
            "Saving model, epoch: 11346, train_loss: 0.9002537131309509, val_loss: 0.866207480430603\n",
            "Saving model, epoch: 11347, train_loss: 0.9002518653869629, val_loss: 0.8662065267562866\n",
            "Saving model, epoch: 11348, train_loss: 0.9002503752708435, val_loss: 0.8662055730819702\n",
            "Saving model, epoch: 11349, train_loss: 0.900248646736145, val_loss: 0.8662042021751404\n",
            "Saving model, epoch: 11350, train_loss: 0.9002471566200256, val_loss: 0.8662035465240479\n",
            "Saving model, epoch: 11351, train_loss: 0.9002453684806824, val_loss: 0.8662028908729553\n",
            "Saving model, epoch: 11352, train_loss: 0.9002438187599182, val_loss: 0.8662019968032837\n",
            "Saving model, epoch: 11353, train_loss: 0.9002424478530884, val_loss: 0.8662014603614807\n",
            "Saving model, epoch: 11354, train_loss: 0.9002405405044556, val_loss: 0.8662000298500061\n",
            "Saving model, epoch: 11355, train_loss: 0.9002389311790466, val_loss: 0.8661994934082031\n",
            "Saving model, epoch: 11356, train_loss: 0.9002372026443481, val_loss: 0.8661985993385315\n",
            "Saving model, epoch: 11357, train_loss: 0.9002357125282288, val_loss: 0.8661974668502808\n",
            "Saving model, epoch: 11358, train_loss: 0.9002341032028198, val_loss: 0.8661969304084778\n",
            "Saving model, epoch: 11359, train_loss: 0.9002322554588318, val_loss: 0.8661959171295166\n",
            "Saving model, epoch: 11360, train_loss: 0.9002307653427124, val_loss: 0.8661951422691345\n",
            "Saving model, epoch: 11361, train_loss: 0.900229275226593, val_loss: 0.8661938309669495\n",
            "Saving model, epoch: 11362, train_loss: 0.9002273678779602, val_loss: 0.8661929368972778\n",
            "Saving model, epoch: 11363, train_loss: 0.9002257585525513, val_loss: 0.8661919236183167\n",
            "Saving model, epoch: 11364, train_loss: 0.9002240300178528, val_loss: 0.8661913871765137\n",
            "Saving model, epoch: 11365, train_loss: 0.9002223610877991, val_loss: 0.8661903738975525\n",
            "Saving model, epoch: 11366, train_loss: 0.9002208113670349, val_loss: 0.8661895394325256\n",
            "Saving model, epoch: 11367, train_loss: 0.900219202041626, val_loss: 0.866188645362854\n",
            "Saving model, epoch: 11368, train_loss: 0.9002175331115723, val_loss: 0.8661878705024719\n",
            "Saving model, epoch: 11369, train_loss: 0.9002158045768738, val_loss: 0.8661867380142212\n",
            "Saving model, epoch: 11370, train_loss: 0.9002141952514648, val_loss: 0.86618572473526\n",
            "Saving model, epoch: 11371, train_loss: 0.9002125859260559, val_loss: 0.8661848306655884\n",
            "Saving model, epoch: 11372, train_loss: 0.9002108573913574, val_loss: 0.8661839365959167\n",
            "Saving model, epoch: 11373, train_loss: 0.9002092480659485, val_loss: 0.866182804107666\n",
            "Saving model, epoch: 11374, train_loss: 0.9002077579498291, val_loss: 0.8661823272705078\n",
            "Saving model, epoch: 11375, train_loss: 0.9002059698104858, val_loss: 0.8661816716194153\n",
            "Saving model, epoch: 11376, train_loss: 0.9002041816711426, val_loss: 0.8661805987358093\n",
            "Saving model, epoch: 11377, train_loss: 0.9002026319503784, val_loss: 0.8661797046661377\n",
            "Saving model, epoch: 11378, train_loss: 0.900201141834259, val_loss: 0.8661785125732422\n",
            "Saving model, epoch: 11379, train_loss: 0.9001993536949158, val_loss: 0.8661770820617676\n",
            "Saving model, epoch: 11380, train_loss: 0.9001978039741516, val_loss: 0.8661765456199646\n",
            "Saving model, epoch: 11381, train_loss: 0.9001958966255188, val_loss: 0.8661763072013855\n",
            "Saving model, epoch: 11382, train_loss: 0.9001942873001099, val_loss: 0.8661748766899109\n",
            "Saving model, epoch: 11383, train_loss: 0.9001926183700562, val_loss: 0.8661741614341736\n",
            "Saving model, epoch: 11384, train_loss: 0.9001910090446472, val_loss: 0.866173267364502\n",
            "Saving model, epoch: 11385, train_loss: 0.9001893997192383, val_loss: 0.8661720752716064\n",
            "Saving model, epoch: 11386, train_loss: 0.9001876711845398, val_loss: 0.8661714792251587\n",
            "Saving model, epoch: 11387, train_loss: 0.9001858830451965, val_loss: 0.8661704659461975\n",
            "Saving model, epoch: 11388, train_loss: 0.9001844525337219, val_loss: 0.8661694526672363\n",
            "Saving model, epoch: 11389, train_loss: 0.9001827239990234, val_loss: 0.8661684393882751\n",
            "Saving model, epoch: 11390, train_loss: 0.9001810550689697, val_loss: 0.8661677241325378\n",
            "Saving model, epoch: 11391, train_loss: 0.9001794457435608, val_loss: 0.8661667108535767\n",
            "Saving model, epoch: 11392, train_loss: 0.9001778364181519, val_loss: 0.8661662936210632\n",
            "Saving model, epoch: 11393, train_loss: 0.9001760482788086, val_loss: 0.8661653399467468\n",
            "Saving model, epoch: 11394, train_loss: 0.9001744389533997, val_loss: 0.8661640286445618\n",
            "Saving model, epoch: 11395, train_loss: 0.9001725912094116, val_loss: 0.8661631941795349\n",
            "Saving model, epoch: 11396, train_loss: 0.9001711010932922, val_loss: 0.8661621809005737\n",
            "Saving model, epoch: 11397, train_loss: 0.900169312953949, val_loss: 0.8661617636680603\n",
            "Saving model, epoch: 11398, train_loss: 0.9001677632331848, val_loss: 0.86616051197052\n",
            "Saving model, epoch: 11399, train_loss: 0.9001659750938416, val_loss: 0.8661598563194275\n",
            "Saving model, epoch: 11400, train_loss: 0.9001643657684326, val_loss: 0.8661587834358215\n",
            "epoch: 11401, train_loss: 0.9001627564430237, val_loss: 0.8661577701568604\n",
            "Saving model, epoch: 11401, train_loss: 0.9001627564430237, val_loss: 0.8661577701568604\n",
            "Saving model, epoch: 11402, train_loss: 0.9001608490943909, val_loss: 0.8661564588546753\n",
            "Saving model, epoch: 11403, train_loss: 0.9001593589782715, val_loss: 0.866155743598938\n",
            "Saving model, epoch: 11404, train_loss: 0.9001575112342834, val_loss: 0.8661548495292664\n",
            "Saving model, epoch: 11405, train_loss: 0.9001559019088745, val_loss: 0.8661538362503052\n",
            "Saving model, epoch: 11406, train_loss: 0.9001542329788208, val_loss: 0.8661534190177917\n",
            "Saving model, epoch: 11407, train_loss: 0.9001526832580566, val_loss: 0.8661527037620544\n",
            "Saving model, epoch: 11408, train_loss: 0.9001510143280029, val_loss: 0.8661514520645142\n",
            "Saving model, epoch: 11409, train_loss: 0.9001491665840149, val_loss: 0.8661501407623291\n",
            "Saving model, epoch: 11410, train_loss: 0.9001476168632507, val_loss: 0.8661493062973022\n",
            "Saving model, epoch: 11411, train_loss: 0.9001460075378418, val_loss: 0.8661486506462097\n",
            "Saving model, epoch: 11412, train_loss: 0.9001442790031433, val_loss: 0.8661478757858276\n",
            "Saving model, epoch: 11413, train_loss: 0.9001425504684448, val_loss: 0.8661468625068665\n",
            "Saving model, epoch: 11414, train_loss: 0.9001407623291016, val_loss: 0.8661459684371948\n",
            "Saving model, epoch: 11415, train_loss: 0.9001392722129822, val_loss: 0.8661453127861023\n",
            "Saving model, epoch: 11416, train_loss: 0.9001374244689941, val_loss: 0.8661441206932068\n",
            "Saving model, epoch: 11417, train_loss: 0.9001360535621643, val_loss: 0.8661433458328247\n",
            "Saving model, epoch: 11418, train_loss: 0.9001340270042419, val_loss: 0.8661420941352844\n",
            "Saving model, epoch: 11419, train_loss: 0.900132417678833, val_loss: 0.8661414980888367\n",
            "Saving model, epoch: 11420, train_loss: 0.9001308083534241, val_loss: 0.8661407232284546\n",
            "Saving model, epoch: 11421, train_loss: 0.9001291990280151, val_loss: 0.8661393523216248\n",
            "Saving model, epoch: 11422, train_loss: 0.9001274108886719, val_loss: 0.8661386370658875\n",
            "Saving model, epoch: 11423, train_loss: 0.9001255631446838, val_loss: 0.8661379218101501\n",
            "Saving model, epoch: 11424, train_loss: 0.9001239538192749, val_loss: 0.8661359548568726\n",
            "Saving model, epoch: 11425, train_loss: 0.9001221656799316, val_loss: 0.8661355376243591\n",
            "Saving model, epoch: 11426, train_loss: 0.9001205563545227, val_loss: 0.8661348223686218\n",
            "Saving model, epoch: 11427, train_loss: 0.9001188278198242, val_loss: 0.8661338090896606\n",
            "Saving model, epoch: 11428, train_loss: 0.9001173377037048, val_loss: 0.866132915019989\n",
            "Saving model, epoch: 11429, train_loss: 0.9001156091690063, val_loss: 0.8661323189735413\n",
            "Saving model, epoch: 11430, train_loss: 0.9001139402389526, val_loss: 0.8661309480667114\n",
            "Saving model, epoch: 11431, train_loss: 0.9001120924949646, val_loss: 0.8661301136016846\n",
            "Saving model, epoch: 11432, train_loss: 0.9001103043556213, val_loss: 0.8661293983459473\n",
            "Saving model, epoch: 11433, train_loss: 0.9001089334487915, val_loss: 0.8661283254623413\n",
            "Saving model, epoch: 11434, train_loss: 0.9001070857048035, val_loss: 0.8661274313926697\n",
            "Saving model, epoch: 11435, train_loss: 0.9001054763793945, val_loss: 0.8661268353462219\n",
            "Saving model, epoch: 11436, train_loss: 0.9001036882400513, val_loss: 0.8661259412765503\n",
            "Saving model, epoch: 11437, train_loss: 0.900101900100708, val_loss: 0.8661245703697205\n",
            "Saving model, epoch: 11438, train_loss: 0.9001002907752991, val_loss: 0.8661234378814697\n",
            "Saving model, epoch: 11439, train_loss: 0.9000985622406006, val_loss: 0.8661227822303772\n",
            "Saving model, epoch: 11440, train_loss: 0.9000967741012573, val_loss: 0.866121768951416\n",
            "Saving model, epoch: 11441, train_loss: 0.9000952243804932, val_loss: 0.8661211729049683\n",
            "Saving model, epoch: 11442, train_loss: 0.9000934362411499, val_loss: 0.8661200404167175\n",
            "Saving model, epoch: 11443, train_loss: 0.9000917077064514, val_loss: 0.866119384765625\n",
            "Saving model, epoch: 11444, train_loss: 0.9000900983810425, val_loss: 0.8661180138587952\n",
            "Saving model, epoch: 11445, train_loss: 0.9000884294509888, val_loss: 0.866117000579834\n",
            "Saving model, epoch: 11446, train_loss: 0.9000865817070007, val_loss: 0.8661165237426758\n",
            "Saving model, epoch: 11447, train_loss: 0.9000847935676575, val_loss: 0.866115152835846\n",
            "Saving model, epoch: 11448, train_loss: 0.9000831842422485, val_loss: 0.8661143779754639\n",
            "Saving model, epoch: 11449, train_loss: 0.9000816941261292, val_loss: 0.8661136031150818\n",
            "Saving model, epoch: 11450, train_loss: 0.9000797867774963, val_loss: 0.866112470626831\n",
            "Saving model, epoch: 11451, train_loss: 0.9000779390335083, val_loss: 0.866111695766449\n",
            "Saving model, epoch: 11452, train_loss: 0.9000763297080994, val_loss: 0.8661107420921326\n",
            "Saving model, epoch: 11453, train_loss: 0.9000746607780457, val_loss: 0.8661098480224609\n",
            "Saving model, epoch: 11454, train_loss: 0.9000729322433472, val_loss: 0.8661088347434998\n",
            "Saving model, epoch: 11455, train_loss: 0.9000713229179382, val_loss: 0.8661078810691833\n",
            "Saving model, epoch: 11456, train_loss: 0.900069534778595, val_loss: 0.866107165813446\n",
            "Saving model, epoch: 11457, train_loss: 0.9000678062438965, val_loss: 0.8661062717437744\n",
            "Saving model, epoch: 11458, train_loss: 0.900066077709198, val_loss: 0.866105318069458\n",
            "Saving model, epoch: 11459, train_loss: 0.9000645279884338, val_loss: 0.8661041259765625\n",
            "Saving model, epoch: 11460, train_loss: 0.9000626802444458, val_loss: 0.86610347032547\n",
            "Saving model, epoch: 11461, train_loss: 0.9000608921051025, val_loss: 0.8661025166511536\n",
            "Saving model, epoch: 11462, train_loss: 0.9000591039657593, val_loss: 0.8661014437675476\n",
            "Saving model, epoch: 11463, train_loss: 0.9000574946403503, val_loss: 0.8661008477210999\n",
            "Saving model, epoch: 11464, train_loss: 0.9000558853149414, val_loss: 0.8660995364189148\n",
            "Saving model, epoch: 11465, train_loss: 0.9000540375709534, val_loss: 0.8660987019538879\n",
            "Saving model, epoch: 11466, train_loss: 0.9000524282455444, val_loss: 0.8660973906517029\n",
            "Saving model, epoch: 11467, train_loss: 0.9000506401062012, val_loss: 0.8660966753959656\n",
            "Saving model, epoch: 11468, train_loss: 0.9000488519668579, val_loss: 0.8660956621170044\n",
            "Saving model, epoch: 11469, train_loss: 0.900047242641449, val_loss: 0.8660950660705566\n",
            "Saving model, epoch: 11470, train_loss: 0.9000453948974609, val_loss: 0.8660939931869507\n",
            "Saving model, epoch: 11471, train_loss: 0.900043785572052, val_loss: 0.8660930395126343\n",
            "Saving model, epoch: 11472, train_loss: 0.9000419974327087, val_loss: 0.8660920262336731\n",
            "Saving model, epoch: 11473, train_loss: 0.9000403881072998, val_loss: 0.8660914301872253\n",
            "Saving model, epoch: 11474, train_loss: 0.9000385999679565, val_loss: 0.8660899996757507\n",
            "Saving model, epoch: 11475, train_loss: 0.9000368714332581, val_loss: 0.8660889267921448\n",
            "Saving model, epoch: 11476, train_loss: 0.9000351428985596, val_loss: 0.8660881519317627\n",
            "Saving model, epoch: 11477, train_loss: 0.9000334739685059, val_loss: 0.8660878539085388\n",
            "Saving model, epoch: 11478, train_loss: 0.9000316262245178, val_loss: 0.8660866022109985\n",
            "Saving model, epoch: 11479, train_loss: 0.9000299572944641, val_loss: 0.8660852313041687\n",
            "Saving model, epoch: 11480, train_loss: 0.9000281095504761, val_loss: 0.8660843968391418\n",
            "Saving model, epoch: 11481, train_loss: 0.9000265002250671, val_loss: 0.866083562374115\n",
            "Saving model, epoch: 11482, train_loss: 0.9000247120857239, val_loss: 0.8660823106765747\n",
            "Saving model, epoch: 11483, train_loss: 0.9000229239463806, val_loss: 0.8660815954208374\n",
            "Saving model, epoch: 11484, train_loss: 0.9000211954116821, val_loss: 0.8660808205604553\n",
            "Saving model, epoch: 11485, train_loss: 0.9000194668769836, val_loss: 0.8660799264907837\n",
            "Saving model, epoch: 11486, train_loss: 0.9000178575515747, val_loss: 0.866078794002533\n",
            "Saving model, epoch: 11487, train_loss: 0.9000160694122314, val_loss: 0.8660775423049927\n",
            "Saving model, epoch: 11488, train_loss: 0.9000142812728882, val_loss: 0.8660764694213867\n",
            "Saving model, epoch: 11489, train_loss: 0.9000124335289001, val_loss: 0.8660759329795837\n",
            "Saving model, epoch: 11490, train_loss: 0.9000107645988464, val_loss: 0.8660755157470703\n",
            "Saving model, epoch: 11491, train_loss: 0.900009036064148, val_loss: 0.8660743832588196\n",
            "Saving model, epoch: 11492, train_loss: 0.9000073075294495, val_loss: 0.8660733103752136\n",
            "Saving model, epoch: 11493, train_loss: 0.9000056385993958, val_loss: 0.8660715818405151\n",
            "Saving model, epoch: 11494, train_loss: 0.9000039100646973, val_loss: 0.8660712242126465\n",
            "Saving model, epoch: 11495, train_loss: 0.900002121925354, val_loss: 0.8660703301429749\n",
            "Saving model, epoch: 11496, train_loss: 0.900000274181366, val_loss: 0.8660691976547241\n",
            "Saving model, epoch: 11497, train_loss: 0.899998664855957, val_loss: 0.8660681843757629\n",
            "Saving model, epoch: 11498, train_loss: 0.8999968767166138, val_loss: 0.8660677075386047\n",
            "Saving model, epoch: 11499, train_loss: 0.8999951481819153, val_loss: 0.8660663366317749\n",
            "Saving model, epoch: 11500, train_loss: 0.8999934792518616, val_loss: 0.8660656809806824\n",
            "epoch: 11501, train_loss: 0.8999917507171631, val_loss: 0.8660641312599182\n",
            "Saving model, epoch: 11501, train_loss: 0.8999917507171631, val_loss: 0.8660641312599182\n",
            "Saving model, epoch: 11502, train_loss: 0.8999899625778198, val_loss: 0.8660638928413391\n",
            "Saving model, epoch: 11503, train_loss: 0.8999881148338318, val_loss: 0.8660628199577332\n",
            "Saving model, epoch: 11504, train_loss: 0.8999863266944885, val_loss: 0.8660615682601929\n",
            "Saving model, epoch: 11505, train_loss: 0.89998459815979, val_loss: 0.8660608530044556\n",
            "Saving model, epoch: 11506, train_loss: 0.8999829292297363, val_loss: 0.8660597801208496\n",
            "Saving model, epoch: 11507, train_loss: 0.8999810814857483, val_loss: 0.8660586476325989\n",
            "Saving model, epoch: 11508, train_loss: 0.8999794125556946, val_loss: 0.8660575151443481\n",
            "Saving model, epoch: 11509, train_loss: 0.8999776840209961, val_loss: 0.8660566210746765\n",
            "Saving model, epoch: 11510, train_loss: 0.8999758958816528, val_loss: 0.8660557270050049\n",
            "Saving model, epoch: 11511, train_loss: 0.8999741673469543, val_loss: 0.866054892539978\n",
            "Saving model, epoch: 11512, train_loss: 0.8999723792076111, val_loss: 0.8660545349121094\n",
            "Saving model, epoch: 11513, train_loss: 0.8999704718589783, val_loss: 0.8660528063774109\n",
            "Saving model, epoch: 11514, train_loss: 0.8999687433242798, val_loss: 0.8660520315170288\n",
            "Saving model, epoch: 11515, train_loss: 0.8999670147895813, val_loss: 0.8660512566566467\n",
            "Saving model, epoch: 11516, train_loss: 0.8999653458595276, val_loss: 0.8660499453544617\n",
            "Saving model, epoch: 11517, train_loss: 0.8999636173248291, val_loss: 0.8660492897033691\n",
            "Saving model, epoch: 11518, train_loss: 0.8999618291854858, val_loss: 0.8660488128662109\n",
            "Saving model, epoch: 11519, train_loss: 0.8999600410461426, val_loss: 0.8660474419593811\n",
            "Saving model, epoch: 11520, train_loss: 0.8999581933021545, val_loss: 0.8660465478897095\n",
            "Saving model, epoch: 11521, train_loss: 0.8999565243721008, val_loss: 0.8660454154014587\n",
            "Saving model, epoch: 11522, train_loss: 0.8999546766281128, val_loss: 0.8660446405410767\n",
            "Saving model, epoch: 11523, train_loss: 0.8999530076980591, val_loss: 0.8660433888435364\n",
            "Saving model, epoch: 11524, train_loss: 0.899951159954071, val_loss: 0.8660422563552856\n",
            "Saving model, epoch: 11525, train_loss: 0.8999493718147278, val_loss: 0.8660417199134827\n",
            "Saving model, epoch: 11526, train_loss: 0.8999477624893188, val_loss: 0.8660409450531006\n",
            "Saving model, epoch: 11527, train_loss: 0.8999459743499756, val_loss: 0.8660399317741394\n",
            "Saving model, epoch: 11528, train_loss: 0.8999441266059875, val_loss: 0.8660389184951782\n",
            "Saving model, epoch: 11529, train_loss: 0.8999424576759338, val_loss: 0.8660377860069275\n",
            "Saving model, epoch: 11530, train_loss: 0.8999406099319458, val_loss: 0.8660366535186768\n",
            "Saving model, epoch: 11531, train_loss: 0.899938702583313, val_loss: 0.8660356998443604\n",
            "Saving model, epoch: 11532, train_loss: 0.8999370336532593, val_loss: 0.8660351037979126\n",
            "Saving model, epoch: 11533, train_loss: 0.8999353051185608, val_loss: 0.8660339117050171\n",
            "Saving model, epoch: 11534, train_loss: 0.8999335169792175, val_loss: 0.866033136844635\n",
            "Saving model, epoch: 11535, train_loss: 0.8999316692352295, val_loss: 0.8660321235656738\n",
            "Saving model, epoch: 11536, train_loss: 0.8999300599098206, val_loss: 0.8660308122634888\n",
            "Saving model, epoch: 11537, train_loss: 0.8999283909797668, val_loss: 0.8660299777984619\n",
            "Saving model, epoch: 11538, train_loss: 0.8999263644218445, val_loss: 0.8660291433334351\n",
            "Saving model, epoch: 11539, train_loss: 0.8999245166778564, val_loss: 0.8660279512405396\n",
            "Saving model, epoch: 11540, train_loss: 0.8999229669570923, val_loss: 0.8660273551940918\n",
            "Saving model, epoch: 11541, train_loss: 0.8999209403991699, val_loss: 0.8660262227058411\n",
            "Saving model, epoch: 11542, train_loss: 0.899919331073761, val_loss: 0.866024911403656\n",
            "Saving model, epoch: 11543, train_loss: 0.8999176025390625, val_loss: 0.8660244345664978\n",
            "Saving model, epoch: 11544, train_loss: 0.8999155759811401, val_loss: 0.866023063659668\n",
            "Saving model, epoch: 11545, train_loss: 0.8999139070510864, val_loss: 0.8660221695899963\n",
            "Saving model, epoch: 11546, train_loss: 0.8999121785163879, val_loss: 0.8660213947296143\n",
            "Saving model, epoch: 11547, train_loss: 0.8999103903770447, val_loss: 0.8660204410552979\n",
            "Saving model, epoch: 11548, train_loss: 0.8999085426330566, val_loss: 0.8660193085670471\n",
            "Saving model, epoch: 11549, train_loss: 0.8999067544937134, val_loss: 0.8660182952880859\n",
            "Saving model, epoch: 11550, train_loss: 0.8999050259590149, val_loss: 0.8660171627998352\n",
            "Saving model, epoch: 11551, train_loss: 0.8999033570289612, val_loss: 0.8660162687301636\n",
            "Saving model, epoch: 11552, train_loss: 0.8999014496803284, val_loss: 0.8660153746604919\n",
            "Saving model, epoch: 11553, train_loss: 0.8998996019363403, val_loss: 0.8660144805908203\n",
            "Saving model, epoch: 11554, train_loss: 0.8998976945877075, val_loss: 0.8660134673118591\n",
            "Saving model, epoch: 11555, train_loss: 0.8998962044715881, val_loss: 0.8660129904747009\n",
            "Saving model, epoch: 11556, train_loss: 0.8998942971229553, val_loss: 0.8660111427307129\n",
            "Saving model, epoch: 11557, train_loss: 0.8998925089836121, val_loss: 0.866010308265686\n",
            "Saving model, epoch: 11558, train_loss: 0.8998907804489136, val_loss: 0.866009533405304\n",
            "Saving model, epoch: 11559, train_loss: 0.8998887538909912, val_loss: 0.8660084009170532\n",
            "Saving model, epoch: 11560, train_loss: 0.8998872637748718, val_loss: 0.8660074472427368\n",
            "Saving model, epoch: 11561, train_loss: 0.899885356426239, val_loss: 0.8660065531730652\n",
            "Saving model, epoch: 11562, train_loss: 0.8998835682868958, val_loss: 0.8660056591033936\n",
            "Saving model, epoch: 11563, train_loss: 0.8998817205429077, val_loss: 0.8660044074058533\n",
            "Saving model, epoch: 11564, train_loss: 0.8998799324035645, val_loss: 0.8660032153129578\n",
            "Saving model, epoch: 11565, train_loss: 0.8998781442642212, val_loss: 0.86600261926651\n",
            "Saving model, epoch: 11566, train_loss: 0.8998764157295227, val_loss: 0.8660019040107727\n",
            "Saving model, epoch: 11567, train_loss: 0.8998746275901794, val_loss: 0.8660008311271667\n",
            "Saving model, epoch: 11568, train_loss: 0.8998727798461914, val_loss: 0.8660003542900085\n",
            "Saving model, epoch: 11569, train_loss: 0.8998708724975586, val_loss: 0.8659987449645996\n",
            "Saving model, epoch: 11570, train_loss: 0.8998692035675049, val_loss: 0.8659980297088623\n",
            "Saving model, epoch: 11571, train_loss: 0.8998672962188721, val_loss: 0.8659967184066772\n",
            "Saving model, epoch: 11572, train_loss: 0.8998655676841736, val_loss: 0.8659959435462952\n",
            "Saving model, epoch: 11573, train_loss: 0.8998637795448303, val_loss: 0.8659948110580444\n",
            "Saving model, epoch: 11574, train_loss: 0.8998619318008423, val_loss: 0.8659940361976624\n",
            "Saving model, epoch: 11575, train_loss: 0.899860143661499, val_loss: 0.865993320941925\n",
            "Saving model, epoch: 11576, train_loss: 0.8998583555221558, val_loss: 0.8659918904304504\n",
            "Saving model, epoch: 11577, train_loss: 0.8998565077781677, val_loss: 0.8659914135932922\n",
            "Saving model, epoch: 11578, train_loss: 0.899854838848114, val_loss: 0.8659902811050415\n",
            "Saving model, epoch: 11579, train_loss: 0.8998529314994812, val_loss: 0.8659888505935669\n",
            "Saving model, epoch: 11580, train_loss: 0.8998510837554932, val_loss: 0.8659878969192505\n",
            "Saving model, epoch: 11581, train_loss: 0.8998494148254395, val_loss: 0.8659870028495789\n",
            "Saving model, epoch: 11582, train_loss: 0.8998475074768066, val_loss: 0.8659859895706177\n",
            "Saving model, epoch: 11583, train_loss: 0.8998456597328186, val_loss: 0.8659852743148804\n",
            "Saving model, epoch: 11584, train_loss: 0.8998438715934753, val_loss: 0.8659841418266296\n",
            "Saving model, epoch: 11585, train_loss: 0.8998420834541321, val_loss: 0.8659831285476685\n",
            "Saving model, epoch: 11586, train_loss: 0.8998403549194336, val_loss: 0.865981936454773\n",
            "Saving model, epoch: 11587, train_loss: 0.8998384475708008, val_loss: 0.8659811615943909\n",
            "Saving model, epoch: 11588, train_loss: 0.8998366594314575, val_loss: 0.8659799098968506\n",
            "Saving model, epoch: 11589, train_loss: 0.8998346328735352, val_loss: 0.8659791350364685\n",
            "Saving model, epoch: 11590, train_loss: 0.8998328447341919, val_loss: 0.8659781813621521\n",
            "Saving model, epoch: 11591, train_loss: 0.899831235408783, val_loss: 0.865976870059967\n",
            "Saving model, epoch: 11592, train_loss: 0.8998293876647949, val_loss: 0.8659759163856506\n",
            "Saving model, epoch: 11593, train_loss: 0.8998275995254517, val_loss: 0.8659752011299133\n",
            "Saving model, epoch: 11594, train_loss: 0.8998255729675293, val_loss: 0.8659743070602417\n",
            "Saving model, epoch: 11595, train_loss: 0.8998239040374756, val_loss: 0.8659733533859253\n",
            "Saving model, epoch: 11596, train_loss: 0.8998221755027771, val_loss: 0.8659721612930298\n",
            "Saving model, epoch: 11597, train_loss: 0.8998203873634338, val_loss: 0.8659710884094238\n",
            "Saving model, epoch: 11598, train_loss: 0.8998182415962219, val_loss: 0.8659703135490417\n",
            "Saving model, epoch: 11599, train_loss: 0.899816632270813, val_loss: 0.8659694790840149\n",
            "Saving model, epoch: 11600, train_loss: 0.8998147249221802, val_loss: 0.8659681081771851\n",
            "epoch: 11601, train_loss: 0.8998129367828369, val_loss: 0.865966796875\n",
            "Saving model, epoch: 11601, train_loss: 0.8998129367828369, val_loss: 0.865966796875\n",
            "Saving model, epoch: 11602, train_loss: 0.8998109102249146, val_loss: 0.8659659028053284\n",
            "Saving model, epoch: 11603, train_loss: 0.8998093008995056, val_loss: 0.8659648895263672\n",
            "Saving model, epoch: 11604, train_loss: 0.8998075127601624, val_loss: 0.8659638166427612\n",
            "Saving model, epoch: 11605, train_loss: 0.89980548620224, val_loss: 0.865963339805603\n",
            "Saving model, epoch: 11606, train_loss: 0.899803876876831, val_loss: 0.8659622669219971\n",
            "Saving model, epoch: 11607, train_loss: 0.8998020887374878, val_loss: 0.8659612536430359\n",
            "Saving model, epoch: 11608, train_loss: 0.899800181388855, val_loss: 0.8659602403640747\n",
            "Saving model, epoch: 11609, train_loss: 0.8997982740402222, val_loss: 0.8659591674804688\n",
            "Saving model, epoch: 11610, train_loss: 0.8997963666915894, val_loss: 0.865958034992218\n",
            "Saving model, epoch: 11611, train_loss: 0.8997946381568909, val_loss: 0.865957498550415\n",
            "Saving model, epoch: 11612, train_loss: 0.8997928500175476, val_loss: 0.8659564852714539\n",
            "Saving model, epoch: 11613, train_loss: 0.8997910022735596, val_loss: 0.8659554123878479\n",
            "Saving model, epoch: 11614, train_loss: 0.8997892141342163, val_loss: 0.8659543395042419\n",
            "Saving model, epoch: 11615, train_loss: 0.8997873067855835, val_loss: 0.8659530282020569\n",
            "Saving model, epoch: 11616, train_loss: 0.8997853994369507, val_loss: 0.8659520745277405\n",
            "Saving model, epoch: 11617, train_loss: 0.8997836112976074, val_loss: 0.8659514784812927\n",
            "Saving model, epoch: 11618, train_loss: 0.8997815847396851, val_loss: 0.8659506440162659\n",
            "Saving model, epoch: 11619, train_loss: 0.8997799754142761, val_loss: 0.865949273109436\n",
            "Saving model, epoch: 11620, train_loss: 0.8997780680656433, val_loss: 0.8659483194351196\n",
            "Saving model, epoch: 11621, train_loss: 0.8997761607170105, val_loss: 0.8659473657608032\n",
            "Saving model, epoch: 11622, train_loss: 0.8997743725776672, val_loss: 0.8659464120864868\n",
            "Saving model, epoch: 11623, train_loss: 0.8997725248336792, val_loss: 0.8659452795982361\n",
            "Saving model, epoch: 11624, train_loss: 0.8997709155082703, val_loss: 0.8659440279006958\n",
            "Saving model, epoch: 11625, train_loss: 0.8997688889503479, val_loss: 0.8659429550170898\n",
            "Saving model, epoch: 11626, train_loss: 0.8997671008110046, val_loss: 0.8659421801567078\n",
            "Saving model, epoch: 11627, train_loss: 0.8997650146484375, val_loss: 0.8659409880638123\n",
            "Saving model, epoch: 11628, train_loss: 0.8997634053230286, val_loss: 0.8659398555755615\n",
            "Saving model, epoch: 11629, train_loss: 0.8997614979743958, val_loss: 0.8659389019012451\n",
            "Saving model, epoch: 11630, train_loss: 0.8997596502304077, val_loss: 0.8659383058547974\n",
            "Saving model, epoch: 11631, train_loss: 0.8997577428817749, val_loss: 0.8659375309944153\n",
            "Saving model, epoch: 11632, train_loss: 0.8997559547424316, val_loss: 0.8659365177154541\n",
            "Saving model, epoch: 11633, train_loss: 0.8997540473937988, val_loss: 0.8659356236457825\n",
            "Saving model, epoch: 11634, train_loss: 0.8997522592544556, val_loss: 0.8659341931343079\n",
            "Saving model, epoch: 11635, train_loss: 0.8997502326965332, val_loss: 0.8659329414367676\n",
            "Saving model, epoch: 11636, train_loss: 0.8997485041618347, val_loss: 0.865932285785675\n",
            "Saving model, epoch: 11637, train_loss: 0.8997465968132019, val_loss: 0.8659308552742004\n",
            "Saving model, epoch: 11638, train_loss: 0.8997448086738586, val_loss: 0.8659299612045288\n",
            "Saving model, epoch: 11639, train_loss: 0.8997430801391602, val_loss: 0.865929126739502\n",
            "Saving model, epoch: 11640, train_loss: 0.8997411131858826, val_loss: 0.8659282326698303\n",
            "Saving model, epoch: 11641, train_loss: 0.8997392654418945, val_loss: 0.8659266233444214\n",
            "Saving model, epoch: 11642, train_loss: 0.8997373580932617, val_loss: 0.8659260869026184\n",
            "Saving model, epoch: 11643, train_loss: 0.8997355699539185, val_loss: 0.8659249544143677\n",
            "Saving model, epoch: 11644, train_loss: 0.8997335433959961, val_loss: 0.8659234642982483\n",
            "Saving model, epoch: 11645, train_loss: 0.8997318744659424, val_loss: 0.8659225702285767\n",
            "Saving model, epoch: 11646, train_loss: 0.8997300267219543, val_loss: 0.8659219741821289\n",
            "Saving model, epoch: 11647, train_loss: 0.899728000164032, val_loss: 0.8659209609031677\n",
            "Saving model, epoch: 11648, train_loss: 0.8997262120246887, val_loss: 0.8659199476242065\n",
            "Saving model, epoch: 11649, train_loss: 0.8997243046760559, val_loss: 0.8659189939498901\n",
            "Saving model, epoch: 11650, train_loss: 0.8997225761413574, val_loss: 0.8659177422523499\n",
            "Saving model, epoch: 11651, train_loss: 0.8997206687927246, val_loss: 0.8659170269966125\n",
            "Saving model, epoch: 11652, train_loss: 0.8997187614440918, val_loss: 0.8659160137176514\n",
            "Saving model, epoch: 11653, train_loss: 0.899716854095459, val_loss: 0.8659148216247559\n",
            "Saving model, epoch: 11654, train_loss: 0.8997150659561157, val_loss: 0.8659136295318604\n",
            "Saving model, epoch: 11655, train_loss: 0.8997132778167725, val_loss: 0.8659130930900574\n",
            "Saving model, epoch: 11656, train_loss: 0.8997112512588501, val_loss: 0.8659120798110962\n",
            "Saving model, epoch: 11657, train_loss: 0.8997093439102173, val_loss: 0.8659108281135559\n",
            "Saving model, epoch: 11658, train_loss: 0.8997076153755188, val_loss: 0.8659096360206604\n",
            "Saving model, epoch: 11659, train_loss: 0.8997055292129517, val_loss: 0.8659089207649231\n",
            "Saving model, epoch: 11660, train_loss: 0.8997040390968323, val_loss: 0.8659075498580933\n",
            "Saving model, epoch: 11661, train_loss: 0.8997018337249756, val_loss: 0.8659065365791321\n",
            "Saving model, epoch: 11662, train_loss: 0.8996999859809875, val_loss: 0.8659055233001709\n",
            "Saving model, epoch: 11663, train_loss: 0.8996981978416443, val_loss: 0.8659049272537231\n",
            "Saving model, epoch: 11664, train_loss: 0.8996961712837219, val_loss: 0.865903913974762\n",
            "Saving model, epoch: 11665, train_loss: 0.8996943831443787, val_loss: 0.865902304649353\n",
            "Saving model, epoch: 11666, train_loss: 0.8996924757957458, val_loss: 0.8659014105796814\n",
            "Saving model, epoch: 11667, train_loss: 0.8996906280517578, val_loss: 0.8659002184867859\n",
            "Saving model, epoch: 11668, train_loss: 0.8996888399124146, val_loss: 0.8658995628356934\n",
            "Saving model, epoch: 11669, train_loss: 0.8996868133544922, val_loss: 0.8658984899520874\n",
            "Saving model, epoch: 11670, train_loss: 0.8996851444244385, val_loss: 0.8658974170684814\n",
            "Saving model, epoch: 11671, train_loss: 0.8996831178665161, val_loss: 0.8658965229988098\n",
            "Saving model, epoch: 11672, train_loss: 0.8996813297271729, val_loss: 0.8658954501152039\n",
            "Saving model, epoch: 11673, train_loss: 0.8996793031692505, val_loss: 0.8658941388130188\n",
            "Saving model, epoch: 11674, train_loss: 0.8996775150299072, val_loss: 0.8658930659294128\n",
            "Saving model, epoch: 11675, train_loss: 0.8996756672859192, val_loss: 0.8658919930458069\n",
            "Saving model, epoch: 11676, train_loss: 0.8996737003326416, val_loss: 0.8658914566040039\n",
            "Saving model, epoch: 11677, train_loss: 0.8996718525886536, val_loss: 0.8658904433250427\n",
            "Saving model, epoch: 11678, train_loss: 0.8996699452400208, val_loss: 0.8658894896507263\n",
            "Saving model, epoch: 11679, train_loss: 0.8996681571006775, val_loss: 0.8658878207206726\n",
            "Saving model, epoch: 11680, train_loss: 0.8996662497520447, val_loss: 0.8658867478370667\n",
            "Saving model, epoch: 11681, train_loss: 0.8996644020080566, val_loss: 0.865886390209198\n",
            "Saving model, epoch: 11682, train_loss: 0.899662435054779, val_loss: 0.8658850789070129\n",
            "Saving model, epoch: 11683, train_loss: 0.8996605277061462, val_loss: 0.8658842444419861\n",
            "Saving model, epoch: 11684, train_loss: 0.8996586203575134, val_loss: 0.865883469581604\n",
            "Saving model, epoch: 11685, train_loss: 0.8996567726135254, val_loss: 0.865882158279419\n",
            "Saving model, epoch: 11686, train_loss: 0.8996548652648926, val_loss: 0.8658806681632996\n",
            "Saving model, epoch: 11687, train_loss: 0.8996529579162598, val_loss: 0.8658801913261414\n",
            "Saving model, epoch: 11688, train_loss: 0.8996509909629822, val_loss: 0.8658790588378906\n",
            "Saving model, epoch: 11689, train_loss: 0.8996493816375732, val_loss: 0.8658780455589294\n",
            "Saving model, epoch: 11690, train_loss: 0.8996472358703613, val_loss: 0.8658771514892578\n",
            "Saving model, epoch: 11691, train_loss: 0.8996453285217285, val_loss: 0.8658760190010071\n",
            "Saving model, epoch: 11692, train_loss: 0.8996433615684509, val_loss: 0.8658746480941772\n",
            "Saving model, epoch: 11693, train_loss: 0.8996415138244629, val_loss: 0.8658737540245056\n",
            "Saving model, epoch: 11694, train_loss: 0.8996397256851196, val_loss: 0.8658727407455444\n",
            "Saving model, epoch: 11695, train_loss: 0.8996376991271973, val_loss: 0.8658718466758728\n",
            "Saving model, epoch: 11696, train_loss: 0.899635910987854, val_loss: 0.8658708333969116\n",
            "Saving model, epoch: 11697, train_loss: 0.899634063243866, val_loss: 0.8658699989318848\n",
            "Saving model, epoch: 11698, train_loss: 0.8996320962905884, val_loss: 0.8658689856529236\n",
            "Saving model, epoch: 11699, train_loss: 0.899630069732666, val_loss: 0.8658676147460938\n",
            "Saving model, epoch: 11700, train_loss: 0.8996282815933228, val_loss: 0.8658664226531982\n",
            "epoch: 11701, train_loss: 0.8996264338493347, val_loss: 0.8658658862113953\n",
            "Saving model, epoch: 11701, train_loss: 0.8996264338493347, val_loss: 0.8658658862113953\n",
            "Saving model, epoch: 11702, train_loss: 0.8996243476867676, val_loss: 0.8658645153045654\n",
            "Saving model, epoch: 11703, train_loss: 0.8996225595474243, val_loss: 0.8658638000488281\n",
            "Saving model, epoch: 11704, train_loss: 0.8996206521987915, val_loss: 0.8658621907234192\n",
            "Saving model, epoch: 11705, train_loss: 0.8996185064315796, val_loss: 0.8658612966537476\n",
            "Saving model, epoch: 11706, train_loss: 0.8996170163154602, val_loss: 0.8658604621887207\n",
            "Saving model, epoch: 11707, train_loss: 0.8996148109436035, val_loss: 0.8658593893051147\n",
            "Saving model, epoch: 11708, train_loss: 0.8996129035949707, val_loss: 0.8658587336540222\n",
            "Saving model, epoch: 11709, train_loss: 0.8996109962463379, val_loss: 0.8658571243286133\n",
            "Saving model, epoch: 11710, train_loss: 0.8996089696884155, val_loss: 0.8658564686775208\n",
            "Saving model, epoch: 11711, train_loss: 0.899607241153717, val_loss: 0.8658550977706909\n",
            "Saving model, epoch: 11712, train_loss: 0.8996053338050842, val_loss: 0.8658541440963745\n",
            "Saving model, epoch: 11713, train_loss: 0.8996033668518066, val_loss: 0.8658535480499268\n",
            "Saving model, epoch: 11714, train_loss: 0.8996015191078186, val_loss: 0.865852415561676\n",
            "Saving model, epoch: 11715, train_loss: 0.899599552154541, val_loss: 0.8658506870269775\n",
            "Saving model, epoch: 11716, train_loss: 0.899597704410553, val_loss: 0.8658497929573059\n",
            "Saving model, epoch: 11717, train_loss: 0.8995957374572754, val_loss: 0.8658491373062134\n",
            "Saving model, epoch: 11718, train_loss: 0.899593710899353, val_loss: 0.8658480048179626\n",
            "Saving model, epoch: 11719, train_loss: 0.899591863155365, val_loss: 0.8658469915390015\n",
            "Saving model, epoch: 11720, train_loss: 0.8995900750160217, val_loss: 0.8658458590507507\n",
            "Saving model, epoch: 11721, train_loss: 0.8995879888534546, val_loss: 0.8658445477485657\n",
            "Saving model, epoch: 11722, train_loss: 0.8995860815048218, val_loss: 0.865843653678894\n",
            "Saving model, epoch: 11723, train_loss: 0.899584174156189, val_loss: 0.8658428192138672\n",
            "Saving model, epoch: 11724, train_loss: 0.8995822668075562, val_loss: 0.8658415675163269\n",
            "Saving model, epoch: 11725, train_loss: 0.8995802402496338, val_loss: 0.8658406138420105\n",
            "Saving model, epoch: 11726, train_loss: 0.899578332901001, val_loss: 0.8658393621444702\n",
            "Saving model, epoch: 11727, train_loss: 0.8995764255523682, val_loss: 0.8658384084701538\n",
            "Saving model, epoch: 11728, train_loss: 0.8995743989944458, val_loss: 0.8658373355865479\n",
            "Saving model, epoch: 11729, train_loss: 0.8995726108551025, val_loss: 0.8658365607261658\n",
            "Saving model, epoch: 11730, train_loss: 0.8995708227157593, val_loss: 0.8658351898193359\n",
            "Saving model, epoch: 11731, train_loss: 0.8995688557624817, val_loss: 0.8658342957496643\n",
            "Saving model, epoch: 11732, train_loss: 0.8995667695999146, val_loss: 0.8658332824707031\n",
            "Saving model, epoch: 11733, train_loss: 0.8995647430419922, val_loss: 0.8658320307731628\n",
            "Saving model, epoch: 11734, train_loss: 0.8995630741119385, val_loss: 0.8658314347267151\n",
            "Saving model, epoch: 11735, train_loss: 0.8995609283447266, val_loss: 0.8658299446105957\n",
            "Saving model, epoch: 11736, train_loss: 0.8995591402053833, val_loss: 0.8658292889595032\n",
            "Saving model, epoch: 11737, train_loss: 0.89955735206604, val_loss: 0.8658281564712524\n",
            "Saving model, epoch: 11738, train_loss: 0.8995551466941833, val_loss: 0.8658268451690674\n",
            "Saving model, epoch: 11739, train_loss: 0.8995532989501953, val_loss: 0.8658255338668823\n",
            "Saving model, epoch: 11740, train_loss: 0.8995513916015625, val_loss: 0.8658247590065002\n",
            "Saving model, epoch: 11741, train_loss: 0.8995493650436401, val_loss: 0.8658236861228943\n",
            "Saving model, epoch: 11742, train_loss: 0.8995474576950073, val_loss: 0.8658226728439331\n",
            "Saving model, epoch: 11743, train_loss: 0.8995454907417297, val_loss: 0.8658215403556824\n",
            "Saving model, epoch: 11744, train_loss: 0.8995435833930969, val_loss: 0.8658204078674316\n",
            "Saving model, epoch: 11745, train_loss: 0.8995417356491089, val_loss: 0.8658192753791809\n",
            "Saving model, epoch: 11746, train_loss: 0.8995398283004761, val_loss: 0.8658185005187988\n",
            "Saving model, epoch: 11747, train_loss: 0.8995378613471985, val_loss: 0.8658175468444824\n",
            "Saving model, epoch: 11748, train_loss: 0.8995358347892761, val_loss: 0.8658162355422974\n",
            "Saving model, epoch: 11749, train_loss: 0.8995338082313538, val_loss: 0.8658153414726257\n",
            "Saving model, epoch: 11750, train_loss: 0.8995320200920105, val_loss: 0.8658139705657959\n",
            "Saving model, epoch: 11751, train_loss: 0.8995299935340881, val_loss: 0.8658134937286377\n",
            "Saving model, epoch: 11752, train_loss: 0.8995282053947449, val_loss: 0.8658119440078735\n",
            "Saving model, epoch: 11753, train_loss: 0.899526059627533, val_loss: 0.8658110499382019\n",
            "Saving model, epoch: 11754, train_loss: 0.8995239734649658, val_loss: 0.8658104538917542\n",
            "Saving model, epoch: 11755, train_loss: 0.8995222449302673, val_loss: 0.86580890417099\n",
            "Saving model, epoch: 11756, train_loss: 0.8995201587677002, val_loss: 0.8658076524734497\n",
            "Saving model, epoch: 11757, train_loss: 0.8995183706283569, val_loss: 0.8658064603805542\n",
            "Saving model, epoch: 11758, train_loss: 0.8995163440704346, val_loss: 0.8658057451248169\n",
            "Saving model, epoch: 11759, train_loss: 0.8995143175125122, val_loss: 0.8658046722412109\n",
            "Saving model, epoch: 11760, train_loss: 0.8995124101638794, val_loss: 0.8658035397529602\n",
            "Saving model, epoch: 11761, train_loss: 0.8995106220245361, val_loss: 0.8658028841018677\n",
            "Saving model, epoch: 11762, train_loss: 0.8995084762573242, val_loss: 0.8658013343811035\n",
            "Saving model, epoch: 11763, train_loss: 0.8995065093040466, val_loss: 0.8658003211021423\n",
            "Saving model, epoch: 11764, train_loss: 0.8995046615600586, val_loss: 0.8657993674278259\n",
            "Saving model, epoch: 11765, train_loss: 0.899502694606781, val_loss: 0.8657982349395752\n",
            "Saving model, epoch: 11766, train_loss: 0.8995006680488586, val_loss: 0.8657974004745483\n",
            "Saving model, epoch: 11767, train_loss: 0.8994987607002258, val_loss: 0.8657960891723633\n",
            "Saving model, epoch: 11768, train_loss: 0.899496853351593, val_loss: 0.865794837474823\n",
            "Saving model, epoch: 11769, train_loss: 0.8994949460029602, val_loss: 0.8657943606376648\n",
            "Saving model, epoch: 11770, train_loss: 0.8994928002357483, val_loss: 0.8657928705215454\n",
            "Saving model, epoch: 11771, train_loss: 0.8994908928871155, val_loss: 0.8657915592193604\n",
            "Saving model, epoch: 11772, train_loss: 0.8994888067245483, val_loss: 0.8657909035682678\n",
            "Saving model, epoch: 11773, train_loss: 0.8994870185852051, val_loss: 0.8657899498939514\n",
            "Saving model, epoch: 11774, train_loss: 0.8994849920272827, val_loss: 0.8657884001731873\n",
            "Saving model, epoch: 11775, train_loss: 0.8994829654693604, val_loss: 0.8657878041267395\n",
            "Saving model, epoch: 11776, train_loss: 0.8994811773300171, val_loss: 0.8657863736152649\n",
            "Saving model, epoch: 11777, train_loss: 0.8994789719581604, val_loss: 0.8657851219177246\n",
            "Saving model, epoch: 11778, train_loss: 0.8994771838188171, val_loss: 0.8657843470573425\n",
            "Saving model, epoch: 11779, train_loss: 0.8994751572608948, val_loss: 0.8657832145690918\n",
            "Saving model, epoch: 11780, train_loss: 0.8994731307029724, val_loss: 0.8657822012901306\n",
            "Saving model, epoch: 11781, train_loss: 0.89947110414505, val_loss: 0.8657814264297485\n",
            "Saving model, epoch: 11782, train_loss: 0.8994691967964172, val_loss: 0.8657803535461426\n",
            "Saving model, epoch: 11783, train_loss: 0.8994672894477844, val_loss: 0.8657786846160889\n",
            "Saving model, epoch: 11784, train_loss: 0.8994653224945068, val_loss: 0.8657776117324829\n",
            "Saving model, epoch: 11785, train_loss: 0.899463415145874, val_loss: 0.865776777267456\n",
            "Saving model, epoch: 11786, train_loss: 0.8994613885879517, val_loss: 0.865776002407074\n",
            "Saving model, epoch: 11787, train_loss: 0.8994592428207397, val_loss: 0.8657749891281128\n",
            "Saving model, epoch: 11788, train_loss: 0.8994574546813965, val_loss: 0.8657736778259277\n",
            "Saving model, epoch: 11789, train_loss: 0.8994553685188293, val_loss: 0.8657724857330322\n",
            "Saving model, epoch: 11790, train_loss: 0.8994534611701965, val_loss: 0.8657712936401367\n",
            "Saving model, epoch: 11791, train_loss: 0.8994515538215637, val_loss: 0.8657704591751099\n",
            "Saving model, epoch: 11792, train_loss: 0.8994494080543518, val_loss: 0.8657695055007935\n",
            "Saving model, epoch: 11793, train_loss: 0.8994474411010742, val_loss: 0.8657681941986084\n",
            "Saving model, epoch: 11794, train_loss: 0.8994455337524414, val_loss: 0.8657668232917786\n",
            "Saving model, epoch: 11795, train_loss: 0.8994433879852295, val_loss: 0.8657661080360413\n",
            "Saving model, epoch: 11796, train_loss: 0.8994415998458862, val_loss: 0.8657647967338562\n",
            "Saving model, epoch: 11797, train_loss: 0.8994395732879639, val_loss: 0.8657638430595398\n",
            "Saving model, epoch: 11798, train_loss: 0.8994374871253967, val_loss: 0.8657631278038025\n",
            "Saving model, epoch: 11799, train_loss: 0.8994355797767639, val_loss: 0.8657620549201965\n",
            "Saving model, epoch: 11800, train_loss: 0.8994336724281311, val_loss: 0.8657603859901428\n",
            "epoch: 11801, train_loss: 0.8994315266609192, val_loss: 0.8657594919204712\n",
            "Saving model, epoch: 11801, train_loss: 0.8994315266609192, val_loss: 0.8657594919204712\n",
            "Saving model, epoch: 11802, train_loss: 0.8994297385215759, val_loss: 0.8657588958740234\n",
            "Saving model, epoch: 11803, train_loss: 0.8994277119636536, val_loss: 0.8657577633857727\n",
            "Saving model, epoch: 11804, train_loss: 0.8994256258010864, val_loss: 0.8657565712928772\n",
            "Saving model, epoch: 11805, train_loss: 0.8994235992431641, val_loss: 0.8657550811767578\n",
            "Saving model, epoch: 11806, train_loss: 0.8994216918945312, val_loss: 0.8657543063163757\n",
            "Saving model, epoch: 11807, train_loss: 0.8994196653366089, val_loss: 0.8657529950141907\n",
            "Saving model, epoch: 11808, train_loss: 0.8994177579879761, val_loss: 0.8657519221305847\n",
            "Saving model, epoch: 11809, train_loss: 0.8994156718254089, val_loss: 0.8657511472702026\n",
            "Saving model, epoch: 11810, train_loss: 0.8994136452674866, val_loss: 0.8657500147819519\n",
            "Saving model, epoch: 11811, train_loss: 0.899411678314209, val_loss: 0.865749180316925\n",
            "Saving model, epoch: 11812, train_loss: 0.8994096517562866, val_loss: 0.8657478094100952\n",
            "Saving model, epoch: 11813, train_loss: 0.8994075059890747, val_loss: 0.8657472729682922\n",
            "Saving model, epoch: 11814, train_loss: 0.8994055986404419, val_loss: 0.8657453656196594\n",
            "Saving model, epoch: 11815, train_loss: 0.8994036912918091, val_loss: 0.8657442331314087\n",
            "Saving model, epoch: 11816, train_loss: 0.8994017839431763, val_loss: 0.8657433986663818\n",
            "Saving model, epoch: 11817, train_loss: 0.8993995785713196, val_loss: 0.8657423853874207\n",
            "Saving model, epoch: 11818, train_loss: 0.899397611618042, val_loss: 0.8657411932945251\n",
            "Saving model, epoch: 11819, train_loss: 0.8993955850601196, val_loss: 0.8657402992248535\n",
            "Saving model, epoch: 11820, train_loss: 0.8993935585021973, val_loss: 0.8657384514808655\n",
            "Saving model, epoch: 11821, train_loss: 0.8993915319442749, val_loss: 0.865737795829773\n",
            "Saving model, epoch: 11822, train_loss: 0.8993895649909973, val_loss: 0.8657367825508118\n",
            "Saving model, epoch: 11823, train_loss: 0.899387538433075, val_loss: 0.8657355904579163\n",
            "Saving model, epoch: 11824, train_loss: 0.8993854522705078, val_loss: 0.8657343983650208\n",
            "Saving model, epoch: 11825, train_loss: 0.899383544921875, val_loss: 0.8657332062721252\n",
            "Saving model, epoch: 11826, train_loss: 0.8993815183639526, val_loss: 0.8657322525978088\n",
            "Saving model, epoch: 11827, train_loss: 0.8993794918060303, val_loss: 0.8657313585281372\n",
            "Saving model, epoch: 11828, train_loss: 0.899377703666687, val_loss: 0.8657306432723999\n",
            "Saving model, epoch: 11829, train_loss: 0.8993754982948303, val_loss: 0.8657292127609253\n",
            "Saving model, epoch: 11830, train_loss: 0.899373471736908, val_loss: 0.8657280206680298\n",
            "Saving model, epoch: 11831, train_loss: 0.8993715643882751, val_loss: 0.8657267093658447\n",
            "Saving model, epoch: 11832, train_loss: 0.8993695378303528, val_loss: 0.8657257556915283\n",
            "Saving model, epoch: 11833, train_loss: 0.8993674516677856, val_loss: 0.8657244443893433\n",
            "Saving model, epoch: 11834, train_loss: 0.8993654251098633, val_loss: 0.8657239675521851\n",
            "Saving model, epoch: 11835, train_loss: 0.8993635177612305, val_loss: 0.8657225966453552\n",
            "Saving model, epoch: 11836, train_loss: 0.8993614315986633, val_loss: 0.8657211661338806\n",
            "Saving model, epoch: 11837, train_loss: 0.8993595838546753, val_loss: 0.8657203912734985\n",
            "Saving model, epoch: 11838, train_loss: 0.8993573784828186, val_loss: 0.8657192587852478\n",
            "Saving model, epoch: 11839, train_loss: 0.899355411529541, val_loss: 0.8657182455062866\n",
            "Saving model, epoch: 11840, train_loss: 0.8993532657623291, val_loss: 0.8657171130180359\n",
            "Saving model, epoch: 11841, train_loss: 0.8993513584136963, val_loss: 0.865715503692627\n",
            "Saving model, epoch: 11842, train_loss: 0.8993493318557739, val_loss: 0.8657146692276001\n",
            "Saving model, epoch: 11843, train_loss: 0.8993472456932068, val_loss: 0.8657141327857971\n",
            "Saving model, epoch: 11844, train_loss: 0.8993454575538635, val_loss: 0.8657127022743225\n",
            "Saving model, epoch: 11845, train_loss: 0.8993431329727173, val_loss: 0.865711510181427\n",
            "Saving model, epoch: 11846, train_loss: 0.8993412256240845, val_loss: 0.865710437297821\n",
            "Saving model, epoch: 11847, train_loss: 0.8993391990661621, val_loss: 0.8657093048095703\n",
            "Saving model, epoch: 11848, train_loss: 0.899337112903595, val_loss: 0.8657078742980957\n",
            "Saving model, epoch: 11849, train_loss: 0.8993350863456726, val_loss: 0.8657069206237793\n",
            "Saving model, epoch: 11850, train_loss: 0.8993331789970398, val_loss: 0.8657060265541077\n",
            "Saving model, epoch: 11851, train_loss: 0.8993310928344727, val_loss: 0.8657048344612122\n",
            "Saving model, epoch: 11852, train_loss: 0.8993290662765503, val_loss: 0.8657035231590271\n",
            "Saving model, epoch: 11853, train_loss: 0.8993268609046936, val_loss: 0.8657025694847107\n",
            "Saving model, epoch: 11854, train_loss: 0.8993249535560608, val_loss: 0.8657016158103943\n",
            "Saving model, epoch: 11855, train_loss: 0.899323046207428, val_loss: 0.8657005429267883\n",
            "Saving model, epoch: 11856, train_loss: 0.8993208408355713, val_loss: 0.865699291229248\n",
            "Saving model, epoch: 11857, train_loss: 0.8993189930915833, val_loss: 0.8656983375549316\n",
            "Saving model, epoch: 11858, train_loss: 0.8993167877197266, val_loss: 0.8656975030899048\n",
            "Saving model, epoch: 11859, train_loss: 0.8993148803710938, val_loss: 0.8656958341598511\n",
            "Saving model, epoch: 11860, train_loss: 0.8993127942085266, val_loss: 0.8656948208808899\n",
            "Saving model, epoch: 11861, train_loss: 0.8993107676506042, val_loss: 0.8656942248344421\n",
            "Saving model, epoch: 11862, train_loss: 0.8993087410926819, val_loss: 0.8656928539276123\n",
            "Saving model, epoch: 11863, train_loss: 0.8993063569068909, val_loss: 0.8656918406486511\n",
            "Saving model, epoch: 11864, train_loss: 0.8993046283721924, val_loss: 0.8656906485557556\n",
            "Saving model, epoch: 11865, train_loss: 0.8993025422096252, val_loss: 0.8656890988349915\n",
            "Saving model, epoch: 11866, train_loss: 0.8993003368377686, val_loss: 0.8656882643699646\n",
            "Saving model, epoch: 11867, train_loss: 0.8992984294891357, val_loss: 0.8656869530677795\n",
            "Saving model, epoch: 11868, train_loss: 0.8992962837219238, val_loss: 0.8656863570213318\n",
            "Saving model, epoch: 11869, train_loss: 0.8992943167686462, val_loss: 0.865685224533081\n",
            "Saving model, epoch: 11870, train_loss: 0.8992921710014343, val_loss: 0.8656838536262512\n",
            "Saving model, epoch: 11871, train_loss: 0.8992902636528015, val_loss: 0.86568284034729\n",
            "Saving model, epoch: 11872, train_loss: 0.8992882966995239, val_loss: 0.8656816482543945\n",
            "Saving model, epoch: 11873, train_loss: 0.899286150932312, val_loss: 0.865680456161499\n",
            "Saving model, epoch: 11874, train_loss: 0.8992841243743896, val_loss: 0.8656793236732483\n",
            "Saving model, epoch: 11875, train_loss: 0.8992820382118225, val_loss: 0.8656783103942871\n",
            "Saving model, epoch: 11876, train_loss: 0.8992800116539001, val_loss: 0.8656771183013916\n",
            "Saving model, epoch: 11877, train_loss: 0.899277925491333, val_loss: 0.8656764626502991\n",
            "Saving model, epoch: 11878, train_loss: 0.8992758393287659, val_loss: 0.865674614906311\n",
            "Saving model, epoch: 11879, train_loss: 0.8992738127708435, val_loss: 0.8656737208366394\n",
            "Saving model, epoch: 11880, train_loss: 0.8992717862129211, val_loss: 0.8656725883483887\n",
            "Saving model, epoch: 11881, train_loss: 0.8992697596549988, val_loss: 0.8656719923019409\n",
            "Saving model, epoch: 11882, train_loss: 0.8992677927017212, val_loss: 0.8656702637672424\n",
            "Saving model, epoch: 11883, train_loss: 0.8992656469345093, val_loss: 0.8656691908836365\n",
            "Saving model, epoch: 11884, train_loss: 0.8992634415626526, val_loss: 0.8656681776046753\n",
            "Saving model, epoch: 11885, train_loss: 0.8992613554000854, val_loss: 0.8656673431396484\n",
            "Saving model, epoch: 11886, train_loss: 0.8992593288421631, val_loss: 0.8656661510467529\n",
            "Saving model, epoch: 11887, train_loss: 0.8992573022842407, val_loss: 0.8656644821166992\n",
            "Saving model, epoch: 11888, train_loss: 0.8992553353309631, val_loss: 0.8656635880470276\n",
            "Saving model, epoch: 11889, train_loss: 0.8992531299591064, val_loss: 0.8656628131866455\n",
            "Saving model, epoch: 11890, train_loss: 0.8992512226104736, val_loss: 0.8656612634658813\n",
            "Saving model, epoch: 11891, train_loss: 0.8992490768432617, val_loss: 0.8656603097915649\n",
            "Saving model, epoch: 11892, train_loss: 0.8992471098899841, val_loss: 0.8656588196754456\n",
            "Saving model, epoch: 11893, train_loss: 0.8992450833320618, val_loss: 0.8656584024429321\n",
            "Saving model, epoch: 11894, train_loss: 0.8992429375648499, val_loss: 0.8656569719314575\n",
            "Saving model, epoch: 11895, train_loss: 0.8992408514022827, val_loss: 0.8656558394432068\n",
            "Saving model, epoch: 11896, train_loss: 0.8992388248443604, val_loss: 0.8656547665596008\n",
            "Saving model, epoch: 11897, train_loss: 0.8992368578910828, val_loss: 0.865653395652771\n",
            "Saving model, epoch: 11898, train_loss: 0.8992345929145813, val_loss: 0.8656523823738098\n",
            "Saving model, epoch: 11899, train_loss: 0.8992326259613037, val_loss: 0.8656507730484009\n",
            "Saving model, epoch: 11900, train_loss: 0.8992305994033813, val_loss: 0.8656495213508606\n",
            "epoch: 11901, train_loss: 0.8992283940315247, val_loss: 0.8656492233276367\n",
            "Saving model, epoch: 11901, train_loss: 0.8992283940315247, val_loss: 0.8656492233276367\n",
            "Saving model, epoch: 11902, train_loss: 0.8992263674736023, val_loss: 0.865648090839386\n",
            "Saving model, epoch: 11903, train_loss: 0.8992242813110352, val_loss: 0.8656467199325562\n",
            "Saving model, epoch: 11904, train_loss: 0.8992223739624023, val_loss: 0.8656455278396606\n",
            "Saving model, epoch: 11905, train_loss: 0.8992201685905457, val_loss: 0.8656445145606995\n",
            "Saving model, epoch: 11906, train_loss: 0.8992180228233337, val_loss: 0.8656435012817383\n",
            "Saving model, epoch: 11907, train_loss: 0.8992159366607666, val_loss: 0.8656424880027771\n",
            "Saving model, epoch: 11908, train_loss: 0.8992137312889099, val_loss: 0.8656414747238159\n",
            "Saving model, epoch: 11909, train_loss: 0.8992118239402771, val_loss: 0.8656400442123413\n",
            "Saving model, epoch: 11910, train_loss: 0.8992096781730652, val_loss: 0.8656386733055115\n",
            "Saving model, epoch: 11911, train_loss: 0.8992077112197876, val_loss: 0.8656381964683533\n",
            "Saving model, epoch: 11912, train_loss: 0.8992055654525757, val_loss: 0.8656361103057861\n",
            "Saving model, epoch: 11913, train_loss: 0.8992034792900085, val_loss: 0.8656356930732727\n",
            "Saving model, epoch: 11914, train_loss: 0.8992012739181519, val_loss: 0.8656344413757324\n",
            "Saving model, epoch: 11915, train_loss: 0.8991992473602295, val_loss: 0.8656330704689026\n",
            "Saving model, epoch: 11916, train_loss: 0.8991972208023071, val_loss: 0.8656322956085205\n",
            "Saving model, epoch: 11917, train_loss: 0.89919513463974, val_loss: 0.8656310439109802\n",
            "Saving model, epoch: 11918, train_loss: 0.8991930484771729, val_loss: 0.8656296133995056\n",
            "Saving model, epoch: 11919, train_loss: 0.8991909027099609, val_loss: 0.8656288385391235\n",
            "Saving model, epoch: 11920, train_loss: 0.8991888165473938, val_loss: 0.8656279444694519\n",
            "Saving model, epoch: 11921, train_loss: 0.8991867899894714, val_loss: 0.8656265139579773\n",
            "Saving model, epoch: 11922, train_loss: 0.8991847038269043, val_loss: 0.8656251430511475\n",
            "Saving model, epoch: 11923, train_loss: 0.8991826772689819, val_loss: 0.8656240105628967\n",
            "Saving model, epoch: 11924, train_loss: 0.8991804718971252, val_loss: 0.8656229376792908\n",
            "Saving model, epoch: 11925, train_loss: 0.8991783857345581, val_loss: 0.8656218647956848\n",
            "Saving model, epoch: 11926, train_loss: 0.8991763591766357, val_loss: 0.8656207919120789\n",
            "Saving model, epoch: 11927, train_loss: 0.899174153804779, val_loss: 0.8656198978424072\n",
            "Saving model, epoch: 11928, train_loss: 0.8991720080375671, val_loss: 0.8656184077262878\n",
            "Saving model, epoch: 11929, train_loss: 0.8991701006889343, val_loss: 0.8656172752380371\n",
            "Saving model, epoch: 11930, train_loss: 0.8991677165031433, val_loss: 0.8656162023544312\n",
            "Saving model, epoch: 11931, train_loss: 0.899165689945221, val_loss: 0.8656148910522461\n",
            "Saving model, epoch: 11932, train_loss: 0.8991636633872986, val_loss: 0.8656142354011536\n",
            "Saving model, epoch: 11933, train_loss: 0.8991615772247314, val_loss: 0.865612804889679\n",
            "Saving model, epoch: 11934, train_loss: 0.8991594910621643, val_loss: 0.8656115531921387\n",
            "Saving model, epoch: 11935, train_loss: 0.8991571664810181, val_loss: 0.8656105399131775\n",
            "Saving model, epoch: 11936, train_loss: 0.8991552591323853, val_loss: 0.8656096458435059\n",
            "Saving model, epoch: 11937, train_loss: 0.8991530537605286, val_loss: 0.865608274936676\n",
            "Saving model, epoch: 11938, train_loss: 0.8991510272026062, val_loss: 0.8656071424484253\n",
            "Saving model, epoch: 11939, train_loss: 0.8991488218307495, val_loss: 0.8656061291694641\n",
            "Saving model, epoch: 11940, train_loss: 0.8991467952728271, val_loss: 0.8656048774719238\n",
            "Saving model, epoch: 11941, train_loss: 0.89914470911026, val_loss: 0.8656036853790283\n",
            "Saving model, epoch: 11942, train_loss: 0.8991425037384033, val_loss: 0.8656023740768433\n",
            "Saving model, epoch: 11943, train_loss: 0.8991405963897705, val_loss: 0.8656010627746582\n",
            "Saving model, epoch: 11944, train_loss: 0.8991383910179138, val_loss: 0.8656004667282104\n",
            "Saving model, epoch: 11945, train_loss: 0.8991362452507019, val_loss: 0.8655988574028015\n",
            "Saving model, epoch: 11946, train_loss: 0.8991341590881348, val_loss: 0.865598201751709\n",
            "Saving model, epoch: 11947, train_loss: 0.8991319537162781, val_loss: 0.8655964732170105\n",
            "Saving model, epoch: 11948, train_loss: 0.8991299271583557, val_loss: 0.8655956983566284\n",
            "Saving model, epoch: 11949, train_loss: 0.899127721786499, val_loss: 0.8655945062637329\n",
            "Saving model, epoch: 11950, train_loss: 0.8991256356239319, val_loss: 0.8655931353569031\n",
            "Saving model, epoch: 11951, train_loss: 0.8991234302520752, val_loss: 0.8655921220779419\n",
            "Saving model, epoch: 11952, train_loss: 0.8991215229034424, val_loss: 0.8655909895896912\n",
            "Saving model, epoch: 11953, train_loss: 0.8991192579269409, val_loss: 0.8655897378921509\n",
            "Saving model, epoch: 11954, train_loss: 0.8991172909736633, val_loss: 0.8655887246131897\n",
            "Saving model, epoch: 11955, train_loss: 0.8991151452064514, val_loss: 0.8655874729156494\n",
            "Saving model, epoch: 11956, train_loss: 0.8991130590438843, val_loss: 0.8655864596366882\n",
            "Saving model, epoch: 11957, train_loss: 0.899110734462738, val_loss: 0.8655853271484375\n",
            "Saving model, epoch: 11958, train_loss: 0.8991088271141052, val_loss: 0.865584135055542\n",
            "Saving model, epoch: 11959, train_loss: 0.8991066217422485, val_loss: 0.8655829429626465\n",
            "Saving model, epoch: 11960, train_loss: 0.8991045355796814, val_loss: 0.8655816316604614\n",
            "Saving model, epoch: 11961, train_loss: 0.8991023898124695, val_loss: 0.8655802011489868\n",
            "Saving model, epoch: 11962, train_loss: 0.8991001844406128, val_loss: 0.8655797243118286\n",
            "Saving model, epoch: 11963, train_loss: 0.8990979790687561, val_loss: 0.8655779361724854\n",
            "Saving model, epoch: 11964, train_loss: 0.899095892906189, val_loss: 0.8655770421028137\n",
            "Saving model, epoch: 11965, train_loss: 0.8990939855575562, val_loss: 0.8655757308006287\n",
            "Saving model, epoch: 11966, train_loss: 0.8990917801856995, val_loss: 0.8655748963356018\n",
            "Saving model, epoch: 11967, train_loss: 0.899089515209198, val_loss: 0.8655736446380615\n",
            "Saving model, epoch: 11968, train_loss: 0.8990875482559204, val_loss: 0.8655725121498108\n",
            "Saving model, epoch: 11969, train_loss: 0.8990852236747742, val_loss: 0.865571141242981\n",
            "Saving model, epoch: 11970, train_loss: 0.899083137512207, val_loss: 0.8655702471733093\n",
            "Saving model, epoch: 11971, train_loss: 0.8990809917449951, val_loss: 0.8655688166618347\n",
            "Saving model, epoch: 11972, train_loss: 0.899078905582428, val_loss: 0.8655677437782288\n",
            "Saving model, epoch: 11973, train_loss: 0.8990767002105713, val_loss: 0.8655669689178467\n",
            "Saving model, epoch: 11974, train_loss: 0.8990746736526489, val_loss: 0.8655650615692139\n",
            "Saving model, epoch: 11975, train_loss: 0.8990724682807922, val_loss: 0.8655641674995422\n",
            "Saving model, epoch: 11976, train_loss: 0.8990702629089355, val_loss: 0.8655630946159363\n",
            "Saving model, epoch: 11977, train_loss: 0.8990682363510132, val_loss: 0.865562379360199\n",
            "Saving model, epoch: 11978, train_loss: 0.8990660309791565, val_loss: 0.8655607104301453\n",
            "Saving model, epoch: 11979, train_loss: 0.8990639448165894, val_loss: 0.8655597567558289\n",
            "Saving model, epoch: 11980, train_loss: 0.8990617990493774, val_loss: 0.8655586242675781\n",
            "Saving model, epoch: 11981, train_loss: 0.8990594744682312, val_loss: 0.8655569553375244\n",
            "Saving model, epoch: 11982, train_loss: 0.8990573883056641, val_loss: 0.8655562996864319\n",
            "Saving model, epoch: 11983, train_loss: 0.8990553617477417, val_loss: 0.865554690361023\n",
            "Saving model, epoch: 11984, train_loss: 0.899053156375885, val_loss: 0.8655539155006409\n",
            "Saving model, epoch: 11985, train_loss: 0.8990508913993835, val_loss: 0.8655528426170349\n",
            "Saving model, epoch: 11986, train_loss: 0.8990489840507507, val_loss: 0.8655515909194946\n",
            "Saving model, epoch: 11987, train_loss: 0.8990466594696045, val_loss: 0.8655502796173096\n",
            "Saving model, epoch: 11988, train_loss: 0.8990447521209717, val_loss: 0.8655486702919006\n",
            "Saving model, epoch: 11989, train_loss: 0.8990422487258911, val_loss: 0.8655480146408081\n",
            "Saving model, epoch: 11990, train_loss: 0.8990403413772583, val_loss: 0.8655468821525574\n",
            "Saving model, epoch: 11991, train_loss: 0.8990380764007568, val_loss: 0.8655456900596619\n",
            "Saving model, epoch: 11992, train_loss: 0.8990358710289001, val_loss: 0.8655441999435425\n",
            "Saving model, epoch: 11993, train_loss: 0.8990336656570435, val_loss: 0.8655431270599365\n",
            "Saving model, epoch: 11994, train_loss: 0.8990316987037659, val_loss: 0.8655423521995544\n",
            "Saving model, epoch: 11995, train_loss: 0.8990294933319092, val_loss: 0.8655413389205933\n",
            "Saving model, epoch: 11996, train_loss: 0.8990273475646973, val_loss: 0.8655397891998291\n",
            "Saving model, epoch: 11997, train_loss: 0.899025022983551, val_loss: 0.8655385971069336\n",
            "Saving model, epoch: 11998, train_loss: 0.8990230560302734, val_loss: 0.8655375242233276\n",
            "Saving model, epoch: 11999, train_loss: 0.8990208506584167, val_loss: 0.8655362725257874\n",
            "Saving model, epoch: 12000, train_loss: 0.8990188241004944, val_loss: 0.8655351996421814\n",
            "epoch: 12001, train_loss: 0.8990166187286377, val_loss: 0.8655338287353516\n",
            "Saving model, epoch: 12001, train_loss: 0.8990166187286377, val_loss: 0.8655338287353516\n",
            "Saving model, epoch: 12002, train_loss: 0.899014413356781, val_loss: 0.865533173084259\n",
            "Saving model, epoch: 12003, train_loss: 0.8990122675895691, val_loss: 0.8655319809913635\n",
            "Saving model, epoch: 12004, train_loss: 0.8990100026130676, val_loss: 0.8655303120613098\n",
            "Saving model, epoch: 12005, train_loss: 0.8990079760551453, val_loss: 0.8655292391777039\n",
            "Saving model, epoch: 12006, train_loss: 0.8990055322647095, val_loss: 0.8655278086662292\n",
            "Saving model, epoch: 12007, train_loss: 0.8990035653114319, val_loss: 0.8655266761779785\n",
            "Saving model, epoch: 12008, train_loss: 0.8990013599395752, val_loss: 0.8655259013175964\n",
            "Saving model, epoch: 12009, train_loss: 0.8989993333816528, val_loss: 0.8655245304107666\n",
            "Saving model, epoch: 12010, train_loss: 0.8989971280097961, val_loss: 0.8655232787132263\n",
            "Saving model, epoch: 12011, train_loss: 0.8989949226379395, val_loss: 0.8655223250389099\n",
            "Saving model, epoch: 12012, train_loss: 0.8989927172660828, val_loss: 0.8655210137367249\n",
            "Saving model, epoch: 12013, train_loss: 0.8989905714988708, val_loss: 0.8655194640159607\n",
            "Saving model, epoch: 12014, train_loss: 0.8989883661270142, val_loss: 0.8655182719230652\n",
            "Saving model, epoch: 12015, train_loss: 0.8989861607551575, val_loss: 0.8655176162719727\n",
            "Saving model, epoch: 12016, train_loss: 0.8989840745925903, val_loss: 0.8655164837837219\n",
            "Saving model, epoch: 12017, train_loss: 0.8989819288253784, val_loss: 0.8655153512954712\n",
            "Saving model, epoch: 12018, train_loss: 0.8989797234535217, val_loss: 0.8655136227607727\n",
            "Saving model, epoch: 12019, train_loss: 0.8989773988723755, val_loss: 0.8655126094818115\n",
            "Saving model, epoch: 12020, train_loss: 0.8989751935005188, val_loss: 0.8655112385749817\n",
            "Saving model, epoch: 12021, train_loss: 0.8989729881286621, val_loss: 0.8655104637145996\n",
            "Saving model, epoch: 12022, train_loss: 0.8989709615707397, val_loss: 0.8655094504356384\n",
            "Saving model, epoch: 12023, train_loss: 0.8989685773849487, val_loss: 0.8655077815055847\n",
            "Saving model, epoch: 12024, train_loss: 0.8989665508270264, val_loss: 0.8655065298080444\n",
            "Saving model, epoch: 12025, train_loss: 0.8989643454551697, val_loss: 0.8655053973197937\n",
            "Saving model, epoch: 12026, train_loss: 0.8989622592926025, val_loss: 0.8655046224594116\n",
            "Saving model, epoch: 12027, train_loss: 0.8989599347114563, val_loss: 0.8655032515525818\n",
            "Saving model, epoch: 12028, train_loss: 0.8989578485488892, val_loss: 0.865501880645752\n",
            "Saving model, epoch: 12029, train_loss: 0.8989555239677429, val_loss: 0.8655008673667908\n",
            "Saving model, epoch: 12030, train_loss: 0.8989534378051758, val_loss: 0.8654999136924744\n",
            "Saving model, epoch: 12031, train_loss: 0.8989511132240295, val_loss: 0.8654986023902893\n",
            "Saving model, epoch: 12032, train_loss: 0.8989489674568176, val_loss: 0.8654972910881042\n",
            "Saving model, epoch: 12033, train_loss: 0.8989468812942505, val_loss: 0.8654959201812744\n",
            "Saving model, epoch: 12034, train_loss: 0.8989446759223938, val_loss: 0.8654951453208923\n",
            "Saving model, epoch: 12035, train_loss: 0.8989424705505371, val_loss: 0.8654938340187073\n",
            "Saving model, epoch: 12036, train_loss: 0.8989403247833252, val_loss: 0.8654925227165222\n",
            "Saving model, epoch: 12037, train_loss: 0.8989380598068237, val_loss: 0.8654913902282715\n",
            "Saving model, epoch: 12038, train_loss: 0.898935854434967, val_loss: 0.8654900193214417\n",
            "Saving model, epoch: 12039, train_loss: 0.8989335894584656, val_loss: 0.8654892444610596\n",
            "Saving model, epoch: 12040, train_loss: 0.8989315032958984, val_loss: 0.8654875755310059\n",
            "Saving model, epoch: 12041, train_loss: 0.8989291787147522, val_loss: 0.8654866218566895\n",
            "Saving model, epoch: 12042, train_loss: 0.8989269733428955, val_loss: 0.8654856085777283\n",
            "Saving model, epoch: 12043, train_loss: 0.8989249467849731, val_loss: 0.8654842376708984\n",
            "Saving model, epoch: 12044, train_loss: 0.8989227414131165, val_loss: 0.8654834032058716\n",
            "Saving model, epoch: 12045, train_loss: 0.8989206552505493, val_loss: 0.8654817342758179\n",
            "Saving model, epoch: 12046, train_loss: 0.8989181518554688, val_loss: 0.8654804229736328\n",
            "Saving model, epoch: 12047, train_loss: 0.8989161252975464, val_loss: 0.8654791712760925\n",
            "Saving model, epoch: 12048, train_loss: 0.8989138007164001, val_loss: 0.8654781579971313\n",
            "Saving model, epoch: 12049, train_loss: 0.898911714553833, val_loss: 0.8654768466949463\n",
            "Saving model, epoch: 12050, train_loss: 0.898909330368042, val_loss: 0.8654756546020508\n",
            "Saving model, epoch: 12051, train_loss: 0.8989073038101196, val_loss: 0.8654745221138\n",
            "Saving model, epoch: 12052, train_loss: 0.8989050984382629, val_loss: 0.8654732704162598\n",
            "Saving model, epoch: 12053, train_loss: 0.8989028930664062, val_loss: 0.8654718399047852\n",
            "Saving model, epoch: 12054, train_loss: 0.8989006876945496, val_loss: 0.8654710650444031\n",
            "Saving model, epoch: 12055, train_loss: 0.8988983631134033, val_loss: 0.8654699325561523\n",
            "Saving model, epoch: 12056, train_loss: 0.8988962173461914, val_loss: 0.8654686808586121\n",
            "Saving model, epoch: 12057, train_loss: 0.8988940119743347, val_loss: 0.8654673099517822\n",
            "Saving model, epoch: 12058, train_loss: 0.8988917469978333, val_loss: 0.8654661774635315\n",
            "Saving model, epoch: 12059, train_loss: 0.8988896012306213, val_loss: 0.8654651045799255\n",
            "Saving model, epoch: 12060, train_loss: 0.8988872170448303, val_loss: 0.8654637336730957\n",
            "Saving model, epoch: 12061, train_loss: 0.8988850712776184, val_loss: 0.8654623627662659\n",
            "Saving model, epoch: 12062, train_loss: 0.8988829851150513, val_loss: 0.8654616475105286\n",
            "Saving model, epoch: 12063, train_loss: 0.8988805413246155, val_loss: 0.8654597401618958\n",
            "Saving model, epoch: 12064, train_loss: 0.8988784551620483, val_loss: 0.8654588460922241\n",
            "Saving model, epoch: 12065, train_loss: 0.8988762497901917, val_loss: 0.8654577732086182\n",
            "Saving model, epoch: 12066, train_loss: 0.898874044418335, val_loss: 0.8654564619064331\n",
            "Saving model, epoch: 12067, train_loss: 0.8988717198371887, val_loss: 0.8654557466506958\n",
            "Saving model, epoch: 12068, train_loss: 0.898869514465332, val_loss: 0.8654540777206421\n",
            "Saving model, epoch: 12069, train_loss: 0.8988673090934753, val_loss: 0.8654531240463257\n",
            "Saving model, epoch: 12070, train_loss: 0.8988651037216187, val_loss: 0.8654516935348511\n",
            "Saving model, epoch: 12071, train_loss: 0.898862898349762, val_loss: 0.8654506206512451\n",
            "Saving model, epoch: 12072, train_loss: 0.89886075258255, val_loss: 0.8654496073722839\n",
            "Saving model, epoch: 12073, train_loss: 0.8988584876060486, val_loss: 0.865447998046875\n",
            "Saving model, epoch: 12074, train_loss: 0.8988562822341919, val_loss: 0.865446925163269\n",
            "Saving model, epoch: 12075, train_loss: 0.8988539576530457, val_loss: 0.8654457926750183\n",
            "Saving model, epoch: 12076, train_loss: 0.8988518118858337, val_loss: 0.8654440641403198\n",
            "Saving model, epoch: 12077, train_loss: 0.8988494277000427, val_loss: 0.8654430508613586\n",
            "Saving model, epoch: 12078, train_loss: 0.898847222328186, val_loss: 0.8654420971870422\n",
            "Saving model, epoch: 12079, train_loss: 0.8988450169563293, val_loss: 0.865440845489502\n",
            "Saving model, epoch: 12080, train_loss: 0.8988428711891174, val_loss: 0.8654395341873169\n",
            "Saving model, epoch: 12081, train_loss: 0.8988407850265503, val_loss: 0.8654386401176453\n",
            "Saving model, epoch: 12082, train_loss: 0.8988382816314697, val_loss: 0.8654369115829468\n",
            "Saving model, epoch: 12083, train_loss: 0.8988361358642578, val_loss: 0.865436315536499\n",
            "Saving model, epoch: 12084, train_loss: 0.8988337516784668, val_loss: 0.8654345870018005\n",
            "Saving model, epoch: 12085, train_loss: 0.8988316655158997, val_loss: 0.8654336929321289\n",
            "Saving model, epoch: 12086, train_loss: 0.8988295197486877, val_loss: 0.8654319047927856\n",
            "Saving model, epoch: 12087, train_loss: 0.8988270163536072, val_loss: 0.8654311299324036\n",
            "Saving model, epoch: 12088, train_loss: 0.8988248109817505, val_loss: 0.8654299974441528\n",
            "Saving model, epoch: 12089, train_loss: 0.8988227248191833, val_loss: 0.8654284477233887\n",
            "Saving model, epoch: 12090, train_loss: 0.8988204002380371, val_loss: 0.8654274344444275\n",
            "Saving model, epoch: 12091, train_loss: 0.8988180756568909, val_loss: 0.8654260635375977\n",
            "Saving model, epoch: 12092, train_loss: 0.8988160490989685, val_loss: 0.8654248714447021\n",
            "Saving model, epoch: 12093, train_loss: 0.8988136649131775, val_loss: 0.865423321723938\n",
            "Saving model, epoch: 12094, train_loss: 0.8988112211227417, val_loss: 0.8654226660728455\n",
            "Saving model, epoch: 12095, train_loss: 0.8988092541694641, val_loss: 0.8654220104217529\n",
            "Saving model, epoch: 12096, train_loss: 0.8988070487976074, val_loss: 0.8654204607009888\n",
            "Saving model, epoch: 12097, train_loss: 0.8988047242164612, val_loss: 0.8654192686080933\n",
            "Saving model, epoch: 12098, train_loss: 0.8988023996353149, val_loss: 0.865418016910553\n",
            "Saving model, epoch: 12099, train_loss: 0.8988001942634583, val_loss: 0.8654162287712097\n",
            "Saving model, epoch: 12100, train_loss: 0.8987979888916016, val_loss: 0.8654153347015381\n",
            "epoch: 12101, train_loss: 0.8987956643104553, val_loss: 0.8654134273529053\n",
            "Saving model, epoch: 12101, train_loss: 0.8987956643104553, val_loss: 0.8654134273529053\n",
            "Saving model, epoch: 12102, train_loss: 0.8987935781478882, val_loss: 0.8654133081436157\n",
            "Saving model, epoch: 12103, train_loss: 0.8987912535667419, val_loss: 0.8654118180274963\n",
            "Saving model, epoch: 12104, train_loss: 0.8987889289855957, val_loss: 0.865410327911377\n",
            "Saving model, epoch: 12105, train_loss: 0.898786723613739, val_loss: 0.8654091954231262\n",
            "Saving model, epoch: 12106, train_loss: 0.898784339427948, val_loss: 0.8654077649116516\n",
            "Saving model, epoch: 12107, train_loss: 0.8987821936607361, val_loss: 0.8654066324234009\n",
            "Saving model, epoch: 12108, train_loss: 0.8987798690795898, val_loss: 0.8654053807258606\n",
            "Saving model, epoch: 12109, train_loss: 0.8987776041030884, val_loss: 0.8654041290283203\n",
            "Saving model, epoch: 12110, train_loss: 0.8987752795219421, val_loss: 0.8654027581214905\n",
            "Saving model, epoch: 12111, train_loss: 0.8987730741500854, val_loss: 0.8654014468193054\n",
            "Saving model, epoch: 12112, train_loss: 0.8987706303596497, val_loss: 0.865400493144989\n",
            "Saving model, epoch: 12113, train_loss: 0.898768424987793, val_loss: 0.8653992414474487\n",
            "Saving model, epoch: 12114, train_loss: 0.8987663388252258, val_loss: 0.8653980493545532\n",
            "Saving model, epoch: 12115, train_loss: 0.8987641334533691, val_loss: 0.8653965592384338\n",
            "Saving model, epoch: 12116, train_loss: 0.8987618088722229, val_loss: 0.8653952479362488\n",
            "Saving model, epoch: 12117, train_loss: 0.8987594246864319, val_loss: 0.8653943538665771\n",
            "Saving model, epoch: 12118, train_loss: 0.8987572193145752, val_loss: 0.8653930425643921\n",
            "Saving model, epoch: 12119, train_loss: 0.8987549543380737, val_loss: 0.8653919100761414\n",
            "Saving model, epoch: 12120, train_loss: 0.898752748966217, val_loss: 0.8653905987739563\n",
            "Saving model, epoch: 12121, train_loss: 0.898750364780426, val_loss: 0.8653894066810608\n",
            "Saving model, epoch: 12122, train_loss: 0.8987481594085693, val_loss: 0.8653882741928101\n",
            "Saving model, epoch: 12123, train_loss: 0.8987459540367126, val_loss: 0.8653870224952698\n",
            "Saving model, epoch: 12124, train_loss: 0.8987435102462769, val_loss: 0.8653860092163086\n",
            "Saving model, epoch: 12125, train_loss: 0.8987413048744202, val_loss: 0.8653843998908997\n",
            "Saving model, epoch: 12126, train_loss: 0.8987390995025635, val_loss: 0.865383505821228\n",
            "Saving model, epoch: 12127, train_loss: 0.8987367153167725, val_loss: 0.865382194519043\n",
            "Saving model, epoch: 12128, train_loss: 0.8987345099449158, val_loss: 0.865381121635437\n",
            "Saving model, epoch: 12129, train_loss: 0.8987323045730591, val_loss: 0.8653794527053833\n",
            "Saving model, epoch: 12130, train_loss: 0.8987299799919128, val_loss: 0.8653783202171326\n",
            "Saving model, epoch: 12131, train_loss: 0.8987276554107666, val_loss: 0.8653771281242371\n",
            "Saving model, epoch: 12132, train_loss: 0.8987253308296204, val_loss: 0.8653757572174072\n",
            "Saving model, epoch: 12133, train_loss: 0.8987230062484741, val_loss: 0.8653745651245117\n",
            "Saving model, epoch: 12134, train_loss: 0.8987208008766174, val_loss: 0.8653737902641296\n",
            "Saving model, epoch: 12135, train_loss: 0.8987184166908264, val_loss: 0.8653723001480103\n",
            "Saving model, epoch: 12136, train_loss: 0.8987162113189697, val_loss: 0.8653706908226013\n",
            "Saving model, epoch: 12137, train_loss: 0.898714005947113, val_loss: 0.8653693795204163\n",
            "Saving model, epoch: 12138, train_loss: 0.8987115621566772, val_loss: 0.8653681874275208\n",
            "Saving model, epoch: 12139, train_loss: 0.8987093567848206, val_loss: 0.8653672337532043\n",
            "Saving model, epoch: 12140, train_loss: 0.8987070322036743, val_loss: 0.8653658628463745\n",
            "Saving model, epoch: 12141, train_loss: 0.8987047672271729, val_loss: 0.8653643727302551\n",
            "Saving model, epoch: 12142, train_loss: 0.8987025618553162, val_loss: 0.8653635382652283\n",
            "Saving model, epoch: 12143, train_loss: 0.8987002372741699, val_loss: 0.8653622269630432\n",
            "Saving model, epoch: 12144, train_loss: 0.8986980319023132, val_loss: 0.8653608560562134\n",
            "Saving model, epoch: 12145, train_loss: 0.8986955881118774, val_loss: 0.8653596043586731\n",
            "Saving model, epoch: 12146, train_loss: 0.8986933827400208, val_loss: 0.8653583526611328\n",
            "Saving model, epoch: 12147, train_loss: 0.8986909985542297, val_loss: 0.8653570413589478\n",
            "Saving model, epoch: 12148, train_loss: 0.8986886739730835, val_loss: 0.8653560280799866\n",
            "Saving model, epoch: 12149, train_loss: 0.8986864686012268, val_loss: 0.8653547167778015\n",
            "Saving model, epoch: 12150, train_loss: 0.898684024810791, val_loss: 0.8653537034988403\n",
            "Saving model, epoch: 12151, train_loss: 0.8986818194389343, val_loss: 0.865351676940918\n",
            "Saving model, epoch: 12152, train_loss: 0.8986796140670776, val_loss: 0.8653507828712463\n",
            "Saving model, epoch: 12153, train_loss: 0.8986772298812866, val_loss: 0.8653497695922852\n",
            "Saving model, epoch: 12154, train_loss: 0.8986747860908508, val_loss: 0.8653485774993896\n",
            "Saving model, epoch: 12155, train_loss: 0.8986725807189941, val_loss: 0.8653474450111389\n",
            "Saving model, epoch: 12156, train_loss: 0.8986701965332031, val_loss: 0.8653464317321777\n",
            "Saving model, epoch: 12157, train_loss: 0.8986680507659912, val_loss: 0.8653451204299927\n",
            "Saving model, epoch: 12158, train_loss: 0.8986657857894897, val_loss: 0.8653430938720703\n",
            "Saving model, epoch: 12159, train_loss: 0.898663341999054, val_loss: 0.8653421998023987\n",
            "Saving model, epoch: 12160, train_loss: 0.8986609578132629, val_loss: 0.865341305732727\n",
            "Saving model, epoch: 12161, train_loss: 0.8986587524414062, val_loss: 0.8653398156166077\n",
            "Saving model, epoch: 12162, train_loss: 0.8986565470695496, val_loss: 0.8653383851051331\n",
            "Saving model, epoch: 12163, train_loss: 0.8986541032791138, val_loss: 0.8653375506401062\n",
            "Saving model, epoch: 12164, train_loss: 0.8986518979072571, val_loss: 0.8653357625007629\n",
            "Saving model, epoch: 12165, train_loss: 0.8986495137214661, val_loss: 0.8653345704078674\n",
            "Saving model, epoch: 12166, train_loss: 0.8986471891403198, val_loss: 0.8653337359428406\n",
            "Saving model, epoch: 12167, train_loss: 0.8986448645591736, val_loss: 0.8653321266174316\n",
            "Saving model, epoch: 12168, train_loss: 0.8986426591873169, val_loss: 0.8653310537338257\n",
            "Saving model, epoch: 12169, train_loss: 0.8986401557922363, val_loss: 0.8653292655944824\n",
            "Saving model, epoch: 12170, train_loss: 0.8986378312110901, val_loss: 0.8653286099433899\n",
            "Saving model, epoch: 12171, train_loss: 0.8986358046531677, val_loss: 0.8653270602226257\n",
            "Saving model, epoch: 12172, train_loss: 0.8986333012580872, val_loss: 0.8653261661529541\n",
            "Saving model, epoch: 12173, train_loss: 0.8986309170722961, val_loss: 0.8653250932693481\n",
            "Saving model, epoch: 12174, train_loss: 0.8986285924911499, val_loss: 0.8653233647346497\n",
            "Saving model, epoch: 12175, train_loss: 0.8986263871192932, val_loss: 0.8653220534324646\n",
            "Saving model, epoch: 12176, train_loss: 0.898624062538147, val_loss: 0.865321159362793\n",
            "Saving model, epoch: 12177, train_loss: 0.898621678352356, val_loss: 0.8653199672698975\n",
            "Saving model, epoch: 12178, train_loss: 0.8986193537712097, val_loss: 0.8653183579444885\n",
            "Saving model, epoch: 12179, train_loss: 0.8986170291900635, val_loss: 0.8653167486190796\n",
            "Saving model, epoch: 12180, train_loss: 0.8986147046089172, val_loss: 0.8653157353401184\n",
            "Saving model, epoch: 12181, train_loss: 0.8986123204231262, val_loss: 0.8653146028518677\n",
            "Saving model, epoch: 12182, train_loss: 0.8986101150512695, val_loss: 0.8653131127357483\n",
            "Saving model, epoch: 12183, train_loss: 0.8986076712608337, val_loss: 0.8653119802474976\n",
            "Saving model, epoch: 12184, train_loss: 0.898605465888977, val_loss: 0.8653104305267334\n",
            "Saving model, epoch: 12185, train_loss: 0.8986029624938965, val_loss: 0.8653097748756409\n",
            "Saving model, epoch: 12186, train_loss: 0.8986006379127502, val_loss: 0.8653084635734558\n",
            "Saving model, epoch: 12187, train_loss: 0.8985984325408936, val_loss: 0.8653069138526917\n",
            "Saving model, epoch: 12188, train_loss: 0.8985958099365234, val_loss: 0.86530601978302\n",
            "Saving model, epoch: 12189, train_loss: 0.8985936045646667, val_loss: 0.8653046488761902\n",
            "Saving model, epoch: 12190, train_loss: 0.8985913395881653, val_loss: 0.8653030395507812\n",
            "Saving model, epoch: 12191, train_loss: 0.898589015007019, val_loss: 0.8653020858764648\n",
            "Saving model, epoch: 12192, train_loss: 0.8985865712165833, val_loss: 0.8653006553649902\n",
            "Saving model, epoch: 12193, train_loss: 0.8985843658447266, val_loss: 0.8652993440628052\n",
            "Saving model, epoch: 12194, train_loss: 0.8985819816589355, val_loss: 0.8652978539466858\n",
            "Saving model, epoch: 12195, train_loss: 0.8985796570777893, val_loss: 0.8652969598770142\n",
            "Saving model, epoch: 12196, train_loss: 0.8985773324966431, val_loss: 0.8652950525283813\n",
            "Saving model, epoch: 12197, train_loss: 0.898574948310852, val_loss: 0.8652939796447754\n",
            "Saving model, epoch: 12198, train_loss: 0.8985725045204163, val_loss: 0.865293025970459\n",
            "Saving model, epoch: 12199, train_loss: 0.8985702991485596, val_loss: 0.8652918934822083\n",
            "Saving model, epoch: 12200, train_loss: 0.898567795753479, val_loss: 0.8652902841567993\n",
            "epoch: 12201, train_loss: 0.8985657095909119, val_loss: 0.8652889728546143\n",
            "Saving model, epoch: 12201, train_loss: 0.8985657095909119, val_loss: 0.8652889728546143\n",
            "Saving model, epoch: 12202, train_loss: 0.8985632658004761, val_loss: 0.8652878403663635\n",
            "Saving model, epoch: 12203, train_loss: 0.8985608816146851, val_loss: 0.8652865886688232\n",
            "Saving model, epoch: 12204, train_loss: 0.8985584378242493, val_loss: 0.865285336971283\n",
            "Saving model, epoch: 12205, train_loss: 0.8985561728477478, val_loss: 0.8652840852737427\n",
            "Saving model, epoch: 12206, train_loss: 0.898553729057312, val_loss: 0.8652825355529785\n",
            "Saving model, epoch: 12207, train_loss: 0.8985515236854553, val_loss: 0.8652817010879517\n",
            "Saving model, epoch: 12208, train_loss: 0.8985489010810852, val_loss: 0.8652805685997009\n",
            "Saving model, epoch: 12209, train_loss: 0.8985466957092285, val_loss: 0.865278959274292\n",
            "Saving model, epoch: 12210, train_loss: 0.8985444903373718, val_loss: 0.8652779459953308\n",
            "Saving model, epoch: 12211, train_loss: 0.8985419869422913, val_loss: 0.8652767539024353\n",
            "Saving model, epoch: 12212, train_loss: 0.898539662361145, val_loss: 0.8652756214141846\n",
            "Saving model, epoch: 12213, train_loss: 0.8985375761985779, val_loss: 0.865273654460907\n",
            "Saving model, epoch: 12214, train_loss: 0.8985348343849182, val_loss: 0.8652727007865906\n",
            "Saving model, epoch: 12215, train_loss: 0.8985325694084167, val_loss: 0.8652713894844055\n",
            "Saving model, epoch: 12216, train_loss: 0.898530125617981, val_loss: 0.8652699589729309\n",
            "Saving model, epoch: 12217, train_loss: 0.8985279202461243, val_loss: 0.8652691841125488\n",
            "Saving model, epoch: 12218, train_loss: 0.8985255360603333, val_loss: 0.8652674555778503\n",
            "Saving model, epoch: 12219, train_loss: 0.8985230922698975, val_loss: 0.8652663826942444\n",
            "Saving model, epoch: 12220, train_loss: 0.8985208868980408, val_loss: 0.865264892578125\n",
            "Saving model, epoch: 12221, train_loss: 0.8985185623168945, val_loss: 0.8652635216712952\n",
            "Saving model, epoch: 12222, train_loss: 0.898516058921814, val_loss: 0.8652628064155579\n",
            "Saving model, epoch: 12223, train_loss: 0.8985137343406677, val_loss: 0.8652609586715698\n",
            "Saving model, epoch: 12224, train_loss: 0.8985114693641663, val_loss: 0.8652597069740295\n",
            "Saving model, epoch: 12225, train_loss: 0.8985089063644409, val_loss: 0.8652586936950684\n",
            "Saving model, epoch: 12226, train_loss: 0.8985065221786499, val_loss: 0.865257203578949\n",
            "Saving model, epoch: 12227, train_loss: 0.8985044360160828, val_loss: 0.8652560710906982\n",
            "Saving model, epoch: 12228, train_loss: 0.8985018134117126, val_loss: 0.8652547597885132\n",
            "Saving model, epoch: 12229, train_loss: 0.8984993696212769, val_loss: 0.8652533888816833\n",
            "Saving model, epoch: 12230, train_loss: 0.8984971046447754, val_loss: 0.8652517795562744\n",
            "Saving model, epoch: 12231, train_loss: 0.8984946608543396, val_loss: 0.8652509450912476\n",
            "Saving model, epoch: 12232, train_loss: 0.8984922766685486, val_loss: 0.8652492165565491\n",
            "Saving model, epoch: 12233, train_loss: 0.8984900712966919, val_loss: 0.8652480244636536\n",
            "Saving model, epoch: 12234, train_loss: 0.8984876275062561, val_loss: 0.8652467131614685\n",
            "Saving model, epoch: 12235, train_loss: 0.8984852433204651, val_loss: 0.8652456402778625\n",
            "Saving model, epoch: 12236, train_loss: 0.8984826803207397, val_loss: 0.8652440905570984\n",
            "Saving model, epoch: 12237, train_loss: 0.8984804749488831, val_loss: 0.8652436137199402\n",
            "Saving model, epoch: 12238, train_loss: 0.898478090763092, val_loss: 0.8652418255805969\n",
            "Saving model, epoch: 12239, train_loss: 0.8984754681587219, val_loss: 0.8652405738830566\n",
            "Saving model, epoch: 12240, train_loss: 0.8984732627868652, val_loss: 0.8652397394180298\n",
            "Saving model, epoch: 12241, train_loss: 0.898470938205719, val_loss: 0.8652376532554626\n",
            "Saving model, epoch: 12242, train_loss: 0.898468554019928, val_loss: 0.865236759185791\n",
            "Saving model, epoch: 12243, train_loss: 0.8984660506248474, val_loss: 0.8652352094650269\n",
            "Saving model, epoch: 12244, train_loss: 0.8984636068344116, val_loss: 0.865233838558197\n",
            "Saving model, epoch: 12245, train_loss: 0.8984614014625549, val_loss: 0.8652328252792358\n",
            "Saving model, epoch: 12246, train_loss: 0.8984590172767639, val_loss: 0.8652316331863403\n",
            "Saving model, epoch: 12247, train_loss: 0.8984566926956177, val_loss: 0.8652308583259583\n",
            "Saving model, epoch: 12248, train_loss: 0.8984541893005371, val_loss: 0.8652287125587463\n",
            "Saving model, epoch: 12249, train_loss: 0.8984518647193909, val_loss: 0.8652276396751404\n",
            "Saving model, epoch: 12250, train_loss: 0.8984493613243103, val_loss: 0.8652260899543762\n",
            "Saving model, epoch: 12251, train_loss: 0.8984469175338745, val_loss: 0.8652249574661255\n",
            "Saving model, epoch: 12252, train_loss: 0.8984447121620178, val_loss: 0.8652235269546509\n",
            "Saving model, epoch: 12253, train_loss: 0.8984421491622925, val_loss: 0.8652224540710449\n",
            "Saving model, epoch: 12254, train_loss: 0.898439884185791, val_loss: 0.8652209043502808\n",
            "Saving model, epoch: 12255, train_loss: 0.8984375, val_loss: 0.86521977186203\n",
            "Saving model, epoch: 12256, train_loss: 0.8984349966049194, val_loss: 0.8652182817459106\n",
            "Saving model, epoch: 12257, train_loss: 0.8984326720237732, val_loss: 0.8652172684669495\n",
            "Saving model, epoch: 12258, train_loss: 0.8984300494194031, val_loss: 0.865215539932251\n",
            "Saving model, epoch: 12259, train_loss: 0.8984278440475464, val_loss: 0.8652145862579346\n",
            "Saving model, epoch: 12260, train_loss: 0.8984254598617554, val_loss: 0.8652134537696838\n",
            "Saving model, epoch: 12261, train_loss: 0.8984231352806091, val_loss: 0.8652119040489197\n",
            "Saving model, epoch: 12262, train_loss: 0.8984203934669495, val_loss: 0.8652102947235107\n",
            "Saving model, epoch: 12263, train_loss: 0.8984181880950928, val_loss: 0.8652095198631287\n",
            "Saving model, epoch: 12264, train_loss: 0.8984156847000122, val_loss: 0.8652082085609436\n",
            "Saving model, epoch: 12265, train_loss: 0.8984133005142212, val_loss: 0.865206778049469\n",
            "Saving model, epoch: 12266, train_loss: 0.898410975933075, val_loss: 0.8652054667472839\n",
            "Saving model, epoch: 12267, train_loss: 0.8984085917472839, val_loss: 0.8652040362358093\n",
            "Saving model, epoch: 12268, train_loss: 0.8984060287475586, val_loss: 0.8652026057243347\n",
            "Saving model, epoch: 12269, train_loss: 0.8984038233757019, val_loss: 0.8652016520500183\n",
            "Saving model, epoch: 12270, train_loss: 0.8984013199806213, val_loss: 0.8652003407478333\n",
            "Saving model, epoch: 12271, train_loss: 0.8983989357948303, val_loss: 0.865199089050293\n",
            "Saving model, epoch: 12272, train_loss: 0.8983964920043945, val_loss: 0.8651975393295288\n",
            "Saving model, epoch: 12273, train_loss: 0.898393988609314, val_loss: 0.8651963472366333\n",
            "Saving model, epoch: 12274, train_loss: 0.8983916640281677, val_loss: 0.8651948571205139\n",
            "Saving model, epoch: 12275, train_loss: 0.8983891010284424, val_loss: 0.8651935458183289\n",
            "Saving model, epoch: 12276, train_loss: 0.89838707447052, val_loss: 0.8651921153068542\n",
            "Saving model, epoch: 12277, train_loss: 0.8983844518661499, val_loss: 0.8651911020278931\n",
            "Saving model, epoch: 12278, train_loss: 0.8983820676803589, val_loss: 0.8651897311210632\n",
            "Saving model, epoch: 12279, train_loss: 0.8983796238899231, val_loss: 0.8651880621910095\n",
            "Saving model, epoch: 12280, train_loss: 0.898377001285553, val_loss: 0.865186870098114\n",
            "Saving model, epoch: 12281, train_loss: 0.8983747959136963, val_loss: 0.8651856184005737\n",
            "Saving model, epoch: 12282, train_loss: 0.8983721733093262, val_loss: 0.8651843070983887\n",
            "Saving model, epoch: 12283, train_loss: 0.8983697891235352, val_loss: 0.8651831150054932\n",
            "Saving model, epoch: 12284, train_loss: 0.8983673453330994, val_loss: 0.865181565284729\n",
            "Saving model, epoch: 12285, train_loss: 0.8983649611473083, val_loss: 0.8651801347732544\n",
            "Saving model, epoch: 12286, train_loss: 0.8983625769615173, val_loss: 0.8651790022850037\n",
            "Saving model, epoch: 12287, train_loss: 0.8983601331710815, val_loss: 0.8651778697967529\n",
            "Saving model, epoch: 12288, train_loss: 0.898357629776001, val_loss: 0.8651766180992126\n",
            "Saving model, epoch: 12289, train_loss: 0.8983551263809204, val_loss: 0.8651752471923828\n",
            "Saving model, epoch: 12290, train_loss: 0.8983529210090637, val_loss: 0.865173876285553\n",
            "Saving model, epoch: 12291, train_loss: 0.8983504772186279, val_loss: 0.8651723861694336\n",
            "Saving model, epoch: 12292, train_loss: 0.8983478546142578, val_loss: 0.8651708960533142\n",
            "Saving model, epoch: 12293, train_loss: 0.8983455896377563, val_loss: 0.8651700615882874\n",
            "Saving model, epoch: 12294, train_loss: 0.8983431458473206, val_loss: 0.865168571472168\n",
            "Saving model, epoch: 12295, train_loss: 0.8983407616615295, val_loss: 0.8651672601699829\n",
            "Saving model, epoch: 12296, train_loss: 0.8983384370803833, val_loss: 0.8651660680770874\n",
            "Saving model, epoch: 12297, train_loss: 0.8983356952667236, val_loss: 0.8651645183563232\n",
            "Saving model, epoch: 12298, train_loss: 0.8983333110809326, val_loss: 0.8651632070541382\n",
            "Saving model, epoch: 12299, train_loss: 0.898330807685852, val_loss: 0.8651620149612427\n",
            "Saving model, epoch: 12300, train_loss: 0.8983284831047058, val_loss: 0.8651605248451233\n",
            "epoch: 12301, train_loss: 0.8983259797096252, val_loss: 0.8651594519615173\n",
            "Saving model, epoch: 12301, train_loss: 0.8983259797096252, val_loss: 0.8651594519615173\n",
            "Saving model, epoch: 12302, train_loss: 0.8983234763145447, val_loss: 0.8651574850082397\n",
            "Saving model, epoch: 12303, train_loss: 0.8983209729194641, val_loss: 0.8651565909385681\n",
            "Saving model, epoch: 12304, train_loss: 0.8983187675476074, val_loss: 0.8651556372642517\n",
            "Saving model, epoch: 12305, train_loss: 0.8983163237571716, val_loss: 0.8651544451713562\n",
            "Saving model, epoch: 12306, train_loss: 0.8983137011528015, val_loss: 0.8651527166366577\n",
            "Saving model, epoch: 12307, train_loss: 0.8983114957809448, val_loss: 0.8651515245437622\n",
            "Saving model, epoch: 12308, train_loss: 0.8983088731765747, val_loss: 0.8651500344276428\n",
            "Saving model, epoch: 12309, train_loss: 0.8983063697814941, val_loss: 0.865148663520813\n",
            "Saving model, epoch: 12310, train_loss: 0.8983040452003479, val_loss: 0.8651476502418518\n",
            "Saving model, epoch: 12311, train_loss: 0.8983016610145569, val_loss: 0.8651459217071533\n",
            "Saving model, epoch: 12312, train_loss: 0.8982990384101868, val_loss: 0.8651447296142578\n",
            "Saving model, epoch: 12313, train_loss: 0.8982966542243958, val_loss: 0.8651432991027832\n",
            "Saving model, epoch: 12314, train_loss: 0.8982941508293152, val_loss: 0.8651419878005981\n",
            "Saving model, epoch: 12315, train_loss: 0.898291826248169, val_loss: 0.8651408553123474\n",
            "Saving model, epoch: 12316, train_loss: 0.8982893228530884, val_loss: 0.8651394844055176\n",
            "Saving model, epoch: 12317, train_loss: 0.8982867002487183, val_loss: 0.8651379346847534\n",
            "Saving model, epoch: 12318, train_loss: 0.898284375667572, val_loss: 0.8651366829872131\n",
            "Saving model, epoch: 12319, train_loss: 0.8982817530632019, val_loss: 0.8651351928710938\n",
            "Saving model, epoch: 12320, train_loss: 0.8982793688774109, val_loss: 0.8651342391967773\n",
            "Saving model, epoch: 12321, train_loss: 0.8982769250869751, val_loss: 0.8651328682899475\n",
            "Saving model, epoch: 12322, train_loss: 0.8982745409011841, val_loss: 0.8651315569877625\n",
            "Saving model, epoch: 12323, train_loss: 0.8982717990875244, val_loss: 0.8651300072669983\n",
            "Saving model, epoch: 12324, train_loss: 0.8982695937156677, val_loss: 0.8651286959648132\n",
            "Saving model, epoch: 12325, train_loss: 0.8982670903205872, val_loss: 0.8651273846626282\n",
            "Saving model, epoch: 12326, train_loss: 0.898264467716217, val_loss: 0.8651258945465088\n",
            "Saving model, epoch: 12327, train_loss: 0.898262083530426, val_loss: 0.8651249408721924\n",
            "Saving model, epoch: 12328, train_loss: 0.8982595801353455, val_loss: 0.8651235699653625\n",
            "Saving model, epoch: 12329, train_loss: 0.8982570767402649, val_loss: 0.8651222586631775\n",
            "Saving model, epoch: 12330, train_loss: 0.8982547521591187, val_loss: 0.8651207685470581\n",
            "Saving model, epoch: 12331, train_loss: 0.8982522487640381, val_loss: 0.8651195764541626\n",
            "Saving model, epoch: 12332, train_loss: 0.8982498049736023, val_loss: 0.8651180267333984\n",
            "Saving model, epoch: 12333, train_loss: 0.8982473015785217, val_loss: 0.8651168942451477\n",
            "Saving model, epoch: 12334, train_loss: 0.8982447981834412, val_loss: 0.8651158213615417\n",
            "Saving model, epoch: 12335, train_loss: 0.8982424139976501, val_loss: 0.8651137351989746\n",
            "Saving model, epoch: 12336, train_loss: 0.89823979139328, val_loss: 0.8651130199432373\n",
            "Saving model, epoch: 12337, train_loss: 0.8982373476028442, val_loss: 0.8651109933853149\n",
            "Saving model, epoch: 12338, train_loss: 0.8982348442077637, val_loss: 0.8651100993156433\n",
            "Saving model, epoch: 12339, train_loss: 0.8982324600219727, val_loss: 0.8651090860366821\n",
            "Saving model, epoch: 12340, train_loss: 0.8982299566268921, val_loss: 0.8651073575019836\n",
            "Saving model, epoch: 12341, train_loss: 0.8982275128364563, val_loss: 0.8651058673858643\n",
            "Saving model, epoch: 12342, train_loss: 0.8982248902320862, val_loss: 0.8651044368743896\n",
            "Saving model, epoch: 12343, train_loss: 0.8982223868370056, val_loss: 0.8651034832000732\n",
            "Saving model, epoch: 12344, train_loss: 0.8982200622558594, val_loss: 0.8651016354560852\n",
            "Saving model, epoch: 12345, train_loss: 0.898217499256134, val_loss: 0.8651003241539001\n",
            "Saving model, epoch: 12346, train_loss: 0.8982150554656982, val_loss: 0.8650992512702942\n",
            "Saving model, epoch: 12347, train_loss: 0.8982126712799072, val_loss: 0.865098237991333\n",
            "Saving model, epoch: 12348, train_loss: 0.8982101082801819, val_loss: 0.8650963306427002\n",
            "Saving model, epoch: 12349, train_loss: 0.8982075452804565, val_loss: 0.8650953769683838\n",
            "Saving model, epoch: 12350, train_loss: 0.8982052206993103, val_loss: 0.8650938868522644\n",
            "Saving model, epoch: 12351, train_loss: 0.8982025980949402, val_loss: 0.8650925755500793\n",
            "Saving model, epoch: 12352, train_loss: 0.8981999754905701, val_loss: 0.86509108543396\n",
            "Saving model, epoch: 12353, train_loss: 0.898197591304779, val_loss: 0.8650899529457092\n",
            "Saving model, epoch: 12354, train_loss: 0.8981951475143433, val_loss: 0.8650885820388794\n",
            "Saving model, epoch: 12355, train_loss: 0.8981925249099731, val_loss: 0.8650868535041809\n",
            "Saving model, epoch: 12356, train_loss: 0.8981899619102478, val_loss: 0.8650853037834167\n",
            "Saving model, epoch: 12357, train_loss: 0.898187518119812, val_loss: 0.8650845885276794\n",
            "Saving model, epoch: 12358, train_loss: 0.898185133934021, val_loss: 0.8650829792022705\n",
            "Saving model, epoch: 12359, train_loss: 0.8981826901435852, val_loss: 0.8650816679000854\n",
            "Saving model, epoch: 12360, train_loss: 0.8981800675392151, val_loss: 0.8650801777839661\n",
            "Saving model, epoch: 12361, train_loss: 0.8981776833534241, val_loss: 0.8650790452957153\n",
            "Saving model, epoch: 12362, train_loss: 0.8981750011444092, val_loss: 0.865077793598175\n",
            "Saving model, epoch: 12363, train_loss: 0.8981726765632629, val_loss: 0.8650766015052795\n",
            "Saving model, epoch: 12364, train_loss: 0.8981700539588928, val_loss: 0.865074634552002\n",
            "Saving model, epoch: 12365, train_loss: 0.898167610168457, val_loss: 0.8650733232498169\n",
            "Saving model, epoch: 12366, train_loss: 0.8981650471687317, val_loss: 0.8650721907615662\n",
            "Saving model, epoch: 12367, train_loss: 0.8981624841690063, val_loss: 0.8650711178779602\n",
            "Saving model, epoch: 12368, train_loss: 0.8981599807739258, val_loss: 0.8650694489479065\n",
            "Saving model, epoch: 12369, train_loss: 0.8981574773788452, val_loss: 0.8650679588317871\n",
            "Saving model, epoch: 12370, train_loss: 0.8981550931930542, val_loss: 0.8650667667388916\n",
            "Saving model, epoch: 12371, train_loss: 0.8981525897979736, val_loss: 0.8650650978088379\n",
            "Saving model, epoch: 12372, train_loss: 0.898149847984314, val_loss: 0.8650638461112976\n",
            "Saving model, epoch: 12373, train_loss: 0.8981475234031677, val_loss: 0.8650621771812439\n",
            "Saving model, epoch: 12374, train_loss: 0.8981449604034424, val_loss: 0.8650611639022827\n",
            "Saving model, epoch: 12375, train_loss: 0.8981425166130066, val_loss: 0.865060031414032\n",
            "Saving model, epoch: 12376, train_loss: 0.8981398940086365, val_loss: 0.8650588989257812\n",
            "Saving model, epoch: 12377, train_loss: 0.8981375098228455, val_loss: 0.8650568127632141\n",
            "Saving model, epoch: 12378, train_loss: 0.8981348872184753, val_loss: 0.8650557994842529\n",
            "Saving model, epoch: 12379, train_loss: 0.8981323838233948, val_loss: 0.8650544285774231\n",
            "Saving model, epoch: 12380, train_loss: 0.8981298804283142, val_loss: 0.8650532960891724\n",
            "Saving model, epoch: 12381, train_loss: 0.8981273770332336, val_loss: 0.8650519251823425\n",
            "Saving model, epoch: 12382, train_loss: 0.8981245160102844, val_loss: 0.8650504946708679\n",
            "Saving model, epoch: 12383, train_loss: 0.8981223106384277, val_loss: 0.8650491237640381\n",
            "Saving model, epoch: 12384, train_loss: 0.8981196284294128, val_loss: 0.8650478720664978\n",
            "Saving model, epoch: 12385, train_loss: 0.898117184638977, val_loss: 0.8650463819503784\n",
            "Saving model, epoch: 12386, train_loss: 0.898114800453186, val_loss: 0.8650450110435486\n",
            "Saving model, epoch: 12387, train_loss: 0.8981121778488159, val_loss: 0.8650438189506531\n",
            "Saving model, epoch: 12388, train_loss: 0.8981095552444458, val_loss: 0.8650422692298889\n",
            "Saving model, epoch: 12389, train_loss: 0.8981070518493652, val_loss: 0.8650400638580322\n",
            "Saving model, epoch: 12390, train_loss: 0.8981045484542847, val_loss: 0.8650391697883606\n",
            "Saving model, epoch: 12391, train_loss: 0.8981019258499146, val_loss: 0.8650381565093994\n",
            "Saving model, epoch: 12392, train_loss: 0.8980995416641235, val_loss: 0.8650367856025696\n",
            "Saving model, epoch: 12393, train_loss: 0.8980969786643982, val_loss: 0.8650354146957397\n",
            "Saving model, epoch: 12394, train_loss: 0.8980944156646729, val_loss: 0.865033745765686\n",
            "Saving model, epoch: 12395, train_loss: 0.8980919122695923, val_loss: 0.8650320768356323\n",
            "Saving model, epoch: 12396, train_loss: 0.8980892896652222, val_loss: 0.8650308847427368\n",
            "Saving model, epoch: 12397, train_loss: 0.8980869650840759, val_loss: 0.8650296330451965\n",
            "Saving model, epoch: 12398, train_loss: 0.8980843424797058, val_loss: 0.8650282025337219\n",
            "Saving model, epoch: 12399, train_loss: 0.8980816602706909, val_loss: 0.8650270700454712\n",
            "Saving model, epoch: 12400, train_loss: 0.8980792164802551, val_loss: 0.8650259971618652\n",
            "epoch: 12401, train_loss: 0.8980767130851746, val_loss: 0.8650243878364563\n",
            "Saving model, epoch: 12401, train_loss: 0.8980767130851746, val_loss: 0.8650243878364563\n",
            "Saving model, epoch: 12402, train_loss: 0.8980739712715149, val_loss: 0.8650228381156921\n",
            "Saving model, epoch: 12403, train_loss: 0.8980715870857239, val_loss: 0.8650217652320862\n",
            "Saving model, epoch: 12404, train_loss: 0.8980690836906433, val_loss: 0.8650205135345459\n",
            "Saving model, epoch: 12405, train_loss: 0.8980665802955627, val_loss: 0.8650187253952026\n",
            "Saving model, epoch: 12406, train_loss: 0.8980638384819031, val_loss: 0.8650175929069519\n",
            "Saving model, epoch: 12407, train_loss: 0.8980613350868225, val_loss: 0.8650159239768982\n",
            "Saving model, epoch: 12408, train_loss: 0.8980587124824524, val_loss: 0.8650146126747131\n",
            "Saving model, epoch: 12409, train_loss: 0.8980562090873718, val_loss: 0.8650133013725281\n",
            "Saving model, epoch: 12410, train_loss: 0.8980537056922913, val_loss: 0.8650120496749878\n",
            "Saving model, epoch: 12411, train_loss: 0.8980510830879211, val_loss: 0.8650100231170654\n",
            "Saving model, epoch: 12412, train_loss: 0.8980485796928406, val_loss: 0.8650096654891968\n",
            "Saving model, epoch: 12413, train_loss: 0.89804607629776, val_loss: 0.8650080561637878\n",
            "Saving model, epoch: 12414, train_loss: 0.8980436325073242, val_loss: 0.8650063276290894\n",
            "Saving model, epoch: 12415, train_loss: 0.8980409502983093, val_loss: 0.8650052547454834\n",
            "Saving model, epoch: 12416, train_loss: 0.8980384469032288, val_loss: 0.8650035858154297\n",
            "Saving model, epoch: 12417, train_loss: 0.8980359435081482, val_loss: 0.8650020360946655\n",
            "Saving model, epoch: 12418, train_loss: 0.8980333209037781, val_loss: 0.8650007247924805\n",
            "Saving model, epoch: 12419, train_loss: 0.8980308175086975, val_loss: 0.8649994730949402\n",
            "Saving model, epoch: 12420, train_loss: 0.8980281949043274, val_loss: 0.8649983406066895\n",
            "Saving model, epoch: 12421, train_loss: 0.8980254530906677, val_loss: 0.8649969696998596\n",
            "Saving model, epoch: 12422, train_loss: 0.8980231881141663, val_loss: 0.8649953603744507\n",
            "Saving model, epoch: 12423, train_loss: 0.898020327091217, val_loss: 0.8649937510490417\n",
            "Saving model, epoch: 12424, train_loss: 0.8980177640914917, val_loss: 0.864992618560791\n",
            "Saving model, epoch: 12425, train_loss: 0.8980153203010559, val_loss: 0.8649910092353821\n",
            "Saving model, epoch: 12426, train_loss: 0.8980128169059753, val_loss: 0.864989697933197\n",
            "Saving model, epoch: 12427, train_loss: 0.8980103135108948, val_loss: 0.8649887442588806\n",
            "Saving model, epoch: 12428, train_loss: 0.8980076909065247, val_loss: 0.8649867177009583\n",
            "Saving model, epoch: 12429, train_loss: 0.898004949092865, val_loss: 0.8649854063987732\n",
            "Saving model, epoch: 12430, train_loss: 0.8980024456977844, val_loss: 0.8649840950965881\n",
            "Saving model, epoch: 12431, train_loss: 0.8979998826980591, val_loss: 0.8649826645851135\n",
            "Saving model, epoch: 12432, train_loss: 0.8979973196983337, val_loss: 0.8649814128875732\n",
            "Saving model, epoch: 12433, train_loss: 0.8979947566986084, val_loss: 0.8649802803993225\n",
            "Saving model, epoch: 12434, train_loss: 0.8979921340942383, val_loss: 0.8649786114692688\n",
            "Saving model, epoch: 12435, train_loss: 0.8979895114898682, val_loss: 0.8649774789810181\n",
            "Saving model, epoch: 12436, train_loss: 0.8979870080947876, val_loss: 0.8649758696556091\n",
            "Saving model, epoch: 12437, train_loss: 0.897984504699707, val_loss: 0.8649746179580688\n",
            "Saving model, epoch: 12438, train_loss: 0.8979819416999817, val_loss: 0.8649728298187256\n",
            "Saving model, epoch: 12439, train_loss: 0.8979793787002563, val_loss: 0.8649715781211853\n",
            "Saving model, epoch: 12440, train_loss: 0.8979766368865967, val_loss: 0.8649699687957764\n",
            "Saving model, epoch: 12441, train_loss: 0.8979742527008057, val_loss: 0.8649690747261047\n",
            "Saving model, epoch: 12442, train_loss: 0.8979716300964355, val_loss: 0.8649675846099854\n",
            "Saving model, epoch: 12443, train_loss: 0.8979690074920654, val_loss: 0.8649662733078003\n",
            "Saving model, epoch: 12444, train_loss: 0.8979663848876953, val_loss: 0.8649644255638123\n",
            "Saving model, epoch: 12445, train_loss: 0.8979637622833252, val_loss: 0.8649635910987854\n",
            "Saving model, epoch: 12446, train_loss: 0.8979612588882446, val_loss: 0.8649618625640869\n",
            "Saving model, epoch: 12447, train_loss: 0.8979586362838745, val_loss: 0.8649604916572571\n",
            "Saving model, epoch: 12448, train_loss: 0.8979559540748596, val_loss: 0.8649590015411377\n",
            "Saving model, epoch: 12449, train_loss: 0.897953450679779, val_loss: 0.8649575114250183\n",
            "Saving model, epoch: 12450, train_loss: 0.8979508280754089, val_loss: 0.8649560213088989\n",
            "Saving model, epoch: 12451, train_loss: 0.8979482054710388, val_loss: 0.8649551272392273\n",
            "Saving model, epoch: 12452, train_loss: 0.8979458212852478, val_loss: 0.8649535775184631\n",
            "Saving model, epoch: 12453, train_loss: 0.8979430794715881, val_loss: 0.8649522662162781\n",
            "Saving model, epoch: 12454, train_loss: 0.8979405760765076, val_loss: 0.8649508953094482\n",
            "Saving model, epoch: 12455, train_loss: 0.8979379534721375, val_loss: 0.8649488687515259\n",
            "Saving model, epoch: 12456, train_loss: 0.8979353308677673, val_loss: 0.8649479746818542\n",
            "Saving model, epoch: 12457, train_loss: 0.897932767868042, val_loss: 0.8649466037750244\n",
            "Saving model, epoch: 12458, train_loss: 0.8979300260543823, val_loss: 0.864945113658905\n",
            "Saving model, epoch: 12459, train_loss: 0.8979276418685913, val_loss: 0.8649436831474304\n",
            "Saving model, epoch: 12460, train_loss: 0.8979247808456421, val_loss: 0.8649421334266663\n",
            "Saving model, epoch: 12461, train_loss: 0.8979222774505615, val_loss: 0.8649402856826782\n",
            "Saving model, epoch: 12462, train_loss: 0.8979196548461914, val_loss: 0.8649401068687439\n",
            "Saving model, epoch: 12463, train_loss: 0.8979169726371765, val_loss: 0.864937961101532\n",
            "Saving model, epoch: 12464, train_loss: 0.897914469242096, val_loss: 0.8649365901947021\n",
            "Saving model, epoch: 12465, train_loss: 0.8979118466377258, val_loss: 0.8649353981018066\n",
            "Saving model, epoch: 12466, train_loss: 0.8979092240333557, val_loss: 0.864933967590332\n",
            "Saving model, epoch: 12467, train_loss: 0.8979064226150513, val_loss: 0.8649323582649231\n",
            "Saving model, epoch: 12468, train_loss: 0.8979039788246155, val_loss: 0.8649314641952515\n",
            "Saving model, epoch: 12469, train_loss: 0.8979014158248901, val_loss: 0.8649294972419739\n",
            "Saving model, epoch: 12470, train_loss: 0.89789879322052, val_loss: 0.8649283647537231\n",
            "Saving model, epoch: 12471, train_loss: 0.8978961706161499, val_loss: 0.8649265170097351\n",
            "Saving model, epoch: 12472, train_loss: 0.8978935480117798, val_loss: 0.8649252653121948\n",
            "Saving model, epoch: 12473, train_loss: 0.8978907465934753, val_loss: 0.8649243116378784\n",
            "Saving model, epoch: 12474, train_loss: 0.8978883028030396, val_loss: 0.8649221062660217\n",
            "Saving model, epoch: 12475, train_loss: 0.8978857398033142, val_loss: 0.8649210929870605\n",
            "Saving model, epoch: 12476, train_loss: 0.8978832364082336, val_loss: 0.8649194836616516\n",
            "Saving model, epoch: 12477, train_loss: 0.8978803753852844, val_loss: 0.8649181127548218\n",
            "Saving model, epoch: 12478, train_loss: 0.8978778719902039, val_loss: 0.8649166822433472\n",
            "Saving model, epoch: 12479, train_loss: 0.8978753685951233, val_loss: 0.8649153709411621\n",
            "Saving model, epoch: 12480, train_loss: 0.8978726863861084, val_loss: 0.8649145364761353\n",
            "Saving model, epoch: 12481, train_loss: 0.8978698253631592, val_loss: 0.8649128079414368\n",
            "Saving model, epoch: 12482, train_loss: 0.8978672623634338, val_loss: 0.8649112582206726\n",
            "Saving model, epoch: 12483, train_loss: 0.8978646993637085, val_loss: 0.8649102449417114\n",
            "Saving model, epoch: 12484, train_loss: 0.8978621959686279, val_loss: 0.8649085164070129\n",
            "Saving model, epoch: 12485, train_loss: 0.8978595733642578, val_loss: 0.8649072647094727\n",
            "Saving model, epoch: 12486, train_loss: 0.8978570103645325, val_loss: 0.8649053573608398\n",
            "Saving model, epoch: 12487, train_loss: 0.8978540897369385, val_loss: 0.8649038076400757\n",
            "Saving model, epoch: 12488, train_loss: 0.8978515863418579, val_loss: 0.8649024963378906\n",
            "Saving model, epoch: 12489, train_loss: 0.8978489637374878, val_loss: 0.8649014830589294\n",
            "Saving model, epoch: 12490, train_loss: 0.8978462219238281, val_loss: 0.8648995757102966\n",
            "Saving model, epoch: 12491, train_loss: 0.897843599319458, val_loss: 0.8648985624313354\n",
            "Saving model, epoch: 12492, train_loss: 0.8978410959243774, val_loss: 0.864896833896637\n",
            "Saving model, epoch: 12493, train_loss: 0.8978384733200073, val_loss: 0.8648957014083862\n",
            "Saving model, epoch: 12494, train_loss: 0.8978356719017029, val_loss: 0.8648940324783325\n",
            "Saving model, epoch: 12495, train_loss: 0.8978330492973328, val_loss: 0.8648927211761475\n",
            "Saving model, epoch: 12496, train_loss: 0.8978304862976074, val_loss: 0.8648912906646729\n",
            "Saving model, epoch: 12497, train_loss: 0.8978279232978821, val_loss: 0.8648898005485535\n",
            "Saving model, epoch: 12498, train_loss: 0.8978252410888672, val_loss: 0.8648884892463684\n",
            "Saving model, epoch: 12499, train_loss: 0.8978226184844971, val_loss: 0.8648868203163147\n",
            "Saving model, epoch: 12500, train_loss: 0.897819995880127, val_loss: 0.8648853898048401\n",
            "epoch: 12501, train_loss: 0.8978173136711121, val_loss: 0.864884078502655\n",
            "Saving model, epoch: 12501, train_loss: 0.8978173136711121, val_loss: 0.864884078502655\n",
            "Saving model, epoch: 12502, train_loss: 0.8978145718574524, val_loss: 0.8648825883865356\n",
            "Saving model, epoch: 12503, train_loss: 0.8978119492530823, val_loss: 0.8648812174797058\n",
            "Saving model, epoch: 12504, train_loss: 0.8978093862533569, val_loss: 0.8648797869682312\n",
            "Saving model, epoch: 12505, train_loss: 0.8978067636489868, val_loss: 0.8648782968521118\n",
            "Saving model, epoch: 12506, train_loss: 0.8978041410446167, val_loss: 0.8648771047592163\n",
            "Saving model, epoch: 12507, train_loss: 0.897801399230957, val_loss: 0.8648756146430969\n",
            "Saving model, epoch: 12508, train_loss: 0.8977988362312317, val_loss: 0.864874005317688\n",
            "Saving model, epoch: 12509, train_loss: 0.897796094417572, val_loss: 0.8648725152015686\n",
            "Saving model, epoch: 12510, train_loss: 0.8977934718132019, val_loss: 0.8648715615272522\n",
            "Saving model, epoch: 12511, train_loss: 0.8977906703948975, val_loss: 0.8648694753646851\n",
            "Saving model, epoch: 12512, train_loss: 0.8977884650230408, val_loss: 0.8648682832717896\n",
            "Saving model, epoch: 12513, train_loss: 0.8977855443954468, val_loss: 0.8648666739463806\n",
            "Saving model, epoch: 12514, train_loss: 0.8977828621864319, val_loss: 0.8648656010627747\n",
            "Saving model, epoch: 12515, train_loss: 0.8977802395820618, val_loss: 0.8648639917373657\n",
            "Saving model, epoch: 12516, train_loss: 0.8977776169776917, val_loss: 0.8648625016212463\n",
            "Saving model, epoch: 12517, train_loss: 0.8977748155593872, val_loss: 0.8648614883422852\n",
            "Saving model, epoch: 12518, train_loss: 0.8977723121643066, val_loss: 0.8648597002029419\n",
            "Saving model, epoch: 12519, train_loss: 0.897769570350647, val_loss: 0.8648586869239807\n",
            "Saving model, epoch: 12520, train_loss: 0.8977669477462769, val_loss: 0.8648566603660583\n",
            "Saving model, epoch: 12521, train_loss: 0.897764265537262, val_loss: 0.8648554086685181\n",
            "Saving model, epoch: 12522, train_loss: 0.8977616429328918, val_loss: 0.8648543953895569\n",
            "Saving model, epoch: 12523, train_loss: 0.8977589011192322, val_loss: 0.8648526072502136\n",
            "Saving model, epoch: 12524, train_loss: 0.8977563381195068, val_loss: 0.8648511171340942\n",
            "Saving model, epoch: 12525, train_loss: 0.8977534770965576, val_loss: 0.8648497462272644\n",
            "Saving model, epoch: 12526, train_loss: 0.8977509140968323, val_loss: 0.8648483753204346\n",
            "Saving model, epoch: 12527, train_loss: 0.8977482914924622, val_loss: 0.86484694480896\n",
            "Saving model, epoch: 12528, train_loss: 0.8977455496788025, val_loss: 0.8648452758789062\n",
            "Saving model, epoch: 12529, train_loss: 0.897742748260498, val_loss: 0.8648438453674316\n",
            "Saving model, epoch: 12530, train_loss: 0.8977402448654175, val_loss: 0.8648425936698914\n",
            "Saving model, epoch: 12531, train_loss: 0.897737443447113, val_loss: 0.8648408055305481\n",
            "Saving model, epoch: 12532, train_loss: 0.8977348208427429, val_loss: 0.8648394346237183\n",
            "Saving model, epoch: 12533, train_loss: 0.8977320790290833, val_loss: 0.8648381233215332\n",
            "Saving model, epoch: 12534, train_loss: 0.8977295756340027, val_loss: 0.8648364543914795\n",
            "Saving model, epoch: 12535, train_loss: 0.8977267742156982, val_loss: 0.8648349642753601\n",
            "Saving model, epoch: 12536, train_loss: 0.8977240920066833, val_loss: 0.8648338913917542\n",
            "Saving model, epoch: 12537, train_loss: 0.8977213501930237, val_loss: 0.8648326396942139\n",
            "Saving model, epoch: 12538, train_loss: 0.8977187275886536, val_loss: 0.8648306131362915\n",
            "Saving model, epoch: 12539, train_loss: 0.8977161049842834, val_loss: 0.8648297190666199\n",
            "Saving model, epoch: 12540, train_loss: 0.897713303565979, val_loss: 0.8648280501365662\n",
            "Saving model, epoch: 12541, train_loss: 0.8977106809616089, val_loss: 0.8648269176483154\n",
            "Saving model, epoch: 12542, train_loss: 0.897707998752594, val_loss: 0.8648250699043274\n",
            "Saving model, epoch: 12543, train_loss: 0.8977051973342896, val_loss: 0.8648235201835632\n",
            "Saving model, epoch: 12544, train_loss: 0.8977025747299194, val_loss: 0.8648219704627991\n",
            "Saving model, epoch: 12545, train_loss: 0.8976998329162598, val_loss: 0.8648205399513245\n",
            "Saving model, epoch: 12546, train_loss: 0.8976973295211792, val_loss: 0.864819347858429\n",
            "Saving model, epoch: 12547, train_loss: 0.8976946473121643, val_loss: 0.8648179173469543\n",
            "Saving model, epoch: 12548, train_loss: 0.8976917862892151, val_loss: 0.8648167252540588\n",
            "Saving model, epoch: 12549, train_loss: 0.8976892232894897, val_loss: 0.8648149967193604\n",
            "Saving model, epoch: 12550, train_loss: 0.8976863622665405, val_loss: 0.864813506603241\n",
            "Saving model, epoch: 12551, train_loss: 0.8976837992668152, val_loss: 0.864811897277832\n",
            "Saving model, epoch: 12552, train_loss: 0.8976811766624451, val_loss: 0.8648107051849365\n",
            "Saving model, epoch: 12553, train_loss: 0.8976783752441406, val_loss: 0.8648089170455933\n",
            "Saving model, epoch: 12554, train_loss: 0.8976757526397705, val_loss: 0.8648072481155396\n",
            "Saving model, epoch: 12555, train_loss: 0.8976731300354004, val_loss: 0.8648061156272888\n",
            "Saving model, epoch: 12556, train_loss: 0.897670328617096, val_loss: 0.8648043870925903\n",
            "Saving model, epoch: 12557, train_loss: 0.8976675868034363, val_loss: 0.8648033738136292\n",
            "Saving model, epoch: 12558, train_loss: 0.8976649641990662, val_loss: 0.8648014664649963\n",
            "Saving model, epoch: 12559, train_loss: 0.8976621031761169, val_loss: 0.8648000359535217\n",
            "Saving model, epoch: 12560, train_loss: 0.8976594805717468, val_loss: 0.8647991418838501\n",
            "Saving model, epoch: 12561, train_loss: 0.8976566791534424, val_loss: 0.8647971749305725\n",
            "Saving model, epoch: 12562, train_loss: 0.8976540565490723, val_loss: 0.8647964000701904\n",
            "Saving model, epoch: 12563, train_loss: 0.8976514339447021, val_loss: 0.8647944331169128\n",
            "Saving model, epoch: 12564, train_loss: 0.8976486325263977, val_loss: 0.8647930026054382\n",
            "Saving model, epoch: 12565, train_loss: 0.8976460099220276, val_loss: 0.8647915720939636\n",
            "Saving model, epoch: 12566, train_loss: 0.8976430892944336, val_loss: 0.8647900223731995\n",
            "Saving model, epoch: 12567, train_loss: 0.897640585899353, val_loss: 0.8647884130477905\n",
            "Saving model, epoch: 12568, train_loss: 0.8976378440856934, val_loss: 0.864787220954895\n",
            "Saving model, epoch: 12569, train_loss: 0.8976350426673889, val_loss: 0.8647863268852234\n",
            "Saving model, epoch: 12570, train_loss: 0.8976324200630188, val_loss: 0.8647843599319458\n",
            "Saving model, epoch: 12571, train_loss: 0.8976296186447144, val_loss: 0.8647828698158264\n",
            "Saving model, epoch: 12572, train_loss: 0.8976269364356995, val_loss: 0.8647810220718384\n",
            "Saving model, epoch: 12573, train_loss: 0.8976241946220398, val_loss: 0.8647799491882324\n",
            "Saving model, epoch: 12574, train_loss: 0.8976215124130249, val_loss: 0.8647779226303101\n",
            "Saving model, epoch: 12575, train_loss: 0.8976188898086548, val_loss: 0.864776611328125\n",
            "Saving model, epoch: 12576, train_loss: 0.8976160883903503, val_loss: 0.8647752404212952\n",
            "Saving model, epoch: 12577, train_loss: 0.8976132273674011, val_loss: 0.8647738099098206\n",
            "Saving model, epoch: 12578, train_loss: 0.8976106643676758, val_loss: 0.8647724986076355\n",
            "Saving model, epoch: 12579, train_loss: 0.8976079225540161, val_loss: 0.8647711873054504\n",
            "Saving model, epoch: 12580, train_loss: 0.8976051211357117, val_loss: 0.8647693991661072\n",
            "Saving model, epoch: 12581, train_loss: 0.897602379322052, val_loss: 0.8647683262825012\n",
            "Saving model, epoch: 12582, train_loss: 0.8975997567176819, val_loss: 0.8647668361663818\n",
            "Saving model, epoch: 12583, train_loss: 0.897597074508667, val_loss: 0.8647646307945251\n",
            "Saving model, epoch: 12584, train_loss: 0.8975942730903625, val_loss: 0.8647637367248535\n",
            "Saving model, epoch: 12585, train_loss: 0.8975915312767029, val_loss: 0.8647618293762207\n",
            "Saving model, epoch: 12586, train_loss: 0.8975887298583984, val_loss: 0.8647607564926147\n",
            "Saving model, epoch: 12587, train_loss: 0.8975861072540283, val_loss: 0.8647593259811401\n",
            "Saving model, epoch: 12588, train_loss: 0.8975834846496582, val_loss: 0.8647577166557312\n",
            "Saving model, epoch: 12589, train_loss: 0.8975805044174194, val_loss: 0.8647562265396118\n",
            "Saving model, epoch: 12590, train_loss: 0.8975780010223389, val_loss: 0.8647550344467163\n",
            "Saving model, epoch: 12591, train_loss: 0.8975750803947449, val_loss: 0.8647530674934387\n",
            "Saving model, epoch: 12592, train_loss: 0.8975724577903748, val_loss: 0.8647518754005432\n",
            "Saving model, epoch: 12593, train_loss: 0.8975696563720703, val_loss: 0.8647506237030029\n",
            "Saving model, epoch: 12594, train_loss: 0.8975668549537659, val_loss: 0.8647493720054626\n",
            "Saving model, epoch: 12595, train_loss: 0.8975642323493958, val_loss: 0.8647474050521851\n",
            "Saving model, epoch: 12596, train_loss: 0.8975614309310913, val_loss: 0.86474609375\n",
            "Saving model, epoch: 12597, train_loss: 0.8975585699081421, val_loss: 0.8647441864013672\n",
            "Saving model, epoch: 12598, train_loss: 0.8975557684898376, val_loss: 0.8647426962852478\n",
            "Saving model, epoch: 12599, train_loss: 0.8975532650947571, val_loss: 0.8647412657737732\n",
            "Saving model, epoch: 12600, train_loss: 0.8975504636764526, val_loss: 0.8647400736808777\n",
            "epoch: 12601, train_loss: 0.8975475430488586, val_loss: 0.864738404750824\n",
            "Saving model, epoch: 12601, train_loss: 0.8975475430488586, val_loss: 0.864738404750824\n",
            "Saving model, epoch: 12602, train_loss: 0.8975449204444885, val_loss: 0.8647368550300598\n",
            "Saving model, epoch: 12603, train_loss: 0.8975422978401184, val_loss: 0.8647359013557434\n",
            "Saving model, epoch: 12604, train_loss: 0.897539496421814, val_loss: 0.8647335767745972\n",
            "Saving model, epoch: 12605, train_loss: 0.8975368142127991, val_loss: 0.8647330403327942\n",
            "Saving model, epoch: 12606, train_loss: 0.8975339531898499, val_loss: 0.8647310733795166\n",
            "Saving model, epoch: 12607, train_loss: 0.8975311517715454, val_loss: 0.8647293448448181\n",
            "Saving model, epoch: 12608, train_loss: 0.8975284695625305, val_loss: 0.8647281527519226\n",
            "Saving model, epoch: 12609, train_loss: 0.8975258469581604, val_loss: 0.8647271990776062\n",
            "Saving model, epoch: 12610, train_loss: 0.897523045539856, val_loss: 0.8647251725196838\n",
            "Saving model, epoch: 12611, train_loss: 0.8975201845169067, val_loss: 0.8647241592407227\n",
            "Saving model, epoch: 12612, train_loss: 0.8975176215171814, val_loss: 0.8647223711013794\n",
            "Saving model, epoch: 12613, train_loss: 0.8975147008895874, val_loss: 0.8647207021713257\n",
            "Saving model, epoch: 12614, train_loss: 0.897511899471283, val_loss: 0.8647193908691406\n",
            "Saving model, epoch: 12615, train_loss: 0.8975091576576233, val_loss: 0.8647178411483765\n",
            "Saving model, epoch: 12616, train_loss: 0.8975065350532532, val_loss: 0.8647164106369019\n",
            "Saving model, epoch: 12617, train_loss: 0.8975035548210144, val_loss: 0.8647143840789795\n",
            "Saving model, epoch: 12618, train_loss: 0.8975009322166443, val_loss: 0.8647134304046631\n",
            "Saving model, epoch: 12619, train_loss: 0.8974981307983398, val_loss: 0.8647116422653198\n",
            "Saving model, epoch: 12620, train_loss: 0.8974955081939697, val_loss: 0.8647105097770691\n",
            "Saving model, epoch: 12621, train_loss: 0.8974924683570862, val_loss: 0.8647087812423706\n",
            "Saving model, epoch: 12622, train_loss: 0.8974897861480713, val_loss: 0.8647072911262512\n",
            "Saving model, epoch: 12623, train_loss: 0.8974869847297668, val_loss: 0.8647056221961975\n",
            "Saving model, epoch: 12624, train_loss: 0.8974840641021729, val_loss: 0.8647041916847229\n",
            "Saving model, epoch: 12625, train_loss: 0.8974814414978027, val_loss: 0.8647027611732483\n",
            "Saving model, epoch: 12626, train_loss: 0.8974786400794983, val_loss: 0.8647014498710632\n",
            "Saving model, epoch: 12627, train_loss: 0.8974760174751282, val_loss: 0.8646999597549438\n",
            "Saving model, epoch: 12628, train_loss: 0.8974732160568237, val_loss: 0.8646979928016663\n",
            "Saving model, epoch: 12629, train_loss: 0.8974703550338745, val_loss: 0.8646968603134155\n",
            "Saving model, epoch: 12630, train_loss: 0.8974677920341492, val_loss: 0.8646953701972961\n",
            "Saving model, epoch: 12631, train_loss: 0.8974647521972656, val_loss: 0.8646937608718872\n",
            "Saving model, epoch: 12632, train_loss: 0.8974620699882507, val_loss: 0.8646923899650574\n",
            "Saving model, epoch: 12633, train_loss: 0.8974591493606567, val_loss: 0.8646904826164246\n",
            "Saving model, epoch: 12634, train_loss: 0.8974562883377075, val_loss: 0.8646889328956604\n",
            "Saving model, epoch: 12635, train_loss: 0.8974537253379822, val_loss: 0.8646876811981201\n",
            "Saving model, epoch: 12636, train_loss: 0.8974511027336121, val_loss: 0.8646860718727112\n",
            "Saving model, epoch: 12637, train_loss: 0.8974480628967285, val_loss: 0.8646848201751709\n",
            "Saving model, epoch: 12638, train_loss: 0.8974454402923584, val_loss: 0.8646835088729858\n",
            "Saving model, epoch: 12639, train_loss: 0.897442638874054, val_loss: 0.8646819591522217\n",
            "Saving model, epoch: 12640, train_loss: 0.8974398374557495, val_loss: 0.8646801710128784\n",
            "Saving model, epoch: 12641, train_loss: 0.8974369168281555, val_loss: 0.8646786212921143\n",
            "Saving model, epoch: 12642, train_loss: 0.8974341154098511, val_loss: 0.8646774888038635\n",
            "Saving model, epoch: 12643, train_loss: 0.8974313139915466, val_loss: 0.8646755218505859\n",
            "Saving model, epoch: 12644, train_loss: 0.897428572177887, val_loss: 0.8646739721298218\n",
            "Saving model, epoch: 12645, train_loss: 0.8974257707595825, val_loss: 0.8646726608276367\n",
            "Saving model, epoch: 12646, train_loss: 0.8974229693412781, val_loss: 0.8646709322929382\n",
            "Saving model, epoch: 12647, train_loss: 0.8974201679229736, val_loss: 0.8646695017814636\n",
            "Saving model, epoch: 12648, train_loss: 0.8974173069000244, val_loss: 0.864668607711792\n",
            "Saving model, epoch: 12649, train_loss: 0.8974146246910095, val_loss: 0.8646666407585144\n",
            "Saving model, epoch: 12650, train_loss: 0.8974118828773499, val_loss: 0.8646648526191711\n",
            "Saving model, epoch: 12651, train_loss: 0.8974089026451111, val_loss: 0.86466383934021\n",
            "Saving model, epoch: 12652, train_loss: 0.8974061608314514, val_loss: 0.8646621108055115\n",
            "Saving model, epoch: 12653, train_loss: 0.897403359413147, val_loss: 0.8646602034568787\n",
            "Saving model, epoch: 12654, train_loss: 0.8974006772041321, val_loss: 0.8646592497825623\n",
            "Saving model, epoch: 12655, train_loss: 0.8973979353904724, val_loss: 0.8646576404571533\n",
            "Saving model, epoch: 12656, train_loss: 0.8973949551582336, val_loss: 0.8646563291549683\n",
            "Saving model, epoch: 12657, train_loss: 0.8973920345306396, val_loss: 0.8646544218063354\n",
            "Saving model, epoch: 12658, train_loss: 0.8973894119262695, val_loss: 0.8646531701087952\n",
            "Saving model, epoch: 12659, train_loss: 0.8973866701126099, val_loss: 0.8646515011787415\n",
            "Saving model, epoch: 12660, train_loss: 0.8973837494850159, val_loss: 0.8646503686904907\n",
            "Saving model, epoch: 12661, train_loss: 0.8973809480667114, val_loss: 0.8646488785743713\n",
            "Saving model, epoch: 12662, train_loss: 0.897378146648407, val_loss: 0.8646471500396729\n",
            "Saving model, epoch: 12663, train_loss: 0.8973753452301025, val_loss: 0.8646453619003296\n",
            "Saving model, epoch: 12664, train_loss: 0.8973725438117981, val_loss: 0.8646439909934998\n",
            "Saving model, epoch: 12665, train_loss: 0.8973696827888489, val_loss: 0.864642858505249\n",
            "Saving model, epoch: 12666, train_loss: 0.8973668813705444, val_loss: 0.8646410703659058\n",
            "Saving model, epoch: 12667, train_loss: 0.8973642587661743, val_loss: 0.8646398782730103\n",
            "Saving model, epoch: 12668, train_loss: 0.8973612785339355, val_loss: 0.8646381497383118\n",
            "Saving model, epoch: 12669, train_loss: 0.8973583579063416, val_loss: 0.8646365404129028\n",
            "Saving model, epoch: 12670, train_loss: 0.8973556160926819, val_loss: 0.8646352291107178\n",
            "Saving model, epoch: 12671, train_loss: 0.8973527550697327, val_loss: 0.8646329641342163\n",
            "Saving model, epoch: 12672, train_loss: 0.8973498940467834, val_loss: 0.8646317720413208\n",
            "Saving model, epoch: 12673, train_loss: 0.897347092628479, val_loss: 0.8646300435066223\n",
            "Saving model, epoch: 12674, train_loss: 0.897344172000885, val_loss: 0.8646286725997925\n",
            "Saving model, epoch: 12675, train_loss: 0.8973415493965149, val_loss: 0.8646275401115417\n",
            "Saving model, epoch: 12676, train_loss: 0.8973385691642761, val_loss: 0.8646255135536194\n",
            "Saving model, epoch: 12677, train_loss: 0.897335946559906, val_loss: 0.864624559879303\n",
            "Saving model, epoch: 12678, train_loss: 0.897333025932312, val_loss: 0.864622950553894\n",
            "Saving model, epoch: 12679, train_loss: 0.897330105304718, val_loss: 0.8646209239959717\n",
            "Saving model, epoch: 12680, train_loss: 0.897327184677124, val_loss: 0.8646194934844971\n",
            "Saving model, epoch: 12681, train_loss: 0.8973245024681091, val_loss: 0.8646180033683777\n",
            "Saving model, epoch: 12682, train_loss: 0.8973215818405151, val_loss: 0.8646167516708374\n",
            "Saving model, epoch: 12683, train_loss: 0.8973188996315002, val_loss: 0.8646150231361389\n",
            "Saving model, epoch: 12684, train_loss: 0.897316038608551, val_loss: 0.8646135330200195\n",
            "Saving model, epoch: 12685, train_loss: 0.8973132371902466, val_loss: 0.8646122813224792\n",
            "Saving model, epoch: 12686, train_loss: 0.8973104357719421, val_loss: 0.8646102547645569\n",
            "Saving model, epoch: 12687, train_loss: 0.8973075151443481, val_loss: 0.8646088242530823\n",
            "Saving model, epoch: 12688, train_loss: 0.8973047137260437, val_loss: 0.8646076321601868\n",
            "Saving model, epoch: 12689, train_loss: 0.8973017930984497, val_loss: 0.8646056056022644\n",
            "Saving model, epoch: 12690, train_loss: 0.8972989916801453, val_loss: 0.8646039962768555\n",
            "Saving model, epoch: 12691, train_loss: 0.8972961902618408, val_loss: 0.86460280418396\n",
            "Saving model, epoch: 12692, train_loss: 0.8972933292388916, val_loss: 0.8646013736724854\n",
            "Saving model, epoch: 12693, train_loss: 0.8972905278205872, val_loss: 0.8645995855331421\n",
            "Saving model, epoch: 12694, train_loss: 0.8972875475883484, val_loss: 0.8645980954170227\n",
            "Saving model, epoch: 12695, train_loss: 0.8972846865653992, val_loss: 0.8645966649055481\n",
            "Saving model, epoch: 12696, train_loss: 0.8972818851470947, val_loss: 0.8645951151847839\n",
            "Saving model, epoch: 12697, train_loss: 0.8972790837287903, val_loss: 0.8645938038825989\n",
            "Saving model, epoch: 12698, train_loss: 0.8972762823104858, val_loss: 0.8645921349525452\n",
            "Saving model, epoch: 12699, train_loss: 0.8972732424736023, val_loss: 0.8645904660224915\n",
            "Saving model, epoch: 12700, train_loss: 0.8972705602645874, val_loss: 0.8645888566970825\n",
            "epoch: 12701, train_loss: 0.897267758846283, val_loss: 0.8645878434181213\n",
            "Saving model, epoch: 12701, train_loss: 0.897267758846283, val_loss: 0.8645878434181213\n",
            "Saving model, epoch: 12702, train_loss: 0.897264838218689, val_loss: 0.864585816860199\n",
            "Saving model, epoch: 12703, train_loss: 0.8972619771957397, val_loss: 0.8645843267440796\n",
            "Saving model, epoch: 12704, train_loss: 0.8972591757774353, val_loss: 0.8645826578140259\n",
            "Saving model, epoch: 12705, train_loss: 0.8972561955451965, val_loss: 0.8645811676979065\n",
            "Saving model, epoch: 12706, train_loss: 0.897253155708313, val_loss: 0.8645797967910767\n",
            "Saving model, epoch: 12707, train_loss: 0.8972504734992981, val_loss: 0.8645785450935364\n",
            "Saving model, epoch: 12708, train_loss: 0.8972475528717041, val_loss: 0.864576518535614\n",
            "Saving model, epoch: 12709, train_loss: 0.8972448110580444, val_loss: 0.8645753264427185\n",
            "Saving model, epoch: 12710, train_loss: 0.89724200963974, val_loss: 0.8645738363265991\n",
            "Saving model, epoch: 12711, train_loss: 0.8972392082214355, val_loss: 0.8645719885826111\n",
            "Saving model, epoch: 12712, train_loss: 0.8972362875938416, val_loss: 0.8645709156990051\n",
            "Saving model, epoch: 12713, train_loss: 0.8972333669662476, val_loss: 0.8645690083503723\n",
            "Saving model, epoch: 12714, train_loss: 0.8972305655479431, val_loss: 0.8645676970481873\n",
            "Saving model, epoch: 12715, train_loss: 0.8972276449203491, val_loss: 0.8645661473274231\n",
            "Saving model, epoch: 12716, train_loss: 0.8972248435020447, val_loss: 0.8645644783973694\n",
            "Saving model, epoch: 12717, train_loss: 0.8972219228744507, val_loss: 0.8645628094673157\n",
            "Saving model, epoch: 12718, train_loss: 0.8972190022468567, val_loss: 0.8645610809326172\n",
            "Saving model, epoch: 12719, train_loss: 0.8972159624099731, val_loss: 0.8645596504211426\n",
            "Saving model, epoch: 12720, train_loss: 0.8972133994102478, val_loss: 0.8645579218864441\n",
            "Saving model, epoch: 12721, train_loss: 0.8972104787826538, val_loss: 0.8645566701889038\n",
            "Saving model, epoch: 12722, train_loss: 0.8972074389457703, val_loss: 0.8645551800727844\n",
            "Saving model, epoch: 12723, train_loss: 0.8972047567367554, val_loss: 0.8645535707473755\n",
            "Saving model, epoch: 12724, train_loss: 0.8972018361091614, val_loss: 0.8645520210266113\n",
            "Saving model, epoch: 12725, train_loss: 0.8971987962722778, val_loss: 0.8645501732826233\n",
            "Saving model, epoch: 12726, train_loss: 0.8971958756446838, val_loss: 0.8645490407943726\n",
            "Saving model, epoch: 12727, train_loss: 0.8971929550170898, val_loss: 0.8645474910736084\n",
            "Saving model, epoch: 12728, train_loss: 0.8971901535987854, val_loss: 0.8645458817481995\n",
            "Saving model, epoch: 12729, train_loss: 0.8971872329711914, val_loss: 0.8645445704460144\n",
            "Saving model, epoch: 12730, train_loss: 0.897184431552887, val_loss: 0.8645420670509338\n",
            "Saving model, epoch: 12731, train_loss: 0.897181510925293, val_loss: 0.8645411133766174\n",
            "Saving model, epoch: 12732, train_loss: 0.897178590297699, val_loss: 0.8645393252372742\n",
            "Saving model, epoch: 12733, train_loss: 0.8971757292747498, val_loss: 0.8645378947257996\n",
            "Saving model, epoch: 12734, train_loss: 0.8971728086471558, val_loss: 0.864536702632904\n",
            "Saving model, epoch: 12735, train_loss: 0.8971700668334961, val_loss: 0.8645349144935608\n",
            "Saving model, epoch: 12736, train_loss: 0.8971671462059021, val_loss: 0.8645333051681519\n",
            "Saving model, epoch: 12737, train_loss: 0.8971643447875977, val_loss: 0.8645313382148743\n",
            "Saving model, epoch: 12738, train_loss: 0.8971615433692932, val_loss: 0.864530086517334\n",
            "Saving model, epoch: 12739, train_loss: 0.8971585035324097, val_loss: 0.8645289540290833\n",
            "Saving model, epoch: 12740, train_loss: 0.8971555233001709, val_loss: 0.8645269870758057\n",
            "Saving model, epoch: 12741, train_loss: 0.8971527218818665, val_loss: 0.8645250797271729\n",
            "Saving model, epoch: 12742, train_loss: 0.8971498012542725, val_loss: 0.8645235300064087\n",
            "Saving model, epoch: 12743, train_loss: 0.8971468806266785, val_loss: 0.864522397518158\n",
            "Saving model, epoch: 12744, train_loss: 0.8971439599990845, val_loss: 0.8645205497741699\n",
            "Saving model, epoch: 12745, train_loss: 0.8971410393714905, val_loss: 0.8645192384719849\n",
            "Saving model, epoch: 12746, train_loss: 0.897138237953186, val_loss: 0.864517331123352\n",
            "Saving model, epoch: 12747, train_loss: 0.8971354365348816, val_loss: 0.864516019821167\n",
            "Saving model, epoch: 12748, train_loss: 0.8971322774887085, val_loss: 0.8645147085189819\n",
            "Saving model, epoch: 12749, train_loss: 0.8971295952796936, val_loss: 0.8645131587982178\n",
            "Saving model, epoch: 12750, train_loss: 0.8971266746520996, val_loss: 0.8645115494728088\n",
            "Saving model, epoch: 12751, train_loss: 0.8971236348152161, val_loss: 0.8645098209381104\n",
            "Saving model, epoch: 12752, train_loss: 0.8971208333969116, val_loss: 0.8645083904266357\n",
            "Saving model, epoch: 12753, train_loss: 0.8971179127693176, val_loss: 0.8645069003105164\n",
            "Saving model, epoch: 12754, train_loss: 0.8971148133277893, val_loss: 0.8645052313804626\n",
            "Saving model, epoch: 12755, train_loss: 0.8971118927001953, val_loss: 0.8645038604736328\n",
            "Saving model, epoch: 12756, train_loss: 0.8971090912818909, val_loss: 0.8645021319389343\n",
            "Saving model, epoch: 12757, train_loss: 0.8971060514450073, val_loss: 0.8645000457763672\n",
            "Saving model, epoch: 12758, train_loss: 0.8971031904220581, val_loss: 0.8644992113113403\n",
            "Saving model, epoch: 12759, train_loss: 0.8971002697944641, val_loss: 0.8644972443580627\n",
            "Saving model, epoch: 12760, train_loss: 0.8970974087715149, val_loss: 0.8644956946372986\n",
            "Saving model, epoch: 12761, train_loss: 0.8970945477485657, val_loss: 0.8644942045211792\n",
            "Saving model, epoch: 12762, train_loss: 0.8970916271209717, val_loss: 0.8644924759864807\n",
            "Saving model, epoch: 12763, train_loss: 0.8970885872840881, val_loss: 0.8644908666610718\n",
            "Saving model, epoch: 12764, train_loss: 0.8970857858657837, val_loss: 0.8644894957542419\n",
            "Saving model, epoch: 12765, train_loss: 0.8970829844474792, val_loss: 0.8644880056381226\n",
            "Saving model, epoch: 12766, train_loss: 0.8970799446105957, val_loss: 0.8644865155220032\n",
            "Saving model, epoch: 12767, train_loss: 0.8970771431922913, val_loss: 0.8644847869873047\n",
            "Saving model, epoch: 12768, train_loss: 0.8970739245414734, val_loss: 0.8644832968711853\n",
            "Saving model, epoch: 12769, train_loss: 0.897071123123169, val_loss: 0.8644816875457764\n",
            "Saving model, epoch: 12770, train_loss: 0.897068202495575, val_loss: 0.8644800186157227\n",
            "Saving model, epoch: 12771, train_loss: 0.8970651030540466, val_loss: 0.8644782304763794\n",
            "Saving model, epoch: 12772, train_loss: 0.8970624804496765, val_loss: 0.8644766211509705\n",
            "Saving model, epoch: 12773, train_loss: 0.8970593810081482, val_loss: 0.8644754886627197\n",
            "Saving model, epoch: 12774, train_loss: 0.8970564603805542, val_loss: 0.8644738793373108\n",
            "Saving model, epoch: 12775, train_loss: 0.8970535397529602, val_loss: 0.8644723296165466\n",
            "Saving model, epoch: 12776, train_loss: 0.8970506191253662, val_loss: 0.8644708395004272\n",
            "Saving model, epoch: 12777, train_loss: 0.8970475792884827, val_loss: 0.8644693493843079\n",
            "Saving model, epoch: 12778, train_loss: 0.8970447778701782, val_loss: 0.8644675016403198\n",
            "Saving model, epoch: 12779, train_loss: 0.8970419764518738, val_loss: 0.8644654750823975\n",
            "Saving model, epoch: 12780, train_loss: 0.8970387578010559, val_loss: 0.8644644618034363\n",
            "Saving model, epoch: 12781, train_loss: 0.8970358371734619, val_loss: 0.8644629120826721\n",
            "Saving model, epoch: 12782, train_loss: 0.8970329165458679, val_loss: 0.8644610047340393\n",
            "Saving model, epoch: 12783, train_loss: 0.8970301151275635, val_loss: 0.8644593358039856\n",
            "Saving model, epoch: 12784, train_loss: 0.897027313709259, val_loss: 0.8644577860832214\n",
            "Saving model, epoch: 12785, train_loss: 0.8970240950584412, val_loss: 0.8644558787345886\n",
            "Saving model, epoch: 12786, train_loss: 0.8970212936401367, val_loss: 0.8644548058509827\n",
            "Saving model, epoch: 12787, train_loss: 0.8970182538032532, val_loss: 0.8644529581069946\n",
            "Saving model, epoch: 12788, train_loss: 0.8970153331756592, val_loss: 0.86445152759552\n",
            "Saving model, epoch: 12789, train_loss: 0.8970123529434204, val_loss: 0.864450216293335\n",
            "Saving model, epoch: 12790, train_loss: 0.8970093131065369, val_loss: 0.8644480109214783\n",
            "Saving model, epoch: 12791, train_loss: 0.8970066905021667, val_loss: 0.8644466400146484\n",
            "Saving model, epoch: 12792, train_loss: 0.8970035910606384, val_loss: 0.8644455075263977\n",
            "Saving model, epoch: 12793, train_loss: 0.8970006108283997, val_loss: 0.8644433617591858\n",
            "Saving model, epoch: 12794, train_loss: 0.8969977498054504, val_loss: 0.8644421696662903\n",
            "Saving model, epoch: 12795, train_loss: 0.8969947695732117, val_loss: 0.8644404411315918\n",
            "Saving model, epoch: 12796, train_loss: 0.8969917297363281, val_loss: 0.8644386529922485\n",
            "Saving model, epoch: 12797, train_loss: 0.8969887495040894, val_loss: 0.8644371628761292\n",
            "Saving model, epoch: 12798, train_loss: 0.8969858884811401, val_loss: 0.8644357919692993\n",
            "Saving model, epoch: 12799, train_loss: 0.8969829082489014, val_loss: 0.8644339442253113\n",
            "Saving model, epoch: 12800, train_loss: 0.8969801068305969, val_loss: 0.8644324541091919\n",
            "epoch: 12801, train_loss: 0.896976888179779, val_loss: 0.8644310832023621\n",
            "Saving model, epoch: 12801, train_loss: 0.896976888179779, val_loss: 0.8644310832023621\n",
            "Saving model, epoch: 12802, train_loss: 0.8969738483428955, val_loss: 0.8644293546676636\n",
            "Saving model, epoch: 12803, train_loss: 0.8969710469245911, val_loss: 0.8644275665283203\n",
            "Saving model, epoch: 12804, train_loss: 0.8969680070877075, val_loss: 0.8644260168075562\n",
            "Saving model, epoch: 12805, train_loss: 0.8969650268554688, val_loss: 0.8644249439239502\n",
            "Saving model, epoch: 12806, train_loss: 0.8969619870185852, val_loss: 0.8644229769706726\n",
            "Saving model, epoch: 12807, train_loss: 0.8969591856002808, val_loss: 0.8644208908081055\n",
            "Saving model, epoch: 12808, train_loss: 0.896956205368042, val_loss: 0.8644199371337891\n",
            "Saving model, epoch: 12809, train_loss: 0.8969531655311584, val_loss: 0.8644180297851562\n",
            "Saving model, epoch: 12810, train_loss: 0.8969501256942749, val_loss: 0.8644164204597473\n",
            "Saving model, epoch: 12811, train_loss: 0.8969471454620361, val_loss: 0.8644149303436279\n",
            "Saving model, epoch: 12812, train_loss: 0.8969443440437317, val_loss: 0.8644132018089294\n",
            "Saving model, epoch: 12813, train_loss: 0.8969413042068481, val_loss: 0.8644117116928101\n",
            "Saving model, epoch: 12814, train_loss: 0.8969382643699646, val_loss: 0.8644098043441772\n",
            "Saving model, epoch: 12815, train_loss: 0.8969354629516602, val_loss: 0.864408016204834\n",
            "Saving model, epoch: 12816, train_loss: 0.8969322443008423, val_loss: 0.8644070029258728\n",
            "Saving model, epoch: 12817, train_loss: 0.8969293236732483, val_loss: 0.8644052147865295\n",
            "Saving model, epoch: 12818, train_loss: 0.8969264626502991, val_loss: 0.8644039630889893\n",
            "Saving model, epoch: 12819, train_loss: 0.8969234228134155, val_loss: 0.8644019365310669\n",
            "Saving model, epoch: 12820, train_loss: 0.8969205021858215, val_loss: 0.8644003868103027\n",
            "Saving model, epoch: 12821, train_loss: 0.8969175815582275, val_loss: 0.8643988370895386\n",
            "Saving model, epoch: 12822, train_loss: 0.8969146013259888, val_loss: 0.8643972873687744\n",
            "Saving model, epoch: 12823, train_loss: 0.8969115614891052, val_loss: 0.8643957376480103\n",
            "Saving model, epoch: 12824, train_loss: 0.8969086408615112, val_loss: 0.8643943071365356\n",
            "Saving model, epoch: 12825, train_loss: 0.8969055414199829, val_loss: 0.8643927574157715\n",
            "Saving model, epoch: 12826, train_loss: 0.8969025015830994, val_loss: 0.8643908500671387\n",
            "Saving model, epoch: 12827, train_loss: 0.8968995213508606, val_loss: 0.8643890023231506\n",
            "Saving model, epoch: 12828, train_loss: 0.8968966007232666, val_loss: 0.8643876910209656\n",
            "Saving model, epoch: 12829, train_loss: 0.8968935608863831, val_loss: 0.8643860816955566\n",
            "Saving model, epoch: 12830, train_loss: 0.8968905806541443, val_loss: 0.8643844723701477\n",
            "Saving model, epoch: 12831, train_loss: 0.8968876600265503, val_loss: 0.8643829226493835\n",
            "Saving model, epoch: 12832, train_loss: 0.8968846201896667, val_loss: 0.8643811941146851\n",
            "Saving model, epoch: 12833, train_loss: 0.8968814015388489, val_loss: 0.8643794059753418\n",
            "Saving model, epoch: 12834, train_loss: 0.8968786001205444, val_loss: 0.8643781542778015\n",
            "Saving model, epoch: 12835, train_loss: 0.8968756794929504, val_loss: 0.864376425743103\n",
            "Saving model, epoch: 12836, train_loss: 0.8968727588653564, val_loss: 0.8643746972084045\n",
            "Saving model, epoch: 12837, train_loss: 0.8968695998191833, val_loss: 0.8643730282783508\n",
            "Saving model, epoch: 12838, train_loss: 0.8968665599822998, val_loss: 0.8643718361854553\n",
            "Saving model, epoch: 12839, train_loss: 0.8968636393547058, val_loss: 0.8643699884414673\n",
            "Saving model, epoch: 12840, train_loss: 0.8968605399131775, val_loss: 0.8643682599067688\n",
            "Saving model, epoch: 12841, train_loss: 0.896857500076294, val_loss: 0.8643662929534912\n",
            "Saving model, epoch: 12842, train_loss: 0.8968545794487, val_loss: 0.8643648624420166\n",
            "Saving model, epoch: 12843, train_loss: 0.8968515992164612, val_loss: 0.8643635511398315\n",
            "Saving model, epoch: 12844, train_loss: 0.8968486785888672, val_loss: 0.8643622398376465\n",
            "Saving model, epoch: 12845, train_loss: 0.8968454599380493, val_loss: 0.8643602132797241\n",
            "Saving model, epoch: 12846, train_loss: 0.8968426585197449, val_loss: 0.8643585443496704\n",
            "Saving model, epoch: 12847, train_loss: 0.8968396186828613, val_loss: 0.8643568754196167\n",
            "Saving model, epoch: 12848, train_loss: 0.8968366384506226, val_loss: 0.8643553256988525\n",
            "Saving model, epoch: 12849, train_loss: 0.8968334197998047, val_loss: 0.8643539547920227\n",
            "Saving model, epoch: 12850, train_loss: 0.8968304991722107, val_loss: 0.8643524050712585\n",
            "Saving model, epoch: 12851, train_loss: 0.8968275785446167, val_loss: 0.8643505573272705\n",
            "Saving model, epoch: 12852, train_loss: 0.8968243598937988, val_loss: 0.8643488883972168\n",
            "Saving model, epoch: 12853, train_loss: 0.8968213200569153, val_loss: 0.864346981048584\n",
            "Saving model, epoch: 12854, train_loss: 0.8968185186386108, val_loss: 0.864345908164978\n",
            "Saving model, epoch: 12855, train_loss: 0.896815299987793, val_loss: 0.8643442988395691\n",
            "Saving model, epoch: 12856, train_loss: 0.8968123197555542, val_loss: 0.8643425107002258\n",
            "Saving model, epoch: 12857, train_loss: 0.8968093991279602, val_loss: 0.8643407225608826\n",
            "Saving model, epoch: 12858, train_loss: 0.8968062996864319, val_loss: 0.8643388748168945\n",
            "Saving model, epoch: 12859, train_loss: 0.8968032598495483, val_loss: 0.8643376231193542\n",
            "Saving model, epoch: 12860, train_loss: 0.89680016040802, val_loss: 0.8643360137939453\n",
            "Saving model, epoch: 12861, train_loss: 0.896797239780426, val_loss: 0.8643342852592468\n",
            "Saving model, epoch: 12862, train_loss: 0.8967941999435425, val_loss: 0.8643326759338379\n",
            "Saving model, epoch: 12863, train_loss: 0.8967912793159485, val_loss: 0.8643311262130737\n",
            "Saving model, epoch: 12864, train_loss: 0.8967881798744202, val_loss: 0.8643298745155334\n",
            "Saving model, epoch: 12865, train_loss: 0.8967852592468262, val_loss: 0.8643278479576111\n",
            "Saving model, epoch: 12866, train_loss: 0.8967821598052979, val_loss: 0.8643263578414917\n",
            "Saving model, epoch: 12867, train_loss: 0.8967791795730591, val_loss: 0.8643247485160828\n",
            "Saving model, epoch: 12868, train_loss: 0.8967761397361755, val_loss: 0.8643227219581604\n",
            "Saving model, epoch: 12869, train_loss: 0.896773099899292, val_loss: 0.8643211126327515\n",
            "Saving model, epoch: 12870, train_loss: 0.8967701196670532, val_loss: 0.864319920539856\n",
            "Saving model, epoch: 12871, train_loss: 0.8967669010162354, val_loss: 0.8643181920051575\n",
            "Saving model, epoch: 12872, train_loss: 0.8967638611793518, val_loss: 0.8643165230751038\n",
            "Saving model, epoch: 12873, train_loss: 0.896760880947113, val_loss: 0.86431485414505\n",
            "Saving model, epoch: 12874, train_loss: 0.8967577219009399, val_loss: 0.8643130660057068\n",
            "Saving model, epoch: 12875, train_loss: 0.8967547416687012, val_loss: 0.8643110394477844\n",
            "Saving model, epoch: 12876, train_loss: 0.8967518210411072, val_loss: 0.8643100261688232\n",
            "Saving model, epoch: 12877, train_loss: 0.8967487812042236, val_loss: 0.8643081784248352\n",
            "Saving model, epoch: 12878, train_loss: 0.8967456817626953, val_loss: 0.8643063902854919\n",
            "Saving model, epoch: 12879, train_loss: 0.8967427611351013, val_loss: 0.8643051385879517\n",
            "Saving model, epoch: 12880, train_loss: 0.896739661693573, val_loss: 0.8643031716346741\n",
            "Saving model, epoch: 12881, train_loss: 0.8967365622520447, val_loss: 0.8643015027046204\n",
            "Saving model, epoch: 12882, train_loss: 0.8967335224151611, val_loss: 0.864300012588501\n",
            "Saving model, epoch: 12883, train_loss: 0.8967301249504089, val_loss: 0.8642985820770264\n",
            "Saving model, epoch: 12884, train_loss: 0.8967273831367493, val_loss: 0.8642967939376831\n",
            "Saving model, epoch: 12885, train_loss: 0.8967244029045105, val_loss: 0.8642948269844055\n",
            "Saving model, epoch: 12886, train_loss: 0.8967213034629822, val_loss: 0.86429363489151\n",
            "Saving model, epoch: 12887, train_loss: 0.8967182636260986, val_loss: 0.8642920851707458\n",
            "Saving model, epoch: 12888, train_loss: 0.8967150449752808, val_loss: 0.864290714263916\n",
            "Saving model, epoch: 12889, train_loss: 0.8967122435569763, val_loss: 0.8642888069152832\n",
            "Saving model, epoch: 12890, train_loss: 0.8967090249061584, val_loss: 0.8642869591712952\n",
            "Saving model, epoch: 12891, train_loss: 0.8967059850692749, val_loss: 0.8642852902412415\n",
            "Saving model, epoch: 12892, train_loss: 0.8967028856277466, val_loss: 0.8642839789390564\n",
            "Saving model, epoch: 12893, train_loss: 0.8966999053955078, val_loss: 0.8642818331718445\n",
            "Saving model, epoch: 12894, train_loss: 0.8966967463493347, val_loss: 0.8642799258232117\n",
            "Saving model, epoch: 12895, train_loss: 0.8966936469078064, val_loss: 0.8642785549163818\n",
            "Saving model, epoch: 12896, train_loss: 0.8966907262802124, val_loss: 0.8642765879631042\n",
            "Saving model, epoch: 12897, train_loss: 0.8966876268386841, val_loss: 0.8642756342887878\n",
            "Saving model, epoch: 12898, train_loss: 0.8966845273971558, val_loss: 0.8642736077308655\n",
            "Saving model, epoch: 12899, train_loss: 0.8966814875602722, val_loss: 0.8642719984054565\n",
            "Saving model, epoch: 12900, train_loss: 0.8966782689094543, val_loss: 0.8642703294754028\n",
            "epoch: 12901, train_loss: 0.8966753482818604, val_loss: 0.8642686605453491\n",
            "Saving model, epoch: 12901, train_loss: 0.8966753482818604, val_loss: 0.8642686605453491\n",
            "Saving model, epoch: 12902, train_loss: 0.8966721296310425, val_loss: 0.8642671704292297\n",
            "Saving model, epoch: 12903, train_loss: 0.8966692090034485, val_loss: 0.8642652630805969\n",
            "Saving model, epoch: 12904, train_loss: 0.8966661095619202, val_loss: 0.8642635941505432\n",
            "Saving model, epoch: 12905, train_loss: 0.8966630101203918, val_loss: 0.8642619848251343\n",
            "Saving model, epoch: 12906, train_loss: 0.8966599106788635, val_loss: 0.8642603754997253\n",
            "Saving model, epoch: 12907, train_loss: 0.8966569900512695, val_loss: 0.8642591238021851\n",
            "Saving model, epoch: 12908, train_loss: 0.8966537714004517, val_loss: 0.864257276058197\n",
            "Saving model, epoch: 12909, train_loss: 0.8966507315635681, val_loss: 0.8642552495002747\n",
            "Saving model, epoch: 12910, train_loss: 0.8966475129127502, val_loss: 0.8642538785934448\n",
            "Saving model, epoch: 12911, train_loss: 0.8966445326805115, val_loss: 0.8642524480819702\n",
            "Saving model, epoch: 12912, train_loss: 0.8966414928436279, val_loss: 0.8642503619194031\n",
            "Saving model, epoch: 12913, train_loss: 0.8966382741928101, val_loss: 0.8642486929893494\n",
            "Saving model, epoch: 12914, train_loss: 0.8966352939605713, val_loss: 0.8642473816871643\n",
            "Saving model, epoch: 12915, train_loss: 0.8966322541236877, val_loss: 0.8642454743385315\n",
            "Saving model, epoch: 12916, train_loss: 0.896629273891449, val_loss: 0.864243745803833\n",
            "Saving model, epoch: 12917, train_loss: 0.8966261148452759, val_loss: 0.8642420768737793\n",
            "Saving model, epoch: 12918, train_loss: 0.896622896194458, val_loss: 0.8642397522926331\n",
            "Saving model, epoch: 12919, train_loss: 0.8966199159622192, val_loss: 0.8642389178276062\n",
            "Saving model, epoch: 12920, train_loss: 0.8966165781021118, val_loss: 0.8642370700836182\n",
            "Saving model, epoch: 12921, train_loss: 0.896613597869873, val_loss: 0.8642351627349854\n",
            "Saving model, epoch: 12922, train_loss: 0.896610677242279, val_loss: 0.8642341494560242\n",
            "Saving model, epoch: 12923, train_loss: 0.8966075778007507, val_loss: 0.8642324209213257\n",
            "Saving model, epoch: 12924, train_loss: 0.8966043591499329, val_loss: 0.8642305731773376\n",
            "Saving model, epoch: 12925, train_loss: 0.8966014385223389, val_loss: 0.8642290830612183\n",
            "Saving model, epoch: 12926, train_loss: 0.8965981006622314, val_loss: 0.864227294921875\n",
            "Saving model, epoch: 12927, train_loss: 0.8965950608253479, val_loss: 0.8642259240150452\n",
            "Saving model, epoch: 12928, train_loss: 0.8965919613838196, val_loss: 0.8642237186431885\n",
            "Saving model, epoch: 12929, train_loss: 0.8965890407562256, val_loss: 0.8642222285270691\n",
            "Saving model, epoch: 12930, train_loss: 0.8965858221054077, val_loss: 0.8642206788063049\n",
            "Saving model, epoch: 12931, train_loss: 0.8965827226638794, val_loss: 0.8642188310623169\n",
            "Saving model, epoch: 12932, train_loss: 0.8965796232223511, val_loss: 0.8642175197601318\n",
            "Saving model, epoch: 12933, train_loss: 0.8965764045715332, val_loss: 0.8642160892486572\n",
            "Saving model, epoch: 12934, train_loss: 0.8965733647346497, val_loss: 0.8642138242721558\n",
            "Saving model, epoch: 12935, train_loss: 0.8965702652931213, val_loss: 0.8642122745513916\n",
            "Saving model, epoch: 12936, train_loss: 0.896567165851593, val_loss: 0.8642109036445618\n",
            "Saving model, epoch: 12937, train_loss: 0.8965641260147095, val_loss: 0.8642086982727051\n",
            "Saving model, epoch: 12938, train_loss: 0.8965609073638916, val_loss: 0.8642072677612305\n",
            "Saving model, epoch: 12939, train_loss: 0.8965578079223633, val_loss: 0.8642057776451111\n",
            "Saving model, epoch: 12940, train_loss: 0.8965545296669006, val_loss: 0.8642039895057678\n",
            "Saving model, epoch: 12941, train_loss: 0.8965516090393066, val_loss: 0.8642017245292664\n",
            "Saving model, epoch: 12942, train_loss: 0.8965484499931335, val_loss: 0.8642005920410156\n",
            "Saving model, epoch: 12943, train_loss: 0.8965453505516052, val_loss: 0.8641991019248962\n",
            "Saving model, epoch: 12944, train_loss: 0.8965422511100769, val_loss: 0.8641979098320007\n",
            "Saving model, epoch: 12945, train_loss: 0.896539032459259, val_loss: 0.8641954064369202\n",
            "Saving model, epoch: 12946, train_loss: 0.8965359926223755, val_loss: 0.8641939163208008\n",
            "Saving model, epoch: 12947, train_loss: 0.8965327739715576, val_loss: 0.8641922473907471\n",
            "Saving model, epoch: 12948, train_loss: 0.8965297937393188, val_loss: 0.864190936088562\n",
            "Saving model, epoch: 12949, train_loss: 0.896526575088501, val_loss: 0.8641889691352844\n",
            "Saving model, epoch: 12950, train_loss: 0.8965235352516174, val_loss: 0.8641872406005859\n",
            "Saving model, epoch: 12951, train_loss: 0.8965203166007996, val_loss: 0.8641856908798218\n",
            "Saving model, epoch: 12952, train_loss: 0.8965173363685608, val_loss: 0.8641839623451233\n",
            "Saving model, epoch: 12953, train_loss: 0.8965142369270325, val_loss: 0.8641824126243591\n",
            "Saving model, epoch: 12954, train_loss: 0.8965110778808594, val_loss: 0.8641806244850159\n",
            "Saving model, epoch: 12955, train_loss: 0.8965078592300415, val_loss: 0.8641790747642517\n",
            "Saving model, epoch: 12956, train_loss: 0.8965047597885132, val_loss: 0.8641773462295532\n",
            "Saving model, epoch: 12957, train_loss: 0.8965016603469849, val_loss: 0.8641752600669861\n",
            "Saving model, epoch: 12958, train_loss: 0.8964986205101013, val_loss: 0.8641738891601562\n",
            "Saving model, epoch: 12959, train_loss: 0.8964953422546387, val_loss: 0.8641720414161682\n",
            "Saving model, epoch: 12960, train_loss: 0.8964921832084656, val_loss: 0.8641700744628906\n",
            "Saving model, epoch: 12961, train_loss: 0.8964889049530029, val_loss: 0.8641689419746399\n",
            "Saving model, epoch: 12962, train_loss: 0.8964858651161194, val_loss: 0.864166796207428\n",
            "Saving model, epoch: 12963, train_loss: 0.8964827656745911, val_loss: 0.8641653656959534\n",
            "Saving model, epoch: 12964, train_loss: 0.8964796662330627, val_loss: 0.864163875579834\n",
            "Saving model, epoch: 12965, train_loss: 0.8964765071868896, val_loss: 0.8641617894172668\n",
            "Saving model, epoch: 12966, train_loss: 0.8964733481407166, val_loss: 0.8641604781150818\n",
            "Saving model, epoch: 12967, train_loss: 0.896470308303833, val_loss: 0.8641588687896729\n",
            "Saving model, epoch: 12968, train_loss: 0.8964670896530151, val_loss: 0.8641570210456848\n",
            "Saving model, epoch: 12969, train_loss: 0.8964638710021973, val_loss: 0.8641552925109863\n",
            "Saving model, epoch: 12970, train_loss: 0.8964606523513794, val_loss: 0.8641533851623535\n",
            "Saving model, epoch: 12971, train_loss: 0.8964576721191406, val_loss: 0.8641517758369446\n",
            "Saving model, epoch: 12972, train_loss: 0.8964544534683228, val_loss: 0.86415034532547\n",
            "Saving model, epoch: 12973, train_loss: 0.8964514136314392, val_loss: 0.8641482591629028\n",
            "Saving model, epoch: 12974, train_loss: 0.8964480757713318, val_loss: 0.8641467094421387\n",
            "Saving model, epoch: 12975, train_loss: 0.8964452147483826, val_loss: 0.8641453385353088\n",
            "Saving model, epoch: 12976, train_loss: 0.8964418768882751, val_loss: 0.8641437292098999\n",
            "Saving model, epoch: 12977, train_loss: 0.8964387774467468, val_loss: 0.8641415238380432\n",
            "Saving model, epoch: 12978, train_loss: 0.8964357376098633, val_loss: 0.864139974117279\n",
            "Saving model, epoch: 12979, train_loss: 0.8964323401451111, val_loss: 0.864138126373291\n",
            "Saving model, epoch: 12980, train_loss: 0.8964293003082275, val_loss: 0.8641366362571716\n",
            "Saving model, epoch: 12981, train_loss: 0.8964260816574097, val_loss: 0.8641347289085388\n",
            "Saving model, epoch: 12982, train_loss: 0.8964231014251709, val_loss: 0.8641332387924194\n",
            "Saving model, epoch: 12983, train_loss: 0.896419882774353, val_loss: 0.8641312122344971\n",
            "Saving model, epoch: 12984, train_loss: 0.8964166641235352, val_loss: 0.8641301989555359\n",
            "Saving model, epoch: 12985, train_loss: 0.8964134454727173, val_loss: 0.8641287684440613\n",
            "Saving model, epoch: 12986, train_loss: 0.8964102268218994, val_loss: 0.8641265630722046\n",
            "Saving model, epoch: 12987, train_loss: 0.8964071273803711, val_loss: 0.8641245365142822\n",
            "Saving model, epoch: 12988, train_loss: 0.8964037895202637, val_loss: 0.8641230463981628\n",
            "Saving model, epoch: 12989, train_loss: 0.8964006900787354, val_loss: 0.8641214966773987\n",
            "Saving model, epoch: 12990, train_loss: 0.8963976502418518, val_loss: 0.8641198873519897\n",
            "Saving model, epoch: 12991, train_loss: 0.8963944315910339, val_loss: 0.8641177415847778\n",
            "Saving model, epoch: 12992, train_loss: 0.8963912129402161, val_loss: 0.8641161322593689\n",
            "Saving model, epoch: 12993, train_loss: 0.8963879942893982, val_loss: 0.8641141653060913\n",
            "Saving model, epoch: 12994, train_loss: 0.8963848948478699, val_loss: 0.8641126751899719\n",
            "Saving model, epoch: 12995, train_loss: 0.8963817954063416, val_loss: 0.864111602306366\n",
            "Saving model, epoch: 12996, train_loss: 0.8963785767555237, val_loss: 0.864109456539154\n",
            "Saving model, epoch: 12997, train_loss: 0.8963752388954163, val_loss: 0.864107608795166\n",
            "Saving model, epoch: 12998, train_loss: 0.8963722586631775, val_loss: 0.8641059994697571\n",
            "Saving model, epoch: 12999, train_loss: 0.8963690400123596, val_loss: 0.8641047477722168\n",
            "Saving model, epoch: 13000, train_loss: 0.8963659405708313, val_loss: 0.8641026020050049\n",
            "epoch: 13001, train_loss: 0.8963626027107239, val_loss: 0.8641011118888855\n",
            "Saving model, epoch: 13001, train_loss: 0.8963626027107239, val_loss: 0.8641011118888855\n",
            "Saving model, epoch: 13002, train_loss: 0.8963595032691956, val_loss: 0.8640989661216736\n",
            "Saving model, epoch: 13003, train_loss: 0.8963564038276672, val_loss: 0.8640974164009094\n",
            "Saving model, epoch: 13004, train_loss: 0.8963531851768494, val_loss: 0.8640957474708557\n",
            "Saving model, epoch: 13005, train_loss: 0.8963499665260315, val_loss: 0.8640938997268677\n",
            "Saving model, epoch: 13006, train_loss: 0.8963468074798584, val_loss: 0.8640927672386169\n",
            "Saving model, epoch: 13007, train_loss: 0.8963435292243958, val_loss: 0.8640907406806946\n",
            "Saving model, epoch: 13008, train_loss: 0.8963403105735779, val_loss: 0.8640888929367065\n",
            "Saving model, epoch: 13009, train_loss: 0.89633709192276, val_loss: 0.8640875816345215\n",
            "Saving model, epoch: 13010, train_loss: 0.8963339924812317, val_loss: 0.864085853099823\n",
            "Saving model, epoch: 13011, train_loss: 0.8963306546211243, val_loss: 0.8640838265419006\n",
            "Saving model, epoch: 13012, train_loss: 0.8963276147842407, val_loss: 0.8640822768211365\n",
            "Saving model, epoch: 13013, train_loss: 0.8963245153427124, val_loss: 0.8640806674957275\n",
            "Saving model, epoch: 13014, train_loss: 0.8963209986686707, val_loss: 0.8640789985656738\n",
            "Saving model, epoch: 13015, train_loss: 0.8963180184364319, val_loss: 0.8640769720077515\n",
            "Saving model, epoch: 13016, train_loss: 0.8963148593902588, val_loss: 0.8640753030776978\n",
            "Saving model, epoch: 13017, train_loss: 0.8963115811347961, val_loss: 0.8640735149383545\n",
            "Saving model, epoch: 13018, train_loss: 0.8963085412979126, val_loss: 0.8640719056129456\n",
            "Saving model, epoch: 13019, train_loss: 0.89630526304245, val_loss: 0.8640699982643127\n",
            "Saving model, epoch: 13020, train_loss: 0.8963019251823425, val_loss: 0.864068329334259\n",
            "Saving model, epoch: 13021, train_loss: 0.8962987065315247, val_loss: 0.8640662431716919\n",
            "Saving model, epoch: 13022, train_loss: 0.8962954878807068, val_loss: 0.8640651106834412\n",
            "Saving model, epoch: 13023, train_loss: 0.8962923884391785, val_loss: 0.8640632033348083\n",
            "Saving model, epoch: 13024, train_loss: 0.896289050579071, val_loss: 0.8640615344047546\n",
            "Saving model, epoch: 13025, train_loss: 0.8962860703468323, val_loss: 0.8640596270561218\n",
            "Saving model, epoch: 13026, train_loss: 0.8962828516960144, val_loss: 0.8640580773353577\n",
            "Saving model, epoch: 13027, train_loss: 0.8962793946266174, val_loss: 0.864056408405304\n",
            "Saving model, epoch: 13028, train_loss: 0.8962764143943787, val_loss: 0.864054799079895\n",
            "Saving model, epoch: 13029, train_loss: 0.8962729573249817, val_loss: 0.8640532493591309\n",
            "Saving model, epoch: 13030, train_loss: 0.8962698578834534, val_loss: 0.864051103591919\n",
            "Saving model, epoch: 13031, train_loss: 0.896266758441925, val_loss: 0.8640496134757996\n",
            "Saving model, epoch: 13032, train_loss: 0.8962634205818176, val_loss: 0.8640481233596802\n",
            "Saving model, epoch: 13033, train_loss: 0.8962603211402893, val_loss: 0.8640462160110474\n",
            "Saving model, epoch: 13034, train_loss: 0.896257221698761, val_loss: 0.8640445470809937\n",
            "Saving model, epoch: 13035, train_loss: 0.8962538838386536, val_loss: 0.8640427589416504\n",
            "Saving model, epoch: 13036, train_loss: 0.8962506651878357, val_loss: 0.8640406131744385\n",
            "Saving model, epoch: 13037, train_loss: 0.8962472677230835, val_loss: 0.8640391230583191\n",
            "Saving model, epoch: 13038, train_loss: 0.8962442278862, val_loss: 0.8640375733375549\n",
            "Saving model, epoch: 13039, train_loss: 0.8962410092353821, val_loss: 0.8640360236167908\n",
            "Saving model, epoch: 13040, train_loss: 0.896237850189209, val_loss: 0.8640338778495789\n",
            "Saving model, epoch: 13041, train_loss: 0.8962346315383911, val_loss: 0.8640323281288147\n",
            "Saving model, epoch: 13042, train_loss: 0.8962311744689941, val_loss: 0.8640307188034058\n",
            "Saving model, epoch: 13043, train_loss: 0.8962279558181763, val_loss: 0.8640286922454834\n",
            "Saving model, epoch: 13044, train_loss: 0.8962246775627136, val_loss: 0.864027202129364\n",
            "Saving model, epoch: 13045, train_loss: 0.8962215185165405, val_loss: 0.8640254139900208\n",
            "Saving model, epoch: 13046, train_loss: 0.8962183594703674, val_loss: 0.8640233874320984\n",
            "Saving model, epoch: 13047, train_loss: 0.8962151408195496, val_loss: 0.8640219569206238\n",
            "Saving model, epoch: 13048, train_loss: 0.8962118029594421, val_loss: 0.8640198111534119\n",
            "Saving model, epoch: 13049, train_loss: 0.8962084650993347, val_loss: 0.864018440246582\n",
            "Saving model, epoch: 13050, train_loss: 0.896205484867096, val_loss: 0.8640169501304626\n",
            "Saving model, epoch: 13051, train_loss: 0.8962021470069885, val_loss: 0.8640146255493164\n",
            "Saving model, epoch: 13052, train_loss: 0.8961990475654602, val_loss: 0.8640133738517761\n",
            "Saving model, epoch: 13053, train_loss: 0.8961958289146423, val_loss: 0.8640115261077881\n",
            "Saving model, epoch: 13054, train_loss: 0.8961926102638245, val_loss: 0.8640098571777344\n",
            "Saving model, epoch: 13055, train_loss: 0.8961892127990723, val_loss: 0.8640082478523254\n",
            "Saving model, epoch: 13056, train_loss: 0.8961860537528992, val_loss: 0.864006519317627\n",
            "Saving model, epoch: 13057, train_loss: 0.896182656288147, val_loss: 0.8640041351318359\n",
            "Saving model, epoch: 13058, train_loss: 0.8961795568466187, val_loss: 0.8640028238296509\n",
            "Saving model, epoch: 13059, train_loss: 0.8961763381958008, val_loss: 0.8640010952949524\n",
            "Saving model, epoch: 13060, train_loss: 0.8961731195449829, val_loss: 0.8639994263648987\n",
            "Saving model, epoch: 13061, train_loss: 0.8961697220802307, val_loss: 0.8639972805976868\n",
            "Saving model, epoch: 13062, train_loss: 0.8961665034294128, val_loss: 0.8639957904815674\n",
            "Saving model, epoch: 13063, train_loss: 0.896163284778595, val_loss: 0.863994300365448\n",
            "Saving model, epoch: 13064, train_loss: 0.8961599469184875, val_loss: 0.8639922142028809\n",
            "Saving model, epoch: 13065, train_loss: 0.8961568474769592, val_loss: 0.8639907836914062\n",
            "Saving model, epoch: 13066, train_loss: 0.896153450012207, val_loss: 0.8639888167381287\n",
            "Saving model, epoch: 13067, train_loss: 0.8961502313613892, val_loss: 0.8639869093894958\n",
            "Saving model, epoch: 13068, train_loss: 0.8961470127105713, val_loss: 0.8639854192733765\n",
            "Saving model, epoch: 13069, train_loss: 0.8961437940597534, val_loss: 0.8639838695526123\n",
            "Saving model, epoch: 13070, train_loss: 0.8961405754089355, val_loss: 0.8639822602272034\n",
            "Saving model, epoch: 13071, train_loss: 0.8961372375488281, val_loss: 0.8639799356460571\n",
            "Saving model, epoch: 13072, train_loss: 0.8961339592933655, val_loss: 0.8639787435531616\n",
            "Saving model, epoch: 13073, train_loss: 0.8961305022239685, val_loss: 0.863976240158081\n",
            "Saving model, epoch: 13074, train_loss: 0.8961272835731506, val_loss: 0.8639749884605408\n",
            "Saving model, epoch: 13075, train_loss: 0.896124005317688, val_loss: 0.8639732003211975\n",
            "Saving model, epoch: 13076, train_loss: 0.8961209654808044, val_loss: 0.8639712929725647\n",
            "Saving model, epoch: 13077, train_loss: 0.8961175680160522, val_loss: 0.8639695048332214\n",
            "Saving model, epoch: 13078, train_loss: 0.8961142301559448, val_loss: 0.8639681339263916\n",
            "Saving model, epoch: 13079, train_loss: 0.896111011505127, val_loss: 0.8639662861824036\n",
            "Saving model, epoch: 13080, train_loss: 0.8961077332496643, val_loss: 0.8639642596244812\n",
            "Saving model, epoch: 13081, train_loss: 0.8961043953895569, val_loss: 0.863962709903717\n",
            "Saving model, epoch: 13082, train_loss: 0.896101176738739, val_loss: 0.8639607429504395\n",
            "Saving model, epoch: 13083, train_loss: 0.8960980772972107, val_loss: 0.8639589548110962\n",
            "Saving model, epoch: 13084, train_loss: 0.8960947394371033, val_loss: 0.8639577627182007\n",
            "Saving model, epoch: 13085, train_loss: 0.8960915207862854, val_loss: 0.8639551997184753\n",
            "Saving model, epoch: 13086, train_loss: 0.8960882425308228, val_loss: 0.8639540672302246\n",
            "Saving model, epoch: 13087, train_loss: 0.896084725856781, val_loss: 0.8639516830444336\n",
            "Saving model, epoch: 13088, train_loss: 0.8960815072059631, val_loss: 0.8639505505561829\n",
            "Saving model, epoch: 13089, train_loss: 0.8960781693458557, val_loss: 0.8639488816261292\n",
            "Saving model, epoch: 13090, train_loss: 0.8960750699043274, val_loss: 0.8639468550682068\n",
            "Saving model, epoch: 13091, train_loss: 0.8960716724395752, val_loss: 0.863944947719574\n",
            "Saving model, epoch: 13092, train_loss: 0.8960684537887573, val_loss: 0.8639432787895203\n",
            "Saving model, epoch: 13093, train_loss: 0.8960649967193604, val_loss: 0.8639417290687561\n",
            "Saving model, epoch: 13094, train_loss: 0.8960617780685425, val_loss: 0.8639399409294128\n",
            "Saving model, epoch: 13095, train_loss: 0.8960584998130798, val_loss: 0.86393803358078\n",
            "Saving model, epoch: 13096, train_loss: 0.8960551619529724, val_loss: 0.863936185836792\n",
            "Saving model, epoch: 13097, train_loss: 0.8960519433021545, val_loss: 0.8639343976974487\n",
            "Saving model, epoch: 13098, train_loss: 0.8960487246513367, val_loss: 0.8639330267906189\n",
            "Saving model, epoch: 13099, train_loss: 0.8960453271865845, val_loss: 0.8639307022094727\n",
            "Saving model, epoch: 13100, train_loss: 0.8960421085357666, val_loss: 0.8639292120933533\n",
            "epoch: 13101, train_loss: 0.8960387706756592, val_loss: 0.8639273643493652\n",
            "Saving model, epoch: 13101, train_loss: 0.8960387706756592, val_loss: 0.8639273643493652\n",
            "Saving model, epoch: 13102, train_loss: 0.8960354924201965, val_loss: 0.8639255166053772\n",
            "Saving model, epoch: 13103, train_loss: 0.8960322737693787, val_loss: 0.8639237880706787\n",
            "Saving model, epoch: 13104, train_loss: 0.8960288166999817, val_loss: 0.863922655582428\n",
            "Saving model, epoch: 13105, train_loss: 0.8960256576538086, val_loss: 0.8639205098152161\n",
            "Saving model, epoch: 13106, train_loss: 0.8960223197937012, val_loss: 0.8639187812805176\n",
            "Saving model, epoch: 13107, train_loss: 0.8960189819335938, val_loss: 0.8639168739318848\n",
            "Saving model, epoch: 13108, train_loss: 0.8960155844688416, val_loss: 0.8639149069786072\n",
            "Saving model, epoch: 13109, train_loss: 0.8960124850273132, val_loss: 0.8639135360717773\n",
            "Saving model, epoch: 13110, train_loss: 0.8960091471672058, val_loss: 0.8639113903045654\n",
            "Saving model, epoch: 13111, train_loss: 0.8960058093070984, val_loss: 0.8639097213745117\n",
            "Saving model, epoch: 13112, train_loss: 0.8960025310516357, val_loss: 0.863908052444458\n",
            "Saving model, epoch: 13113, train_loss: 0.8959993124008179, val_loss: 0.8639061450958252\n",
            "Saving model, epoch: 13114, train_loss: 0.8959957957267761, val_loss: 0.863904595375061\n",
            "Saving model, epoch: 13115, train_loss: 0.8959926962852478, val_loss: 0.8639026284217834\n",
            "Saving model, epoch: 13116, train_loss: 0.8959892392158508, val_loss: 0.8639013767242432\n",
            "Saving model, epoch: 13117, train_loss: 0.8959858417510986, val_loss: 0.8638990521430969\n",
            "Saving model, epoch: 13118, train_loss: 0.8959826231002808, val_loss: 0.8638973832130432\n",
            "Saving model, epoch: 13119, train_loss: 0.8959792256355286, val_loss: 0.8638957738876343\n",
            "Saving model, epoch: 13120, train_loss: 0.8959760069847107, val_loss: 0.8638940453529358\n",
            "Saving model, epoch: 13121, train_loss: 0.8959726691246033, val_loss: 0.8638918399810791\n",
            "Saving model, epoch: 13122, train_loss: 0.895969569683075, val_loss: 0.8638901114463806\n",
            "Saving model, epoch: 13123, train_loss: 0.8959660530090332, val_loss: 0.8638886213302612\n",
            "Saving model, epoch: 13124, train_loss: 0.8959627151489258, val_loss: 0.8638867139816284\n",
            "Saving model, epoch: 13125, train_loss: 0.8959594368934631, val_loss: 0.8638853430747986\n",
            "Saving model, epoch: 13126, train_loss: 0.8959560990333557, val_loss: 0.8638833165168762\n",
            "Saving model, epoch: 13127, train_loss: 0.8959527015686035, val_loss: 0.8638811707496643\n",
            "Saving model, epoch: 13128, train_loss: 0.8959492444992065, val_loss: 0.8638795018196106\n",
            "Saving model, epoch: 13129, train_loss: 0.8959458470344543, val_loss: 0.8638778924942017\n",
            "Saving model, epoch: 13130, train_loss: 0.8959426283836365, val_loss: 0.863876223564148\n",
            "Saving model, epoch: 13131, train_loss: 0.8959394097328186, val_loss: 0.8638743758201599\n",
            "Saving model, epoch: 13132, train_loss: 0.8959361910820007, val_loss: 0.8638723492622375\n",
            "Saving model, epoch: 13133, train_loss: 0.8959327936172485, val_loss: 0.8638710975646973\n",
            "Saving model, epoch: 13134, train_loss: 0.8959295749664307, val_loss: 0.8638691306114197\n",
            "Saving model, epoch: 13135, train_loss: 0.8959260582923889, val_loss: 0.8638671636581421\n",
            "Saving model, epoch: 13136, train_loss: 0.8959227204322815, val_loss: 0.8638651967048645\n",
            "Saving model, epoch: 13137, train_loss: 0.8959193229675293, val_loss: 0.8638630509376526\n",
            "Saving model, epoch: 13138, train_loss: 0.8959161043167114, val_loss: 0.863861620426178\n",
            "Saving model, epoch: 13139, train_loss: 0.8959127068519592, val_loss: 0.8638598918914795\n",
            "Saving model, epoch: 13140, train_loss: 0.8959094882011414, val_loss: 0.863858163356781\n",
            "Saving model, epoch: 13141, train_loss: 0.8959060907363892, val_loss: 0.8638566732406616\n",
            "Saving model, epoch: 13142, train_loss: 0.8959026336669922, val_loss: 0.8638546466827393\n",
            "Saving model, epoch: 13143, train_loss: 0.8958994150161743, val_loss: 0.863852858543396\n",
            "Saving model, epoch: 13144, train_loss: 0.8958961963653564, val_loss: 0.8638511300086975\n",
            "Saving model, epoch: 13145, train_loss: 0.8958926796913147, val_loss: 0.8638493418693542\n",
            "Saving model, epoch: 13146, train_loss: 0.8958895206451416, val_loss: 0.8638477921485901\n",
            "Saving model, epoch: 13147, train_loss: 0.8958860039710999, val_loss: 0.8638457655906677\n",
            "Saving model, epoch: 13148, train_loss: 0.895882785320282, val_loss: 0.8638439178466797\n",
            "Saving model, epoch: 13149, train_loss: 0.895879328250885, val_loss: 0.8638425469398499\n",
            "Saving model, epoch: 13150, train_loss: 0.8958761096000671, val_loss: 0.8638399839401245\n",
            "Saving model, epoch: 13151, train_loss: 0.8958727121353149, val_loss: 0.8638386130332947\n",
            "Saving model, epoch: 13152, train_loss: 0.8958693146705627, val_loss: 0.8638367652893066\n",
            "Saving model, epoch: 13153, train_loss: 0.8958656787872314, val_loss: 0.8638353943824768\n",
            "Saving model, epoch: 13154, train_loss: 0.8958624601364136, val_loss: 0.8638330101966858\n",
            "Saving model, epoch: 13155, train_loss: 0.8958593606948853, val_loss: 0.8638316988945007\n",
            "Saving model, epoch: 13156, train_loss: 0.8958560228347778, val_loss: 0.863829493522644\n",
            "Saving model, epoch: 13157, train_loss: 0.8958524465560913, val_loss: 0.8638280630111694\n",
            "Saving model, epoch: 13158, train_loss: 0.8958492279052734, val_loss: 0.8638260960578918\n",
            "Saving model, epoch: 13159, train_loss: 0.8958457112312317, val_loss: 0.8638243675231934\n",
            "Saving model, epoch: 13160, train_loss: 0.8958424925804138, val_loss: 0.8638224005699158\n",
            "Saving model, epoch: 13161, train_loss: 0.8958390355110168, val_loss: 0.8638208508491516\n",
            "Saving model, epoch: 13162, train_loss: 0.8958357572555542, val_loss: 0.8638187050819397\n",
            "Saving model, epoch: 13163, train_loss: 0.8958322405815125, val_loss: 0.8638167381286621\n",
            "Saving model, epoch: 13164, train_loss: 0.895828902721405, val_loss: 0.8638150691986084\n",
            "Saving model, epoch: 13165, train_loss: 0.8958255052566528, val_loss: 0.8638135194778442\n",
            "Saving model, epoch: 13166, train_loss: 0.895822286605835, val_loss: 0.8638114333152771\n",
            "Saving model, epoch: 13167, train_loss: 0.8958189487457275, val_loss: 0.8638097047805786\n",
            "Saving model, epoch: 13168, train_loss: 0.8958154320716858, val_loss: 0.8638079166412354\n",
            "Saving model, epoch: 13169, train_loss: 0.8958121538162231, val_loss: 0.8638061881065369\n",
            "Saving model, epoch: 13170, train_loss: 0.8958088159561157, val_loss: 0.8638046383857727\n",
            "Saving model, epoch: 13171, train_loss: 0.895805299282074, val_loss: 0.8638024926185608\n",
            "Saving model, epoch: 13172, train_loss: 0.8958019614219666, val_loss: 0.8638014197349548\n",
            "Saving model, epoch: 13173, train_loss: 0.8957986831665039, val_loss: 0.8637990951538086\n",
            "Saving model, epoch: 13174, train_loss: 0.8957953453063965, val_loss: 0.8637972474098206\n",
            "Saving model, epoch: 13175, train_loss: 0.8957919478416443, val_loss: 0.8637951612472534\n",
            "Saving model, epoch: 13176, train_loss: 0.8957884311676025, val_loss: 0.8637933731079102\n",
            "Saving model, epoch: 13177, train_loss: 0.8957849144935608, val_loss: 0.8637915253639221\n",
            "Saving model, epoch: 13178, train_loss: 0.8957816958427429, val_loss: 0.8637896776199341\n",
            "Saving model, epoch: 13179, train_loss: 0.895778477191925, val_loss: 0.8637880682945251\n",
            "Saving model, epoch: 13180, train_loss: 0.8957748413085938, val_loss: 0.8637864589691162\n",
            "Saving model, epoch: 13181, train_loss: 0.8957716226577759, val_loss: 0.8637849688529968\n",
            "Saving model, epoch: 13182, train_loss: 0.8957682251930237, val_loss: 0.8637831807136536\n",
            "Saving model, epoch: 13183, train_loss: 0.8957648277282715, val_loss: 0.8637807965278625\n",
            "Saving model, epoch: 13184, train_loss: 0.8957614898681641, val_loss: 0.8637790083885193\n",
            "Saving model, epoch: 13185, train_loss: 0.8957580924034119, val_loss: 0.8637771010398865\n",
            "Saving model, epoch: 13186, train_loss: 0.8957545757293701, val_loss: 0.863775372505188\n",
            "Saving model, epoch: 13187, train_loss: 0.8957513570785522, val_loss: 0.8637735843658447\n",
            "Saving model, epoch: 13188, train_loss: 0.8957478404045105, val_loss: 0.8637720942497253\n",
            "Saving model, epoch: 13189, train_loss: 0.8957444429397583, val_loss: 0.8637701272964478\n",
            "Saving model, epoch: 13190, train_loss: 0.8957409262657166, val_loss: 0.863768458366394\n",
            "Saving model, epoch: 13191, train_loss: 0.8957375884056091, val_loss: 0.8637666702270508\n",
            "Saving model, epoch: 13192, train_loss: 0.8957341909408569, val_loss: 0.8637646436691284\n",
            "Saving model, epoch: 13193, train_loss: 0.89573073387146, val_loss: 0.8637628555297852\n",
            "Saving model, epoch: 13194, train_loss: 0.8957273364067078, val_loss: 0.8637610673904419\n",
            "Saving model, epoch: 13195, train_loss: 0.895723819732666, val_loss: 0.8637593388557434\n",
            "Saving model, epoch: 13196, train_loss: 0.8957206010818481, val_loss: 0.8637570738792419\n",
            "Saving model, epoch: 13197, train_loss: 0.895717203617096, val_loss: 0.8637556433677673\n",
            "Saving model, epoch: 13198, train_loss: 0.895713746547699, val_loss: 0.8637537956237793\n",
            "Saving model, epoch: 13199, train_loss: 0.8957105875015259, val_loss: 0.8637523055076599\n",
            "Saving model, epoch: 13200, train_loss: 0.8957069516181946, val_loss: 0.8637499809265137\n",
            "epoch: 13201, train_loss: 0.8957035541534424, val_loss: 0.8637483716011047\n",
            "Saving model, epoch: 13201, train_loss: 0.8957035541534424, val_loss: 0.8637483716011047\n",
            "Saving model, epoch: 13202, train_loss: 0.8957000374794006, val_loss: 0.863746702671051\n",
            "Saving model, epoch: 13203, train_loss: 0.8956968784332275, val_loss: 0.8637444376945496\n",
            "Saving model, epoch: 13204, train_loss: 0.8956933617591858, val_loss: 0.8637427091598511\n",
            "Saving model, epoch: 13205, train_loss: 0.8956899642944336, val_loss: 0.863741397857666\n",
            "Saving model, epoch: 13206, train_loss: 0.8956866264343262, val_loss: 0.863739013671875\n",
            "Saving model, epoch: 13207, train_loss: 0.8956831097602844, val_loss: 0.8637374043464661\n",
            "Saving model, epoch: 13208, train_loss: 0.8956798315048218, val_loss: 0.8637356758117676\n",
            "Saving model, epoch: 13209, train_loss: 0.8956760764122009, val_loss: 0.8637336492538452\n",
            "Saving model, epoch: 13210, train_loss: 0.8956727981567383, val_loss: 0.8637319207191467\n",
            "Saving model, epoch: 13211, train_loss: 0.8956694602966309, val_loss: 0.8637305498123169\n",
            "Saving model, epoch: 13212, train_loss: 0.8956661224365234, val_loss: 0.8637281656265259\n",
            "Saving model, epoch: 13213, train_loss: 0.8956626057624817, val_loss: 0.8637263178825378\n",
            "Saving model, epoch: 13214, train_loss: 0.8956592082977295, val_loss: 0.8637245297431946\n",
            "Saving model, epoch: 13215, train_loss: 0.8956556916236877, val_loss: 0.8637227416038513\n",
            "Saving model, epoch: 13216, train_loss: 0.8956522941589355, val_loss: 0.8637210130691528\n",
            "Saving model, epoch: 13217, train_loss: 0.8956488966941833, val_loss: 0.86371910572052\n",
            "Saving model, epoch: 13218, train_loss: 0.8956454396247864, val_loss: 0.8637174367904663\n",
            "Saving model, epoch: 13219, train_loss: 0.8956419229507446, val_loss: 0.8637155890464783\n",
            "Saving model, epoch: 13220, train_loss: 0.895638644695282, val_loss: 0.8637135624885559\n",
            "Saving model, epoch: 13221, train_loss: 0.895635187625885, val_loss: 0.8637117147445679\n",
            "Saving model, epoch: 13222, train_loss: 0.8956316709518433, val_loss: 0.8637099862098694\n",
            "Saving model, epoch: 13223, train_loss: 0.8956284523010254, val_loss: 0.8637081384658813\n",
            "Saving model, epoch: 13224, train_loss: 0.8956249356269836, val_loss: 0.8637060523033142\n",
            "Saving model, epoch: 13225, train_loss: 0.8956215381622314, val_loss: 0.8637046217918396\n",
            "Saving model, epoch: 13226, train_loss: 0.8956180214881897, val_loss: 0.8637024760246277\n",
            "Saving model, epoch: 13227, train_loss: 0.8956148028373718, val_loss: 0.863700807094574\n",
            "Saving model, epoch: 13228, train_loss: 0.8956111669540405, val_loss: 0.863699197769165\n",
            "Saving model, epoch: 13229, train_loss: 0.8956076502799988, val_loss: 0.8636967539787292\n",
            "Saving model, epoch: 13230, train_loss: 0.8956042528152466, val_loss: 0.8636950850486755\n",
            "Saving model, epoch: 13231, train_loss: 0.8956008553504944, val_loss: 0.8636934757232666\n",
            "Saving model, epoch: 13232, train_loss: 0.8955973386764526, val_loss: 0.863692045211792\n",
            "Saving model, epoch: 13233, train_loss: 0.8955940008163452, val_loss: 0.8636898398399353\n",
            "Saving model, epoch: 13234, train_loss: 0.895590603351593, val_loss: 0.8636878132820129\n",
            "Saving model, epoch: 13235, train_loss: 0.8955872058868408, val_loss: 0.8636857867240906\n",
            "Saving model, epoch: 13236, train_loss: 0.89558345079422, val_loss: 0.8636844158172607\n",
            "Saving model, epoch: 13237, train_loss: 0.8955800533294678, val_loss: 0.8636825680732727\n",
            "Saving model, epoch: 13238, train_loss: 0.8955767154693604, val_loss: 0.8636801838874817\n",
            "Saving model, epoch: 13239, train_loss: 0.8955731391906738, val_loss: 0.8636783361434937\n",
            "Saving model, epoch: 13240, train_loss: 0.8955698013305664, val_loss: 0.8636770844459534\n",
            "Saving model, epoch: 13241, train_loss: 0.8955662846565247, val_loss: 0.8636754751205444\n",
            "Saving model, epoch: 13242, train_loss: 0.8955628871917725, val_loss: 0.8636731505393982\n",
            "Saving model, epoch: 13243, train_loss: 0.8955594301223755, val_loss: 0.8636713027954102\n",
            "Saving model, epoch: 13244, train_loss: 0.8955559134483337, val_loss: 0.8636692762374878\n",
            "Saving model, epoch: 13245, train_loss: 0.8955526351928711, val_loss: 0.863667368888855\n",
            "Saving model, epoch: 13246, train_loss: 0.8955489993095398, val_loss: 0.863665759563446\n",
            "Saving model, epoch: 13247, train_loss: 0.8955456018447876, val_loss: 0.8636641502380371\n",
            "Saving model, epoch: 13248, train_loss: 0.8955419659614563, val_loss: 0.8636618852615356\n",
            "Saving model, epoch: 13249, train_loss: 0.8955385684967041, val_loss: 0.8636600971221924\n",
            "Saving model, epoch: 13250, train_loss: 0.8955351114273071, val_loss: 0.8636581897735596\n",
            "Saving model, epoch: 13251, train_loss: 0.8955317139625549, val_loss: 0.8636568784713745\n",
            "Saving model, epoch: 13252, train_loss: 0.8955281972885132, val_loss: 0.8636546730995178\n",
            "Saving model, epoch: 13253, train_loss: 0.8955246806144714, val_loss: 0.8636528849601746\n",
            "Saving model, epoch: 13254, train_loss: 0.8955212831497192, val_loss: 0.8636506795883179\n",
            "Saving model, epoch: 13255, train_loss: 0.895517885684967, val_loss: 0.863649308681488\n",
            "Saving model, epoch: 13256, train_loss: 0.8955144286155701, val_loss: 0.8636474609375\n",
            "Saving model, epoch: 13257, train_loss: 0.8955108523368835, val_loss: 0.8636455535888672\n",
            "Saving model, epoch: 13258, train_loss: 0.8955076336860657, val_loss: 0.8636434078216553\n",
            "Saving model, epoch: 13259, train_loss: 0.8955038785934448, val_loss: 0.8636419773101807\n",
            "Saving model, epoch: 13260, train_loss: 0.8955003619194031, val_loss: 0.863640308380127\n",
            "Saving model, epoch: 13261, train_loss: 0.8954970836639404, val_loss: 0.8636384010314941\n",
            "Saving model, epoch: 13262, train_loss: 0.8954935669898987, val_loss: 0.863636314868927\n",
            "Saving model, epoch: 13263, train_loss: 0.8954899311065674, val_loss: 0.8636344075202942\n",
            "Saving model, epoch: 13264, train_loss: 0.8954865336418152, val_loss: 0.8636324405670166\n",
            "Saving model, epoch: 13265, train_loss: 0.8954830765724182, val_loss: 0.8636308312416077\n",
            "Saving model, epoch: 13266, train_loss: 0.8954795598983765, val_loss: 0.8636286854743958\n",
            "Saving model, epoch: 13267, train_loss: 0.8954759836196899, val_loss: 0.8636260032653809\n",
            "Saving model, epoch: 13268, train_loss: 0.8954724669456482, val_loss: 0.8636249303817749\n",
            "Saving model, epoch: 13269, train_loss: 0.8954691290855408, val_loss: 0.8636232018470764\n",
            "Saving model, epoch: 13270, train_loss: 0.895465612411499, val_loss: 0.863620936870575\n",
            "Saving model, epoch: 13271, train_loss: 0.8954620957374573, val_loss: 0.8636195063591003\n",
            "Saving model, epoch: 13272, train_loss: 0.8954585790634155, val_loss: 0.8636178374290466\n",
            "Saving model, epoch: 13273, train_loss: 0.8954549431800842, val_loss: 0.8636158108711243\n",
            "Saving model, epoch: 13274, train_loss: 0.895451545715332, val_loss: 0.8636142015457153\n",
            "Saving model, epoch: 13275, train_loss: 0.8954481482505798, val_loss: 0.863612174987793\n",
            "Saving model, epoch: 13276, train_loss: 0.8954446911811829, val_loss: 0.8636101484298706\n",
            "Saving model, epoch: 13277, train_loss: 0.8954409956932068, val_loss: 0.8636083006858826\n",
            "Saving model, epoch: 13278, train_loss: 0.8954376578330994, val_loss: 0.8636062741279602\n",
            "Saving model, epoch: 13279, train_loss: 0.8954341411590576, val_loss: 0.8636044859886169\n",
            "Saving model, epoch: 13280, train_loss: 0.8954306244850159, val_loss: 0.8636028170585632\n",
            "Saving model, epoch: 13281, train_loss: 0.8954271078109741, val_loss: 0.8636008501052856\n",
            "Saving model, epoch: 13282, train_loss: 0.8954237103462219, val_loss: 0.8635991811752319\n",
            "Saving model, epoch: 13283, train_loss: 0.8954201936721802, val_loss: 0.8635967969894409\n",
            "Saving model, epoch: 13284, train_loss: 0.895416796207428, val_loss: 0.8635948896408081\n",
            "Saving model, epoch: 13285, train_loss: 0.8954130411148071, val_loss: 0.863593578338623\n",
            "Saving model, epoch: 13286, train_loss: 0.8954097628593445, val_loss: 0.863591730594635\n",
            "Saving model, epoch: 13287, train_loss: 0.8954061269760132, val_loss: 0.8635897040367126\n",
            "Saving model, epoch: 13288, train_loss: 0.895402729511261, val_loss: 0.8635875582695007\n",
            "Saving model, epoch: 13289, train_loss: 0.8953989148139954, val_loss: 0.8635860681533813\n",
            "Saving model, epoch: 13290, train_loss: 0.8953956961631775, val_loss: 0.8635839223861694\n",
            "Saving model, epoch: 13291, train_loss: 0.8953921794891357, val_loss: 0.8635823130607605\n",
            "Saving model, epoch: 13292, train_loss: 0.895388662815094, val_loss: 0.8635804057121277\n",
            "Saving model, epoch: 13293, train_loss: 0.8953849077224731, val_loss: 0.863578200340271\n",
            "Saving model, epoch: 13294, train_loss: 0.8953816294670105, val_loss: 0.8635766506195068\n",
            "Saving model, epoch: 13295, train_loss: 0.8953779935836792, val_loss: 0.8635746240615845\n",
            "Saving model, epoch: 13296, train_loss: 0.895374596118927, val_loss: 0.8635725378990173\n",
            "Saving model, epoch: 13297, train_loss: 0.8953711986541748, val_loss: 0.8635712265968323\n",
            "Saving model, epoch: 13298, train_loss: 0.895367443561554, val_loss: 0.8635690808296204\n",
            "Saving model, epoch: 13299, train_loss: 0.8953639268875122, val_loss: 0.8635671734809875\n",
            "Saving model, epoch: 13300, train_loss: 0.89536052942276, val_loss: 0.8635650277137756\n",
            "epoch: 13301, train_loss: 0.8953568935394287, val_loss: 0.8635632395744324\n",
            "Saving model, epoch: 13301, train_loss: 0.8953568935394287, val_loss: 0.8635632395744324\n",
            "Saving model, epoch: 13302, train_loss: 0.8953533172607422, val_loss: 0.8635613918304443\n",
            "Saving model, epoch: 13303, train_loss: 0.8953499794006348, val_loss: 0.8635596036911011\n",
            "Saving model, epoch: 13304, train_loss: 0.8953462839126587, val_loss: 0.8635578155517578\n",
            "Saving model, epoch: 13305, train_loss: 0.8953428268432617, val_loss: 0.863555908203125\n",
            "Saving model, epoch: 13306, train_loss: 0.8953391313552856, val_loss: 0.8635536432266235\n",
            "Saving model, epoch: 13307, train_loss: 0.8953356146812439, val_loss: 0.8635516166687012\n",
            "Saving model, epoch: 13308, train_loss: 0.8953322768211365, val_loss: 0.8635497689247131\n",
            "Saving model, epoch: 13309, train_loss: 0.8953285813331604, val_loss: 0.8635486364364624\n",
            "Saving model, epoch: 13310, train_loss: 0.8953251838684082, val_loss: 0.8635461926460266\n",
            "Saving model, epoch: 13311, train_loss: 0.8953216671943665, val_loss: 0.8635442852973938\n",
            "Saving model, epoch: 13312, train_loss: 0.8953180313110352, val_loss: 0.8635427951812744\n",
            "Saving model, epoch: 13313, train_loss: 0.8953145146369934, val_loss: 0.8635410666465759\n",
            "Saving model, epoch: 13314, train_loss: 0.8953109979629517, val_loss: 0.863538920879364\n",
            "Saving model, epoch: 13315, train_loss: 0.8953073620796204, val_loss: 0.8635368347167969\n",
            "Saving model, epoch: 13316, train_loss: 0.8953038454055786, val_loss: 0.863535463809967\n",
            "Saving model, epoch: 13317, train_loss: 0.8953004479408264, val_loss: 0.8635333180427551\n",
            "Saving model, epoch: 13318, train_loss: 0.8952968120574951, val_loss: 0.8635316491127014\n",
            "Saving model, epoch: 13319, train_loss: 0.8952932357788086, val_loss: 0.8635292053222656\n",
            "Saving model, epoch: 13320, train_loss: 0.8952897787094116, val_loss: 0.863527774810791\n",
            "Saving model, epoch: 13321, train_loss: 0.8952862024307251, val_loss: 0.8635258078575134\n",
            "Saving model, epoch: 13322, train_loss: 0.8952825665473938, val_loss: 0.8635234832763672\n",
            "Saving model, epoch: 13323, train_loss: 0.8952791690826416, val_loss: 0.8635222315788269\n",
            "Saving model, epoch: 13324, train_loss: 0.8952755331993103, val_loss: 0.8635198473930359\n",
            "Saving model, epoch: 13325, train_loss: 0.895271897315979, val_loss: 0.8635179400444031\n",
            "Saving model, epoch: 13326, train_loss: 0.8952683210372925, val_loss: 0.8635165095329285\n",
            "Saving model, epoch: 13327, train_loss: 0.8952648639678955, val_loss: 0.8635143041610718\n",
            "Saving model, epoch: 13328, train_loss: 0.895261287689209, val_loss: 0.8635122776031494\n",
            "Saving model, epoch: 13329, train_loss: 0.895257830619812, val_loss: 0.8635105490684509\n",
            "Saving model, epoch: 13330, train_loss: 0.8952542543411255, val_loss: 0.8635088801383972\n",
            "Saving model, epoch: 13331, train_loss: 0.8952507972717285, val_loss: 0.8635068535804749\n",
            "Saving model, epoch: 13332, train_loss: 0.8952469825744629, val_loss: 0.8635046482086182\n",
            "Saving model, epoch: 13333, train_loss: 0.8952435851097107, val_loss: 0.8635028600692749\n",
            "Saving model, epoch: 13334, train_loss: 0.8952401876449585, val_loss: 0.8635013103485107\n",
            "Saving model, epoch: 13335, train_loss: 0.8952364325523376, val_loss: 0.863499104976654\n",
            "Saving model, epoch: 13336, train_loss: 0.8952329158782959, val_loss: 0.8634979724884033\n",
            "Saving model, epoch: 13337, train_loss: 0.8952292203903198, val_loss: 0.8634953498840332\n",
            "Saving model, epoch: 13338, train_loss: 0.8952257037162781, val_loss: 0.8634936213493347\n",
            "Saving model, epoch: 13339, train_loss: 0.8952223062515259, val_loss: 0.8634914755821228\n",
            "Saving model, epoch: 13340, train_loss: 0.8952186703681946, val_loss: 0.8634897470474243\n",
            "Saving model, epoch: 13341, train_loss: 0.8952149152755737, val_loss: 0.8634876608848572\n",
            "Saving model, epoch: 13342, train_loss: 0.895211398601532, val_loss: 0.8634861707687378\n",
            "Saving model, epoch: 13343, train_loss: 0.8952080011367798, val_loss: 0.8634838461875916\n",
            "Saving model, epoch: 13344, train_loss: 0.8952043652534485, val_loss: 0.8634817004203796\n",
            "Saving model, epoch: 13345, train_loss: 0.895200788974762, val_loss: 0.8634800314903259\n",
            "Saving model, epoch: 13346, train_loss: 0.8951971530914307, val_loss: 0.8634781837463379\n",
            "Saving model, epoch: 13347, train_loss: 0.8951935172080994, val_loss: 0.8634760975837708\n",
            "Saving model, epoch: 13348, train_loss: 0.8951900005340576, val_loss: 0.8634749054908752\n",
            "Saving model, epoch: 13349, train_loss: 0.8951863050460815, val_loss: 0.8634722232818604\n",
            "Saving model, epoch: 13350, train_loss: 0.8951829075813293, val_loss: 0.8634704947471619\n",
            "Saving model, epoch: 13351, train_loss: 0.895179271697998, val_loss: 0.8634689450263977\n",
            "Saving model, epoch: 13352, train_loss: 0.8951756358146667, val_loss: 0.8634667992591858\n",
            "Saving model, epoch: 13353, train_loss: 0.895172119140625, val_loss: 0.8634653687477112\n",
            "Saving model, epoch: 13354, train_loss: 0.8951684236526489, val_loss: 0.8634635210037231\n",
            "Saving model, epoch: 13355, train_loss: 0.8951650261878967, val_loss: 0.8634608387947083\n",
            "Saving model, epoch: 13356, train_loss: 0.8951612710952759, val_loss: 0.8634592890739441\n",
            "Saving model, epoch: 13357, train_loss: 0.8951576948165894, val_loss: 0.863457441329956\n",
            "Saving model, epoch: 13358, train_loss: 0.8951539397239685, val_loss: 0.8634556531906128\n",
            "Saving model, epoch: 13359, train_loss: 0.8951505422592163, val_loss: 0.8634536862373352\n",
            "Saving model, epoch: 13360, train_loss: 0.8951471447944641, val_loss: 0.8634518384933472\n",
            "Saving model, epoch: 13361, train_loss: 0.8951433897018433, val_loss: 0.8634496927261353\n",
            "Saving model, epoch: 13362, train_loss: 0.8951396942138672, val_loss: 0.8634478449821472\n",
            "Saving model, epoch: 13363, train_loss: 0.895136296749115, val_loss: 0.8634463548660278\n",
            "Saving model, epoch: 13364, train_loss: 0.8951324820518494, val_loss: 0.8634439706802368\n",
            "Saving model, epoch: 13365, train_loss: 0.8951290249824524, val_loss: 0.8634418249130249\n",
            "Saving model, epoch: 13366, train_loss: 0.8951254487037659, val_loss: 0.8634405136108398\n",
            "Saving model, epoch: 13367, train_loss: 0.8951218128204346, val_loss: 0.8634380102157593\n",
            "Saving model, epoch: 13368, train_loss: 0.8951181173324585, val_loss: 0.8634362816810608\n",
            "Saving model, epoch: 13369, train_loss: 0.8951146006584167, val_loss: 0.8634346127510071\n",
            "Saving model, epoch: 13370, train_loss: 0.8951108455657959, val_loss: 0.8634323477745056\n",
            "Saving model, epoch: 13371, train_loss: 0.8951073288917542, val_loss: 0.8634307384490967\n",
            "Saving model, epoch: 13372, train_loss: 0.8951038122177124, val_loss: 0.8634288907051086\n",
            "Saving model, epoch: 13373, train_loss: 0.8951001167297363, val_loss: 0.8634268641471863\n",
            "Saving model, epoch: 13374, train_loss: 0.895096480846405, val_loss: 0.8634247779846191\n",
            "Saving model, epoch: 13375, train_loss: 0.8950928449630737, val_loss: 0.863422691822052\n",
            "Saving model, epoch: 13376, train_loss: 0.8950892686843872, val_loss: 0.863420844078064\n",
            "Saving model, epoch: 13377, train_loss: 0.8950856328010559, val_loss: 0.8634191155433655\n",
            "Saving model, epoch: 13378, train_loss: 0.8950822353363037, val_loss: 0.8634169101715088\n",
            "Saving model, epoch: 13379, train_loss: 0.8950784206390381, val_loss: 0.8634151816368103\n",
            "Saving model, epoch: 13380, train_loss: 0.8950747847557068, val_loss: 0.8634131550788879\n",
            "Saving model, epoch: 13381, train_loss: 0.8950711488723755, val_loss: 0.8634112477302551\n",
            "Saving model, epoch: 13382, train_loss: 0.8950677514076233, val_loss: 0.8634095191955566\n",
            "Saving model, epoch: 13383, train_loss: 0.8950640559196472, val_loss: 0.8634074330329895\n",
            "Saving model, epoch: 13384, train_loss: 0.8950603008270264, val_loss: 0.8634055256843567\n",
            "Saving model, epoch: 13385, train_loss: 0.8950567841529846, val_loss: 0.8634036779403687\n",
            "Saving model, epoch: 13386, train_loss: 0.8950532674789429, val_loss: 0.8634016513824463\n",
            "Saving model, epoch: 13387, train_loss: 0.8950495719909668, val_loss: 0.8634001612663269\n",
            "Saving model, epoch: 13388, train_loss: 0.8950459361076355, val_loss: 0.8633979558944702\n",
            "Saving model, epoch: 13389, train_loss: 0.8950422406196594, val_loss: 0.8633959293365479\n",
            "Saving model, epoch: 13390, train_loss: 0.8950387239456177, val_loss: 0.8633940815925598\n",
            "Saving model, epoch: 13391, train_loss: 0.895034909248352, val_loss: 0.8633921146392822\n",
            "Saving model, epoch: 13392, train_loss: 0.8950313925743103, val_loss: 0.8633905053138733\n",
            "Saving model, epoch: 13393, train_loss: 0.895027756690979, val_loss: 0.8633880019187927\n",
            "Saving model, epoch: 13394, train_loss: 0.8950240612030029, val_loss: 0.863386332988739\n",
            "Saving model, epoch: 13395, train_loss: 0.8950205445289612, val_loss: 0.8633843064308167\n",
            "Saving model, epoch: 13396, train_loss: 0.8950169086456299, val_loss: 0.8633826971054077\n",
            "Saving model, epoch: 13397, train_loss: 0.8950130939483643, val_loss: 0.863380491733551\n",
            "Saving model, epoch: 13398, train_loss: 0.8950095772743225, val_loss: 0.8633785843849182\n",
            "Saving model, epoch: 13399, train_loss: 0.8950059413909912, val_loss: 0.8633766770362854\n",
            "Saving model, epoch: 13400, train_loss: 0.8950021266937256, val_loss: 0.8633748888969421\n",
            "epoch: 13401, train_loss: 0.8949986100196838, val_loss: 0.8633726835250854\n",
            "Saving model, epoch: 13401, train_loss: 0.8949986100196838, val_loss: 0.8633726835250854\n",
            "Saving model, epoch: 13402, train_loss: 0.8949950933456421, val_loss: 0.8633708953857422\n",
            "Saving model, epoch: 13403, train_loss: 0.894991397857666, val_loss: 0.863368570804596\n",
            "Saving model, epoch: 13404, train_loss: 0.8949877023696899, val_loss: 0.8633671402931213\n",
            "Saving model, epoch: 13405, train_loss: 0.8949841856956482, val_loss: 0.8633653521537781\n",
            "Saving model, epoch: 13406, train_loss: 0.8949803709983826, val_loss: 0.8633632063865662\n",
            "Saving model, epoch: 13407, train_loss: 0.8949767351150513, val_loss: 0.863360583782196\n",
            "Saving model, epoch: 13408, train_loss: 0.8949732184410095, val_loss: 0.863359808921814\n",
            "Saving model, epoch: 13409, train_loss: 0.8949695229530334, val_loss: 0.8633573651313782\n",
            "Saving model, epoch: 13410, train_loss: 0.8949660062789917, val_loss: 0.8633553981781006\n",
            "Saving model, epoch: 13411, train_loss: 0.8949621319770813, val_loss: 0.8633532524108887\n",
            "Saving model, epoch: 13412, train_loss: 0.8949584364891052, val_loss: 0.8633515238761902\n",
            "Saving model, epoch: 13413, train_loss: 0.894955039024353, val_loss: 0.8633494973182678\n",
            "Saving model, epoch: 13414, train_loss: 0.8949514031410217, val_loss: 0.863347589969635\n",
            "Saving model, epoch: 13415, train_loss: 0.8949477672576904, val_loss: 0.8633450865745544\n",
            "Saving model, epoch: 13416, train_loss: 0.8949439525604248, val_loss: 0.8633435368537903\n",
            "Saving model, epoch: 13417, train_loss: 0.8949402570724487, val_loss: 0.8633416295051575\n",
            "Saving model, epoch: 13418, train_loss: 0.8949366211891174, val_loss: 0.8633397817611694\n",
            "Saving model, epoch: 13419, train_loss: 0.8949330449104309, val_loss: 0.8633377552032471\n",
            "Saving model, epoch: 13420, train_loss: 0.8949292898178101, val_loss: 0.8633359670639038\n",
            "Saving model, epoch: 13421, train_loss: 0.894925594329834, val_loss: 0.8633338212966919\n",
            "Saving model, epoch: 13422, train_loss: 0.8949220776557922, val_loss: 0.8633321523666382\n",
            "Saving model, epoch: 13423, train_loss: 0.8949182629585266, val_loss: 0.8633301854133606\n",
            "Saving model, epoch: 13424, train_loss: 0.8949146270751953, val_loss: 0.8633280992507935\n",
            "Saving model, epoch: 13425, train_loss: 0.8949108123779297, val_loss: 0.8633260726928711\n",
            "Saving model, epoch: 13426, train_loss: 0.8949072957038879, val_loss: 0.8633244633674622\n",
            "Saving model, epoch: 13427, train_loss: 0.8949036002159119, val_loss: 0.863321840763092\n",
            "Saving model, epoch: 13428, train_loss: 0.8948999643325806, val_loss: 0.8633203506469727\n",
            "Saving model, epoch: 13429, train_loss: 0.8948963284492493, val_loss: 0.8633185029029846\n",
            "Saving model, epoch: 13430, train_loss: 0.8948926329612732, val_loss: 0.8633163571357727\n",
            "Saving model, epoch: 13431, train_loss: 0.8948889374732971, val_loss: 0.8633143305778503\n",
            "Saving model, epoch: 13432, train_loss: 0.8948853015899658, val_loss: 0.8633124232292175\n",
            "Saving model, epoch: 13433, train_loss: 0.8948814868927002, val_loss: 0.8633106350898743\n",
            "Saving model, epoch: 13434, train_loss: 0.8948778510093689, val_loss: 0.8633083701133728\n",
            "Saving model, epoch: 13435, train_loss: 0.8948743343353271, val_loss: 0.8633065819740295\n",
            "Saving model, epoch: 13436, train_loss: 0.8948705196380615, val_loss: 0.8633049726486206\n",
            "Saving model, epoch: 13437, train_loss: 0.8948668241500854, val_loss: 0.8633031845092773\n",
            "Saving model, epoch: 13438, train_loss: 0.8948631882667542, val_loss: 0.8633006811141968\n",
            "Saving model, epoch: 13439, train_loss: 0.8948593735694885, val_loss: 0.8632989525794983\n",
            "Saving model, epoch: 13440, train_loss: 0.8948557376861572, val_loss: 0.8632970452308655\n",
            "Saving model, epoch: 13441, train_loss: 0.8948521614074707, val_loss: 0.8632950186729431\n",
            "Saving model, epoch: 13442, train_loss: 0.8948483467102051, val_loss: 0.8632929921150208\n",
            "Saving model, epoch: 13443, train_loss: 0.8948447108268738, val_loss: 0.8632910847663879\n",
            "Saving model, epoch: 13444, train_loss: 0.8948410749435425, val_loss: 0.8632890582084656\n",
            "Saving model, epoch: 13445, train_loss: 0.8948373794555664, val_loss: 0.8632870316505432\n",
            "Saving model, epoch: 13446, train_loss: 0.8948336839675903, val_loss: 0.8632854223251343\n",
            "Saving model, epoch: 13447, train_loss: 0.8948298692703247, val_loss: 0.8632829785346985\n",
            "Saving model, epoch: 13448, train_loss: 0.894826352596283, val_loss: 0.8632813096046448\n",
            "Saving model, epoch: 13449, train_loss: 0.8948225975036621, val_loss: 0.8632796406745911\n",
            "Saving model, epoch: 13450, train_loss: 0.8948190212249756, val_loss: 0.8632771968841553\n",
            "Saving model, epoch: 13451, train_loss: 0.8948151469230652, val_loss: 0.8632749915122986\n",
            "Saving model, epoch: 13452, train_loss: 0.8948114514350891, val_loss: 0.8632736802101135\n",
            "Saving model, epoch: 13453, train_loss: 0.894807755947113, val_loss: 0.8632715344429016\n",
            "Saving model, epoch: 13454, train_loss: 0.8948042392730713, val_loss: 0.8632688522338867\n",
            "Saving model, epoch: 13455, train_loss: 0.8948003053665161, val_loss: 0.8632671236991882\n",
            "Saving model, epoch: 13456, train_loss: 0.8947969079017639, val_loss: 0.8632651567459106\n",
            "Saving model, epoch: 13457, train_loss: 0.8947931528091431, val_loss: 0.8632636070251465\n",
            "Saving model, epoch: 13458, train_loss: 0.8947892785072327, val_loss: 0.8632618188858032\n",
            "Saving model, epoch: 13459, train_loss: 0.8947857618331909, val_loss: 0.86326003074646\n",
            "Saving model, epoch: 13460, train_loss: 0.8947819471359253, val_loss: 0.8632574081420898\n",
            "Saving model, epoch: 13461, train_loss: 0.8947781920433044, val_loss: 0.8632556796073914\n",
            "Saving model, epoch: 13462, train_loss: 0.8947743773460388, val_loss: 0.8632540106773376\n",
            "Saving model, epoch: 13463, train_loss: 0.8947707414627075, val_loss: 0.8632515072822571\n",
            "Saving model, epoch: 13464, train_loss: 0.894767165184021, val_loss: 0.8632498979568481\n",
            "Saving model, epoch: 13465, train_loss: 0.8947632312774658, val_loss: 0.8632482290267944\n",
            "Saving model, epoch: 13466, train_loss: 0.8947597146034241, val_loss: 0.8632456660270691\n",
            "Saving model, epoch: 13467, train_loss: 0.8947558999061584, val_loss: 0.8632436394691467\n",
            "Saving model, epoch: 13468, train_loss: 0.8947522640228271, val_loss: 0.8632420897483826\n",
            "Saving model, epoch: 13469, train_loss: 0.8947486877441406, val_loss: 0.863240122795105\n",
            "Saving model, epoch: 13470, train_loss: 0.8947446346282959, val_loss: 0.8632379770278931\n",
            "Saving model, epoch: 13471, train_loss: 0.8947409987449646, val_loss: 0.8632358908653259\n",
            "Saving model, epoch: 13472, train_loss: 0.894737184047699, val_loss: 0.8632339239120483\n",
            "Saving model, epoch: 13473, train_loss: 0.8947336077690125, val_loss: 0.8632323145866394\n",
            "Saving model, epoch: 13474, train_loss: 0.8947299718856812, val_loss: 0.863230288028717\n",
            "Saving model, epoch: 13475, train_loss: 0.8947262763977051, val_loss: 0.8632278442382812\n",
            "Saving model, epoch: 13476, train_loss: 0.8947223424911499, val_loss: 0.8632258772850037\n",
            "Saving model, epoch: 13477, train_loss: 0.8947187066078186, val_loss: 0.86322420835495\n",
            "Saving model, epoch: 13478, train_loss: 0.894714891910553, val_loss: 0.8632219433784485\n",
            "Saving model, epoch: 13479, train_loss: 0.8947110772132874, val_loss: 0.8632200360298157\n",
            "Saving model, epoch: 13480, train_loss: 0.8947075605392456, val_loss: 0.8632181882858276\n",
            "Saving model, epoch: 13481, train_loss: 0.8947036266326904, val_loss: 0.8632163405418396\n",
            "Saving model, epoch: 13482, train_loss: 0.8947001099586487, val_loss: 0.8632140159606934\n",
            "Saving model, epoch: 13483, train_loss: 0.8946962952613831, val_loss: 0.8632122874259949\n",
            "Saving model, epoch: 13484, train_loss: 0.894692599773407, val_loss: 0.8632100820541382\n",
            "Saving model, epoch: 13485, train_loss: 0.8946889638900757, val_loss: 0.8632081747055054\n",
            "Saving model, epoch: 13486, train_loss: 0.8946851491928101, val_loss: 0.8632059097290039\n",
            "Saving model, epoch: 13487, train_loss: 0.8946813344955444, val_loss: 0.8632043600082397\n",
            "Saving model, epoch: 13488, train_loss: 0.8946777582168579, val_loss: 0.8632020354270935\n",
            "Saving model, epoch: 13489, train_loss: 0.8946738243103027, val_loss: 0.8632001876831055\n",
            "Saving model, epoch: 13490, train_loss: 0.8946700692176819, val_loss: 0.8631983399391174\n",
            "Saving model, epoch: 13491, train_loss: 0.8946664929389954, val_loss: 0.8631965517997742\n",
            "Saving model, epoch: 13492, train_loss: 0.8946626782417297, val_loss: 0.8631942868232727\n",
            "Saving model, epoch: 13493, train_loss: 0.8946589231491089, val_loss: 0.863192081451416\n",
            "Saving model, epoch: 13494, train_loss: 0.8946552276611328, val_loss: 0.8631905913352966\n",
            "Saving model, epoch: 13495, train_loss: 0.8946514129638672, val_loss: 0.8631879687309265\n",
            "Saving model, epoch: 13496, train_loss: 0.8946475982666016, val_loss: 0.863186240196228\n",
            "Saving model, epoch: 13497, train_loss: 0.8946437835693359, val_loss: 0.8631845712661743\n",
            "Saving model, epoch: 13498, train_loss: 0.8946401476860046, val_loss: 0.863182783126831\n",
            "Saving model, epoch: 13499, train_loss: 0.894636332988739, val_loss: 0.8631805181503296\n",
            "Saving model, epoch: 13500, train_loss: 0.8946325182914734, val_loss: 0.8631783723831177\n",
            "epoch: 13501, train_loss: 0.8946288228034973, val_loss: 0.8631762862205505\n",
            "Saving model, epoch: 13501, train_loss: 0.8946288228034973, val_loss: 0.8631762862205505\n",
            "Saving model, epoch: 13502, train_loss: 0.8946250677108765, val_loss: 0.8631741404533386\n",
            "Saving model, epoch: 13503, train_loss: 0.8946212530136108, val_loss: 0.8631725907325745\n",
            "Saving model, epoch: 13504, train_loss: 0.8946174383163452, val_loss: 0.8631702661514282\n",
            "Saving model, epoch: 13505, train_loss: 0.8946138620376587, val_loss: 0.8631683588027954\n",
            "Saving model, epoch: 13506, train_loss: 0.8946099877357483, val_loss: 0.8631664514541626\n",
            "Saving model, epoch: 13507, train_loss: 0.8946064114570618, val_loss: 0.8631644248962402\n",
            "Saving model, epoch: 13508, train_loss: 0.8946024775505066, val_loss: 0.8631626963615417\n",
            "Saving model, epoch: 13509, train_loss: 0.8945988416671753, val_loss: 0.8631603121757507\n",
            "Saving model, epoch: 13510, train_loss: 0.8945950269699097, val_loss: 0.8631581664085388\n",
            "Saving model, epoch: 13511, train_loss: 0.8945911526679993, val_loss: 0.8631565570831299\n",
            "Saving model, epoch: 13512, train_loss: 0.894587516784668, val_loss: 0.8631545305252075\n",
            "Saving model, epoch: 13513, train_loss: 0.8945837020874023, val_loss: 0.8631523847579956\n",
            "Saving model, epoch: 13514, train_loss: 0.8945797681808472, val_loss: 0.8631507158279419\n",
            "Saving model, epoch: 13515, train_loss: 0.8945761322975159, val_loss: 0.8631485104560852\n",
            "Saving model, epoch: 13516, train_loss: 0.8945724368095398, val_loss: 0.8631466031074524\n",
            "Saving model, epoch: 13517, train_loss: 0.8945685029029846, val_loss: 0.8631449341773987\n",
            "Saving model, epoch: 13518, train_loss: 0.8945648074150085, val_loss: 0.8631424903869629\n",
            "Saving model, epoch: 13519, train_loss: 0.8945611715316772, val_loss: 0.8631405830383301\n",
            "Saving model, epoch: 13520, train_loss: 0.8945571780204773, val_loss: 0.8631385564804077\n",
            "Saving model, epoch: 13521, train_loss: 0.8945533633232117, val_loss: 0.8631365299224854\n",
            "Saving model, epoch: 13522, train_loss: 0.8945496082305908, val_loss: 0.8631348609924316\n",
            "Saving model, epoch: 13523, train_loss: 0.8945460319519043, val_loss: 0.8631329536437988\n",
            "Saving model, epoch: 13524, train_loss: 0.8945420980453491, val_loss: 0.8631306290626526\n",
            "Saving model, epoch: 13525, train_loss: 0.894538402557373, val_loss: 0.8631284832954407\n",
            "Saving model, epoch: 13526, train_loss: 0.8945346474647522, val_loss: 0.8631266355514526\n",
            "Saving model, epoch: 13527, train_loss: 0.8945308327674866, val_loss: 0.8631245493888855\n",
            "Saving model, epoch: 13528, train_loss: 0.894527018070221, val_loss: 0.8631225228309631\n",
            "Saving model, epoch: 13529, train_loss: 0.8945232033729553, val_loss: 0.8631203770637512\n",
            "Saving model, epoch: 13530, train_loss: 0.8945195078849792, val_loss: 0.8631182312965393\n",
            "Saving model, epoch: 13531, train_loss: 0.8945156931877136, val_loss: 0.8631163239479065\n",
            "Saving model, epoch: 13532, train_loss: 0.8945117592811584, val_loss: 0.8631146550178528\n",
            "Saving model, epoch: 13533, train_loss: 0.8945081233978271, val_loss: 0.8631125688552856\n",
            "Saving model, epoch: 13534, train_loss: 0.8945042490959167, val_loss: 0.863110363483429\n",
            "Saving model, epoch: 13535, train_loss: 0.8945004940032959, val_loss: 0.86310875415802\n",
            "Saving model, epoch: 13536, train_loss: 0.8944966793060303, val_loss: 0.8631066679954529\n",
            "Saving model, epoch: 13537, train_loss: 0.8944928646087646, val_loss: 0.8631045818328857\n",
            "Saving model, epoch: 13538, train_loss: 0.894489049911499, val_loss: 0.863102912902832\n",
            "Saving model, epoch: 13539, train_loss: 0.8944852352142334, val_loss: 0.8631005883216858\n",
            "Saving model, epoch: 13540, train_loss: 0.8944814205169678, val_loss: 0.8630985617637634\n",
            "Saving model, epoch: 13541, train_loss: 0.8944776058197021, val_loss: 0.8630966544151306\n",
            "Saving model, epoch: 13542, train_loss: 0.8944737911224365, val_loss: 0.863094687461853\n",
            "Saving model, epoch: 13543, train_loss: 0.8944699764251709, val_loss: 0.8630924224853516\n",
            "Saving model, epoch: 13544, train_loss: 0.89446622133255, val_loss: 0.8630902767181396\n",
            "Saving model, epoch: 13545, train_loss: 0.8944624066352844, val_loss: 0.8630880117416382\n",
            "Saving model, epoch: 13546, train_loss: 0.8944585919380188, val_loss: 0.8630861639976501\n",
            "Saving model, epoch: 13547, train_loss: 0.8944547772407532, val_loss: 0.8630844354629517\n",
            "Saving model, epoch: 13548, train_loss: 0.8944510817527771, val_loss: 0.8630824089050293\n",
            "Saving model, epoch: 13549, train_loss: 0.8944470286369324, val_loss: 0.8630801439285278\n",
            "Saving model, epoch: 13550, train_loss: 0.8944434523582458, val_loss: 0.8630780577659607\n",
            "Saving model, epoch: 13551, train_loss: 0.8944396376609802, val_loss: 0.863076388835907\n",
            "Saving model, epoch: 13552, train_loss: 0.894435703754425, val_loss: 0.8630741834640503\n",
            "Saving model, epoch: 13553, train_loss: 0.8944318890571594, val_loss: 0.8630724549293518\n",
            "Saving model, epoch: 13554, train_loss: 0.8944281935691833, val_loss: 0.8630700707435608\n",
            "Saving model, epoch: 13555, train_loss: 0.8944242596626282, val_loss: 0.8630684018135071\n",
            "Saving model, epoch: 13556, train_loss: 0.8944204449653625, val_loss: 0.8630660772323608\n",
            "Saving model, epoch: 13557, train_loss: 0.8944166898727417, val_loss: 0.863064169883728\n",
            "Saving model, epoch: 13558, train_loss: 0.8944129943847656, val_loss: 0.8630620837211609\n",
            "Saving model, epoch: 13559, train_loss: 0.8944090604782104, val_loss: 0.8630603551864624\n",
            "Saving model, epoch: 13560, train_loss: 0.8944050669670105, val_loss: 0.8630577921867371\n",
            "Saving model, epoch: 13561, train_loss: 0.8944014310836792, val_loss: 0.8630560040473938\n",
            "Saving model, epoch: 13562, train_loss: 0.894397497177124, val_loss: 0.8630543351173401\n",
            "Saving model, epoch: 13563, train_loss: 0.8943936824798584, val_loss: 0.8630518913269043\n",
            "Saving model, epoch: 13564, train_loss: 0.894389808177948, val_loss: 0.8630498647689819\n",
            "Saving model, epoch: 13565, train_loss: 0.8943860530853271, val_loss: 0.8630480766296387\n",
            "Saving model, epoch: 13566, train_loss: 0.8943821787834167, val_loss: 0.8630455136299133\n",
            "Saving model, epoch: 13567, train_loss: 0.8943783044815063, val_loss: 0.8630435466766357\n",
            "Saving model, epoch: 13568, train_loss: 0.8943744897842407, val_loss: 0.863041877746582\n",
            "Saving model, epoch: 13569, train_loss: 0.8943707942962646, val_loss: 0.863040030002594\n",
            "Saving model, epoch: 13570, train_loss: 0.8943668603897095, val_loss: 0.8630379438400269\n",
            "Saving model, epoch: 13571, train_loss: 0.8943628668785095, val_loss: 0.8630357384681702\n",
            "Saving model, epoch: 13572, train_loss: 0.8943592309951782, val_loss: 0.8630344867706299\n",
            "Saving model, epoch: 13573, train_loss: 0.8943554162979126, val_loss: 0.8630318641662598\n",
            "Saving model, epoch: 13574, train_loss: 0.8943514823913574, val_loss: 0.8630294799804688\n",
            "Saving model, epoch: 13575, train_loss: 0.894347608089447, val_loss: 0.8630277514457703\n",
            "Saving model, epoch: 13576, train_loss: 0.8943437933921814, val_loss: 0.8630255460739136\n",
            "Saving model, epoch: 13577, train_loss: 0.8943401575088501, val_loss: 0.8630238175392151\n",
            "Saving model, epoch: 13578, train_loss: 0.8943361639976501, val_loss: 0.863021731376648\n",
            "Saving model, epoch: 13579, train_loss: 0.8943322896957397, val_loss: 0.863019585609436\n",
            "Saving model, epoch: 13580, train_loss: 0.8943284749984741, val_loss: 0.863017737865448\n",
            "Saving model, epoch: 13581, train_loss: 0.8943246603012085, val_loss: 0.8630155324935913\n",
            "Saving model, epoch: 13582, train_loss: 0.8943207859992981, val_loss: 0.8630134463310242\n",
            "Saving model, epoch: 13583, train_loss: 0.8943169713020325, val_loss: 0.8630114793777466\n",
            "Saving model, epoch: 13584, train_loss: 0.8943130373954773, val_loss: 0.8630093336105347\n",
            "Saving model, epoch: 13585, train_loss: 0.8943092226982117, val_loss: 0.8630076050758362\n",
            "Saving model, epoch: 13586, train_loss: 0.8943051695823669, val_loss: 0.8630053997039795\n",
            "Saving model, epoch: 13587, train_loss: 0.8943014740943909, val_loss: 0.8630029559135437\n",
            "Saving model, epoch: 13588, train_loss: 0.8942976593971252, val_loss: 0.8630009293556213\n",
            "Saving model, epoch: 13589, train_loss: 0.8942936658859253, val_loss: 0.8629992008209229\n",
            "Saving model, epoch: 13590, train_loss: 0.8942897915840149, val_loss: 0.8629974126815796\n",
            "Saving model, epoch: 13591, train_loss: 0.8942860960960388, val_loss: 0.8629951477050781\n",
            "Saving model, epoch: 13592, train_loss: 0.8942821025848389, val_loss: 0.8629929423332214\n",
            "Saving model, epoch: 13593, train_loss: 0.8942784667015076, val_loss: 0.8629906177520752\n",
            "Saving model, epoch: 13594, train_loss: 0.8942744731903076, val_loss: 0.8629889488220215\n",
            "Saving model, epoch: 13595, train_loss: 0.8942704200744629, val_loss: 0.8629869818687439\n",
            "Saving model, epoch: 13596, train_loss: 0.8942667245864868, val_loss: 0.8629844188690186\n",
            "Saving model, epoch: 13597, train_loss: 0.8942627906799316, val_loss: 0.8629828691482544\n",
            "Saving model, epoch: 13598, train_loss: 0.8942588567733765, val_loss: 0.8629804849624634\n",
            "Saving model, epoch: 13599, train_loss: 0.8942550420761108, val_loss: 0.8629785180091858\n",
            "Saving model, epoch: 13600, train_loss: 0.8942513465881348, val_loss: 0.8629769086837769\n",
            "epoch: 13601, train_loss: 0.8942475318908691, val_loss: 0.8629747629165649\n",
            "Saving model, epoch: 13601, train_loss: 0.8942475318908691, val_loss: 0.8629747629165649\n",
            "Saving model, epoch: 13602, train_loss: 0.8942434787750244, val_loss: 0.8629724383354187\n",
            "Saving model, epoch: 13603, train_loss: 0.8942394852638245, val_loss: 0.8629705309867859\n",
            "Saving model, epoch: 13604, train_loss: 0.8942356705665588, val_loss: 0.862968385219574\n",
            "Saving model, epoch: 13605, train_loss: 0.8942318558692932, val_loss: 0.8629664182662964\n",
            "Saving model, epoch: 13606, train_loss: 0.8942280411720276, val_loss: 0.8629645109176636\n",
            "Saving model, epoch: 13607, train_loss: 0.8942239880561829, val_loss: 0.8629621267318726\n",
            "Saving model, epoch: 13608, train_loss: 0.8942201137542725, val_loss: 0.8629600405693054\n",
            "Saving model, epoch: 13609, train_loss: 0.8942162990570068, val_loss: 0.8629581928253174\n",
            "Saving model, epoch: 13610, train_loss: 0.8942123651504517, val_loss: 0.8629565834999084\n",
            "Saving model, epoch: 13611, train_loss: 0.8942086696624756, val_loss: 0.8629541397094727\n",
            "Saving model, epoch: 13612, train_loss: 0.8942047357559204, val_loss: 0.8629523515701294\n",
            "Saving model, epoch: 13613, train_loss: 0.8942008018493652, val_loss: 0.8629498481750488\n",
            "Saving model, epoch: 13614, train_loss: 0.8941968679428101, val_loss: 0.8629481196403503\n",
            "Saving model, epoch: 13615, train_loss: 0.8941928744316101, val_loss: 0.8629458546638489\n",
            "Saving model, epoch: 13616, train_loss: 0.8941892385482788, val_loss: 0.8629440069198608\n",
            "Saving model, epoch: 13617, train_loss: 0.8941852450370789, val_loss: 0.8629418611526489\n",
            "Saving model, epoch: 13618, train_loss: 0.8941814303398132, val_loss: 0.8629396557807922\n",
            "Saving model, epoch: 13619, train_loss: 0.8941776156425476, val_loss: 0.8629376292228699\n",
            "Saving model, epoch: 13620, train_loss: 0.8941735625267029, val_loss: 0.8629359006881714\n",
            "Saving model, epoch: 13621, train_loss: 0.8941697478294373, val_loss: 0.8629336357116699\n",
            "Saving model, epoch: 13622, train_loss: 0.8941656351089478, val_loss: 0.862931489944458\n",
            "Saving model, epoch: 13623, train_loss: 0.8941618204116821, val_loss: 0.86292964220047\n",
            "Saving model, epoch: 13624, train_loss: 0.894157886505127, val_loss: 0.8629273176193237\n",
            "Saving model, epoch: 13625, train_loss: 0.8941540718078613, val_loss: 0.862925112247467\n",
            "Saving model, epoch: 13626, train_loss: 0.8941502571105957, val_loss: 0.8629230260848999\n",
            "Saving model, epoch: 13627, train_loss: 0.8941463232040405, val_loss: 0.8629213571548462\n",
            "Saving model, epoch: 13628, train_loss: 0.8941423296928406, val_loss: 0.8629194498062134\n",
            "Saving model, epoch: 13629, train_loss: 0.894138514995575, val_loss: 0.8629169464111328\n",
            "Saving model, epoch: 13630, train_loss: 0.8941345810890198, val_loss: 0.8629149198532104\n",
            "Saving model, epoch: 13631, train_loss: 0.8941307663917542, val_loss: 0.8629131317138672\n",
            "Saving model, epoch: 13632, train_loss: 0.8941265344619751, val_loss: 0.8629108667373657\n",
            "Saving model, epoch: 13633, train_loss: 0.894122838973999, val_loss: 0.8629087209701538\n",
            "Saving model, epoch: 13634, train_loss: 0.8941190242767334, val_loss: 0.8629067540168762\n",
            "Saving model, epoch: 13635, train_loss: 0.8941149115562439, val_loss: 0.8629048466682434\n",
            "Saving model, epoch: 13636, train_loss: 0.894111156463623, val_loss: 0.8629029393196106\n",
            "Saving model, epoch: 13637, train_loss: 0.8941072821617126, val_loss: 0.8629005551338196\n",
            "Saving model, epoch: 13638, train_loss: 0.8941032290458679, val_loss: 0.8628982901573181\n",
            "Saving model, epoch: 13639, train_loss: 0.894099235534668, val_loss: 0.8628961443901062\n",
            "Saving model, epoch: 13640, train_loss: 0.8940953016281128, val_loss: 0.8628945350646973\n",
            "Saving model, epoch: 13641, train_loss: 0.8940916061401367, val_loss: 0.8628923296928406\n",
            "Saving model, epoch: 13642, train_loss: 0.894087553024292, val_loss: 0.8628902435302734\n",
            "Saving model, epoch: 13643, train_loss: 0.8940837383270264, val_loss: 0.8628882169723511\n",
            "Saving model, epoch: 13644, train_loss: 0.8940796256065369, val_loss: 0.8628861904144287\n",
            "Saving model, epoch: 13645, train_loss: 0.8940759301185608, val_loss: 0.862883985042572\n",
            "Saving model, epoch: 13646, train_loss: 0.8940718770027161, val_loss: 0.8628819584846497\n",
            "Saving model, epoch: 13647, train_loss: 0.8940680623054504, val_loss: 0.8628796935081482\n",
            "Saving model, epoch: 13648, train_loss: 0.8940642476081848, val_loss: 0.8628783822059631\n",
            "Saving model, epoch: 13649, train_loss: 0.8940601348876953, val_loss: 0.8628756999969482\n",
            "Saving model, epoch: 13650, train_loss: 0.8940562009811401, val_loss: 0.8628733158111572\n",
            "Saving model, epoch: 13651, train_loss: 0.8940522074699402, val_loss: 0.8628718256950378\n",
            "Saving model, epoch: 13652, train_loss: 0.8940483927726746, val_loss: 0.862869381904602\n",
            "Saving model, epoch: 13653, train_loss: 0.8940443992614746, val_loss: 0.8628674149513245\n",
            "Saving model, epoch: 13654, train_loss: 0.8940404653549194, val_loss: 0.862865149974823\n",
            "Saving model, epoch: 13655, train_loss: 0.8940365314483643, val_loss: 0.8628633618354797\n",
            "Saving model, epoch: 13656, train_loss: 0.8940325975418091, val_loss: 0.8628613352775574\n",
            "Saving model, epoch: 13657, train_loss: 0.8940286040306091, val_loss: 0.8628586530685425\n",
            "Saving model, epoch: 13658, train_loss: 0.8940247893333435, val_loss: 0.8628575205802917\n",
            "Saving model, epoch: 13659, train_loss: 0.8940208554267883, val_loss: 0.8628553152084351\n",
            "Saving model, epoch: 13660, train_loss: 0.8940168619155884, val_loss: 0.862852931022644\n",
            "Saving model, epoch: 13661, train_loss: 0.8940129280090332, val_loss: 0.8628504276275635\n",
            "Saving model, epoch: 13662, train_loss: 0.8940088152885437, val_loss: 0.8628489971160889\n",
            "Saving model, epoch: 13663, train_loss: 0.8940051794052124, val_loss: 0.8628466129302979\n",
            "Saving model, epoch: 13664, train_loss: 0.8940011858940125, val_loss: 0.8628445863723755\n",
            "Saving model, epoch: 13665, train_loss: 0.8939971327781677, val_loss: 0.8628425598144531\n",
            "Saving model, epoch: 13666, train_loss: 0.8939931392669678, val_loss: 0.8628405332565308\n",
            "Saving model, epoch: 13667, train_loss: 0.8939893245697021, val_loss: 0.8628382682800293\n",
            "Saving model, epoch: 13668, train_loss: 0.8939852714538574, val_loss: 0.8628361225128174\n",
            "Saving model, epoch: 13669, train_loss: 0.8939812779426575, val_loss: 0.862834095954895\n",
            "Saving model, epoch: 13670, train_loss: 0.8939773440361023, val_loss: 0.8628317713737488\n",
            "Saving model, epoch: 13671, train_loss: 0.8939734697341919, val_loss: 0.862829864025116\n",
            "Saving model, epoch: 13672, train_loss: 0.8939694166183472, val_loss: 0.8628275394439697\n",
            "Saving model, epoch: 13673, train_loss: 0.893965482711792, val_loss: 0.8628253936767578\n",
            "Saving model, epoch: 13674, train_loss: 0.8939616084098816, val_loss: 0.862823486328125\n",
            "Saving model, epoch: 13675, train_loss: 0.8939575552940369, val_loss: 0.862821638584137\n",
            "Saving model, epoch: 13676, train_loss: 0.8939536213874817, val_loss: 0.8628195524215698\n",
            "Saving model, epoch: 13677, train_loss: 0.8939497470855713, val_loss: 0.8628177642822266\n",
            "Saving model, epoch: 13678, train_loss: 0.8939456939697266, val_loss: 0.8628155589103699\n",
            "Saving model, epoch: 13679, train_loss: 0.8939417004585266, val_loss: 0.8628132343292236\n",
            "Saving model, epoch: 13680, train_loss: 0.893937885761261, val_loss: 0.8628110289573669\n",
            "Saving model, epoch: 13681, train_loss: 0.8939337730407715, val_loss: 0.8628091812133789\n",
            "Saving model, epoch: 13682, train_loss: 0.8939298391342163, val_loss: 0.8628072142601013\n",
            "Saving model, epoch: 13683, train_loss: 0.8939260244369507, val_loss: 0.8628048300743103\n",
            "Saving model, epoch: 13684, train_loss: 0.8939220309257507, val_loss: 0.8628025650978088\n",
            "Saving model, epoch: 13685, train_loss: 0.8939178586006165, val_loss: 0.8628004193305969\n",
            "Saving model, epoch: 13686, train_loss: 0.893913984298706, val_loss: 0.8627989292144775\n",
            "Saving model, epoch: 13687, train_loss: 0.8939100503921509, val_loss: 0.8627961874008179\n",
            "Saving model, epoch: 13688, train_loss: 0.8939060568809509, val_loss: 0.8627945184707642\n",
            "Saving model, epoch: 13689, train_loss: 0.8939021229743958, val_loss: 0.8627920746803284\n",
            "Saving model, epoch: 13690, train_loss: 0.893898069858551, val_loss: 0.8627903461456299\n",
            "Saving model, epoch: 13691, train_loss: 0.8938941955566406, val_loss: 0.862788200378418\n",
            "Saving model, epoch: 13692, train_loss: 0.8938902616500854, val_loss: 0.8627858757972717\n",
            "Saving model, epoch: 13693, train_loss: 0.893886148929596, val_loss: 0.8627843856811523\n",
            "Saving model, epoch: 13694, train_loss: 0.8938820958137512, val_loss: 0.8627820611000061\n",
            "Saving model, epoch: 13695, train_loss: 0.8938782215118408, val_loss: 0.862779974937439\n",
            "Saving model, epoch: 13696, train_loss: 0.8938744068145752, val_loss: 0.862777829170227\n",
            "Saving model, epoch: 13697, train_loss: 0.8938702344894409, val_loss: 0.8627756834030151\n",
            "Saving model, epoch: 13698, train_loss: 0.8938661217689514, val_loss: 0.862773597240448\n",
            "Saving model, epoch: 13699, train_loss: 0.893862247467041, val_loss: 0.8627711534500122\n",
            "Saving model, epoch: 13700, train_loss: 0.8938583135604858, val_loss: 0.8627694845199585\n",
            "epoch: 13701, train_loss: 0.8938542604446411, val_loss: 0.8627669215202332\n",
            "Saving model, epoch: 13701, train_loss: 0.8938542604446411, val_loss: 0.8627669215202332\n",
            "Saving model, epoch: 13702, train_loss: 0.8938501477241516, val_loss: 0.8627651333808899\n",
            "Saving model, epoch: 13703, train_loss: 0.893846333026886, val_loss: 0.8627631068229675\n",
            "Saving model, epoch: 13704, train_loss: 0.893842339515686, val_loss: 0.8627607226371765\n",
            "Saving model, epoch: 13705, train_loss: 0.8938384056091309, val_loss: 0.862758457660675\n",
            "Saving model, epoch: 13706, train_loss: 0.8938344120979309, val_loss: 0.862756609916687\n",
            "Saving model, epoch: 13707, train_loss: 0.8938301801681519, val_loss: 0.8627546429634094\n",
            "Saving model, epoch: 13708, train_loss: 0.8938263654708862, val_loss: 0.8627524971961975\n",
            "Saving model, epoch: 13709, train_loss: 0.8938222527503967, val_loss: 0.8627505898475647\n",
            "Saving model, epoch: 13710, train_loss: 0.8938183188438416, val_loss: 0.8627487421035767\n",
            "Saving model, epoch: 13711, train_loss: 0.8938143253326416, val_loss: 0.8627461791038513\n",
            "Saving model, epoch: 13712, train_loss: 0.8938103914260864, val_loss: 0.8627442121505737\n",
            "Saving model, epoch: 13713, train_loss: 0.8938063383102417, val_loss: 0.8627421259880066\n",
            "Saving model, epoch: 13714, train_loss: 0.8938023447990417, val_loss: 0.8627395033836365\n",
            "Saving model, epoch: 13715, train_loss: 0.8937982320785522, val_loss: 0.862737774848938\n",
            "Saving model, epoch: 13716, train_loss: 0.8937944173812866, val_loss: 0.8627355694770813\n",
            "Saving model, epoch: 13717, train_loss: 0.8937901854515076, val_loss: 0.8627336025238037\n",
            "Saving model, epoch: 13718, train_loss: 0.8937863707542419, val_loss: 0.8627309799194336\n",
            "Saving model, epoch: 13719, train_loss: 0.8937821388244629, val_loss: 0.8627293705940247\n",
            "Saving model, epoch: 13720, train_loss: 0.8937782049179077, val_loss: 0.8627268671989441\n",
            "Saving model, epoch: 13721, train_loss: 0.8937740921974182, val_loss: 0.8627250790596008\n",
            "Saving model, epoch: 13722, train_loss: 0.8937703967094421, val_loss: 0.8627225756645203\n",
            "Saving model, epoch: 13723, train_loss: 0.8937662839889526, val_loss: 0.8627212643623352\n",
            "Saving model, epoch: 13724, train_loss: 0.8937621712684631, val_loss: 0.8627186417579651\n",
            "Saving model, epoch: 13725, train_loss: 0.893758237361908, val_loss: 0.862716555595398\n",
            "Saving model, epoch: 13726, train_loss: 0.893754243850708, val_loss: 0.862714409828186\n",
            "Saving model, epoch: 13727, train_loss: 0.8937501907348633, val_loss: 0.8627125024795532\n",
            "Saving model, epoch: 13728, train_loss: 0.8937461972236633, val_loss: 0.8627105951309204\n",
            "Saving model, epoch: 13729, train_loss: 0.8937421441078186, val_loss: 0.8627077341079712\n",
            "Saving model, epoch: 13730, train_loss: 0.8937382698059082, val_loss: 0.8627060055732727\n",
            "Saving model, epoch: 13731, train_loss: 0.8937340974807739, val_loss: 0.862703800201416\n",
            "Saving model, epoch: 13732, train_loss: 0.8937302231788635, val_loss: 0.8627013564109802\n",
            "Saving model, epoch: 13733, train_loss: 0.8937258720397949, val_loss: 0.8626996278762817\n",
            "Saving model, epoch: 13734, train_loss: 0.8937219381332397, val_loss: 0.862697422504425\n",
            "Saving model, epoch: 13735, train_loss: 0.8937179446220398, val_loss: 0.8626950979232788\n",
            "Saving model, epoch: 13736, train_loss: 0.8937138319015503, val_loss: 0.8626934885978699\n",
            "Saving model, epoch: 13737, train_loss: 0.8937098383903503, val_loss: 0.8626912236213684\n",
            "Saving model, epoch: 13738, train_loss: 0.8937057852745056, val_loss: 0.8626887202262878\n",
            "Saving model, epoch: 13739, train_loss: 0.8937018513679504, val_loss: 0.8626869320869446\n",
            "Saving model, epoch: 13740, train_loss: 0.8936977386474609, val_loss: 0.8626853823661804\n",
            "Saving model, epoch: 13741, train_loss: 0.893693745136261, val_loss: 0.8626827597618103\n",
            "Saving model, epoch: 13742, train_loss: 0.8936896920204163, val_loss: 0.8626806735992432\n",
            "Saving model, epoch: 13743, train_loss: 0.8936856985092163, val_loss: 0.8626787066459656\n",
            "Saving model, epoch: 13744, train_loss: 0.8936815857887268, val_loss: 0.8626767992973328\n",
            "Saving model, epoch: 13745, train_loss: 0.8936776518821716, val_loss: 0.8626740574836731\n",
            "Saving model, epoch: 13746, train_loss: 0.8936736583709717, val_loss: 0.8626719117164612\n",
            "Saving model, epoch: 13747, train_loss: 0.893669605255127, val_loss: 0.8626700639724731\n",
            "Saving model, epoch: 13748, train_loss: 0.893665611743927, val_loss: 0.8626682758331299\n",
            "Saving model, epoch: 13749, train_loss: 0.893661379814148, val_loss: 0.8626655340194702\n",
            "Saving model, epoch: 13750, train_loss: 0.893657386302948, val_loss: 0.8626633882522583\n",
            "Saving model, epoch: 13751, train_loss: 0.8936533331871033, val_loss: 0.862661600112915\n",
            "Saving model, epoch: 13752, train_loss: 0.8936493396759033, val_loss: 0.8626591563224792\n",
            "Saving model, epoch: 13753, train_loss: 0.8936454057693481, val_loss: 0.8626571297645569\n",
            "Saving model, epoch: 13754, train_loss: 0.8936411738395691, val_loss: 0.8626547455787659\n",
            "Saving model, epoch: 13755, train_loss: 0.8936372995376587, val_loss: 0.8626529574394226\n",
            "Saving model, epoch: 13756, train_loss: 0.8936331272125244, val_loss: 0.8626508116722107\n",
            "Saving model, epoch: 13757, train_loss: 0.8936291337013245, val_loss: 0.8626482486724854\n",
            "Saving model, epoch: 13758, train_loss: 0.8936251401901245, val_loss: 0.8626466989517212\n",
            "Saving model, epoch: 13759, train_loss: 0.8936209678649902, val_loss: 0.862644612789154\n",
            "Saving model, epoch: 13760, train_loss: 0.8936168551445007, val_loss: 0.8626422882080078\n",
            "Saving model, epoch: 13761, train_loss: 0.8936129808425903, val_loss: 0.8626397848129272\n",
            "Saving model, epoch: 13762, train_loss: 0.8936087489128113, val_loss: 0.8626382350921631\n",
            "Saving model, epoch: 13763, train_loss: 0.8936046957969666, val_loss: 0.862635612487793\n",
            "Saving model, epoch: 13764, train_loss: 0.8936007022857666, val_loss: 0.8626337051391602\n",
            "Saving model, epoch: 13765, train_loss: 0.8935967087745667, val_loss: 0.8626317977905273\n",
            "Saving model, epoch: 13766, train_loss: 0.8935924768447876, val_loss: 0.8626294732093811\n",
            "Saving model, epoch: 13767, train_loss: 0.8935883641242981, val_loss: 0.8626272678375244\n",
            "Saving model, epoch: 13768, train_loss: 0.8935844302177429, val_loss: 0.8626253008842468\n",
            "Saving model, epoch: 13769, train_loss: 0.8935801982879639, val_loss: 0.8626231551170349\n",
            "Saving model, epoch: 13770, train_loss: 0.8935762047767639, val_loss: 0.8626213073730469\n",
            "Saving model, epoch: 13771, train_loss: 0.8935721516609192, val_loss: 0.8626188039779663\n",
            "Saving model, epoch: 13772, train_loss: 0.8935680389404297, val_loss: 0.8626163601875305\n",
            "Saving model, epoch: 13773, train_loss: 0.8935639262199402, val_loss: 0.8626149296760559\n",
            "Saving model, epoch: 13774, train_loss: 0.893559992313385, val_loss: 0.8626126646995544\n",
            "Saving model, epoch: 13775, train_loss: 0.8935559988021851, val_loss: 0.8626101016998291\n",
            "Saving model, epoch: 13776, train_loss: 0.8935516476631165, val_loss: 0.862608015537262\n",
            "Saving model, epoch: 13777, train_loss: 0.8935478329658508, val_loss: 0.8626055121421814\n",
            "Saving model, epoch: 13778, train_loss: 0.8935437202453613, val_loss: 0.8626037836074829\n",
            "Saving model, epoch: 13779, train_loss: 0.8935394883155823, val_loss: 0.8626012802124023\n",
            "Saving model, epoch: 13780, train_loss: 0.8935353755950928, val_loss: 0.8625997304916382\n",
            "Saving model, epoch: 13781, train_loss: 0.8935313820838928, val_loss: 0.8625977039337158\n",
            "Saving model, epoch: 13782, train_loss: 0.8935273289680481, val_loss: 0.8625954985618591\n",
            "Saving model, epoch: 13783, train_loss: 0.8935232162475586, val_loss: 0.8625929951667786\n",
            "Saving model, epoch: 13784, train_loss: 0.8935191035270691, val_loss: 0.8625910878181458\n",
            "Saving model, epoch: 13785, train_loss: 0.8935151100158691, val_loss: 0.8625889420509338\n",
            "Saving model, epoch: 13786, train_loss: 0.8935110569000244, val_loss: 0.8625867962837219\n",
            "Saving model, epoch: 13787, train_loss: 0.8935068845748901, val_loss: 0.8625841736793518\n",
            "Saving model, epoch: 13788, train_loss: 0.8935028314590454, val_loss: 0.8625825047492981\n",
            "Saving model, epoch: 13789, train_loss: 0.8934985995292664, val_loss: 0.8625802993774414\n",
            "Saving model, epoch: 13790, train_loss: 0.8934946060180664, val_loss: 0.8625779151916504\n",
            "Saving model, epoch: 13791, train_loss: 0.8934903740882874, val_loss: 0.8625759482383728\n",
            "Saving model, epoch: 13792, train_loss: 0.8934863805770874, val_loss: 0.862573504447937\n",
            "Saving model, epoch: 13793, train_loss: 0.8934823274612427, val_loss: 0.8625717759132385\n",
            "Saving model, epoch: 13794, train_loss: 0.8934783339500427, val_loss: 0.8625690937042236\n",
            "Saving model, epoch: 13795, train_loss: 0.8934742212295532, val_loss: 0.8625674247741699\n",
            "Saving model, epoch: 13796, train_loss: 0.8934701085090637, val_loss: 0.8625653386116028\n",
            "Saving model, epoch: 13797, train_loss: 0.893466055393219, val_loss: 0.8625630736351013\n",
            "Saving model, epoch: 13798, train_loss: 0.8934618234634399, val_loss: 0.8625608086585999\n",
            "Saving model, epoch: 13799, train_loss: 0.8934576511383057, val_loss: 0.8625586032867432\n",
            "Saving model, epoch: 13800, train_loss: 0.8934535980224609, val_loss: 0.8625563383102417\n",
            "epoch: 13801, train_loss: 0.8934494853019714, val_loss: 0.8625543713569641\n",
            "Saving model, epoch: 13801, train_loss: 0.8934494853019714, val_loss: 0.8625543713569641\n",
            "Saving model, epoch: 13802, train_loss: 0.8934453725814819, val_loss: 0.8625518083572388\n",
            "Saving model, epoch: 13803, train_loss: 0.893441379070282, val_loss: 0.8625502586364746\n",
            "Saving model, epoch: 13804, train_loss: 0.8934373259544373, val_loss: 0.8625480532646179\n",
            "Saving model, epoch: 13805, train_loss: 0.8934330940246582, val_loss: 0.8625457882881165\n",
            "Saving model, epoch: 13806, train_loss: 0.8934289216995239, val_loss: 0.8625434041023254\n",
            "Saving model, epoch: 13807, train_loss: 0.8934247493743896, val_loss: 0.8625410199165344\n",
            "Saving model, epoch: 13808, train_loss: 0.8934207558631897, val_loss: 0.8625391721725464\n",
            "Saving model, epoch: 13809, train_loss: 0.8934165239334106, val_loss: 0.8625365495681763\n",
            "Saving model, epoch: 13810, train_loss: 0.8934126496315002, val_loss: 0.8625348210334778\n",
            "Saving model, epoch: 13811, train_loss: 0.893408477306366, val_loss: 0.8625327944755554\n",
            "Saving model, epoch: 13812, train_loss: 0.8934043049812317, val_loss: 0.8625304102897644\n",
            "Saving model, epoch: 13813, train_loss: 0.8934001922607422, val_loss: 0.8625280261039734\n",
            "Saving model, epoch: 13814, train_loss: 0.8933961391448975, val_loss: 0.8625264167785645\n",
            "Saving model, epoch: 13815, train_loss: 0.893392026424408, val_loss: 0.862524151802063\n",
            "Saving model, epoch: 13816, train_loss: 0.8933877944946289, val_loss: 0.8625220656394958\n",
            "Saving model, epoch: 13817, train_loss: 0.8933838605880737, val_loss: 0.8625195622444153\n",
            "Saving model, epoch: 13818, train_loss: 0.8933794498443604, val_loss: 0.8625174760818481\n",
            "Saving model, epoch: 13819, train_loss: 0.8933754563331604, val_loss: 0.8625156879425049\n",
            "Saving model, epoch: 13820, train_loss: 0.8933712244033813, val_loss: 0.8625134229660034\n",
            "Saving model, epoch: 13821, train_loss: 0.8933672308921814, val_loss: 0.8625109195709229\n",
            "Saving model, epoch: 13822, train_loss: 0.8933631181716919, val_loss: 0.8625089526176453\n",
            "Saving model, epoch: 13823, train_loss: 0.8933588862419128, val_loss: 0.8625065684318542\n",
            "Saving model, epoch: 13824, train_loss: 0.8933549523353577, val_loss: 0.8625047206878662\n",
            "Saving model, epoch: 13825, train_loss: 0.8933507204055786, val_loss: 0.8625022172927856\n",
            "Saving model, epoch: 13826, train_loss: 0.8933464884757996, val_loss: 0.862500011920929\n",
            "Saving model, epoch: 13827, train_loss: 0.8933423757553101, val_loss: 0.8624980449676514\n",
            "Saving model, epoch: 13828, train_loss: 0.8933382034301758, val_loss: 0.862496018409729\n",
            "Saving model, epoch: 13829, train_loss: 0.8933342695236206, val_loss: 0.8624936938285828\n",
            "Saving model, epoch: 13830, train_loss: 0.8933300375938416, val_loss: 0.8624914884567261\n",
            "Saving model, epoch: 13831, train_loss: 0.8933260440826416, val_loss: 0.8624892234802246\n",
            "Saving model, epoch: 13832, train_loss: 0.8933218121528625, val_loss: 0.8624871373176575\n",
            "Saving model, epoch: 13833, train_loss: 0.8933175802230835, val_loss: 0.8624852895736694\n",
            "Saving model, epoch: 13834, train_loss: 0.8933133482933044, val_loss: 0.8624835014343262\n",
            "Saving model, epoch: 13835, train_loss: 0.8933091163635254, val_loss: 0.8624804615974426\n",
            "Saving model, epoch: 13836, train_loss: 0.8933051228523254, val_loss: 0.8624786138534546\n",
            "Saving model, epoch: 13837, train_loss: 0.8933011293411255, val_loss: 0.8624765276908875\n",
            "Saving model, epoch: 13838, train_loss: 0.8932968974113464, val_loss: 0.8624739646911621\n",
            "Saving model, epoch: 13839, train_loss: 0.8932925462722778, val_loss: 0.8624721169471741\n",
            "Saving model, epoch: 13840, train_loss: 0.8932885527610779, val_loss: 0.8624698519706726\n",
            "Saving model, epoch: 13841, train_loss: 0.8932843208312988, val_loss: 0.8624676465988159\n",
            "Saving model, epoch: 13842, train_loss: 0.8932803273200989, val_loss: 0.8624656200408936\n",
            "Saving model, epoch: 13843, train_loss: 0.8932761549949646, val_loss: 0.8624632954597473\n",
            "Saving model, epoch: 13844, train_loss: 0.8932719826698303, val_loss: 0.8624614477157593\n",
            "Saving model, epoch: 13845, train_loss: 0.8932677507400513, val_loss: 0.8624589443206787\n",
            "Saving model, epoch: 13846, train_loss: 0.8932636380195618, val_loss: 0.8624565601348877\n",
            "Saving model, epoch: 13847, train_loss: 0.8932592868804932, val_loss: 0.8624547123908997\n",
            "Saving model, epoch: 13848, train_loss: 0.8932554125785828, val_loss: 0.8624529242515564\n",
            "Saving model, epoch: 13849, train_loss: 0.8932512402534485, val_loss: 0.862450122833252\n",
            "Saving model, epoch: 13850, train_loss: 0.8932472467422485, val_loss: 0.86244797706604\n",
            "Saving model, epoch: 13851, train_loss: 0.8932428359985352, val_loss: 0.8624457716941833\n",
            "Saving model, epoch: 13852, train_loss: 0.89323890209198, val_loss: 0.8624431490898132\n",
            "Saving model, epoch: 13853, train_loss: 0.893234372138977, val_loss: 0.8624416589736938\n",
            "Saving model, epoch: 13854, train_loss: 0.8932303786277771, val_loss: 0.8624394536018372\n",
            "Saving model, epoch: 13855, train_loss: 0.8932260274887085, val_loss: 0.86243736743927\n",
            "Saving model, epoch: 13856, train_loss: 0.893221914768219, val_loss: 0.8624351024627686\n",
            "Saving model, epoch: 13857, train_loss: 0.893217921257019, val_loss: 0.862433135509491\n",
            "Saving model, epoch: 13858, train_loss: 0.89321368932724, val_loss: 0.8624309301376343\n",
            "Saving model, epoch: 13859, train_loss: 0.8932094573974609, val_loss: 0.8624283075332642\n",
            "Saving model, epoch: 13860, train_loss: 0.893205463886261, val_loss: 0.8624265193939209\n",
            "Saving model, epoch: 13861, train_loss: 0.8932011127471924, val_loss: 0.8624242544174194\n",
            "Saving model, epoch: 13862, train_loss: 0.8931970000267029, val_loss: 0.8624217510223389\n",
            "Saving model, epoch: 13863, train_loss: 0.8931927680969238, val_loss: 0.8624197244644165\n",
            "Saving model, epoch: 13864, train_loss: 0.8931885957717896, val_loss: 0.8624173998832703\n",
            "Saving model, epoch: 13865, train_loss: 0.8931845426559448, val_loss: 0.8624153137207031\n",
            "Saving model, epoch: 13866, train_loss: 0.8931803107261658, val_loss: 0.8624132871627808\n",
            "Saving model, epoch: 13867, train_loss: 0.8931758999824524, val_loss: 0.8624106049537659\n",
            "Saving model, epoch: 13868, train_loss: 0.8931719064712524, val_loss: 0.8624082803726196\n",
            "Saving model, epoch: 13869, train_loss: 0.8931678533554077, val_loss: 0.8624066114425659\n",
            "Saving model, epoch: 13870, train_loss: 0.8931635618209839, val_loss: 0.8624042272567749\n",
            "Saving model, epoch: 13871, train_loss: 0.8931593298912048, val_loss: 0.8624023199081421\n",
            "Saving model, epoch: 13872, train_loss: 0.8931553959846497, val_loss: 0.8623998761177063\n",
            "Saving model, epoch: 13873, train_loss: 0.8931509852409363, val_loss: 0.8623976111412048\n",
            "Saving model, epoch: 13874, train_loss: 0.8931467533111572, val_loss: 0.8623952269554138\n",
            "Saving model, epoch: 13875, train_loss: 0.8931426405906677, val_loss: 0.8623933792114258\n",
            "Saving model, epoch: 13876, train_loss: 0.8931383490562439, val_loss: 0.8623910546302795\n",
            "Saving model, epoch: 13877, train_loss: 0.8931341171264648, val_loss: 0.8623890280723572\n",
            "Saving model, epoch: 13878, train_loss: 0.8931301236152649, val_loss: 0.8623867034912109\n",
            "Saving model, epoch: 13879, train_loss: 0.8931260108947754, val_loss: 0.8623848557472229\n",
            "Saving model, epoch: 13880, train_loss: 0.8931216597557068, val_loss: 0.8623824715614319\n",
            "Saving model, epoch: 13881, train_loss: 0.8931175470352173, val_loss: 0.8623798489570618\n",
            "Saving model, epoch: 13882, train_loss: 0.8931134343147278, val_loss: 0.862377941608429\n",
            "Saving model, epoch: 13883, train_loss: 0.8931090831756592, val_loss: 0.8623760938644409\n",
            "Saving model, epoch: 13884, train_loss: 0.8931047916412354, val_loss: 0.8623737096786499\n",
            "Saving model, epoch: 13885, train_loss: 0.8931006789207458, val_loss: 0.8623710870742798\n",
            "Saving model, epoch: 13886, train_loss: 0.8930963277816772, val_loss: 0.8623696565628052\n",
            "Saving model, epoch: 13887, train_loss: 0.8930922150611877, val_loss: 0.8623666167259216\n",
            "Saving model, epoch: 13888, train_loss: 0.8930879831314087, val_loss: 0.8623647093772888\n",
            "Saving model, epoch: 13889, train_loss: 0.8930841088294983, val_loss: 0.8623629808425903\n",
            "Saving model, epoch: 13890, train_loss: 0.8930794596672058, val_loss: 0.8623605966567993\n",
            "Saving model, epoch: 13891, train_loss: 0.8930754661560059, val_loss: 0.862358033657074\n",
            "Saving model, epoch: 13892, train_loss: 0.8930713534355164, val_loss: 0.8623558878898621\n",
            "Saving model, epoch: 13893, train_loss: 0.8930670022964478, val_loss: 0.8623536825180054\n",
            "Saving model, epoch: 13894, train_loss: 0.8930627703666687, val_loss: 0.8623517751693726\n",
            "Saving model, epoch: 13895, train_loss: 0.8930584788322449, val_loss: 0.862349271774292\n",
            "Saving model, epoch: 13896, train_loss: 0.8930543661117554, val_loss: 0.8623470664024353\n",
            "Saving model, epoch: 13897, train_loss: 0.8930502533912659, val_loss: 0.8623448610305786\n",
            "Saving model, epoch: 13898, train_loss: 0.8930460810661316, val_loss: 0.862342894077301\n",
            "Saving model, epoch: 13899, train_loss: 0.8930416703224182, val_loss: 0.8623403906822205\n",
            "Saving model, epoch: 13900, train_loss: 0.8930374979972839, val_loss: 0.8623387217521667\n",
            "epoch: 13901, train_loss: 0.8930332660675049, val_loss: 0.8623365163803101\n",
            "Saving model, epoch: 13901, train_loss: 0.8930332660675049, val_loss: 0.8623365163803101\n",
            "Saving model, epoch: 13902, train_loss: 0.8930292129516602, val_loss: 0.8623337149620056\n",
            "Saving model, epoch: 13903, train_loss: 0.8930248022079468, val_loss: 0.8623318076133728\n",
            "Saving model, epoch: 13904, train_loss: 0.8930205702781677, val_loss: 0.8623297214508057\n",
            "Saving model, epoch: 13905, train_loss: 0.8930163979530334, val_loss: 0.8623278141021729\n",
            "Saving model, epoch: 13906, train_loss: 0.8930123448371887, val_loss: 0.8623252511024475\n",
            "Saving model, epoch: 13907, train_loss: 0.8930080533027649, val_loss: 0.8623226881027222\n",
            "Saving model, epoch: 13908, train_loss: 0.8930037021636963, val_loss: 0.8623207807540894\n",
            "Saving model, epoch: 13909, train_loss: 0.8929995894432068, val_loss: 0.8623186945915222\n",
            "Saving model, epoch: 13910, train_loss: 0.892995297908783, val_loss: 0.8623162508010864\n",
            "Saving model, epoch: 13911, train_loss: 0.8929911255836487, val_loss: 0.8623141646385193\n",
            "Saving model, epoch: 13912, train_loss: 0.8929869532585144, val_loss: 0.8623121380805969\n",
            "Saving model, epoch: 13913, train_loss: 0.8929824233055115, val_loss: 0.8623097538948059\n",
            "Saving model, epoch: 13914, train_loss: 0.8929781913757324, val_loss: 0.8623065948486328\n",
            "Saving model, epoch: 13915, train_loss: 0.8929741978645325, val_loss: 0.8623055219650269\n",
            "Saving model, epoch: 13916, train_loss: 0.8929697275161743, val_loss: 0.8623031973838806\n",
            "Saving model, epoch: 13917, train_loss: 0.8929657340049744, val_loss: 0.8623009324073792\n",
            "Saving model, epoch: 13918, train_loss: 0.8929616212844849, val_loss: 0.862298309803009\n",
            "Saving model, epoch: 13919, train_loss: 0.8929570913314819, val_loss: 0.8622963428497314\n",
            "Saving model, epoch: 13920, train_loss: 0.892953097820282, val_loss: 0.8622944951057434\n",
            "Saving model, epoch: 13921, train_loss: 0.8929486274719238, val_loss: 0.8622914552688599\n",
            "Saving model, epoch: 13922, train_loss: 0.8929444551467896, val_loss: 0.8622896671295166\n",
            "Saving model, epoch: 13923, train_loss: 0.892940104007721, val_loss: 0.8622874021530151\n",
            "Saving model, epoch: 13924, train_loss: 0.8929358720779419, val_loss: 0.8622850179672241\n",
            "Saving model, epoch: 13925, train_loss: 0.8929317593574524, val_loss: 0.8622828722000122\n",
            "Saving model, epoch: 13926, train_loss: 0.892927348613739, val_loss: 0.8622808456420898\n",
            "Saving model, epoch: 13927, train_loss: 0.89292311668396, val_loss: 0.8622785210609436\n",
            "Saving model, epoch: 13928, train_loss: 0.8929190039634705, val_loss: 0.8622758388519287\n",
            "Saving model, epoch: 13929, train_loss: 0.8929147124290466, val_loss: 0.8622745871543884\n",
            "Saving model, epoch: 13930, train_loss: 0.8929104804992676, val_loss: 0.8622715473175049\n",
            "Saving model, epoch: 13931, train_loss: 0.892906129360199, val_loss: 0.8622693419456482\n",
            "Saving model, epoch: 13932, train_loss: 0.8929020166397095, val_loss: 0.862267255783081\n",
            "Saving model, epoch: 13933, train_loss: 0.8928978443145752, val_loss: 0.8622652292251587\n",
            "Saving model, epoch: 13934, train_loss: 0.8928934931755066, val_loss: 0.8622632622718811\n",
            "Saving model, epoch: 13935, train_loss: 0.8928889632225037, val_loss: 0.8622604012489319\n",
            "Saving model, epoch: 13936, train_loss: 0.8928849697113037, val_loss: 0.862258791923523\n",
            "Saving model, epoch: 13937, train_loss: 0.8928807377815247, val_loss: 0.8622564077377319\n",
            "Saving model, epoch: 13938, train_loss: 0.8928764462471008, val_loss: 0.8622539043426514\n",
            "Saving model, epoch: 13939, train_loss: 0.8928720951080322, val_loss: 0.8622516393661499\n",
            "Saving model, epoch: 13940, train_loss: 0.8928678631782532, val_loss: 0.8622499704360962\n",
            "Saving model, epoch: 13941, train_loss: 0.8928635716438293, val_loss: 0.8622472286224365\n",
            "Saving model, epoch: 13942, train_loss: 0.8928593397140503, val_loss: 0.8622454404830933\n",
            "Saving model, epoch: 13943, train_loss: 0.8928551077842712, val_loss: 0.8622434139251709\n",
            "Saving model, epoch: 13944, train_loss: 0.8928508162498474, val_loss: 0.8622409701347351\n",
            "Saving model, epoch: 13945, train_loss: 0.8928464651107788, val_loss: 0.8622384071350098\n",
            "Saving model, epoch: 13946, train_loss: 0.892842173576355, val_loss: 0.8622360825538635\n",
            "Saving model, epoch: 13947, train_loss: 0.8928380608558655, val_loss: 0.8622347116470337\n",
            "Saving model, epoch: 13948, train_loss: 0.8928337097167969, val_loss: 0.8622322082519531\n",
            "Saving model, epoch: 13949, train_loss: 0.892829418182373, val_loss: 0.8622295260429382\n",
            "Saving model, epoch: 13950, train_loss: 0.8928253054618835, val_loss: 0.8622271418571472\n",
            "Saving model, epoch: 13951, train_loss: 0.8928210735321045, val_loss: 0.862225353717804\n",
            "Saving model, epoch: 13952, train_loss: 0.8928166627883911, val_loss: 0.8622227907180786\n",
            "Saving model, epoch: 13953, train_loss: 0.8928124308586121, val_loss: 0.862220823764801\n",
            "Saving model, epoch: 13954, train_loss: 0.8928080201148987, val_loss: 0.8622188568115234\n",
            "Saving model, epoch: 13955, train_loss: 0.8928037881851196, val_loss: 0.8622162938117981\n",
            "Saving model, epoch: 13956, train_loss: 0.8927993774414062, val_loss: 0.862213671207428\n",
            "Saving model, epoch: 13957, train_loss: 0.8927951455116272, val_loss: 0.8622119426727295\n",
            "Saving model, epoch: 13958, train_loss: 0.8927909135818481, val_loss: 0.8622096180915833\n",
            "Saving model, epoch: 13959, train_loss: 0.8927866816520691, val_loss: 0.8622071743011475\n",
            "Saving model, epoch: 13960, train_loss: 0.89278244972229, val_loss: 0.8622053265571594\n",
            "Saving model, epoch: 13961, train_loss: 0.8927780389785767, val_loss: 0.8622027635574341\n",
            "Saving model, epoch: 13962, train_loss: 0.8927738666534424, val_loss: 0.8622006177902222\n",
            "Saving model, epoch: 13963, train_loss: 0.8927695155143738, val_loss: 0.8621984124183655\n",
            "Saving model, epoch: 13964, train_loss: 0.89276522397995, val_loss: 0.8621962070465088\n",
            "Saving model, epoch: 13965, train_loss: 0.8927608728408813, val_loss: 0.8621941208839417\n",
            "Saving model, epoch: 13966, train_loss: 0.8927567601203918, val_loss: 0.8621914386749268\n",
            "Saving model, epoch: 13967, train_loss: 0.8927523493766785, val_loss: 0.8621892929077148\n",
            "Saving model, epoch: 13968, train_loss: 0.8927481174468994, val_loss: 0.862187385559082\n",
            "Saving model, epoch: 13969, train_loss: 0.8927438259124756, val_loss: 0.8621849417686462\n",
            "Saving model, epoch: 13970, train_loss: 0.8927393555641174, val_loss: 0.8621826171875\n",
            "Saving model, epoch: 13971, train_loss: 0.8927350640296936, val_loss: 0.8621803522109985\n",
            "Saving model, epoch: 13972, train_loss: 0.8927306532859802, val_loss: 0.8621783256530762\n",
            "Saving model, epoch: 13973, train_loss: 0.8927266001701355, val_loss: 0.8621760606765747\n",
            "Saving model, epoch: 13974, train_loss: 0.8927223682403564, val_loss: 0.8621734380722046\n",
            "Saving model, epoch: 13975, train_loss: 0.8927179574966431, val_loss: 0.8621715903282166\n",
            "Saving model, epoch: 13976, train_loss: 0.8927137851715088, val_loss: 0.8621695041656494\n",
            "Saving model, epoch: 13977, train_loss: 0.8927093148231506, val_loss: 0.8621668815612793\n",
            "Saving model, epoch: 13978, train_loss: 0.8927050232887268, val_loss: 0.8621644973754883\n",
            "Saving model, epoch: 13979, train_loss: 0.8927007913589478, val_loss: 0.8621626496315002\n",
            "Saving model, epoch: 13980, train_loss: 0.8926963806152344, val_loss: 0.862160325050354\n",
            "Saving model, epoch: 13981, train_loss: 0.8926920294761658, val_loss: 0.8621580600738525\n",
            "Saving model, epoch: 13982, train_loss: 0.8926876187324524, val_loss: 0.8621557950973511\n",
            "Saving model, epoch: 13983, train_loss: 0.8926835060119629, val_loss: 0.8621529936790466\n",
            "Saving model, epoch: 13984, train_loss: 0.8926790952682495, val_loss: 0.8621507287025452\n",
            "Saving model, epoch: 13985, train_loss: 0.8926748037338257, val_loss: 0.8621494174003601\n",
            "Saving model, epoch: 13986, train_loss: 0.8926705718040466, val_loss: 0.86214679479599\n",
            "Saving model, epoch: 13987, train_loss: 0.8926661610603333, val_loss: 0.8621445298194885\n",
            "Saving model, epoch: 13988, train_loss: 0.8926618099212646, val_loss: 0.8621418476104736\n",
            "Saving model, epoch: 13989, train_loss: 0.8926575779914856, val_loss: 0.8621401190757751\n",
            "Saving model, epoch: 13990, train_loss: 0.8926532864570618, val_loss: 0.8621377348899841\n",
            "Saving model, epoch: 13991, train_loss: 0.8926489353179932, val_loss: 0.8621350526809692\n",
            "Saving model, epoch: 13992, train_loss: 0.8926444053649902, val_loss: 0.8621329069137573\n",
            "Saving model, epoch: 13993, train_loss: 0.8926404118537903, val_loss: 0.862130880355835\n",
            "Saving model, epoch: 13994, train_loss: 0.8926358222961426, val_loss: 0.8621283769607544\n",
            "Saving model, epoch: 13995, train_loss: 0.8926315903663635, val_loss: 0.8621261715888977\n",
            "Saving model, epoch: 13996, train_loss: 0.8926273584365845, val_loss: 0.8621243834495544\n",
            "Saving model, epoch: 13997, train_loss: 0.8926229476928711, val_loss: 0.8621219396591187\n",
            "Saving model, epoch: 13998, train_loss: 0.8926185965538025, val_loss: 0.8621192574501038\n",
            "Saving model, epoch: 13999, train_loss: 0.8926143050193787, val_loss: 0.8621174693107605\n",
            "Saving model, epoch: 14000, train_loss: 0.8926098942756653, val_loss: 0.8621149659156799\n",
            "epoch: 14001, train_loss: 0.8926057815551758, val_loss: 0.8621129989624023\n",
            "Saving model, epoch: 14001, train_loss: 0.8926057815551758, val_loss: 0.8621129989624023\n",
            "Saving model, epoch: 14002, train_loss: 0.8926012516021729, val_loss: 0.8621106147766113\n",
            "Saving model, epoch: 14003, train_loss: 0.8925970196723938, val_loss: 0.862108588218689\n",
            "Saving model, epoch: 14004, train_loss: 0.8925926685333252, val_loss: 0.8621060252189636\n",
            "Saving model, epoch: 14005, train_loss: 0.8925883769989014, val_loss: 0.8621042370796204\n",
            "Saving model, epoch: 14006, train_loss: 0.892583966255188, val_loss: 0.8621014356613159\n",
            "Saving model, epoch: 14007, train_loss: 0.8925795555114746, val_loss: 0.8620991110801697\n",
            "Saving model, epoch: 14008, train_loss: 0.8925750851631165, val_loss: 0.862096905708313\n",
            "Saving model, epoch: 14009, train_loss: 0.8925709128379822, val_loss: 0.8620949983596802\n",
            "Saving model, epoch: 14010, train_loss: 0.8925665616989136, val_loss: 0.8620926141738892\n",
            "Saving model, epoch: 14011, train_loss: 0.8925622701644897, val_loss: 0.8620903491973877\n",
            "Saving model, epoch: 14012, train_loss: 0.8925580382347107, val_loss: 0.8620879054069519\n",
            "Saving model, epoch: 14013, train_loss: 0.8925536274909973, val_loss: 0.8620858788490295\n",
            "Saving model, epoch: 14014, train_loss: 0.8925492167472839, val_loss: 0.8620831966400146\n",
            "Saving model, epoch: 14015, train_loss: 0.8925449848175049, val_loss: 0.862080991268158\n",
            "Saving model, epoch: 14016, train_loss: 0.8925405740737915, val_loss: 0.8620790243148804\n",
            "Saving model, epoch: 14017, train_loss: 0.8925361037254333, val_loss: 0.8620765805244446\n",
            "Saving model, epoch: 14018, train_loss: 0.8925318121910095, val_loss: 0.8620745539665222\n",
            "Saving model, epoch: 14019, train_loss: 0.8925274610519409, val_loss: 0.8620720505714417\n",
            "Saving model, epoch: 14020, train_loss: 0.8925230503082275, val_loss: 0.8620699048042297\n",
            "Saving model, epoch: 14021, train_loss: 0.8925186395645142, val_loss: 0.8620672225952148\n",
            "Saving model, epoch: 14022, train_loss: 0.8925143480300903, val_loss: 0.8620653748512268\n",
            "Saving model, epoch: 14023, train_loss: 0.8925098180770874, val_loss: 0.862062394618988\n",
            "Saving model, epoch: 14024, train_loss: 0.8925057649612427, val_loss: 0.8620604276657104\n",
            "Saving model, epoch: 14025, train_loss: 0.8925013542175293, val_loss: 0.8620584607124329\n",
            "Saving model, epoch: 14026, train_loss: 0.8924969434738159, val_loss: 0.8620560765266418\n",
            "Saving model, epoch: 14027, train_loss: 0.8924925327301025, val_loss: 0.8620538115501404\n",
            "Saving model, epoch: 14028, train_loss: 0.8924883008003235, val_loss: 0.8620514869689941\n",
            "Saving model, epoch: 14029, train_loss: 0.8924838900566101, val_loss: 0.8620491027832031\n",
            "Saving model, epoch: 14030, train_loss: 0.892479658126831, val_loss: 0.8620469570159912\n",
            "Saving model, epoch: 14031, train_loss: 0.8924751281738281, val_loss: 0.8620451092720032\n",
            "Saving model, epoch: 14032, train_loss: 0.8924708366394043, val_loss: 0.8620425462722778\n",
            "Saving model, epoch: 14033, train_loss: 0.8924664258956909, val_loss: 0.8620404601097107\n",
            "Saving model, epoch: 14034, train_loss: 0.8924620747566223, val_loss: 0.8620378375053406\n",
            "Saving model, epoch: 14035, train_loss: 0.8924576640129089, val_loss: 0.8620355725288391\n",
            "Saving model, epoch: 14036, train_loss: 0.8924533128738403, val_loss: 0.8620333075523376\n",
            "Saving model, epoch: 14037, train_loss: 0.8924488425254822, val_loss: 0.8620311617851257\n",
            "Saving model, epoch: 14038, train_loss: 0.8924446702003479, val_loss: 0.8620291948318481\n",
            "Saving model, epoch: 14039, train_loss: 0.8924402594566345, val_loss: 0.8620266914367676\n",
            "Saving model, epoch: 14040, train_loss: 0.8924357295036316, val_loss: 0.8620245456695557\n",
            "Saving model, epoch: 14041, train_loss: 0.8924313187599182, val_loss: 0.8620222210884094\n",
            "Saving model, epoch: 14042, train_loss: 0.8924271464347839, val_loss: 0.8620198369026184\n",
            "Saving model, epoch: 14043, train_loss: 0.892422616481781, val_loss: 0.8620172739028931\n",
            "Saving model, epoch: 14044, train_loss: 0.892418384552002, val_loss: 0.8620153665542603\n",
            "Saving model, epoch: 14045, train_loss: 0.892413854598999, val_loss: 0.8620131611824036\n",
            "Saving model, epoch: 14046, train_loss: 0.8924097418785095, val_loss: 0.8620103597640991\n",
            "Saving model, epoch: 14047, train_loss: 0.892405092716217, val_loss: 0.8620085120201111\n",
            "Saving model, epoch: 14048, train_loss: 0.8924006223678589, val_loss: 0.8620061874389648\n",
            "Saving model, epoch: 14049, train_loss: 0.8923962712287903, val_loss: 0.8620038032531738\n",
            "Saving model, epoch: 14050, train_loss: 0.8923919796943665, val_loss: 0.8620019555091858\n",
            "Saving model, epoch: 14051, train_loss: 0.8923876285552979, val_loss: 0.8619992733001709\n",
            "Saving model, epoch: 14052, train_loss: 0.8923832178115845, val_loss: 0.8619968891143799\n",
            "Saving model, epoch: 14053, train_loss: 0.8923788666725159, val_loss: 0.8619949221611023\n",
            "Saving model, epoch: 14054, train_loss: 0.8923743963241577, val_loss: 0.8619927167892456\n",
            "Saving model, epoch: 14055, train_loss: 0.8923699259757996, val_loss: 0.8619901537895203\n",
            "Saving model, epoch: 14056, train_loss: 0.8923655152320862, val_loss: 0.8619877099990845\n",
            "Saving model, epoch: 14057, train_loss: 0.8923612236976624, val_loss: 0.8619859218597412\n",
            "Saving model, epoch: 14058, train_loss: 0.892356812953949, val_loss: 0.8619831204414368\n",
            "Saving model, epoch: 14059, train_loss: 0.8923524618148804, val_loss: 0.8619807958602905\n",
            "Saving model, epoch: 14060, train_loss: 0.892348051071167, val_loss: 0.8619788289070129\n",
            "Saving model, epoch: 14061, train_loss: 0.8923436403274536, val_loss: 0.8619762063026428\n",
            "Saving model, epoch: 14062, train_loss: 0.8923392295837402, val_loss: 0.8619742393493652\n",
            "Saving model, epoch: 14063, train_loss: 0.8923347592353821, val_loss: 0.8619718551635742\n",
            "Saving model, epoch: 14064, train_loss: 0.8923305869102478, val_loss: 0.8619694113731384\n",
            "Saving model, epoch: 14065, train_loss: 0.8923261165618896, val_loss: 0.8619672656059265\n",
            "Saving model, epoch: 14066, train_loss: 0.8923216462135315, val_loss: 0.8619647026062012\n",
            "Saving model, epoch: 14067, train_loss: 0.8923171758651733, val_loss: 0.8619627356529236\n",
            "Saving model, epoch: 14068, train_loss: 0.8923127055168152, val_loss: 0.8619604706764221\n",
            "Saving model, epoch: 14069, train_loss: 0.892308235168457, val_loss: 0.8619580864906311\n",
            "Saving model, epoch: 14070, train_loss: 0.8923040628433228, val_loss: 0.8619559407234192\n",
            "Saving model, epoch: 14071, train_loss: 0.8922995924949646, val_loss: 0.8619534969329834\n",
            "Saving model, epoch: 14072, train_loss: 0.8922950029373169, val_loss: 0.8619510531425476\n",
            "Saving model, epoch: 14073, train_loss: 0.8922906517982483, val_loss: 0.8619488477706909\n",
            "Saving model, epoch: 14074, train_loss: 0.8922863602638245, val_loss: 0.8619463443756104\n",
            "Saving model, epoch: 14075, train_loss: 0.8922819495201111, val_loss: 0.8619441390037537\n",
            "Saving model, epoch: 14076, train_loss: 0.8922775387763977, val_loss: 0.8619419932365417\n",
            "Saving model, epoch: 14077, train_loss: 0.8922730088233948, val_loss: 0.8619400262832642\n",
            "Saving model, epoch: 14078, train_loss: 0.8922684788703918, val_loss: 0.8619373440742493\n",
            "Saving model, epoch: 14079, train_loss: 0.8922641277313232, val_loss: 0.8619347214698792\n",
            "Saving model, epoch: 14080, train_loss: 0.8922598361968994, val_loss: 0.8619332909584045\n",
            "Saving model, epoch: 14081, train_loss: 0.8922553062438965, val_loss: 0.8619307279586792\n",
            "Saving model, epoch: 14082, train_loss: 0.8922510147094727, val_loss: 0.8619282841682434\n",
            "Saving model, epoch: 14083, train_loss: 0.8922466039657593, val_loss: 0.8619258403778076\n",
            "Saving model, epoch: 14084, train_loss: 0.8922421336174011, val_loss: 0.8619237542152405\n",
            "Saving model, epoch: 14085, train_loss: 0.8922377228736877, val_loss: 0.8619213104248047\n",
            "Saving model, epoch: 14086, train_loss: 0.8922333121299744, val_loss: 0.8619189262390137\n",
            "Saving model, epoch: 14087, train_loss: 0.892228901386261, val_loss: 0.8619170188903809\n",
            "Saving model, epoch: 14088, train_loss: 0.8922242522239685, val_loss: 0.8619147539138794\n",
            "Saving model, epoch: 14089, train_loss: 0.8922200798988342, val_loss: 0.861911952495575\n",
            "Saving model, epoch: 14090, train_loss: 0.8922154307365417, val_loss: 0.8619098663330078\n",
            "Saving model, epoch: 14091, train_loss: 0.8922110795974731, val_loss: 0.8619074821472168\n",
            "Saving model, epoch: 14092, train_loss: 0.892206609249115, val_loss: 0.8619050979614258\n",
            "Saving model, epoch: 14093, train_loss: 0.8922022581100464, val_loss: 0.8619029521942139\n",
            "Saving model, epoch: 14094, train_loss: 0.8921977281570435, val_loss: 0.8619005680084229\n",
            "Saving model, epoch: 14095, train_loss: 0.8921935558319092, val_loss: 0.8618981242179871\n",
            "Saving model, epoch: 14096, train_loss: 0.8921889066696167, val_loss: 0.8618959784507751\n",
            "Saving model, epoch: 14097, train_loss: 0.8921844959259033, val_loss: 0.8618937134742737\n",
            "Saving model, epoch: 14098, train_loss: 0.8921800851821899, val_loss: 0.8618913292884827\n",
            "Saving model, epoch: 14099, train_loss: 0.8921754360198975, val_loss: 0.8618888854980469\n",
            "Saving model, epoch: 14100, train_loss: 0.8921711444854736, val_loss: 0.8618864417076111\n",
            "epoch: 14101, train_loss: 0.8921666741371155, val_loss: 0.8618846535682678\n",
            "Saving model, epoch: 14101, train_loss: 0.8921666741371155, val_loss: 0.8618846535682678\n",
            "Saving model, epoch: 14102, train_loss: 0.8921622633934021, val_loss: 0.8618820905685425\n",
            "Saving model, epoch: 14103, train_loss: 0.8921577334403992, val_loss: 0.8618797659873962\n",
            "Saving model, epoch: 14104, train_loss: 0.8921533226966858, val_loss: 0.8618778586387634\n",
            "Saving model, epoch: 14105, train_loss: 0.8921489119529724, val_loss: 0.8618754744529724\n",
            "Saving model, epoch: 14106, train_loss: 0.892144501209259, val_loss: 0.8618730902671814\n",
            "Saving model, epoch: 14107, train_loss: 0.8921400904655457, val_loss: 0.8618707656860352\n",
            "Saving model, epoch: 14108, train_loss: 0.8921355605125427, val_loss: 0.861868143081665\n",
            "Saving model, epoch: 14109, train_loss: 0.8921311497688293, val_loss: 0.8618661761283875\n",
            "Saving model, epoch: 14110, train_loss: 0.8921266198158264, val_loss: 0.8618640899658203\n",
            "Saving model, epoch: 14111, train_loss: 0.8921220898628235, val_loss: 0.8618617057800293\n",
            "Saving model, epoch: 14112, train_loss: 0.8921176791191101, val_loss: 0.861859142780304\n",
            "Saving model, epoch: 14113, train_loss: 0.8921131491661072, val_loss: 0.861856997013092\n",
            "Saving model, epoch: 14114, train_loss: 0.8921089172363281, val_loss: 0.8618547916412354\n",
            "Saving model, epoch: 14115, train_loss: 0.8921043276786804, val_loss: 0.8618518710136414\n",
            "Saving model, epoch: 14116, train_loss: 0.8920997977256775, val_loss: 0.8618497848510742\n",
            "Saving model, epoch: 14117, train_loss: 0.8920954465866089, val_loss: 0.8618477582931519\n",
            "Saving model, epoch: 14118, train_loss: 0.8920908570289612, val_loss: 0.8618448376655579\n",
            "Saving model, epoch: 14119, train_loss: 0.8920864462852478, val_loss: 0.8618429899215698\n",
            "Saving model, epoch: 14120, train_loss: 0.8920819759368896, val_loss: 0.8618407845497131\n",
            "Saving model, epoch: 14121, train_loss: 0.8920775055885315, val_loss: 0.8618384003639221\n",
            "Saving model, epoch: 14122, train_loss: 0.8920730352401733, val_loss: 0.8618359565734863\n",
            "Saving model, epoch: 14123, train_loss: 0.8920685648918152, val_loss: 0.8618339896202087\n",
            "Saving model, epoch: 14124, train_loss: 0.892064094543457, val_loss: 0.8618316650390625\n",
            "Saving model, epoch: 14125, train_loss: 0.8920596837997437, val_loss: 0.8618291020393372\n",
            "Saving model, epoch: 14126, train_loss: 0.8920552730560303, val_loss: 0.8618265390396118\n",
            "Saving model, epoch: 14127, train_loss: 0.8920506834983826, val_loss: 0.8618245720863342\n",
            "Saving model, epoch: 14128, train_loss: 0.8920462131500244, val_loss: 0.8618218302726746\n",
            "Saving model, epoch: 14129, train_loss: 0.892041802406311, val_loss: 0.861819863319397\n",
            "Saving model, epoch: 14130, train_loss: 0.8920372128486633, val_loss: 0.8618177175521851\n",
            "Saving model, epoch: 14131, train_loss: 0.89203280210495, val_loss: 0.8618149161338806\n",
            "Saving model, epoch: 14132, train_loss: 0.8920283317565918, val_loss: 0.8618126511573792\n",
            "Saving model, epoch: 14133, train_loss: 0.8920237421989441, val_loss: 0.8618106842041016\n",
            "Saving model, epoch: 14134, train_loss: 0.8920193910598755, val_loss: 0.8618083000183105\n",
            "Saving model, epoch: 14135, train_loss: 0.8920146822929382, val_loss: 0.8618059754371643\n",
            "Saving model, epoch: 14136, train_loss: 0.8920103907585144, val_loss: 0.8618032932281494\n",
            "Saving model, epoch: 14137, train_loss: 0.8920058608055115, val_loss: 0.8618015050888062\n",
            "Saving model, epoch: 14138, train_loss: 0.8920014500617981, val_loss: 0.8617985248565674\n",
            "Saving model, epoch: 14139, train_loss: 0.8919968008995056, val_loss: 0.8617964386940002\n",
            "Saving model, epoch: 14140, train_loss: 0.8919925093650818, val_loss: 0.8617944717407227\n",
            "Saving model, epoch: 14141, train_loss: 0.8919878602027893, val_loss: 0.8617916703224182\n",
            "Saving model, epoch: 14142, train_loss: 0.8919836282730103, val_loss: 0.8617894053459167\n",
            "Saving model, epoch: 14143, train_loss: 0.8919790387153625, val_loss: 0.8617879152297974\n",
            "Saving model, epoch: 14144, train_loss: 0.8919746279716492, val_loss: 0.8617845773696899\n",
            "Saving model, epoch: 14145, train_loss: 0.8919699788093567, val_loss: 0.8617823719978333\n",
            "Saving model, epoch: 14146, train_loss: 0.8919654488563538, val_loss: 0.8617802858352661\n",
            "Saving model, epoch: 14147, train_loss: 0.8919610381126404, val_loss: 0.8617778420448303\n",
            "Saving model, epoch: 14148, train_loss: 0.891956627368927, val_loss: 0.8617755770683289\n",
            "Saving model, epoch: 14149, train_loss: 0.8919522166252136, val_loss: 0.8617733716964722\n",
            "Saving model, epoch: 14150, train_loss: 0.8919473886489868, val_loss: 0.8617710471153259\n",
            "Saving model, epoch: 14151, train_loss: 0.8919431567192078, val_loss: 0.8617680668830872\n",
            "Saving model, epoch: 14152, train_loss: 0.8919386267662048, val_loss: 0.8617660999298096\n",
            "Saving model, epoch: 14153, train_loss: 0.8919340968132019, val_loss: 0.8617638945579529\n",
            "Saving model, epoch: 14154, train_loss: 0.8919295072555542, val_loss: 0.8617610931396484\n",
            "Saving model, epoch: 14155, train_loss: 0.8919250965118408, val_loss: 0.8617590665817261\n",
            "Saving model, epoch: 14156, train_loss: 0.8919204473495483, val_loss: 0.8617575168609619\n",
            "Saving model, epoch: 14157, train_loss: 0.891916036605835, val_loss: 0.8617541193962097\n",
            "Saving model, epoch: 14158, train_loss: 0.8919116258621216, val_loss: 0.8617520928382874\n",
            "Saving model, epoch: 14159, train_loss: 0.8919070959091187, val_loss: 0.8617499470710754\n",
            "Saving model, epoch: 14160, train_loss: 0.8919024467468262, val_loss: 0.8617470860481262\n",
            "Saving model, epoch: 14161, train_loss: 0.8918979167938232, val_loss: 0.8617449402809143\n",
            "Saving model, epoch: 14162, train_loss: 0.8918935060501099, val_loss: 0.8617429137229919\n",
            "Saving model, epoch: 14163, train_loss: 0.8918889164924622, val_loss: 0.8617403507232666\n",
            "Saving model, epoch: 14164, train_loss: 0.8918845057487488, val_loss: 0.8617382049560547\n",
            "Saving model, epoch: 14165, train_loss: 0.8918797373771667, val_loss: 0.8617359399795532\n",
            "Saving model, epoch: 14166, train_loss: 0.8918754458427429, val_loss: 0.8617334961891174\n",
            "Saving model, epoch: 14167, train_loss: 0.8918710350990295, val_loss: 0.8617306351661682\n",
            "Saving model, epoch: 14168, train_loss: 0.8918663859367371, val_loss: 0.8617286682128906\n",
            "Saving model, epoch: 14169, train_loss: 0.8918617963790894, val_loss: 0.8617265224456787\n",
            "Saving model, epoch: 14170, train_loss: 0.8918571472167969, val_loss: 0.8617240190505981\n",
            "Saving model, epoch: 14171, train_loss: 0.8918527364730835, val_loss: 0.8617216944694519\n",
            "Saving model, epoch: 14172, train_loss: 0.8918483257293701, val_loss: 0.8617198467254639\n",
            "Saving model, epoch: 14173, train_loss: 0.8918437957763672, val_loss: 0.8617167472839355\n",
            "Saving model, epoch: 14174, train_loss: 0.8918391466140747, val_loss: 0.8617146611213684\n",
            "Saving model, epoch: 14175, train_loss: 0.8918346762657166, val_loss: 0.8617123961448669\n",
            "Saving model, epoch: 14176, train_loss: 0.8918302059173584, val_loss: 0.8617104887962341\n",
            "Saving model, epoch: 14177, train_loss: 0.8918256163597107, val_loss: 0.8617076873779297\n",
            "Saving model, epoch: 14178, train_loss: 0.8918209671974182, val_loss: 0.861705482006073\n",
            "Saving model, epoch: 14179, train_loss: 0.8918166756629944, val_loss: 0.861703097820282\n",
            "Saving model, epoch: 14180, train_loss: 0.8918120265007019, val_loss: 0.8617005348205566\n",
            "Saving model, epoch: 14181, train_loss: 0.891807496547699, val_loss: 0.8616979718208313\n",
            "Saving model, epoch: 14182, train_loss: 0.8918029069900513, val_loss: 0.8616964817047119\n",
            "Saving model, epoch: 14183, train_loss: 0.8917984962463379, val_loss: 0.8616936206817627\n",
            "Saving model, epoch: 14184, train_loss: 0.8917938470840454, val_loss: 0.8616911768913269\n",
            "Saving model, epoch: 14185, train_loss: 0.891789436340332, val_loss: 0.8616894483566284\n",
            "Saving model, epoch: 14186, train_loss: 0.8917847871780396, val_loss: 0.861686646938324\n",
            "Saving model, epoch: 14187, train_loss: 0.8917803764343262, val_loss: 0.8616839051246643\n",
            "Saving model, epoch: 14188, train_loss: 0.8917757868766785, val_loss: 0.8616819977760315\n",
            "Saving model, epoch: 14189, train_loss: 0.891771137714386, val_loss: 0.8616794347763062\n",
            "Saving model, epoch: 14190, train_loss: 0.8917667269706726, val_loss: 0.8616775274276733\n",
            "Saving model, epoch: 14191, train_loss: 0.8917621374130249, val_loss: 0.8616746664047241\n",
            "Saving model, epoch: 14192, train_loss: 0.8917574882507324, val_loss: 0.8616724014282227\n",
            "Saving model, epoch: 14193, train_loss: 0.891753077507019, val_loss: 0.8616700172424316\n",
            "Saving model, epoch: 14194, train_loss: 0.8917483687400818, val_loss: 0.8616676926612854\n",
            "Saving model, epoch: 14195, train_loss: 0.8917438387870789, val_loss: 0.8616654872894287\n",
            "Saving model, epoch: 14196, train_loss: 0.8917391896247864, val_loss: 0.8616625666618347\n",
            "Saving model, epoch: 14197, train_loss: 0.8917349576950073, val_loss: 0.8616604804992676\n",
            "Saving model, epoch: 14198, train_loss: 0.8917301893234253, val_loss: 0.8616586327552795\n",
            "Saving model, epoch: 14199, train_loss: 0.8917256593704224, val_loss: 0.8616560697555542\n",
            "Saving model, epoch: 14200, train_loss: 0.891721248626709, val_loss: 0.861653745174408\n",
            "epoch: 14201, train_loss: 0.891716480255127, val_loss: 0.8616513609886169\n",
            "Saving model, epoch: 14201, train_loss: 0.891716480255127, val_loss: 0.8616513609886169\n",
            "Saving model, epoch: 14202, train_loss: 0.8917120695114136, val_loss: 0.8616490960121155\n",
            "Saving model, epoch: 14203, train_loss: 0.8917076587677002, val_loss: 0.8616471290588379\n",
            "Saving model, epoch: 14204, train_loss: 0.8917030096054077, val_loss: 0.8616437315940857\n",
            "Saving model, epoch: 14205, train_loss: 0.8916983008384705, val_loss: 0.8616420030593872\n",
            "Saving model, epoch: 14206, train_loss: 0.8916937708854675, val_loss: 0.861639678478241\n",
            "Saving model, epoch: 14207, train_loss: 0.8916893601417542, val_loss: 0.8616373538970947\n",
            "Saving model, epoch: 14208, train_loss: 0.8916847705841064, val_loss: 0.8616350889205933\n",
            "Saving model, epoch: 14209, train_loss: 0.891680121421814, val_loss: 0.8616323471069336\n",
            "Saving model, epoch: 14210, train_loss: 0.8916755318641663, val_loss: 0.8616306185722351\n",
            "Saving model, epoch: 14211, train_loss: 0.8916710019111633, val_loss: 0.861627995967865\n",
            "Saving model, epoch: 14212, train_loss: 0.8916664719581604, val_loss: 0.8616250157356262\n",
            "Saving model, epoch: 14213, train_loss: 0.8916617631912231, val_loss: 0.8616230487823486\n",
            "Saving model, epoch: 14214, train_loss: 0.8916572332382202, val_loss: 0.8616204857826233\n",
            "Saving model, epoch: 14215, train_loss: 0.8916527032852173, val_loss: 0.8616184592247009\n",
            "Saving model, epoch: 14216, train_loss: 0.8916482925415039, val_loss: 0.8616160750389099\n",
            "Saving model, epoch: 14217, train_loss: 0.8916435837745667, val_loss: 0.861613392829895\n",
            "Saving model, epoch: 14218, train_loss: 0.8916389346122742, val_loss: 0.8616113662719727\n",
            "Saving model, epoch: 14219, train_loss: 0.8916342854499817, val_loss: 0.8616092205047607\n",
            "Saving model, epoch: 14220, train_loss: 0.8916298747062683, val_loss: 0.861606240272522\n",
            "Saving model, epoch: 14221, train_loss: 0.8916252851486206, val_loss: 0.8616044521331787\n",
            "Saving model, epoch: 14222, train_loss: 0.8916206359863281, val_loss: 0.8616020083427429\n",
            "Saving model, epoch: 14223, train_loss: 0.8916160464286804, val_loss: 0.8615995049476624\n",
            "Saving model, epoch: 14224, train_loss: 0.8916113972663879, val_loss: 0.8615974187850952\n",
            "Saving model, epoch: 14225, train_loss: 0.8916069865226746, val_loss: 0.8615946769714355\n",
            "Saving model, epoch: 14226, train_loss: 0.8916025757789612, val_loss: 0.8615921139717102\n",
            "Saving model, epoch: 14227, train_loss: 0.8915979266166687, val_loss: 0.8615903854370117\n",
            "Saving model, epoch: 14228, train_loss: 0.8915932178497314, val_loss: 0.861587405204773\n",
            "Saving model, epoch: 14229, train_loss: 0.8915886878967285, val_loss: 0.8615853786468506\n",
            "Saving model, epoch: 14230, train_loss: 0.8915840983390808, val_loss: 0.8615832328796387\n",
            "Saving model, epoch: 14231, train_loss: 0.8915794491767883, val_loss: 0.8615800738334656\n",
            "Saving model, epoch: 14232, train_loss: 0.8915748596191406, val_loss: 0.8615783452987671\n",
            "Saving model, epoch: 14233, train_loss: 0.8915703296661377, val_loss: 0.8615764379501343\n",
            "Saving model, epoch: 14234, train_loss: 0.8915656208992004, val_loss: 0.8615735769271851\n",
            "Saving model, epoch: 14235, train_loss: 0.8915611505508423, val_loss: 0.8615708351135254\n",
            "Saving model, epoch: 14236, train_loss: 0.8915565609931946, val_loss: 0.8615687489509583\n",
            "Saving model, epoch: 14237, train_loss: 0.8915519118309021, val_loss: 0.8615663051605225\n",
            "Saving model, epoch: 14238, train_loss: 0.8915473222732544, val_loss: 0.861564040184021\n",
            "Saving model, epoch: 14239, train_loss: 0.8915426731109619, val_loss: 0.8615614175796509\n",
            "Saving model, epoch: 14240, train_loss: 0.8915380835533142, val_loss: 0.8615593910217285\n",
            "Saving model, epoch: 14241, train_loss: 0.8915334343910217, val_loss: 0.8615567684173584\n",
            "Saving model, epoch: 14242, train_loss: 0.8915289044380188, val_loss: 0.8615539073944092\n",
            "Saving model, epoch: 14243, train_loss: 0.8915241956710815, val_loss: 0.8615518808364868\n",
            "Saving model, epoch: 14244, train_loss: 0.8915197849273682, val_loss: 0.8615497350692749\n",
            "Saving model, epoch: 14245, train_loss: 0.8915150761604309, val_loss: 0.8615471124649048\n",
            "Saving model, epoch: 14246, train_loss: 0.891510546207428, val_loss: 0.8615445494651794\n",
            "Saving model, epoch: 14247, train_loss: 0.8915058970451355, val_loss: 0.8615427613258362\n",
            "Saving model, epoch: 14248, train_loss: 0.8915013074874878, val_loss: 0.8615401983261108\n",
            "Saving model, epoch: 14249, train_loss: 0.8914966583251953, val_loss: 0.8615379929542542\n",
            "Saving model, epoch: 14250, train_loss: 0.8914920687675476, val_loss: 0.8615356087684631\n",
            "Saving model, epoch: 14251, train_loss: 0.8914874196052551, val_loss: 0.8615329265594482\n",
            "Saving model, epoch: 14252, train_loss: 0.8914828300476074, val_loss: 0.8615309000015259\n",
            "Saving model, epoch: 14253, train_loss: 0.8914781808853149, val_loss: 0.8615285158157349\n",
            "Saving model, epoch: 14254, train_loss: 0.891473650932312, val_loss: 0.861526370048523\n",
            "Saving model, epoch: 14255, train_loss: 0.8914689421653748, val_loss: 0.8615236282348633\n",
            "Saving model, epoch: 14256, train_loss: 0.8914642930030823, val_loss: 0.8615211844444275\n",
            "Saving model, epoch: 14257, train_loss: 0.8914597034454346, val_loss: 0.861518919467926\n",
            "Saving model, epoch: 14258, train_loss: 0.8914551734924316, val_loss: 0.8615162372589111\n",
            "Saving model, epoch: 14259, train_loss: 0.8914506435394287, val_loss: 0.8615139126777649\n",
            "Saving model, epoch: 14260, train_loss: 0.8914458155632019, val_loss: 0.8615116477012634\n",
            "Saving model, epoch: 14261, train_loss: 0.8914412260055542, val_loss: 0.861509382724762\n",
            "Saving model, epoch: 14262, train_loss: 0.8914365768432617, val_loss: 0.8615071177482605\n",
            "Saving model, epoch: 14263, train_loss: 0.8914320468902588, val_loss: 0.8615047931671143\n",
            "Saving model, epoch: 14264, train_loss: 0.8914273381233215, val_loss: 0.8615024089813232\n",
            "Saving model, epoch: 14265, train_loss: 0.8914227485656738, val_loss: 0.8614997863769531\n",
            "Saving model, epoch: 14266, train_loss: 0.8914180994033813, val_loss: 0.8614973425865173\n",
            "Saving model, epoch: 14267, train_loss: 0.8914135694503784, val_loss: 0.8614950180053711\n",
            "Saving model, epoch: 14268, train_loss: 0.8914090394973755, val_loss: 0.8614928126335144\n",
            "Saving model, epoch: 14269, train_loss: 0.8914042115211487, val_loss: 0.8614902496337891\n",
            "Saving model, epoch: 14270, train_loss: 0.8913997411727905, val_loss: 0.861487865447998\n",
            "Saving model, epoch: 14271, train_loss: 0.8913952112197876, val_loss: 0.8614853024482727\n",
            "Saving model, epoch: 14272, train_loss: 0.8913903832435608, val_loss: 0.8614831566810608\n",
            "Saving model, epoch: 14273, train_loss: 0.8913857340812683, val_loss: 0.8614807724952698\n",
            "Saving model, epoch: 14274, train_loss: 0.8913812041282654, val_loss: 0.8614780902862549\n",
            "Saving model, epoch: 14275, train_loss: 0.8913764357566833, val_loss: 0.8614760637283325\n",
            "Saving model, epoch: 14276, train_loss: 0.8913716673851013, val_loss: 0.8614734411239624\n",
            "Saving model, epoch: 14277, train_loss: 0.8913672566413879, val_loss: 0.8614711761474609\n",
            "Saving model, epoch: 14278, train_loss: 0.8913624286651611, val_loss: 0.8614685535430908\n",
            "Saving model, epoch: 14279, train_loss: 0.8913581371307373, val_loss: 0.8614664673805237\n",
            "Saving model, epoch: 14280, train_loss: 0.891353189945221, val_loss: 0.8614638447761536\n",
            "Saving model, epoch: 14281, train_loss: 0.8913487792015076, val_loss: 0.8614616990089417\n",
            "Saving model, epoch: 14282, train_loss: 0.8913439512252808, val_loss: 0.8614591956138611\n",
            "Saving model, epoch: 14283, train_loss: 0.8913395404815674, val_loss: 0.8614568114280701\n",
            "Saving model, epoch: 14284, train_loss: 0.8913347125053406, val_loss: 0.8614543676376343\n",
            "Saving model, epoch: 14285, train_loss: 0.8913300633430481, val_loss: 0.8614517450332642\n",
            "Saving model, epoch: 14286, train_loss: 0.8913253545761108, val_loss: 0.8614494800567627\n",
            "Saving model, epoch: 14287, train_loss: 0.8913208246231079, val_loss: 0.8614471554756165\n",
            "Saving model, epoch: 14288, train_loss: 0.8913162350654602, val_loss: 0.8614447712898254\n",
            "Saving model, epoch: 14289, train_loss: 0.8913115859031677, val_loss: 0.8614421486854553\n",
            "Saving model, epoch: 14290, train_loss: 0.89130699634552, val_loss: 0.8614403605461121\n",
            "Saving model, epoch: 14291, train_loss: 0.8913023471832275, val_loss: 0.8614373803138733\n",
            "Saving model, epoch: 14292, train_loss: 0.8912975192070007, val_loss: 0.8614349365234375\n",
            "Saving model, epoch: 14293, train_loss: 0.8912928104400635, val_loss: 0.8614330887794495\n",
            "Saving model, epoch: 14294, train_loss: 0.8912882804870605, val_loss: 0.8614307641983032\n",
            "Saving model, epoch: 14295, train_loss: 0.8912837505340576, val_loss: 0.8614276647567749\n",
            "Saving model, epoch: 14296, train_loss: 0.891278862953186, val_loss: 0.8614258170127869\n",
            "Saving model, epoch: 14297, train_loss: 0.8912742137908936, val_loss: 0.8614235520362854\n",
            "Saving model, epoch: 14298, train_loss: 0.8912693858146667, val_loss: 0.8614205718040466\n",
            "Saving model, epoch: 14299, train_loss: 0.8912649750709534, val_loss: 0.8614183068275452\n",
            "Saving model, epoch: 14300, train_loss: 0.8912602663040161, val_loss: 0.8614159822463989\n",
            "epoch: 14301, train_loss: 0.8912555575370789, val_loss: 0.8614138960838318\n",
            "Saving model, epoch: 14301, train_loss: 0.8912555575370789, val_loss: 0.8614138960838318\n",
            "Saving model, epoch: 14302, train_loss: 0.8912510275840759, val_loss: 0.8614112734794617\n",
            "Saving model, epoch: 14303, train_loss: 0.8912463188171387, val_loss: 0.8614090085029602\n",
            "Saving model, epoch: 14304, train_loss: 0.8912414908409119, val_loss: 0.861406683921814\n",
            "Saving model, epoch: 14305, train_loss: 0.8912369608879089, val_loss: 0.8614044785499573\n",
            "Saving model, epoch: 14306, train_loss: 0.891232430934906, val_loss: 0.8614016175270081\n",
            "Saving model, epoch: 14307, train_loss: 0.8912277221679688, val_loss: 0.8613991141319275\n",
            "Saving model, epoch: 14308, train_loss: 0.8912230134010315, val_loss: 0.8613969683647156\n",
            "Saving model, epoch: 14309, train_loss: 0.8912181854248047, val_loss: 0.8613946437835693\n",
            "Saving model, epoch: 14310, train_loss: 0.8912135362625122, val_loss: 0.8613919615745544\n",
            "Saving model, epoch: 14311, train_loss: 0.8912089467048645, val_loss: 0.8613895773887634\n",
            "Saving model, epoch: 14312, train_loss: 0.8912041783332825, val_loss: 0.8613871335983276\n",
            "Saving model, epoch: 14313, train_loss: 0.8911997079849243, val_loss: 0.8613848686218262\n",
            "Saving model, epoch: 14314, train_loss: 0.8911949396133423, val_loss: 0.861382246017456\n",
            "Saving model, epoch: 14315, train_loss: 0.891190230846405, val_loss: 0.8613799810409546\n",
            "Saving model, epoch: 14316, train_loss: 0.8911856412887573, val_loss: 0.8613779544830322\n",
            "Saving model, epoch: 14317, train_loss: 0.8911808133125305, val_loss: 0.861375093460083\n",
            "Saving model, epoch: 14318, train_loss: 0.8911762833595276, val_loss: 0.8613726496696472\n",
            "Saving model, epoch: 14319, train_loss: 0.8911714553833008, val_loss: 0.8613701462745667\n",
            "Saving model, epoch: 14320, train_loss: 0.8911668062210083, val_loss: 0.8613678216934204\n",
            "Saving model, epoch: 14321, train_loss: 0.8911622166633606, val_loss: 0.8613656759262085\n",
            "Saving model, epoch: 14322, train_loss: 0.8911573886871338, val_loss: 0.8613629937171936\n",
            "Saving model, epoch: 14323, train_loss: 0.8911528587341309, val_loss: 0.8613603711128235\n",
            "Saving model, epoch: 14324, train_loss: 0.8911481499671936, val_loss: 0.8613582849502563\n",
            "Saving model, epoch: 14325, train_loss: 0.8911434412002563, val_loss: 0.8613553643226624\n",
            "Saving model, epoch: 14326, train_loss: 0.8911387920379639, val_loss: 0.8613538146018982\n",
            "Saving model, epoch: 14327, train_loss: 0.8911339640617371, val_loss: 0.8613513112068176\n",
            "Saving model, epoch: 14328, train_loss: 0.8911292552947998, val_loss: 0.8613486886024475\n",
            "Saving model, epoch: 14329, train_loss: 0.8911246061325073, val_loss: 0.861346960067749\n",
            "Saving model, epoch: 14330, train_loss: 0.8911198973655701, val_loss: 0.8613441586494446\n",
            "Saving model, epoch: 14331, train_loss: 0.8911151885986328, val_loss: 0.8613412380218506\n",
            "Saving model, epoch: 14332, train_loss: 0.8911105394363403, val_loss: 0.8613388538360596\n",
            "Saving model, epoch: 14333, train_loss: 0.8911058306694031, val_loss: 0.8613364696502686\n",
            "Saving model, epoch: 14334, train_loss: 0.8911012411117554, val_loss: 0.8613340258598328\n",
            "Saving model, epoch: 14335, train_loss: 0.8910964727401733, val_loss: 0.8613319396972656\n",
            "Saving model, epoch: 14336, train_loss: 0.8910917639732361, val_loss: 0.8613294363021851\n",
            "Saving model, epoch: 14337, train_loss: 0.8910870552062988, val_loss: 0.8613272905349731\n",
            "Saving model, epoch: 14338, train_loss: 0.8910823464393616, val_loss: 0.8613250255584717\n",
            "Saving model, epoch: 14339, train_loss: 0.8910776972770691, val_loss: 0.8613222241401672\n",
            "Saving model, epoch: 14340, train_loss: 0.8910728693008423, val_loss: 0.8613196611404419\n",
            "Saving model, epoch: 14341, train_loss: 0.891068160533905, val_loss: 0.8613174557685852\n",
            "Saving model, epoch: 14342, train_loss: 0.8910634517669678, val_loss: 0.8613145351409912\n",
            "Saving model, epoch: 14343, train_loss: 0.8910588026046753, val_loss: 0.861312985420227\n",
            "Saving model, epoch: 14344, train_loss: 0.891054093837738, val_loss: 0.8613101840019226\n",
            "Saving model, epoch: 14345, train_loss: 0.8910495042800903, val_loss: 0.8613075613975525\n",
            "Saving model, epoch: 14346, train_loss: 0.8910447359085083, val_loss: 0.8613055944442749\n",
            "Saving model, epoch: 14347, train_loss: 0.8910399079322815, val_loss: 0.8613026738166809\n",
            "Saving model, epoch: 14348, train_loss: 0.8910351991653442, val_loss: 0.861300528049469\n",
            "Saving model, epoch: 14349, train_loss: 0.891030490398407, val_loss: 0.8612980246543884\n",
            "Saving model, epoch: 14350, train_loss: 0.8910257816314697, val_loss: 0.8612955808639526\n",
            "Saving model, epoch: 14351, train_loss: 0.8910211324691772, val_loss: 0.8612931966781616\n",
            "Saving model, epoch: 14352, train_loss: 0.89101642370224, val_loss: 0.8612908720970154\n",
            "Saving model, epoch: 14353, train_loss: 0.8910117149353027, val_loss: 0.86128830909729\n",
            "Saving model, epoch: 14354, train_loss: 0.8910070061683655, val_loss: 0.8612864017486572\n",
            "Saving model, epoch: 14355, train_loss: 0.8910021781921387, val_loss: 0.8612836599349976\n",
            "Saving model, epoch: 14356, train_loss: 0.8909977674484253, val_loss: 0.8612809777259827\n",
            "Saving model, epoch: 14357, train_loss: 0.8909929394721985, val_loss: 0.8612787127494812\n",
            "Saving model, epoch: 14358, train_loss: 0.8909879922866821, val_loss: 0.8612769246101379\n",
            "Saving model, epoch: 14359, train_loss: 0.8909832835197449, val_loss: 0.8612731695175171\n",
            "Saving model, epoch: 14360, train_loss: 0.8909786343574524, val_loss: 0.8612715005874634\n",
            "Saving model, epoch: 14361, train_loss: 0.8909740447998047, val_loss: 0.8612691164016724\n",
            "Saving model, epoch: 14362, train_loss: 0.8909690976142883, val_loss: 0.8612663745880127\n",
            "Saving model, epoch: 14363, train_loss: 0.8909645676612854, val_loss: 0.8612642288208008\n",
            "Saving model, epoch: 14364, train_loss: 0.8909597396850586, val_loss: 0.8612616062164307\n",
            "Saving model, epoch: 14365, train_loss: 0.8909550309181213, val_loss: 0.8612591624259949\n",
            "Saving model, epoch: 14366, train_loss: 0.8909503221511841, val_loss: 0.8612565994262695\n",
            "Saving model, epoch: 14367, train_loss: 0.8909454941749573, val_loss: 0.8612546920776367\n",
            "Saving model, epoch: 14368, train_loss: 0.8909408450126648, val_loss: 0.8612518310546875\n",
            "Saving model, epoch: 14369, train_loss: 0.8909360766410828, val_loss: 0.8612498044967651\n",
            "Saving model, epoch: 14370, train_loss: 0.8909314274787903, val_loss: 0.8612470030784607\n",
            "Saving model, epoch: 14371, train_loss: 0.8909265995025635, val_loss: 0.8612444400787354\n",
            "Saving model, epoch: 14372, train_loss: 0.8909217715263367, val_loss: 0.8612427711486816\n",
            "Saving model, epoch: 14373, train_loss: 0.8909170627593994, val_loss: 0.8612401485443115\n",
            "Saving model, epoch: 14374, train_loss: 0.8909123539924622, val_loss: 0.8612373471260071\n",
            "Saving model, epoch: 14375, train_loss: 0.8909075260162354, val_loss: 0.8612353801727295\n",
            "Saving model, epoch: 14376, train_loss: 0.8909028768539429, val_loss: 0.8612329363822937\n",
            "Saving model, epoch: 14377, train_loss: 0.8908982872962952, val_loss: 0.8612299561500549\n",
            "Saving model, epoch: 14378, train_loss: 0.8908934593200684, val_loss: 0.8612282276153564\n",
            "Saving model, epoch: 14379, train_loss: 0.8908886313438416, val_loss: 0.8612257242202759\n",
            "Saving model, epoch: 14380, train_loss: 0.8908838033676147, val_loss: 0.8612223863601685\n",
            "Saving model, epoch: 14381, train_loss: 0.890879213809967, val_loss: 0.8612207174301147\n",
            "Saving model, epoch: 14382, train_loss: 0.8908743858337402, val_loss: 0.8612178564071655\n",
            "Saving model, epoch: 14383, train_loss: 0.8908695578575134, val_loss: 0.8612151741981506\n",
            "Saving model, epoch: 14384, train_loss: 0.890864908695221, val_loss: 0.8612130284309387\n",
            "Saving model, epoch: 14385, train_loss: 0.8908600807189941, val_loss: 0.8612111210823059\n",
            "Saving model, epoch: 14386, train_loss: 0.8908553719520569, val_loss: 0.8612080216407776\n",
            "Saving model, epoch: 14387, train_loss: 0.8908505439758301, val_loss: 0.8612060546875\n",
            "Saving model, epoch: 14388, train_loss: 0.8908459544181824, val_loss: 0.8612027764320374\n",
            "Saving model, epoch: 14389, train_loss: 0.8908411860466003, val_loss: 0.8612010478973389\n",
            "Saving model, epoch: 14390, train_loss: 0.8908365964889526, val_loss: 0.8611988425254822\n",
            "Saving model, epoch: 14391, train_loss: 0.8908315896987915, val_loss: 0.8611955046653748\n",
            "Saving model, epoch: 14392, train_loss: 0.890826940536499, val_loss: 0.8611931800842285\n",
            "Saving model, epoch: 14393, train_loss: 0.8908221125602722, val_loss: 0.861190915107727\n",
            "Saving model, epoch: 14394, train_loss: 0.8908172845840454, val_loss: 0.861188530921936\n",
            "Saving model, epoch: 14395, train_loss: 0.8908124566078186, val_loss: 0.8611863851547241\n",
            "Saving model, epoch: 14396, train_loss: 0.8908078670501709, val_loss: 0.8611838221549988\n",
            "Saving model, epoch: 14397, train_loss: 0.8908030390739441, val_loss: 0.8611809015274048\n",
            "Saving model, epoch: 14398, train_loss: 0.8907982110977173, val_loss: 0.8611788749694824\n",
            "Saving model, epoch: 14399, train_loss: 0.89079350233078, val_loss: 0.8611765503883362\n",
            "Saving model, epoch: 14400, train_loss: 0.890788733959198, val_loss: 0.8611739873886108\n",
            "epoch: 14401, train_loss: 0.890783965587616, val_loss: 0.8611721396446228\n",
            "Saving model, epoch: 14401, train_loss: 0.890783965587616, val_loss: 0.8611721396446228\n",
            "Saving model, epoch: 14402, train_loss: 0.8907791376113892, val_loss: 0.8611693978309631\n",
            "Saving model, epoch: 14403, train_loss: 0.8907744884490967, val_loss: 0.8611664772033691\n",
            "Saving model, epoch: 14404, train_loss: 0.8907696604728699, val_loss: 0.8611645698547363\n",
            "Saving model, epoch: 14405, train_loss: 0.8907648324966431, val_loss: 0.8611617684364319\n",
            "Saving model, epoch: 14406, train_loss: 0.8907600045204163, val_loss: 0.8611599206924438\n",
            "Saving model, epoch: 14407, train_loss: 0.8907554149627686, val_loss: 0.8611574172973633\n",
            "Saving model, epoch: 14408, train_loss: 0.8907505869865417, val_loss: 0.8611545562744141\n",
            "Saving model, epoch: 14409, train_loss: 0.8907456398010254, val_loss: 0.8611521124839783\n",
            "Saving model, epoch: 14410, train_loss: 0.8907410502433777, val_loss: 0.8611499667167664\n",
            "Saving model, epoch: 14411, train_loss: 0.8907362222671509, val_loss: 0.8611470460891724\n",
            "Saving model, epoch: 14412, train_loss: 0.8907315135002136, val_loss: 0.8611446619033813\n",
            "Saving model, epoch: 14413, train_loss: 0.8907266855239868, val_loss: 0.8611428141593933\n",
            "Saving model, epoch: 14414, train_loss: 0.89072185754776, val_loss: 0.861139714717865\n",
            "Saving model, epoch: 14415, train_loss: 0.8907170295715332, val_loss: 0.8611372709274292\n",
            "Saving model, epoch: 14416, train_loss: 0.8907123804092407, val_loss: 0.8611353635787964\n",
            "Saving model, epoch: 14417, train_loss: 0.8907076120376587, val_loss: 0.8611326217651367\n",
            "Saving model, epoch: 14418, train_loss: 0.8907027840614319, val_loss: 0.8611301183700562\n",
            "Saving model, epoch: 14419, train_loss: 0.890697717666626, val_loss: 0.8611271977424622\n",
            "Saving model, epoch: 14420, train_loss: 0.8906931281089783, val_loss: 0.861125111579895\n",
            "Saving model, epoch: 14421, train_loss: 0.8906884789466858, val_loss: 0.8611230254173279\n",
            "Saving model, epoch: 14422, train_loss: 0.890683650970459, val_loss: 0.8611204624176025\n",
            "Saving model, epoch: 14423, train_loss: 0.8906788229942322, val_loss: 0.8611175417900085\n",
            "Saving model, epoch: 14424, train_loss: 0.8906739354133606, val_loss: 0.8611154556274414\n",
            "Saving model, epoch: 14425, train_loss: 0.8906694054603577, val_loss: 0.8611131906509399\n",
            "Saving model, epoch: 14426, train_loss: 0.8906643986701965, val_loss: 0.8611103892326355\n",
            "Saving model, epoch: 14427, train_loss: 0.8906595706939697, val_loss: 0.8611086010932922\n",
            "Saving model, epoch: 14428, train_loss: 0.8906548619270325, val_loss: 0.8611062169075012\n",
            "Saving model, epoch: 14429, train_loss: 0.8906500935554504, val_loss: 0.8611030578613281\n",
            "Saving model, epoch: 14430, train_loss: 0.8906450867652893, val_loss: 0.8611009120941162\n",
            "Saving model, epoch: 14431, train_loss: 0.8906404972076416, val_loss: 0.8610981106758118\n",
            "Saving model, epoch: 14432, train_loss: 0.8906356692314148, val_loss: 0.8610955476760864\n",
            "Saving model, epoch: 14433, train_loss: 0.890630841255188, val_loss: 0.8610935211181641\n",
            "Saving model, epoch: 14434, train_loss: 0.8906261324882507, val_loss: 0.8610910177230835\n",
            "Saving model, epoch: 14435, train_loss: 0.8906210660934448, val_loss: 0.8610886931419373\n",
            "Saving model, epoch: 14436, train_loss: 0.890616238117218, val_loss: 0.861086368560791\n",
            "Saving model, epoch: 14437, train_loss: 0.8906115293502808, val_loss: 0.8610836863517761\n",
            "Saving model, epoch: 14438, train_loss: 0.890606701374054, val_loss: 0.861081063747406\n",
            "Saving model, epoch: 14439, train_loss: 0.8906021118164062, val_loss: 0.8610787987709045\n",
            "Saving model, epoch: 14440, train_loss: 0.8905972838401794, val_loss: 0.8610760569572449\n",
            "Saving model, epoch: 14441, train_loss: 0.8905924558639526, val_loss: 0.8610738515853882\n",
            "Saving model, epoch: 14442, train_loss: 0.8905875086784363, val_loss: 0.8610713481903076\n",
            "Saving model, epoch: 14443, train_loss: 0.8905826807022095, val_loss: 0.8610690236091614\n",
            "Saving model, epoch: 14444, train_loss: 0.8905779719352722, val_loss: 0.8610660433769226\n",
            "Saving model, epoch: 14445, train_loss: 0.8905731439590454, val_loss: 0.861063539981842\n",
            "Saving model, epoch: 14446, train_loss: 0.8905683159828186, val_loss: 0.8610614538192749\n",
            "Saving model, epoch: 14447, train_loss: 0.8905634880065918, val_loss: 0.8610591292381287\n",
            "Saving model, epoch: 14448, train_loss: 0.890558660030365, val_loss: 0.8610564470291138\n",
            "Saving model, epoch: 14449, train_loss: 0.8905539512634277, val_loss: 0.8610541224479675\n",
            "Saving model, epoch: 14450, train_loss: 0.8905490636825562, val_loss: 0.8610516786575317\n",
            "Saving model, epoch: 14451, train_loss: 0.8905444145202637, val_loss: 0.8610488176345825\n",
            "Saving model, epoch: 14452, train_loss: 0.8905394077301025, val_loss: 0.8610463738441467\n",
            "Saving model, epoch: 14453, train_loss: 0.8905345797538757, val_loss: 0.8610438108444214\n",
            "Saving model, epoch: 14454, train_loss: 0.8905297517776489, val_loss: 0.8610420227050781\n",
            "Saving model, epoch: 14455, train_loss: 0.8905251026153564, val_loss: 0.8610394597053528\n",
            "Saving model, epoch: 14456, train_loss: 0.8905202150344849, val_loss: 0.8610368967056274\n",
            "Saving model, epoch: 14457, train_loss: 0.8905152678489685, val_loss: 0.8610344529151917\n",
            "Saving model, epoch: 14458, train_loss: 0.8905104398727417, val_loss: 0.8610317707061768\n",
            "Saving model, epoch: 14459, train_loss: 0.8905056118965149, val_loss: 0.8610296845436096\n",
            "Saving model, epoch: 14460, train_loss: 0.8905008435249329, val_loss: 0.86102694272995\n",
            "Saving model, epoch: 14461, train_loss: 0.8904961943626404, val_loss: 0.8610247373580933\n",
            "Saving model, epoch: 14462, train_loss: 0.8904911875724792, val_loss: 0.8610215783119202\n",
            "Saving model, epoch: 14463, train_loss: 0.8904863595962524, val_loss: 0.8610191941261292\n",
            "Saving model, epoch: 14464, train_loss: 0.8904814124107361, val_loss: 0.8610168099403381\n",
            "Saving model, epoch: 14465, train_loss: 0.8904765844345093, val_loss: 0.8610145449638367\n",
            "Saving model, epoch: 14466, train_loss: 0.890471875667572, val_loss: 0.8610117435455322\n",
            "Saving model, epoch: 14467, train_loss: 0.8904668688774109, val_loss: 0.8610098958015442\n",
            "Saving model, epoch: 14468, train_loss: 0.8904621601104736, val_loss: 0.8610068559646606\n",
            "Saving model, epoch: 14469, train_loss: 0.8904573917388916, val_loss: 0.8610051870346069\n",
            "Saving model, epoch: 14470, train_loss: 0.8904525637626648, val_loss: 0.8610023260116577\n",
            "Saving model, epoch: 14471, train_loss: 0.8904476761817932, val_loss: 0.8609995245933533\n",
            "Saving model, epoch: 14472, train_loss: 0.8904427289962769, val_loss: 0.8609974980354309\n",
            "Saving model, epoch: 14473, train_loss: 0.8904378414154053, val_loss: 0.8609943985939026\n",
            "Saving model, epoch: 14474, train_loss: 0.8904331922531128, val_loss: 0.8609921336174011\n",
            "Saving model, epoch: 14475, train_loss: 0.890428364276886, val_loss: 0.8609899282455444\n",
            "Saving model, epoch: 14476, train_loss: 0.8904234766960144, val_loss: 0.8609870076179504\n",
            "Saving model, epoch: 14477, train_loss: 0.8904187083244324, val_loss: 0.8609850406646729\n",
            "Saving model, epoch: 14478, train_loss: 0.8904138207435608, val_loss: 0.8609828352928162\n",
            "Saving model, epoch: 14479, train_loss: 0.890408992767334, val_loss: 0.8609796762466431\n",
            "Saving model, epoch: 14480, train_loss: 0.8904040455818176, val_loss: 0.8609775900840759\n",
            "Saving model, epoch: 14481, train_loss: 0.890399158000946, val_loss: 0.8609755635261536\n",
            "Saving model, epoch: 14482, train_loss: 0.8903944492340088, val_loss: 0.8609724044799805\n",
            "Saving model, epoch: 14483, train_loss: 0.8903895020484924, val_loss: 0.8609706163406372\n",
            "Saving model, epoch: 14484, train_loss: 0.8903846740722656, val_loss: 0.8609676361083984\n",
            "Saving model, epoch: 14485, train_loss: 0.8903797268867493, val_loss: 0.8609654307365417\n",
            "Saving model, epoch: 14486, train_loss: 0.8903749585151672, val_loss: 0.8609630465507507\n",
            "Saving model, epoch: 14487, train_loss: 0.8903700113296509, val_loss: 0.8609601855278015\n",
            "Saving model, epoch: 14488, train_loss: 0.8903651833534241, val_loss: 0.8609575629234314\n",
            "Saving model, epoch: 14489, train_loss: 0.8903603553771973, val_loss: 0.8609554171562195\n",
            "Saving model, epoch: 14490, train_loss: 0.8903554677963257, val_loss: 0.8609527349472046\n",
            "Saving model, epoch: 14491, train_loss: 0.8903506994247437, val_loss: 0.8609500527381897\n",
            "Saving model, epoch: 14492, train_loss: 0.8903456926345825, val_loss: 0.8609483242034912\n",
            "Saving model, epoch: 14493, train_loss: 0.8903408646583557, val_loss: 0.8609452843666077\n",
            "Saving model, epoch: 14494, train_loss: 0.8903360366821289, val_loss: 0.8609430193901062\n",
            "Saving model, epoch: 14495, train_loss: 0.8903311491012573, val_loss: 0.860940158367157\n",
            "Saving model, epoch: 14496, train_loss: 0.8903263211250305, val_loss: 0.8609380722045898\n",
            "Saving model, epoch: 14497, train_loss: 0.8903213739395142, val_loss: 0.8609358072280884\n",
            "Saving model, epoch: 14498, train_loss: 0.8903166651725769, val_loss: 0.8609327077865601\n",
            "Saving model, epoch: 14499, train_loss: 0.8903117179870605, val_loss: 0.8609305620193481\n",
            "Saving model, epoch: 14500, train_loss: 0.8903068900108337, val_loss: 0.8609278798103333\n",
            "epoch: 14501, train_loss: 0.8903020620346069, val_loss: 0.8609253764152527\n",
            "Saving model, epoch: 14501, train_loss: 0.8903020620346069, val_loss: 0.8609253764152527\n",
            "Saving model, epoch: 14502, train_loss: 0.8902970552444458, val_loss: 0.8609231114387512\n",
            "Saving model, epoch: 14503, train_loss: 0.890292227268219, val_loss: 0.860920786857605\n",
            "Saving model, epoch: 14504, train_loss: 0.8902872204780579, val_loss: 0.8609179258346558\n",
            "Saving model, epoch: 14505, train_loss: 0.8902825117111206, val_loss: 0.86091548204422\n",
            "Saving model, epoch: 14506, train_loss: 0.8902775645256042, val_loss: 0.8609128594398499\n",
            "Saving model, epoch: 14507, train_loss: 0.8902729749679565, val_loss: 0.8609107136726379\n",
            "Saving model, epoch: 14508, train_loss: 0.8902678489685059, val_loss: 0.8609083294868469\n",
            "Saving model, epoch: 14509, train_loss: 0.8902629017829895, val_loss: 0.8609057664871216\n",
            "Saving model, epoch: 14510, train_loss: 0.8902580738067627, val_loss: 0.8609033226966858\n",
            "Saving model, epoch: 14511, train_loss: 0.8902530670166016, val_loss: 0.8609005212783813\n",
            "Saving model, epoch: 14512, train_loss: 0.8902483582496643, val_loss: 0.8608978986740112\n",
            "Saving model, epoch: 14513, train_loss: 0.890243411064148, val_loss: 0.8608961701393127\n",
            "Saving model, epoch: 14514, train_loss: 0.8902385830879211, val_loss: 0.8608929514884949\n",
            "Saving model, epoch: 14515, train_loss: 0.8902337551116943, val_loss: 0.8608908653259277\n",
            "Saving model, epoch: 14516, train_loss: 0.8902287483215332, val_loss: 0.8608881831169128\n",
            "Saving model, epoch: 14517, train_loss: 0.8902238011360168, val_loss: 0.860885739326477\n",
            "Saving model, epoch: 14518, train_loss: 0.89021897315979, val_loss: 0.860883355140686\n",
            "Saving model, epoch: 14519, train_loss: 0.8902140855789185, val_loss: 0.860880970954895\n",
            "Saving model, epoch: 14520, train_loss: 0.8902091383934021, val_loss: 0.8608784675598145\n",
            "Saving model, epoch: 14521, train_loss: 0.8902042508125305, val_loss: 0.8608759641647339\n",
            "Saving model, epoch: 14522, train_loss: 0.8901994824409485, val_loss: 0.860873281955719\n",
            "Saving model, epoch: 14523, train_loss: 0.8901944756507874, val_loss: 0.8608710169792175\n",
            "Saving model, epoch: 14524, train_loss: 0.8901897668838501, val_loss: 0.8608683943748474\n",
            "Saving model, epoch: 14525, train_loss: 0.8901848196983337, val_loss: 0.8608660697937012\n",
            "Saving model, epoch: 14526, train_loss: 0.8901799321174622, val_loss: 0.8608635663986206\n",
            "Saving model, epoch: 14527, train_loss: 0.8901749849319458, val_loss: 0.8608610033988953\n",
            "Saving model, epoch: 14528, train_loss: 0.8901700973510742, val_loss: 0.8608585596084595\n",
            "Saving model, epoch: 14529, train_loss: 0.8901652693748474, val_loss: 0.860855758190155\n",
            "Saving model, epoch: 14530, train_loss: 0.8901604413986206, val_loss: 0.8608537316322327\n",
            "Saving model, epoch: 14531, train_loss: 0.8901556134223938, val_loss: 0.860851526260376\n",
            "Saving model, epoch: 14532, train_loss: 0.8901503086090088, val_loss: 0.8608483076095581\n",
            "Saving model, epoch: 14533, train_loss: 0.8901455402374268, val_loss: 0.8608461022377014\n",
            "Saving model, epoch: 14534, train_loss: 0.8901407122612, val_loss: 0.8608434200286865\n",
            "Saving model, epoch: 14535, train_loss: 0.8901357054710388, val_loss: 0.8608407378196716\n",
            "Saving model, epoch: 14536, train_loss: 0.890130877494812, val_loss: 0.8608387112617493\n",
            "Saving model, epoch: 14537, train_loss: 0.8901260495185852, val_loss: 0.8608359694480896\n",
            "Saving model, epoch: 14538, train_loss: 0.8901208639144897, val_loss: 0.8608335256576538\n",
            "Saving model, epoch: 14539, train_loss: 0.8901162147521973, val_loss: 0.8608316779136658\n",
            "Saving model, epoch: 14540, train_loss: 0.8901112079620361, val_loss: 0.8608284592628479\n",
            "Saving model, epoch: 14541, train_loss: 0.8901063799858093, val_loss: 0.8608258366584778\n",
            "Saving model, epoch: 14542, train_loss: 0.8901015520095825, val_loss: 0.8608240485191345\n",
            "Saving model, epoch: 14543, train_loss: 0.8900966048240662, val_loss: 0.8608208894729614\n",
            "Saving model, epoch: 14544, train_loss: 0.8900914788246155, val_loss: 0.8608185648918152\n",
            "Saving model, epoch: 14545, train_loss: 0.8900867104530334, val_loss: 0.8608160018920898\n",
            "Saving model, epoch: 14546, train_loss: 0.8900816440582275, val_loss: 0.860813319683075\n",
            "Saving model, epoch: 14547, train_loss: 0.8900769352912903, val_loss: 0.8608112335205078\n",
            "Saving model, epoch: 14548, train_loss: 0.8900719285011292, val_loss: 0.8608088493347168\n",
            "Saving model, epoch: 14549, train_loss: 0.8900671005249023, val_loss: 0.8608060479164124\n",
            "Saving model, epoch: 14550, train_loss: 0.8900619745254517, val_loss: 0.8608039021492004\n",
            "Saving model, epoch: 14551, train_loss: 0.8900572657585144, val_loss: 0.8608009219169617\n",
            "Saving model, epoch: 14552, train_loss: 0.890052318572998, val_loss: 0.8607983589172363\n",
            "Saving model, epoch: 14553, train_loss: 0.8900474905967712, val_loss: 0.8607962131500244\n",
            "Saving model, epoch: 14554, train_loss: 0.8900423645973206, val_loss: 0.8607932925224304\n",
            "Saving model, epoch: 14555, train_loss: 0.8900375366210938, val_loss: 0.8607916235923767\n",
            "Saving model, epoch: 14556, train_loss: 0.8900326490402222, val_loss: 0.8607885837554932\n",
            "Saving model, epoch: 14557, train_loss: 0.8900275826454163, val_loss: 0.8607860207557678\n",
            "Saving model, epoch: 14558, train_loss: 0.8900225758552551, val_loss: 0.8607836365699768\n",
            "Saving model, epoch: 14559, train_loss: 0.8900177478790283, val_loss: 0.8607809543609619\n",
            "Saving model, epoch: 14560, train_loss: 0.8900128602981567, val_loss: 0.86077880859375\n",
            "Saving model, epoch: 14561, train_loss: 0.8900079131126404, val_loss: 0.8607759475708008\n",
            "Saving model, epoch: 14562, train_loss: 0.890002965927124, val_loss: 0.8607736825942993\n",
            "Saving model, epoch: 14563, train_loss: 0.8899980783462524, val_loss: 0.8607710003852844\n",
            "Saving model, epoch: 14564, train_loss: 0.8899931311607361, val_loss: 0.86076819896698\n",
            "Saving model, epoch: 14565, train_loss: 0.8899882435798645, val_loss: 0.8607660531997681\n",
            "Saving model, epoch: 14566, train_loss: 0.8899832963943481, val_loss: 0.8607631325721741\n",
            "Saving model, epoch: 14567, train_loss: 0.889978289604187, val_loss: 0.8607608079910278\n",
            "Saving model, epoch: 14568, train_loss: 0.8899734616279602, val_loss: 0.8607588410377502\n",
            "Saving model, epoch: 14569, train_loss: 0.8899684548377991, val_loss: 0.8607558608055115\n",
            "Saving model, epoch: 14570, train_loss: 0.8899633884429932, val_loss: 0.8607536554336548\n",
            "Saving model, epoch: 14571, train_loss: 0.8899586796760559, val_loss: 0.8607514500617981\n",
            "Saving model, epoch: 14572, train_loss: 0.8899536728858948, val_loss: 0.8607481718063354\n",
            "Saving model, epoch: 14573, train_loss: 0.8899486660957336, val_loss: 0.860745906829834\n",
            "Saving model, epoch: 14574, train_loss: 0.8899438381195068, val_loss: 0.860743522644043\n",
            "Saving model, epoch: 14575, train_loss: 0.8899387121200562, val_loss: 0.860740602016449\n",
            "Saving model, epoch: 14576, train_loss: 0.8899337649345398, val_loss: 0.86073899269104\n",
            "Saving model, epoch: 14577, train_loss: 0.889928936958313, val_loss: 0.860736072063446\n",
            "Saving model, epoch: 14578, train_loss: 0.8899239301681519, val_loss: 0.8607335090637207\n",
            "Saving model, epoch: 14579, train_loss: 0.8899189829826355, val_loss: 0.8607311248779297\n",
            "Saving model, epoch: 14580, train_loss: 0.8899140954017639, val_loss: 0.8607286214828491\n",
            "Saving model, epoch: 14581, train_loss: 0.889909029006958, val_loss: 0.860725998878479\n",
            "Saving model, epoch: 14582, train_loss: 0.8899042010307312, val_loss: 0.8607237935066223\n",
            "Saving model, epoch: 14583, train_loss: 0.8898991942405701, val_loss: 0.8607205748558044\n",
            "Saving model, epoch: 14584, train_loss: 0.8898941874504089, val_loss: 0.8607186675071716\n",
            "Saving model, epoch: 14585, train_loss: 0.8898892402648926, val_loss: 0.8607156276702881\n",
            "Saving model, epoch: 14586, train_loss: 0.889884352684021, val_loss: 0.8607133626937866\n",
            "Saving model, epoch: 14587, train_loss: 0.8898792266845703, val_loss: 0.8607112169265747\n",
            "Saving model, epoch: 14588, train_loss: 0.889874279499054, val_loss: 0.8607083559036255\n",
            "Saving model, epoch: 14589, train_loss: 0.8898694515228271, val_loss: 0.8607054352760315\n",
            "Saving model, epoch: 14590, train_loss: 0.889864444732666, val_loss: 0.8607035279273987\n",
            "Saving model, epoch: 14591, train_loss: 0.8898594975471497, val_loss: 0.8607005476951599\n",
            "Saving model, epoch: 14592, train_loss: 0.8898547291755676, val_loss: 0.8606983423233032\n",
            "Saving model, epoch: 14593, train_loss: 0.8898496031761169, val_loss: 0.8606956005096436\n",
            "Saving model, epoch: 14594, train_loss: 0.8898446559906006, val_loss: 0.8606929183006287\n",
            "Saving model, epoch: 14595, train_loss: 0.8898397088050842, val_loss: 0.8606906533241272\n",
            "Saving model, epoch: 14596, train_loss: 0.8898347020149231, val_loss: 0.8606883883476257\n",
            "Saving model, epoch: 14597, train_loss: 0.889829695224762, val_loss: 0.8606858253479004\n",
            "Saving model, epoch: 14598, train_loss: 0.8898247480392456, val_loss: 0.860683262348175\n",
            "Saving model, epoch: 14599, train_loss: 0.8898199200630188, val_loss: 0.8606805801391602\n",
            "Saving model, epoch: 14600, train_loss: 0.8898147940635681, val_loss: 0.8606783747673035\n",
            "epoch: 14601, train_loss: 0.889809787273407, val_loss: 0.860676109790802\n",
            "Saving model, epoch: 14601, train_loss: 0.889809787273407, val_loss: 0.860676109790802\n",
            "Saving model, epoch: 14602, train_loss: 0.8898049592971802, val_loss: 0.8606735467910767\n",
            "Saving model, epoch: 14603, train_loss: 0.8898000717163086, val_loss: 0.8606703281402588\n",
            "Saving model, epoch: 14604, train_loss: 0.8897950053215027, val_loss: 0.8606683611869812\n",
            "Saving model, epoch: 14605, train_loss: 0.8897899985313416, val_loss: 0.8606653213500977\n",
            "Saving model, epoch: 14606, train_loss: 0.8897850513458252, val_loss: 0.8606626987457275\n",
            "Saving model, epoch: 14607, train_loss: 0.8897800445556641, val_loss: 0.8606606721878052\n",
            "Saving model, epoch: 14608, train_loss: 0.8897751569747925, val_loss: 0.8606582880020142\n",
            "Saving model, epoch: 14609, train_loss: 0.8897700905799866, val_loss: 0.8606556057929993\n",
            "Saving model, epoch: 14610, train_loss: 0.889765202999115, val_loss: 0.8606529831886292\n",
            "Saving model, epoch: 14611, train_loss: 0.8897601366043091, val_loss: 0.8606506586074829\n",
            "Saving model, epoch: 14612, train_loss: 0.889755129814148, val_loss: 0.860647976398468\n",
            "Saving model, epoch: 14613, train_loss: 0.8897503018379211, val_loss: 0.8606457710266113\n",
            "Saving model, epoch: 14614, train_loss: 0.88974529504776, val_loss: 0.8606426119804382\n",
            "Saving model, epoch: 14615, train_loss: 0.8897402882575989, val_loss: 0.8606401085853577\n",
            "Saving model, epoch: 14616, train_loss: 0.8897353410720825, val_loss: 0.8606382012367249\n",
            "Saving model, epoch: 14617, train_loss: 0.8897302150726318, val_loss: 0.8606352210044861\n",
            "Saving model, epoch: 14618, train_loss: 0.8897252082824707, val_loss: 0.8606331944465637\n",
            "Saving model, epoch: 14619, train_loss: 0.8897203803062439, val_loss: 0.8606303334236145\n",
            "Saving model, epoch: 14620, train_loss: 0.8897152543067932, val_loss: 0.8606277108192444\n",
            "Saving model, epoch: 14621, train_loss: 0.8897102475166321, val_loss: 0.8606256246566772\n",
            "Saving model, epoch: 14622, train_loss: 0.8897053003311157, val_loss: 0.8606227040290833\n",
            "Saving model, epoch: 14623, train_loss: 0.8897003531455994, val_loss: 0.860620379447937\n",
            "Saving model, epoch: 14624, train_loss: 0.8896952271461487, val_loss: 0.8606176972389221\n",
            "Saving model, epoch: 14625, train_loss: 0.8896902203559875, val_loss: 0.8606151342391968\n",
            "Saving model, epoch: 14626, train_loss: 0.889685332775116, val_loss: 0.8606129884719849\n",
            "Saving model, epoch: 14627, train_loss: 0.8896805047988892, val_loss: 0.8606101274490356\n",
            "Saving model, epoch: 14628, train_loss: 0.8896753787994385, val_loss: 0.8606080412864685\n",
            "Saving model, epoch: 14629, train_loss: 0.8896703124046326, val_loss: 0.8606049418449402\n",
            "Saving model, epoch: 14630, train_loss: 0.8896653056144714, val_loss: 0.8606023788452148\n",
            "Saving model, epoch: 14631, train_loss: 0.8896602988243103, val_loss: 0.8605998158454895\n",
            "Saving model, epoch: 14632, train_loss: 0.8896552920341492, val_loss: 0.8605973720550537\n",
            "Saving model, epoch: 14633, train_loss: 0.8896504640579224, val_loss: 0.8605950474739075\n",
            "Saving model, epoch: 14634, train_loss: 0.8896453380584717, val_loss: 0.8605921268463135\n",
            "Saving model, epoch: 14635, train_loss: 0.8896403908729553, val_loss: 0.8605899214744568\n",
            "Saving model, epoch: 14636, train_loss: 0.8896353840827942, val_loss: 0.8605872988700867\n",
            "Saving model, epoch: 14637, train_loss: 0.8896305561065674, val_loss: 0.8605847954750061\n",
            "Saving model, epoch: 14638, train_loss: 0.8896253108978271, val_loss: 0.8605822324752808\n",
            "Saving model, epoch: 14639, train_loss: 0.8896204233169556, val_loss: 0.8605797290802002\n",
            "Saving model, epoch: 14640, train_loss: 0.8896153569221497, val_loss: 0.8605774641036987\n",
            "Saving model, epoch: 14641, train_loss: 0.8896104693412781, val_loss: 0.8605744242668152\n",
            "Saving model, epoch: 14642, train_loss: 0.8896052241325378, val_loss: 0.8605719804763794\n",
            "Saving model, epoch: 14643, train_loss: 0.8896003365516663, val_loss: 0.8605693578720093\n",
            "Saving model, epoch: 14644, train_loss: 0.8895952701568604, val_loss: 0.8605672717094421\n",
            "Saving model, epoch: 14645, train_loss: 0.8895902633666992, val_loss: 0.8605648279190063\n",
            "Saving model, epoch: 14646, train_loss: 0.8895853161811829, val_loss: 0.8605617880821228\n",
            "Saving model, epoch: 14647, train_loss: 0.8895801305770874, val_loss: 0.8605592846870422\n",
            "Saving model, epoch: 14648, train_loss: 0.8895753026008606, val_loss: 0.860556960105896\n",
            "Saving model, epoch: 14649, train_loss: 0.8895702958106995, val_loss: 0.8605543375015259\n",
            "Saving model, epoch: 14650, train_loss: 0.8895652294158936, val_loss: 0.8605523109436035\n",
            "Saving model, epoch: 14651, train_loss: 0.8895602226257324, val_loss: 0.8605495691299438\n",
            "Saving model, epoch: 14652, train_loss: 0.8895552158355713, val_loss: 0.8605475425720215\n",
            "Saving model, epoch: 14653, train_loss: 0.8895502686500549, val_loss: 0.8605443239212036\n",
            "Saving model, epoch: 14654, train_loss: 0.8895451426506042, val_loss: 0.860541582107544\n",
            "Saving model, epoch: 14655, train_loss: 0.8895401358604431, val_loss: 0.8605396151542664\n",
            "Saving model, epoch: 14656, train_loss: 0.8895353078842163, val_loss: 0.8605368137359619\n",
            "Saving model, epoch: 14657, train_loss: 0.8895300626754761, val_loss: 0.8605340719223022\n",
            "Saving model, epoch: 14658, train_loss: 0.8895250558853149, val_loss: 0.8605316877365112\n",
            "Saving model, epoch: 14659, train_loss: 0.8895200490951538, val_loss: 0.8605289459228516\n",
            "Saving model, epoch: 14660, train_loss: 0.8895149827003479, val_loss: 0.8605261445045471\n",
            "Saving model, epoch: 14661, train_loss: 0.8895099759101868, val_loss: 0.8605242967605591\n",
            "Saving model, epoch: 14662, train_loss: 0.88950514793396, val_loss: 0.8605213165283203\n",
            "Saving model, epoch: 14663, train_loss: 0.8894999623298645, val_loss: 0.8605191707611084\n",
            "Saving model, epoch: 14664, train_loss: 0.8894948363304138, val_loss: 0.8605165481567383\n",
            "Saving model, epoch: 14665, train_loss: 0.8894898891448975, val_loss: 0.8605141043663025\n",
            "Saving model, epoch: 14666, train_loss: 0.8894846439361572, val_loss: 0.8605111241340637\n",
            "Saving model, epoch: 14667, train_loss: 0.8894797563552856, val_loss: 0.8605087399482727\n",
            "Saving model, epoch: 14668, train_loss: 0.8894748091697693, val_loss: 0.8605061769485474\n",
            "Saving model, epoch: 14669, train_loss: 0.8894696831703186, val_loss: 0.8605039119720459\n",
            "Saving model, epoch: 14670, train_loss: 0.8894646763801575, val_loss: 0.8605011105537415\n",
            "Saving model, epoch: 14671, train_loss: 0.8894597291946411, val_loss: 0.8604987263679504\n",
            "Saving model, epoch: 14672, train_loss: 0.88945472240448, val_loss: 0.860496461391449\n",
            "Saving model, epoch: 14673, train_loss: 0.8894495964050293, val_loss: 0.8604932427406311\n",
            "Saving model, epoch: 14674, train_loss: 0.8894445896148682, val_loss: 0.8604913949966431\n",
            "Saving model, epoch: 14675, train_loss: 0.889439582824707, val_loss: 0.86048823595047\n",
            "Saving model, epoch: 14676, train_loss: 0.8894344568252563, val_loss: 0.8604856133460999\n",
            "Saving model, epoch: 14677, train_loss: 0.8894293904304504, val_loss: 0.8604835867881775\n",
            "Saving model, epoch: 14678, train_loss: 0.8894245028495789, val_loss: 0.8604807257652283\n",
            "Saving model, epoch: 14679, train_loss: 0.8894193768501282, val_loss: 0.8604784607887268\n",
            "Saving model, epoch: 14680, train_loss: 0.8894141316413879, val_loss: 0.8604755997657776\n",
            "Saving model, epoch: 14681, train_loss: 0.8894091248512268, val_loss: 0.8604733347892761\n",
            "Saving model, epoch: 14682, train_loss: 0.8894041180610657, val_loss: 0.8604702949523926\n",
            "Saving model, epoch: 14683, train_loss: 0.8893990516662598, val_loss: 0.860468327999115\n",
            "Saving model, epoch: 14684, train_loss: 0.8893940448760986, val_loss: 0.8604653477668762\n",
            "Saving model, epoch: 14685, train_loss: 0.8893890380859375, val_loss: 0.86046302318573\n",
            "Saving model, epoch: 14686, train_loss: 0.8893839716911316, val_loss: 0.8604605197906494\n",
            "Saving model, epoch: 14687, train_loss: 0.8893789052963257, val_loss: 0.8604576587677002\n",
            "Saving model, epoch: 14688, train_loss: 0.889373779296875, val_loss: 0.8604550957679749\n",
            "Saving model, epoch: 14689, train_loss: 0.8893689513206482, val_loss: 0.8604525327682495\n",
            "Saving model, epoch: 14690, train_loss: 0.8893638253211975, val_loss: 0.8604507446289062\n",
            "Saving model, epoch: 14691, train_loss: 0.8893586993217468, val_loss: 0.8604481816291809\n",
            "Saving model, epoch: 14692, train_loss: 0.8893536329269409, val_loss: 0.8604449033737183\n",
            "Saving model, epoch: 14693, train_loss: 0.8893486261367798, val_loss: 0.8604425191879272\n",
            "Saving model, epoch: 14694, train_loss: 0.8893435001373291, val_loss: 0.8604402542114258\n",
            "Saving model, epoch: 14695, train_loss: 0.889338493347168, val_loss: 0.8604379892349243\n",
            "Saving model, epoch: 14696, train_loss: 0.8893335461616516, val_loss: 0.8604350090026855\n",
            "Saving model, epoch: 14697, train_loss: 0.8893283605575562, val_loss: 0.8604324460029602\n",
            "Saving model, epoch: 14698, train_loss: 0.8893232941627502, val_loss: 0.8604300618171692\n",
            "Saving model, epoch: 14699, train_loss: 0.8893182873725891, val_loss: 0.8604271411895752\n",
            "Saving model, epoch: 14700, train_loss: 0.8893130421638489, val_loss: 0.8604251146316528\n",
            "epoch: 14701, train_loss: 0.8893082737922668, val_loss: 0.8604222536087036\n",
            "Saving model, epoch: 14701, train_loss: 0.8893082737922668, val_loss: 0.8604222536087036\n",
            "Saving model, epoch: 14702, train_loss: 0.8893032073974609, val_loss: 0.8604199290275574\n",
            "Saving model, epoch: 14703, train_loss: 0.8892980217933655, val_loss: 0.8604174852371216\n",
            "Saving model, epoch: 14704, train_loss: 0.8892929553985596, val_loss: 0.8604140281677246\n",
            "Saving model, epoch: 14705, train_loss: 0.8892878293991089, val_loss: 0.8604123592376709\n",
            "Saving model, epoch: 14706, train_loss: 0.8892827033996582, val_loss: 0.8604095578193665\n",
            "Saving model, epoch: 14707, train_loss: 0.8892776966094971, val_loss: 0.8604069352149963\n",
            "Saving model, epoch: 14708, train_loss: 0.8892724514007568, val_loss: 0.8604047894477844\n",
            "Saving model, epoch: 14709, train_loss: 0.8892675638198853, val_loss: 0.8604015111923218\n",
            "Saving model, epoch: 14710, train_loss: 0.8892626166343689, val_loss: 0.8603996634483337\n",
            "Saving model, epoch: 14711, train_loss: 0.8892574906349182, val_loss: 0.8603970408439636\n",
            "Saving model, epoch: 14712, train_loss: 0.8892523646354675, val_loss: 0.8603944778442383\n",
            "Saving model, epoch: 14713, train_loss: 0.8892471790313721, val_loss: 0.8603917956352234\n",
            "Saving model, epoch: 14714, train_loss: 0.8892421126365662, val_loss: 0.8603888154029846\n",
            "Saving model, epoch: 14715, train_loss: 0.889237105846405, val_loss: 0.8603867888450623\n",
            "Saving model, epoch: 14716, train_loss: 0.8892320990562439, val_loss: 0.8603838682174683\n",
            "Saving model, epoch: 14717, train_loss: 0.8892269730567932, val_loss: 0.8603818416595459\n",
            "Saving model, epoch: 14718, train_loss: 0.8892218470573425, val_loss: 0.8603790402412415\n",
            "Saving model, epoch: 14719, train_loss: 0.8892168998718262, val_loss: 0.8603761792182922\n",
            "Saving model, epoch: 14720, train_loss: 0.8892117738723755, val_loss: 0.8603742122650146\n",
            "Saving model, epoch: 14721, train_loss: 0.88920658826828, val_loss: 0.8603709936141968\n",
            "Saving model, epoch: 14722, train_loss: 0.8892016410827637, val_loss: 0.8603687882423401\n",
            "Saving model, epoch: 14723, train_loss: 0.889196515083313, val_loss: 0.8603656888008118\n",
            "Saving model, epoch: 14724, train_loss: 0.8891913890838623, val_loss: 0.8603634238243103\n",
            "Saving model, epoch: 14725, train_loss: 0.8891862630844116, val_loss: 0.8603608012199402\n",
            "Saving model, epoch: 14726, train_loss: 0.8891810774803162, val_loss: 0.8603576421737671\n",
            "Saving model, epoch: 14727, train_loss: 0.8891760110855103, val_loss: 0.8603560924530029\n",
            "Saving model, epoch: 14728, train_loss: 0.8891710042953491, val_loss: 0.8603532910346985\n",
            "Saving model, epoch: 14729, train_loss: 0.889165997505188, val_loss: 0.8603509068489075\n",
            "Saving model, epoch: 14730, train_loss: 0.8891607522964478, val_loss: 0.8603483438491821\n",
            "Saving model, epoch: 14731, train_loss: 0.8891557455062866, val_loss: 0.860345721244812\n",
            "Saving model, epoch: 14732, train_loss: 0.8891507983207703, val_loss: 0.8603429794311523\n",
            "Saving model, epoch: 14733, train_loss: 0.8891456127166748, val_loss: 0.8603407144546509\n",
            "Saving model, epoch: 14734, train_loss: 0.8891404867172241, val_loss: 0.8603384494781494\n",
            "Saving model, epoch: 14735, train_loss: 0.8891354203224182, val_loss: 0.8603349328041077\n",
            "Saving model, epoch: 14736, train_loss: 0.8891302347183228, val_loss: 0.8603327870368958\n",
            "Saving model, epoch: 14737, train_loss: 0.8891251683235168, val_loss: 0.8603300452232361\n",
            "Saving model, epoch: 14738, train_loss: 0.8891199827194214, val_loss: 0.8603284358978271\n",
            "Saving model, epoch: 14739, train_loss: 0.8891149163246155, val_loss: 0.8603247404098511\n",
            "Saving model, epoch: 14740, train_loss: 0.88910973072052, val_loss: 0.8603229522705078\n",
            "Saving model, epoch: 14741, train_loss: 0.8891049027442932, val_loss: 0.8603198528289795\n",
            "Saving model, epoch: 14742, train_loss: 0.889099657535553, val_loss: 0.8603180050849915\n",
            "Saving model, epoch: 14743, train_loss: 0.8890946507453918, val_loss: 0.8603149056434631\n",
            "Saving model, epoch: 14744, train_loss: 0.8890895247459412, val_loss: 0.8603127002716064\n",
            "Saving model, epoch: 14745, train_loss: 0.88908451795578, val_loss: 0.8603101372718811\n",
            "Saving model, epoch: 14746, train_loss: 0.8890791535377502, val_loss: 0.8603067994117737\n",
            "Saving model, epoch: 14747, train_loss: 0.8890741467475891, val_loss: 0.8603051900863647\n",
            "Saving model, epoch: 14748, train_loss: 0.889069139957428, val_loss: 0.8603020310401917\n",
            "Saving model, epoch: 14749, train_loss: 0.8890638947486877, val_loss: 0.8602996468544006\n",
            "Saving model, epoch: 14750, train_loss: 0.8890588879585266, val_loss: 0.8602969646453857\n",
            "Saving model, epoch: 14751, train_loss: 0.8890537619590759, val_loss: 0.8602946400642395\n",
            "Saving model, epoch: 14752, train_loss: 0.8890486359596252, val_loss: 0.8602917790412903\n",
            "Saving model, epoch: 14753, train_loss: 0.8890436291694641, val_loss: 0.860289454460144\n",
            "Saving model, epoch: 14754, train_loss: 0.8890383839607239, val_loss: 0.8602865934371948\n",
            "Saving model, epoch: 14755, train_loss: 0.8890333771705627, val_loss: 0.8602846264839172\n",
            "Saving model, epoch: 14756, train_loss: 0.8890281319618225, val_loss: 0.8602811694145203\n",
            "Saving model, epoch: 14757, train_loss: 0.8890228867530823, val_loss: 0.8602792024612427\n",
            "Saving model, epoch: 14758, train_loss: 0.8890178799629211, val_loss: 0.8602758049964905\n",
            "Saving model, epoch: 14759, train_loss: 0.8890126943588257, val_loss: 0.8602740168571472\n",
            "Saving model, epoch: 14760, train_loss: 0.8890076279640198, val_loss: 0.8602715730667114\n",
            "Saving model, epoch: 14761, train_loss: 0.8890027403831482, val_loss: 0.860268235206604\n",
            "Saving model, epoch: 14762, train_loss: 0.8889973759651184, val_loss: 0.8602663278579712\n",
            "Saving model, epoch: 14763, train_loss: 0.8889923691749573, val_loss: 0.8602632880210876\n",
            "Saving model, epoch: 14764, train_loss: 0.8889872431755066, val_loss: 0.8602613806724548\n",
            "Saving model, epoch: 14765, train_loss: 0.8889819979667664, val_loss: 0.8602581024169922\n",
            "Saving model, epoch: 14766, train_loss: 0.8889771103858948, val_loss: 0.8602562546730042\n",
            "Saving model, epoch: 14767, train_loss: 0.8889718651771545, val_loss: 0.8602533340454102\n",
            "Saving model, epoch: 14768, train_loss: 0.8889666795730591, val_loss: 0.86025071144104\n",
            "Saving model, epoch: 14769, train_loss: 0.8889615535736084, val_loss: 0.8602482676506042\n",
            "Saving model, epoch: 14770, train_loss: 0.8889564871788025, val_loss: 0.8602452874183655\n",
            "Saving model, epoch: 14771, train_loss: 0.888951301574707, val_loss: 0.8602431416511536\n",
            "Saving model, epoch: 14772, train_loss: 0.8889461755752563, val_loss: 0.8602401614189148\n",
            "Saving model, epoch: 14773, train_loss: 0.8889409303665161, val_loss: 0.8602381944656372\n",
            "Saving model, epoch: 14774, train_loss: 0.8889359831809998, val_loss: 0.8602352142333984\n",
            "Saving model, epoch: 14775, train_loss: 0.8889307975769043, val_loss: 0.8602330088615417\n",
            "Saving model, epoch: 14776, train_loss: 0.8889256715774536, val_loss: 0.8602303862571716\n",
            "Saving model, epoch: 14777, train_loss: 0.8889205455780029, val_loss: 0.8602273464202881\n",
            "Saving model, epoch: 14778, train_loss: 0.8889153003692627, val_loss: 0.8602254986763\n",
            "Saving model, epoch: 14779, train_loss: 0.888910174369812, val_loss: 0.8602219223976135\n",
            "Saving model, epoch: 14780, train_loss: 0.8889051675796509, val_loss: 0.8602198958396912\n",
            "Saving model, epoch: 14781, train_loss: 0.8889001607894897, val_loss: 0.8602169156074524\n",
            "Saving model, epoch: 14782, train_loss: 0.88889479637146, val_loss: 0.8602146506309509\n",
            "Saving model, epoch: 14783, train_loss: 0.8888897895812988, val_loss: 0.8602123260498047\n",
            "Saving model, epoch: 14784, train_loss: 0.8888846635818481, val_loss: 0.8602093458175659\n",
            "Saving model, epoch: 14785, train_loss: 0.8888795375823975, val_loss: 0.860207200050354\n",
            "Saving model, epoch: 14786, train_loss: 0.8888744115829468, val_loss: 0.86020427942276\n",
            "Saving model, epoch: 14787, train_loss: 0.8888689875602722, val_loss: 0.8602025508880615\n",
            "Saving model, epoch: 14788, train_loss: 0.8888638615608215, val_loss: 0.8601991534233093\n",
            "Saving model, epoch: 14789, train_loss: 0.8888588547706604, val_loss: 0.8601969480514526\n",
            "Saving model, epoch: 14790, train_loss: 0.8888537287712097, val_loss: 0.8601939082145691\n",
            "Saving model, epoch: 14791, train_loss: 0.888848602771759, val_loss: 0.8601913452148438\n",
            "Saving model, epoch: 14792, train_loss: 0.8888434767723083, val_loss: 0.8601894974708557\n",
            "Saving model, epoch: 14793, train_loss: 0.8888382315635681, val_loss: 0.8601861000061035\n",
            "Saving model, epoch: 14794, train_loss: 0.8888331055641174, val_loss: 0.8601840734481812\n",
            "Saving model, epoch: 14795, train_loss: 0.8888279795646667, val_loss: 0.8601809740066528\n",
            "Saving model, epoch: 14796, train_loss: 0.8888228535652161, val_loss: 0.8601790070533752\n",
            "Saving model, epoch: 14797, train_loss: 0.8888177275657654, val_loss: 0.860176146030426\n",
            "Saving model, epoch: 14798, train_loss: 0.8888125419616699, val_loss: 0.8601736426353455\n",
            "Saving model, epoch: 14799, train_loss: 0.8888072967529297, val_loss: 0.8601709008216858\n",
            "Saving model, epoch: 14800, train_loss: 0.8888022899627686, val_loss: 0.8601679801940918\n",
            "epoch: 14801, train_loss: 0.8887970447540283, val_loss: 0.860166072845459\n",
            "Saving model, epoch: 14801, train_loss: 0.8887970447540283, val_loss: 0.860166072845459\n",
            "Saving model, epoch: 14802, train_loss: 0.8887918591499329, val_loss: 0.8601625561714172\n",
            "Saving model, epoch: 14803, train_loss: 0.8887867331504822, val_loss: 0.8601610064506531\n",
            "Saving model, epoch: 14804, train_loss: 0.8887816667556763, val_loss: 0.8601577281951904\n",
            "Saving model, epoch: 14805, train_loss: 0.8887764811515808, val_loss: 0.8601556420326233\n",
            "Saving model, epoch: 14806, train_loss: 0.8887713551521301, val_loss: 0.860153079032898\n",
            "Saving model, epoch: 14807, train_loss: 0.8887661099433899, val_loss: 0.8601506352424622\n",
            "Saving model, epoch: 14808, train_loss: 0.8887608647346497, val_loss: 0.8601477742195129\n",
            "Saving model, epoch: 14809, train_loss: 0.8887558579444885, val_loss: 0.860145628452301\n",
            "Saving model, epoch: 14810, train_loss: 0.8887506127357483, val_loss: 0.8601424694061279\n",
            "Saving model, epoch: 14811, train_loss: 0.8887455463409424, val_loss: 0.8601399064064026\n",
            "Saving model, epoch: 14812, train_loss: 0.8887403011322021, val_loss: 0.8601371049880981\n",
            "Saving model, epoch: 14813, train_loss: 0.8887349963188171, val_loss: 0.8601351976394653\n",
            "Saving model, epoch: 14814, train_loss: 0.8887299299240112, val_loss: 0.8601324558258057\n",
            "Saving model, epoch: 14815, train_loss: 0.8887247443199158, val_loss: 0.8601295351982117\n",
            "Saving model, epoch: 14816, train_loss: 0.8887196779251099, val_loss: 0.8601272702217102\n",
            "Saving model, epoch: 14817, train_loss: 0.8887143731117249, val_loss: 0.8601243495941162\n",
            "Saving model, epoch: 14818, train_loss: 0.8887092471122742, val_loss: 0.8601222038269043\n",
            "Saving model, epoch: 14819, train_loss: 0.8887041211128235, val_loss: 0.8601192831993103\n",
            "Saving model, epoch: 14820, train_loss: 0.8886989951133728, val_loss: 0.8601168990135193\n",
            "Saving model, epoch: 14821, train_loss: 0.8886937499046326, val_loss: 0.860114574432373\n",
            "Saving model, epoch: 14822, train_loss: 0.8886885643005371, val_loss: 0.8601111769676208\n",
            "Saving model, epoch: 14823, train_loss: 0.8886833190917969, val_loss: 0.8601095676422119\n",
            "Saving model, epoch: 14824, train_loss: 0.8886783123016357, val_loss: 0.8601062297821045\n",
            "Saving model, epoch: 14825, train_loss: 0.8886730670928955, val_loss: 0.8601040840148926\n",
            "Saving model, epoch: 14826, train_loss: 0.8886679410934448, val_loss: 0.8601011633872986\n",
            "Saving model, epoch: 14827, train_loss: 0.8886626362800598, val_loss: 0.8600988984107971\n",
            "Saving model, epoch: 14828, train_loss: 0.8886576294898987, val_loss: 0.8600962162017822\n",
            "Saving model, epoch: 14829, train_loss: 0.8886522650718689, val_loss: 0.8600934147834778\n",
            "Saving model, epoch: 14830, train_loss: 0.8886471390724182, val_loss: 0.8600913286209106\n",
            "Saving model, epoch: 14831, train_loss: 0.8886420130729675, val_loss: 0.8600878119468689\n",
            "Saving model, epoch: 14832, train_loss: 0.8886367082595825, val_loss: 0.8600857257843018\n",
            "Saving model, epoch: 14833, train_loss: 0.8886315822601318, val_loss: 0.860082745552063\n",
            "Saving model, epoch: 14834, train_loss: 0.8886264562606812, val_loss: 0.8600816130638123\n",
            "Saving model, epoch: 14835, train_loss: 0.8886212110519409, val_loss: 0.860077440738678\n",
            "Saving model, epoch: 14836, train_loss: 0.8886160254478455, val_loss: 0.8600755333900452\n",
            "Saving model, epoch: 14837, train_loss: 0.8886108994483948, val_loss: 0.8600727915763855\n",
            "Saving model, epoch: 14838, train_loss: 0.8886056542396545, val_loss: 0.8600698709487915\n",
            "Saving model, epoch: 14839, train_loss: 0.8886005282402039, val_loss: 0.8600675463676453\n",
            "Saving model, epoch: 14840, train_loss: 0.8885953426361084, val_loss: 0.8600647449493408\n",
            "Saving model, epoch: 14841, train_loss: 0.8885900974273682, val_loss: 0.8600629568099976\n",
            "Saving model, epoch: 14842, train_loss: 0.8885849714279175, val_loss: 0.8600600361824036\n",
            "Saving model, epoch: 14843, train_loss: 0.8885797262191772, val_loss: 0.8600568175315857\n",
            "Saving model, epoch: 14844, train_loss: 0.8885746002197266, val_loss: 0.8600552678108215\n",
            "Saving model, epoch: 14845, train_loss: 0.888569176197052, val_loss: 0.8600513339042664\n",
            "Saving model, epoch: 14846, train_loss: 0.8885641694068909, val_loss: 0.860049843788147\n",
            "Saving model, epoch: 14847, train_loss: 0.8885589241981506, val_loss: 0.860045850276947\n",
            "Saving model, epoch: 14848, train_loss: 0.8885537385940552, val_loss: 0.8600452542304993\n",
            "Saving model, epoch: 14849, train_loss: 0.8885483741760254, val_loss: 0.8600409626960754\n",
            "Saving model, epoch: 14850, train_loss: 0.8885433673858643, val_loss: 0.8600393533706665\n",
            "Saving model, epoch: 14851, train_loss: 0.8885380625724792, val_loss: 0.8600366711616516\n",
            "Saving model, epoch: 14852, train_loss: 0.8885329961776733, val_loss: 0.8600339889526367\n",
            "Saving model, epoch: 14853, train_loss: 0.8885278105735779, val_loss: 0.8600316047668457\n",
            "Saving model, epoch: 14854, train_loss: 0.8885225653648376, val_loss: 0.8600278496742249\n",
            "Saving model, epoch: 14855, train_loss: 0.8885175585746765, val_loss: 0.8600265979766846\n",
            "Saving model, epoch: 14856, train_loss: 0.8885120153427124, val_loss: 0.8600228428840637\n",
            "Saving model, epoch: 14857, train_loss: 0.8885068893432617, val_loss: 0.8600214123725891\n",
            "Saving model, epoch: 14858, train_loss: 0.8885017037391663, val_loss: 0.8600180745124817\n",
            "Saving model, epoch: 14859, train_loss: 0.888496458530426, val_loss: 0.860015869140625\n",
            "Saving model, epoch: 14860, train_loss: 0.8884912133216858, val_loss: 0.860013484954834\n",
            "Saving model, epoch: 14861, train_loss: 0.8884862065315247, val_loss: 0.8600099682807922\n",
            "Saving model, epoch: 14862, train_loss: 0.8884807825088501, val_loss: 0.8600085973739624\n",
            "Saving model, epoch: 14863, train_loss: 0.8884756565093994, val_loss: 0.860004723072052\n",
            "Saving model, epoch: 14864, train_loss: 0.8884705305099487, val_loss: 0.8600031137466431\n",
            "Saving model, epoch: 14865, train_loss: 0.8884652256965637, val_loss: 0.8599994778633118\n",
            "Saving model, epoch: 14866, train_loss: 0.888460099697113, val_loss: 0.8599979281425476\n",
            "Saving model, epoch: 14867, train_loss: 0.8884548544883728, val_loss: 0.8599944710731506\n",
            "Saving model, epoch: 14868, train_loss: 0.8884496092796326, val_loss: 0.8599928021430969\n",
            "Saving model, epoch: 14869, train_loss: 0.8884446024894714, val_loss: 0.8599897623062134\n",
            "Saving model, epoch: 14870, train_loss: 0.8884391784667969, val_loss: 0.8599869608879089\n",
            "Saving model, epoch: 14871, train_loss: 0.8884340524673462, val_loss: 0.8599849939346313\n",
            "Saving model, epoch: 14872, train_loss: 0.8884287476539612, val_loss: 0.8599817752838135\n",
            "Saving model, epoch: 14873, train_loss: 0.888423502445221, val_loss: 0.8599799275398254\n",
            "Saving model, epoch: 14874, train_loss: 0.8884183764457703, val_loss: 0.8599762916564941\n",
            "Saving model, epoch: 14875, train_loss: 0.8884130716323853, val_loss: 0.8599742650985718\n",
            "Saving model, epoch: 14876, train_loss: 0.888407826423645, val_loss: 0.8599711656570435\n",
            "Saving model, epoch: 14877, train_loss: 0.8884028196334839, val_loss: 0.8599691987037659\n",
            "Saving model, epoch: 14878, train_loss: 0.8883973956108093, val_loss: 0.8599658012390137\n",
            "Saving model, epoch: 14879, train_loss: 0.8883923888206482, val_loss: 0.8599642515182495\n",
            "Saving model, epoch: 14880, train_loss: 0.888387143611908, val_loss: 0.8599607348442078\n",
            "Saving model, epoch: 14881, train_loss: 0.8883817195892334, val_loss: 0.8599592447280884\n",
            "Saving model, epoch: 14882, train_loss: 0.8883767127990723, val_loss: 0.8599554896354675\n",
            "Saving model, epoch: 14883, train_loss: 0.8883712887763977, val_loss: 0.8599539995193481\n",
            "Saving model, epoch: 14884, train_loss: 0.888366162776947, val_loss: 0.8599503040313721\n",
            "Saving model, epoch: 14885, train_loss: 0.8883610367774963, val_loss: 0.8599486351013184\n",
            "Saving model, epoch: 14886, train_loss: 0.8883556723594666, val_loss: 0.8599451780319214\n",
            "Saving model, epoch: 14887, train_loss: 0.8883503675460815, val_loss: 0.8599439263343811\n",
            "Saving model, epoch: 14888, train_loss: 0.8883451223373413, val_loss: 0.8599399328231812\n",
            "Saving model, epoch: 14889, train_loss: 0.8883399963378906, val_loss: 0.8599377274513245\n",
            "Saving model, epoch: 14890, train_loss: 0.8883348107337952, val_loss: 0.8599350452423096\n",
            "Saving model, epoch: 14891, train_loss: 0.8883295655250549, val_loss: 0.8599328398704529\n",
            "Saving model, epoch: 14892, train_loss: 0.8883242607116699, val_loss: 0.8599299788475037\n",
            "Saving model, epoch: 14893, train_loss: 0.8883191347122192, val_loss: 0.8599274158477783\n",
            "Saving model, epoch: 14894, train_loss: 0.8883138298988342, val_loss: 0.8599249720573425\n",
            "Saving model, epoch: 14895, train_loss: 0.8883084654808044, val_loss: 0.8599221706390381\n",
            "Saving model, epoch: 14896, train_loss: 0.8883031606674194, val_loss: 0.8599191904067993\n",
            "Saving model, epoch: 14897, train_loss: 0.8882979154586792, val_loss: 0.8599169254302979\n",
            "Saving model, epoch: 14898, train_loss: 0.8882927298545837, val_loss: 0.8599144816398621\n",
            "Saving model, epoch: 14899, train_loss: 0.8882874846458435, val_loss: 0.8599113821983337\n",
            "Saving model, epoch: 14900, train_loss: 0.8882824778556824, val_loss: 0.8599092960357666\n",
            "epoch: 14901, train_loss: 0.8882771134376526, val_loss: 0.8599060773849487\n",
            "Saving model, epoch: 14901, train_loss: 0.8882771134376526, val_loss: 0.8599060773849487\n",
            "Saving model, epoch: 14902, train_loss: 0.8882719874382019, val_loss: 0.8599047660827637\n",
            "Saving model, epoch: 14903, train_loss: 0.8882668018341064, val_loss: 0.8599004745483398\n",
            "Saving model, epoch: 14904, train_loss: 0.8882613778114319, val_loss: 0.8598995804786682\n",
            "Saving model, epoch: 14905, train_loss: 0.8882559537887573, val_loss: 0.8598951101303101\n",
            "Saving model, epoch: 14906, train_loss: 0.8882508873939514, val_loss: 0.8598945736885071\n",
            "Saving model, epoch: 14907, train_loss: 0.8882455825805664, val_loss: 0.859889566898346\n",
            "Saving model, epoch: 14908, train_loss: 0.8882404565811157, val_loss: 0.8598893880844116\n",
            "Saving model, epoch: 14909, train_loss: 0.8882350325584412, val_loss: 0.8598842024803162\n",
            "Saving model, epoch: 14911, train_loss: 0.8882246017456055, val_loss: 0.8598787784576416\n",
            "Saving model, epoch: 14913, train_loss: 0.888214111328125, val_loss: 0.8598729968070984\n",
            "Saving model, epoch: 14915, train_loss: 0.8882035613059998, val_loss: 0.8598670363426208\n",
            "Saving model, epoch: 14917, train_loss: 0.8881930112838745, val_loss: 0.8598616123199463\n",
            "Saving model, epoch: 14919, train_loss: 0.8881824016571045, val_loss: 0.8598557710647583\n",
            "Saving model, epoch: 14921, train_loss: 0.8881720304489136, val_loss: 0.8598493337631226\n",
            "Saving model, epoch: 14923, train_loss: 0.8881614804267883, val_loss: 0.8598424196243286\n",
            "Saving model, epoch: 14925, train_loss: 0.8881510496139526, val_loss: 0.8598348498344421\n",
            "Saving model, epoch: 14927, train_loss: 0.8881406188011169, val_loss: 0.8598266243934631\n",
            "Saving model, epoch: 14929, train_loss: 0.8881301283836365, val_loss: 0.8598163723945618\n",
            "Saving model, epoch: 14931, train_loss: 0.8881196975708008, val_loss: 0.8598043322563171\n",
            "Saving model, epoch: 14933, train_loss: 0.8881092667579651, val_loss: 0.8597886562347412\n",
            "Saving model, epoch: 14935, train_loss: 0.888099193572998, val_loss: 0.859768807888031\n",
            "Saving model, epoch: 14937, train_loss: 0.8880895376205444, val_loss: 0.8597412705421448\n",
            "Saving model, epoch: 14939, train_loss: 0.8880807161331177, val_loss: 0.8597047924995422\n",
            "Saving model, epoch: 14941, train_loss: 0.8880735039710999, val_loss: 0.8596574664115906\n",
            "Saving model, epoch: 14943, train_loss: 0.888068437576294, val_loss: 0.8596066832542419\n",
            "Saving model, epoch: 14945, train_loss: 0.8880624175071716, val_loss: 0.8595735430717468\n",
            "epoch: 15001, train_loss: 0.8877660632133484, val_loss: 0.8596627712249756\n",
            "Saving model, epoch: 15033, train_loss: 0.8876056671142578, val_loss: 0.8595718145370483\n",
            "Saving model, epoch: 15034, train_loss: 0.8876004815101624, val_loss: 0.8595677018165588\n",
            "Saving model, epoch: 15036, train_loss: 0.8875905275344849, val_loss: 0.8595619201660156\n",
            "Saving model, epoch: 15038, train_loss: 0.8875804543495178, val_loss: 0.8595572710037231\n",
            "Saving model, epoch: 15040, train_loss: 0.8875705003738403, val_loss: 0.859552800655365\n",
            "Saving model, epoch: 15041, train_loss: 0.8875653147697449, val_loss: 0.8595518469810486\n",
            "Saving model, epoch: 15042, train_loss: 0.8875601887702942, val_loss: 0.859548032283783\n",
            "Saving model, epoch: 15043, train_loss: 0.8875552415847778, val_loss: 0.8595462441444397\n",
            "Saving model, epoch: 15044, train_loss: 0.8875503540039062, val_loss: 0.8595439195632935\n",
            "Saving model, epoch: 15045, train_loss: 0.8875452280044556, val_loss: 0.859540581703186\n",
            "Saving model, epoch: 15046, train_loss: 0.8875401616096497, val_loss: 0.8595399260520935\n",
            "Saving model, epoch: 15047, train_loss: 0.8875351548194885, val_loss: 0.8595348596572876\n",
            "Saving model, epoch: 15049, train_loss: 0.8875250220298767, val_loss: 0.8595300316810608\n",
            "Saving model, epoch: 15051, train_loss: 0.8875148892402649, val_loss: 0.8595250844955444\n",
            "Saving model, epoch: 15052, train_loss: 0.887509822845459, val_loss: 0.8595247268676758\n",
            "Saving model, epoch: 15053, train_loss: 0.8875048160552979, val_loss: 0.859520435333252\n",
            "Saving model, epoch: 15054, train_loss: 0.8874998688697815, val_loss: 0.8595187664031982\n",
            "Saving model, epoch: 15055, train_loss: 0.8874948024749756, val_loss: 0.8595165014266968\n",
            "Saving model, epoch: 15056, train_loss: 0.8874896764755249, val_loss: 0.8595131039619446\n",
            "Saving model, epoch: 15057, train_loss: 0.8874847292900085, val_loss: 0.8595119714736938\n",
            "Saving model, epoch: 15058, train_loss: 0.8874797224998474, val_loss: 0.8595077395439148\n",
            "Saving model, epoch: 15059, train_loss: 0.8874746561050415, val_loss: 0.8595075011253357\n",
            "Saving model, epoch: 15060, train_loss: 0.887469470500946, val_loss: 0.8595022559165955\n",
            "Saving model, epoch: 15062, train_loss: 0.8874595165252686, val_loss: 0.8594969511032104\n",
            "Saving model, epoch: 15064, train_loss: 0.8874492645263672, val_loss: 0.8594920039176941\n",
            "Saving model, epoch: 15066, train_loss: 0.8874393105506897, val_loss: 0.859486997127533\n",
            "Saving model, epoch: 15068, train_loss: 0.8874290585517883, val_loss: 0.8594828844070435\n",
            "Saving model, epoch: 15069, train_loss: 0.8874242305755615, val_loss: 0.8594818711280823\n",
            "Saving model, epoch: 15070, train_loss: 0.8874192237854004, val_loss: 0.8594776391983032\n",
            "Saving model, epoch: 15071, train_loss: 0.8874137997627258, val_loss: 0.859476625919342\n",
            "Saving model, epoch: 15072, train_loss: 0.887408971786499, val_loss: 0.8594725728034973\n",
            "Saving model, epoch: 15073, train_loss: 0.8874038457870483, val_loss: 0.8594711422920227\n",
            "Saving model, epoch: 15074, train_loss: 0.8873988389968872, val_loss: 0.8594679236412048\n",
            "Saving model, epoch: 15075, train_loss: 0.8873937726020813, val_loss: 0.8594664335250854\n",
            "Saving model, epoch: 15076, train_loss: 0.8873887062072754, val_loss: 0.8594629168510437\n",
            "Saving model, epoch: 15077, train_loss: 0.8873836398124695, val_loss: 0.8594613671302795\n",
            "Saving model, epoch: 15078, train_loss: 0.8873785138130188, val_loss: 0.8594579696655273\n",
            "Saving model, epoch: 15079, train_loss: 0.8873733878135681, val_loss: 0.8594562411308289\n",
            "Saving model, epoch: 15080, train_loss: 0.887368381023407, val_loss: 0.8594534397125244\n",
            "Saving model, epoch: 15081, train_loss: 0.8873631358146667, val_loss: 0.8594509363174438\n",
            "Saving model, epoch: 15082, train_loss: 0.8873582482337952, val_loss: 0.8594483733177185\n",
            "Saving model, epoch: 15083, train_loss: 0.8873531222343445, val_loss: 0.8594455718994141\n",
            "Saving model, epoch: 15084, train_loss: 0.8873481154441833, val_loss: 0.8594440817832947\n",
            "Saving model, epoch: 15085, train_loss: 0.8873430490493774, val_loss: 0.8594400882720947\n",
            "Saving model, epoch: 15086, train_loss: 0.8873379230499268, val_loss: 0.8594393134117126\n",
            "Saving model, epoch: 15087, train_loss: 0.8873329162597656, val_loss: 0.8594337701797485\n",
            "Saving model, epoch: 15089, train_loss: 0.8873227834701538, val_loss: 0.8594290018081665\n",
            "Saving model, epoch: 15091, train_loss: 0.8873127102851868, val_loss: 0.8594233393669128\n",
            "Saving model, epoch: 15093, train_loss: 0.8873024582862854, val_loss: 0.8594173192977905\n",
            "Saving model, epoch: 15095, train_loss: 0.8872923254966736, val_loss: 0.8594121336936951\n",
            "Saving model, epoch: 15097, train_loss: 0.8872821927070618, val_loss: 0.8594062328338623\n",
            "Saving model, epoch: 15099, train_loss: 0.88727205991745, val_loss: 0.8593995571136475\n",
            "epoch: 15101, train_loss: 0.8872618079185486, val_loss: 0.8593929409980774\n",
            "Saving model, epoch: 15101, train_loss: 0.8872618079185486, val_loss: 0.8593929409980774\n",
            "Saving model, epoch: 15103, train_loss: 0.887251615524292, val_loss: 0.8593853116035461\n",
            "Saving model, epoch: 15105, train_loss: 0.8872414827346802, val_loss: 0.8593763113021851\n",
            "Saving model, epoch: 15107, train_loss: 0.8872315287590027, val_loss: 0.8593653440475464\n",
            "Saving model, epoch: 15109, train_loss: 0.8872215151786804, val_loss: 0.8593515753746033\n",
            "Saving model, epoch: 15111, train_loss: 0.8872114419937134, val_loss: 0.8593334555625916\n",
            "Saving model, epoch: 15113, train_loss: 0.8872020244598389, val_loss: 0.859307587146759\n",
            "Saving model, epoch: 15115, train_loss: 0.8871931433677673, val_loss: 0.8592734932899475\n",
            "Saving model, epoch: 15117, train_loss: 0.8871861100196838, val_loss: 0.8592270016670227\n",
            "Saving model, epoch: 15119, train_loss: 0.8871814012527466, val_loss: 0.8591726422309875\n",
            "Saving model, epoch: 15121, train_loss: 0.8871771693229675, val_loss: 0.8591277003288269\n",
            "epoch: 15201, train_loss: 0.886775016784668, val_loss: 0.8591532111167908\n",
            "Saving model, epoch: 15214, train_loss: 0.8867121934890747, val_loss: 0.8591256737709045\n",
            "Saving model, epoch: 15215, train_loss: 0.8867071866989136, val_loss: 0.8591215014457703\n",
            "Saving model, epoch: 15217, train_loss: 0.8866974711418152, val_loss: 0.8591159582138062\n",
            "Saving model, epoch: 15219, train_loss: 0.8866878747940063, val_loss: 0.8591108918190002\n",
            "Saving model, epoch: 15221, train_loss: 0.8866780400276184, val_loss: 0.8591067790985107\n",
            "Saving model, epoch: 15223, train_loss: 0.8866685032844543, val_loss: 0.8591029644012451\n",
            "Saving model, epoch: 15224, train_loss: 0.8866634964942932, val_loss: 0.8591010570526123\n",
            "Saving model, epoch: 15225, train_loss: 0.8866586685180664, val_loss: 0.8590988516807556\n",
            "Saving model, epoch: 15226, train_loss: 0.8866538405418396, val_loss: 0.8590953946113586\n",
            "Saving model, epoch: 15227, train_loss: 0.8866490125656128, val_loss: 0.8590952157974243\n",
            "Saving model, epoch: 15228, train_loss: 0.886644184589386, val_loss: 0.859089732170105\n",
            "Saving model, epoch: 15230, train_loss: 0.8866345286369324, val_loss: 0.8590846061706543\n",
            "Saving model, epoch: 15232, train_loss: 0.8866246342658997, val_loss: 0.8590803146362305\n",
            "Saving model, epoch: 15234, train_loss: 0.8866150379180908, val_loss: 0.859075665473938\n",
            "Saving model, epoch: 15236, train_loss: 0.886605441570282, val_loss: 0.8590709567070007\n",
            "Saving model, epoch: 15237, train_loss: 0.8866006135940552, val_loss: 0.8590702414512634\n",
            "Saving model, epoch: 15238, train_loss: 0.8865955471992493, val_loss: 0.859066903591156\n",
            "Saving model, epoch: 15239, train_loss: 0.886590838432312, val_loss: 0.8590648174285889\n",
            "Saving model, epoch: 15240, train_loss: 0.8865857124328613, val_loss: 0.8590632081031799\n",
            "Saving model, epoch: 15241, train_loss: 0.8865808844566345, val_loss: 0.8590590953826904\n",
            "Saving model, epoch: 15242, train_loss: 0.8865760564804077, val_loss: 0.8590585589408875\n",
            "Saving model, epoch: 15243, train_loss: 0.8865713477134705, val_loss: 0.8590535521507263\n",
            "Saving model, epoch: 15245, train_loss: 0.886561393737793, val_loss: 0.8590489029884338\n",
            "Saving model, epoch: 15247, train_loss: 0.8865517973899841, val_loss: 0.8590438961982727\n",
            "Saving model, epoch: 15249, train_loss: 0.886542022228241, val_loss: 0.8590396046638489\n",
            "Saving model, epoch: 15250, train_loss: 0.8865373134613037, val_loss: 0.8590395450592041\n",
            "Saving model, epoch: 15251, train_loss: 0.8865323066711426, val_loss: 0.8590345978736877\n",
            "Saving model, epoch: 15252, train_loss: 0.8865273594856262, val_loss: 0.859034538269043\n",
            "Saving model, epoch: 15253, train_loss: 0.8865225315093994, val_loss: 0.8590296506881714\n",
            "Saving model, epoch: 15254, train_loss: 0.8865178227424622, val_loss: 0.859029233455658\n",
            "Saving model, epoch: 15255, train_loss: 0.8865127563476562, val_loss: 0.8590250611305237\n",
            "Saving model, epoch: 15256, train_loss: 0.8865079879760742, val_loss: 0.8590234518051147\n",
            "Saving model, epoch: 15257, train_loss: 0.8865031599998474, val_loss: 0.8590210676193237\n",
            "Saving model, epoch: 15258, train_loss: 0.8864983320236206, val_loss: 0.8590189814567566\n",
            "Saving model, epoch: 15259, train_loss: 0.8864932656288147, val_loss: 0.8590168952941895\n",
            "Saving model, epoch: 15260, train_loss: 0.8864882588386536, val_loss: 0.8590130805969238\n",
            "Saving model, epoch: 15261, train_loss: 0.8864834308624268, val_loss: 0.8590123057365417\n",
            "Saving model, epoch: 15262, train_loss: 0.8864786028862, val_loss: 0.8590085506439209\n",
            "Saving model, epoch: 15263, train_loss: 0.8864737749099731, val_loss: 0.8590071201324463\n",
            "Saving model, epoch: 15264, train_loss: 0.886468768119812, val_loss: 0.8590035438537598\n",
            "Saving model, epoch: 15265, train_loss: 0.8864639401435852, val_loss: 0.8590018153190613\n",
            "Saving model, epoch: 15266, train_loss: 0.8864591121673584, val_loss: 0.8589990735054016\n",
            "Saving model, epoch: 15267, train_loss: 0.8864541053771973, val_loss: 0.8589968085289001\n",
            "Saving model, epoch: 15268, train_loss: 0.88644939661026, val_loss: 0.8589949011802673\n",
            "Saving model, epoch: 15269, train_loss: 0.8864444494247437, val_loss: 0.8589916229248047\n",
            "Saving model, epoch: 15270, train_loss: 0.886439323425293, val_loss: 0.8589904308319092\n",
            "Saving model, epoch: 15271, train_loss: 0.8864344954490662, val_loss: 0.8589863181114197\n",
            "Saving model, epoch: 15272, train_loss: 0.8864297866821289, val_loss: 0.8589860796928406\n",
            "Saving model, epoch: 15273, train_loss: 0.8864248991012573, val_loss: 0.8589813113212585\n",
            "Saving model, epoch: 15275, train_loss: 0.8864151239395142, val_loss: 0.8589763641357422\n",
            "Saving model, epoch: 15277, train_loss: 0.8864054083824158, val_loss: 0.8589708805084229\n",
            "Saving model, epoch: 15279, train_loss: 0.8863954544067383, val_loss: 0.8589653968811035\n",
            "Saving model, epoch: 15281, train_loss: 0.8863856792449951, val_loss: 0.8589598536491394\n",
            "Saving model, epoch: 15283, train_loss: 0.8863760232925415, val_loss: 0.8589537143707275\n",
            "Saving model, epoch: 15285, train_loss: 0.8863661289215088, val_loss: 0.8589471578598022\n",
            "Saving model, epoch: 15287, train_loss: 0.8863563537597656, val_loss: 0.8589407205581665\n",
            "Saving model, epoch: 15289, train_loss: 0.8863465189933777, val_loss: 0.8589321374893188\n",
            "Saving model, epoch: 15291, train_loss: 0.8863367438316345, val_loss: 0.8589227199554443\n",
            "Saving model, epoch: 15293, train_loss: 0.8863271474838257, val_loss: 0.8589110970497131\n",
            "Saving model, epoch: 15295, train_loss: 0.8863174915313721, val_loss: 0.8588963747024536\n",
            "Saving model, epoch: 15297, train_loss: 0.886307954788208, val_loss: 0.8588765263557434\n",
            "Saving model, epoch: 15299, train_loss: 0.8862988948822021, val_loss: 0.8588492274284363\n",
            "epoch: 15301, train_loss: 0.8862907290458679, val_loss: 0.8588115572929382\n",
            "Saving model, epoch: 15301, train_loss: 0.8862907290458679, val_loss: 0.8588115572929382\n",
            "Saving model, epoch: 15303, train_loss: 0.8862849473953247, val_loss: 0.8587623238563538\n",
            "Saving model, epoch: 15305, train_loss: 0.8862810134887695, val_loss: 0.8587080836296082\n",
            "Saving model, epoch: 15307, train_loss: 0.8862761855125427, val_loss: 0.8586710095405579\n",
            "epoch: 15401, train_loss: 0.8858218789100647, val_loss: 0.8586792945861816\n",
            "Saving model, epoch: 15405, train_loss: 0.8858031630516052, val_loss: 0.8586694598197937\n",
            "Saving model, epoch: 15407, train_loss: 0.8857936263084412, val_loss: 0.8586656451225281\n",
            "Saving model, epoch: 15409, train_loss: 0.8857845067977905, val_loss: 0.8586610555648804\n",
            "Saving model, epoch: 15410, train_loss: 0.885779857635498, val_loss: 0.8586602807044983\n",
            "Saving model, epoch: 15411, train_loss: 0.8857751488685608, val_loss: 0.8586580157279968\n",
            "Saving model, epoch: 15412, train_loss: 0.8857704997062683, val_loss: 0.8586548566818237\n",
            "Saving model, epoch: 15413, train_loss: 0.8857656717300415, val_loss: 0.858653724193573\n",
            "Saving model, epoch: 15414, train_loss: 0.8857608437538147, val_loss: 0.8586493134498596\n",
            "Saving model, epoch: 15416, train_loss: 0.8857516050338745, val_loss: 0.8586445450782776\n",
            "Saving model, epoch: 15418, train_loss: 0.8857423663139343, val_loss: 0.8586398363113403\n",
            "Saving model, epoch: 15420, train_loss: 0.8857329487800598, val_loss: 0.8586358428001404\n",
            "Saving model, epoch: 15421, train_loss: 0.8857282996177673, val_loss: 0.8586351275444031\n",
            "Saving model, epoch: 15422, train_loss: 0.8857235908508301, val_loss: 0.8586313724517822\n",
            "Saving model, epoch: 15423, train_loss: 0.8857188820838928, val_loss: 0.8586302995681763\n",
            "Saving model, epoch: 15424, train_loss: 0.8857142329216003, val_loss: 0.8586270213127136\n",
            "Saving model, epoch: 15425, train_loss: 0.8857094049453735, val_loss: 0.8586252331733704\n",
            "Saving model, epoch: 15426, train_loss: 0.8857048153877258, val_loss: 0.8586225509643555\n",
            "Saving model, epoch: 15427, train_loss: 0.885699987411499, val_loss: 0.8586200475692749\n",
            "Saving model, epoch: 15428, train_loss: 0.8856953382492065, val_loss: 0.8586177825927734\n",
            "Saving model, epoch: 15429, train_loss: 0.8856907486915588, val_loss: 0.8586153984069824\n",
            "Saving model, epoch: 15430, train_loss: 0.8856860399246216, val_loss: 0.8586134910583496\n",
            "Saving model, epoch: 15431, train_loss: 0.8856815099716187, val_loss: 0.8586097955703735\n",
            "Saving model, epoch: 15432, train_loss: 0.8856766819953918, val_loss: 0.8586094975471497\n",
            "Saving model, epoch: 15433, train_loss: 0.8856720328330994, val_loss: 0.8586053848266602\n",
            "Saving model, epoch: 15434, train_loss: 0.8856672644615173, val_loss: 0.8586046695709229\n",
            "Saving model, epoch: 15435, train_loss: 0.8856624364852905, val_loss: 0.8586004972457886\n",
            "Saving model, epoch: 15437, train_loss: 0.8856531977653503, val_loss: 0.8585955500602722\n",
            "Saving model, epoch: 15439, train_loss: 0.8856438994407654, val_loss: 0.8585910201072693\n",
            "Saving model, epoch: 15441, train_loss: 0.8856343626976013, val_loss: 0.8585861325263977\n",
            "Saving model, epoch: 15443, train_loss: 0.8856249451637268, val_loss: 0.8585812449455261\n",
            "Saving model, epoch: 15445, train_loss: 0.8856154680252075, val_loss: 0.858576774597168\n",
            "Saving model, epoch: 15447, train_loss: 0.8856061697006226, val_loss: 0.8585723638534546\n",
            "Saving model, epoch: 15448, train_loss: 0.8856014013290405, val_loss: 0.8585721850395203\n",
            "Saving model, epoch: 15449, train_loss: 0.8855966925621033, val_loss: 0.8585679531097412\n",
            "Saving model, epoch: 15450, train_loss: 0.8855921030044556, val_loss: 0.8585668802261353\n",
            "Saving model, epoch: 15451, train_loss: 0.8855874538421631, val_loss: 0.8585638403892517\n",
            "Saving model, epoch: 15452, train_loss: 0.8855825662612915, val_loss: 0.8585620522499084\n",
            "Saving model, epoch: 15453, train_loss: 0.885577917098999, val_loss: 0.8585591912269592\n",
            "Saving model, epoch: 15454, train_loss: 0.8855730891227722, val_loss: 0.8585567474365234\n",
            "Saving model, epoch: 15455, train_loss: 0.885568380355835, val_loss: 0.8585549592971802\n",
            "Saving model, epoch: 15456, train_loss: 0.8855637311935425, val_loss: 0.8585523366928101\n",
            "Saving model, epoch: 15457, train_loss: 0.8855591416358948, val_loss: 0.858550488948822\n",
            "Saving model, epoch: 15458, train_loss: 0.885554313659668, val_loss: 0.8585470914840698\n",
            "Saving model, epoch: 15459, train_loss: 0.8855496644973755, val_loss: 0.8585464954376221\n",
            "Saving model, epoch: 15460, train_loss: 0.8855448365211487, val_loss: 0.8585420846939087\n",
            "Saving model, epoch: 15462, train_loss: 0.8855355381965637, val_loss: 0.858536422252655\n",
            "Saving model, epoch: 15464, train_loss: 0.8855260014533997, val_loss: 0.8585314750671387\n",
            "Saving model, epoch: 15466, train_loss: 0.8855165243148804, val_loss: 0.8585259914398193\n",
            "Saving model, epoch: 15468, train_loss: 0.8855071067810059, val_loss: 0.8585208058357239\n",
            "Saving model, epoch: 15470, train_loss: 0.8854978680610657, val_loss: 0.8585142493247986\n",
            "Saving model, epoch: 15472, train_loss: 0.8854882121086121, val_loss: 0.8585085272789001\n",
            "Saving model, epoch: 15474, train_loss: 0.8854787349700928, val_loss: 0.8585008382797241\n",
            "Saving model, epoch: 15476, train_loss: 0.8854694366455078, val_loss: 0.8584927320480347\n",
            "Saving model, epoch: 15478, train_loss: 0.8854600787162781, val_loss: 0.858482301235199\n",
            "Saving model, epoch: 15480, train_loss: 0.8854506015777588, val_loss: 0.8584690690040588\n",
            "Saving model, epoch: 15482, train_loss: 0.8854413628578186, val_loss: 0.8584522008895874\n",
            "Saving model, epoch: 15484, train_loss: 0.8854324221611023, val_loss: 0.8584284782409668\n",
            "Saving model, epoch: 15486, train_loss: 0.8854243159294128, val_loss: 0.8583955764770508\n",
            "Saving model, epoch: 15488, train_loss: 0.8854174613952637, val_loss: 0.8583514094352722\n",
            "Saving model, epoch: 15490, train_loss: 0.8854133486747742, val_loss: 0.8582961559295654\n",
            "Saving model, epoch: 15492, train_loss: 0.8854104280471802, val_loss: 0.8582457304000854\n",
            "Saving model, epoch: 15494, train_loss: 0.8854022026062012, val_loss: 0.8582350015640259\n",
            "epoch: 15501, train_loss: 0.8853598237037659, val_loss: 0.8583016991615295\n",
            "Saving model, epoch: 15597, train_loss: 0.884924054145813, val_loss: 0.858231246471405\n",
            "Saving model, epoch: 15599, train_loss: 0.8849149346351624, val_loss: 0.8582279086112976\n",
            "Saving model, epoch: 15600, train_loss: 0.884910523891449, val_loss: 0.858226478099823\n",
            "epoch: 15601, train_loss: 0.884905993938446, val_loss: 0.858224093914032\n",
            "Saving model, epoch: 15601, train_loss: 0.884905993938446, val_loss: 0.858224093914032\n",
            "Saving model, epoch: 15602, train_loss: 0.8849014639854431, val_loss: 0.8582210540771484\n",
            "Saving model, epoch: 15603, train_loss: 0.8848968148231506, val_loss: 0.8582195043563843\n",
            "Saving model, epoch: 15604, train_loss: 0.8848924040794373, val_loss: 0.8582159876823425\n",
            "Saving model, epoch: 15605, train_loss: 0.8848879933357239, val_loss: 0.8582152724266052\n",
            "Saving model, epoch: 15606, train_loss: 0.8848835825920105, val_loss: 0.8582116961479187\n",
            "Saving model, epoch: 15607, train_loss: 0.884878933429718, val_loss: 0.8582111597061157\n",
            "Saving model, epoch: 15608, train_loss: 0.8848743438720703, val_loss: 0.858206570148468\n",
            "Saving model, epoch: 15609, train_loss: 0.8848698139190674, val_loss: 0.8582063913345337\n",
            "Saving model, epoch: 15610, train_loss: 0.8848652839660645, val_loss: 0.8582025170326233\n",
            "Saving model, epoch: 15611, train_loss: 0.8848608732223511, val_loss: 0.8582020401954651\n",
            "Saving model, epoch: 15612, train_loss: 0.8848564624786377, val_loss: 0.8581982254981995\n",
            "Saving model, epoch: 15613, train_loss: 0.8848518133163452, val_loss: 0.858197033405304\n",
            "Saving model, epoch: 15614, train_loss: 0.8848472237586975, val_loss: 0.8581937551498413\n",
            "Saving model, epoch: 15615, train_loss: 0.8848428130149841, val_loss: 0.858192503452301\n",
            "Saving model, epoch: 15616, train_loss: 0.8848384618759155, val_loss: 0.8581900000572205\n",
            "Saving model, epoch: 15617, train_loss: 0.8848337531089783, val_loss: 0.8581882119178772\n",
            "Saving model, epoch: 15618, train_loss: 0.8848293423652649, val_loss: 0.8581852316856384\n",
            "Saving model, epoch: 15619, train_loss: 0.8848246932029724, val_loss: 0.8581830859184265\n",
            "Saving model, epoch: 15620, train_loss: 0.8848201632499695, val_loss: 0.8581812381744385\n",
            "Saving model, epoch: 15621, train_loss: 0.8848156332969666, val_loss: 0.8581786155700684\n",
            "Saving model, epoch: 15622, train_loss: 0.8848112225532532, val_loss: 0.8581762313842773\n",
            "Saving model, epoch: 15623, train_loss: 0.8848066329956055, val_loss: 0.8581742644309998\n",
            "Saving model, epoch: 15624, train_loss: 0.8848021030426025, val_loss: 0.8581717014312744\n",
            "Saving model, epoch: 15625, train_loss: 0.8847975730895996, val_loss: 0.8581701517105103\n",
            "Saving model, epoch: 15626, train_loss: 0.8847930431365967, val_loss: 0.8581670522689819\n",
            "Saving model, epoch: 15627, train_loss: 0.8847885131835938, val_loss: 0.8581658005714417\n",
            "Saving model, epoch: 15628, train_loss: 0.8847839832305908, val_loss: 0.8581621646881104\n",
            "Saving model, epoch: 15629, train_loss: 0.8847795128822327, val_loss: 0.858161211013794\n",
            "Saving model, epoch: 15630, train_loss: 0.8847748637199402, val_loss: 0.8581569790840149\n",
            "Saving model, epoch: 15632, train_loss: 0.8847658038139343, val_loss: 0.8581525683403015\n",
            "Saving model, epoch: 15634, train_loss: 0.8847568035125732, val_loss: 0.8581483960151672\n",
            "Saving model, epoch: 15636, train_loss: 0.8847476243972778, val_loss: 0.8581438660621643\n",
            "Saving model, epoch: 15638, train_loss: 0.8847388029098511, val_loss: 0.8581392168998718\n",
            "Saving model, epoch: 15639, train_loss: 0.8847342729568481, val_loss: 0.858139157295227\n",
            "Saving model, epoch: 15640, train_loss: 0.8847296833992004, val_loss: 0.8581350445747375\n",
            "Saving model, epoch: 15641, train_loss: 0.884725034236908, val_loss: 0.8581348061561584\n",
            "Saving model, epoch: 15642, train_loss: 0.8847206234931946, val_loss: 0.8581308722496033\n",
            "Saving model, epoch: 15643, train_loss: 0.8847159743309021, val_loss: 0.8581299781799316\n",
            "Saving model, epoch: 15644, train_loss: 0.8847113847732544, val_loss: 0.8581260442733765\n",
            "Saving model, epoch: 15645, train_loss: 0.8847070336341858, val_loss: 0.8581253290176392\n",
            "Saving model, epoch: 15646, train_loss: 0.8847023248672485, val_loss: 0.8581213355064392\n",
            "Saving model, epoch: 15647, train_loss: 0.8846977949142456, val_loss: 0.858120858669281\n",
            "Saving model, epoch: 15648, train_loss: 0.8846932649612427, val_loss: 0.8581172227859497\n",
            "Saving model, epoch: 15649, train_loss: 0.8846887350082397, val_loss: 0.8581168055534363\n",
            "Saving model, epoch: 15650, train_loss: 0.8846840262413025, val_loss: 0.8581117987632751\n",
            "Saving model, epoch: 15652, train_loss: 0.8846752047538757, val_loss: 0.8581072688102722\n",
            "Saving model, epoch: 15654, train_loss: 0.8846659660339355, val_loss: 0.8581020832061768\n",
            "Saving model, epoch: 15656, train_loss: 0.8846570253372192, val_loss: 0.8580966591835022\n",
            "Saving model, epoch: 15658, train_loss: 0.884647786617279, val_loss: 0.858091413974762\n",
            "Saving model, epoch: 15660, train_loss: 0.8846387267112732, val_loss: 0.8580856919288635\n",
            "Saving model, epoch: 15662, train_loss: 0.8846296072006226, val_loss: 0.8580793142318726\n",
            "Saving model, epoch: 15664, train_loss: 0.8846205472946167, val_loss: 0.8580716848373413\n",
            "Saving model, epoch: 15666, train_loss: 0.8846114873886108, val_loss: 0.8580629825592041\n",
            "Saving model, epoch: 15668, train_loss: 0.8846024870872498, val_loss: 0.8580520749092102\n",
            "Saving model, epoch: 15670, train_loss: 0.8845934271812439, val_loss: 0.8580390214920044\n",
            "Saving model, epoch: 15672, train_loss: 0.8845846652984619, val_loss: 0.8580209016799927\n",
            "Saving model, epoch: 15674, train_loss: 0.884576141834259, val_loss: 0.857995867729187\n",
            "Saving model, epoch: 15676, train_loss: 0.8845682740211487, val_loss: 0.8579620122909546\n",
            "Saving model, epoch: 15678, train_loss: 0.8845622539520264, val_loss: 0.8579160571098328\n",
            "Saving model, epoch: 15680, train_loss: 0.8845588564872742, val_loss: 0.8578608632087708\n",
            "Saving model, epoch: 15682, train_loss: 0.8845560550689697, val_loss: 0.85781329870224\n",
            "epoch: 15701, train_loss: 0.884456992149353, val_loss: 0.8580436110496521\n",
            "Saving model, epoch: 15787, train_loss: 0.8840862512588501, val_loss: 0.8578128218650818\n",
            "Saving model, epoch: 15789, train_loss: 0.8840774297714233, val_loss: 0.8578085899353027\n",
            "Saving model, epoch: 15790, train_loss: 0.8840730786323547, val_loss: 0.8578084111213684\n",
            "Saving model, epoch: 15791, train_loss: 0.8840688467025757, val_loss: 0.8578052520751953\n",
            "Saving model, epoch: 15792, train_loss: 0.8840643763542175, val_loss: 0.8578039407730103\n",
            "Saving model, epoch: 15793, train_loss: 0.8840601444244385, val_loss: 0.8578012585639954\n",
            "Saving model, epoch: 15794, train_loss: 0.8840557336807251, val_loss: 0.8577985763549805\n",
            "Saving model, epoch: 15795, train_loss: 0.8840512633323669, val_loss: 0.8577975630760193\n",
            "Saving model, epoch: 15796, train_loss: 0.8840468525886536, val_loss: 0.8577940464019775\n",
            "Saving model, epoch: 15797, train_loss: 0.8840426206588745, val_loss: 0.8577932715415955\n",
            "Saving model, epoch: 15798, train_loss: 0.8840383291244507, val_loss: 0.8577892184257507\n",
            "Saving model, epoch: 15800, train_loss: 0.8840295672416687, val_loss: 0.8577849268913269\n",
            "epoch: 15801, train_loss: 0.8840252757072449, val_loss: 0.8577849268913269\n",
            "Saving model, epoch: 15802, train_loss: 0.8840209245681763, val_loss: 0.8577805757522583\n",
            "Saving model, epoch: 15804, train_loss: 0.8840122222900391, val_loss: 0.857776403427124\n",
            "Saving model, epoch: 15805, train_loss: 0.88400799036026, val_loss: 0.8577761650085449\n",
            "Saving model, epoch: 15806, train_loss: 0.8840035796165466, val_loss: 0.8577719926834106\n",
            "Saving model, epoch: 15807, train_loss: 0.8839991688728333, val_loss: 0.8577715158462524\n",
            "Saving model, epoch: 15808, train_loss: 0.8839948177337646, val_loss: 0.857768177986145\n",
            "Saving model, epoch: 15809, train_loss: 0.8839904069900513, val_loss: 0.8577668070793152\n",
            "Saving model, epoch: 15810, train_loss: 0.8839861750602722, val_loss: 0.8577641844749451\n",
            "Saving model, epoch: 15811, train_loss: 0.8839818835258484, val_loss: 0.8577622771263123\n",
            "Saving model, epoch: 15812, train_loss: 0.8839774131774902, val_loss: 0.8577598333358765\n",
            "Saving model, epoch: 15813, train_loss: 0.8839730024337769, val_loss: 0.8577578663825989\n",
            "Saving model, epoch: 15814, train_loss: 0.883968710899353, val_loss: 0.857755720615387\n",
            "Saving model, epoch: 15815, train_loss: 0.8839643597602844, val_loss: 0.8577535152435303\n",
            "Saving model, epoch: 15816, train_loss: 0.883959949016571, val_loss: 0.8577513098716736\n",
            "Saving model, epoch: 15817, train_loss: 0.8839556574821472, val_loss: 0.857749342918396\n",
            "Saving model, epoch: 15818, train_loss: 0.8839512467384338, val_loss: 0.8577468395233154\n",
            "Saving model, epoch: 15819, train_loss: 0.8839467763900757, val_loss: 0.8577451705932617\n",
            "Saving model, epoch: 15820, train_loss: 0.8839426040649414, val_loss: 0.8577423095703125\n",
            "Saving model, epoch: 15821, train_loss: 0.8839381337165833, val_loss: 0.8577408790588379\n",
            "Saving model, epoch: 15822, train_loss: 0.8839337229728699, val_loss: 0.8577381372451782\n",
            "Saving model, epoch: 15823, train_loss: 0.883929431438446, val_loss: 0.8577365279197693\n",
            "Saving model, epoch: 15824, train_loss: 0.8839250206947327, val_loss: 0.8577331304550171\n",
            "Saving model, epoch: 15825, train_loss: 0.8839206695556641, val_loss: 0.8577327728271484\n",
            "Saving model, epoch: 15826, train_loss: 0.8839163780212402, val_loss: 0.8577291369438171\n",
            "Saving model, epoch: 15827, train_loss: 0.8839119076728821, val_loss: 0.8577279448509216\n",
            "Saving model, epoch: 15828, train_loss: 0.8839076161384583, val_loss: 0.8577238917350769\n",
            "Saving model, epoch: 15829, train_loss: 0.8839032649993896, val_loss: 0.857723593711853\n",
            "Saving model, epoch: 15830, train_loss: 0.8838987946510315, val_loss: 0.8577196002006531\n",
            "Saving model, epoch: 15832, train_loss: 0.8838900327682495, val_loss: 0.8577149510383606\n",
            "Saving model, epoch: 15834, train_loss: 0.8838815093040466, val_loss: 0.8577106595039368\n",
            "Saving model, epoch: 15836, train_loss: 0.8838725686073303, val_loss: 0.8577060699462891\n",
            "Saving model, epoch: 15838, train_loss: 0.8838639259338379, val_loss: 0.8577015995979309\n",
            "Saving model, epoch: 15840, train_loss: 0.8838550448417664, val_loss: 0.8576973080635071\n",
            "Saving model, epoch: 15842, train_loss: 0.8838465213775635, val_loss: 0.857692539691925\n",
            "Saving model, epoch: 15844, train_loss: 0.8838375806808472, val_loss: 0.8576882481575012\n",
            "Saving model, epoch: 15846, train_loss: 0.8838288187980652, val_loss: 0.8576831817626953\n",
            "Saving model, epoch: 15848, train_loss: 0.8838199973106384, val_loss: 0.8576780557632446\n",
            "Saving model, epoch: 15850, train_loss: 0.8838111758232117, val_loss: 0.8576728105545044\n",
            "Saving model, epoch: 15852, train_loss: 0.8838026523590088, val_loss: 0.8576663136482239\n",
            "Saving model, epoch: 15854, train_loss: 0.8837937116622925, val_loss: 0.8576598763465881\n",
            "Saving model, epoch: 15856, train_loss: 0.8837851285934448, val_loss: 0.8576520681381226\n",
            "Saving model, epoch: 15858, train_loss: 0.8837761878967285, val_loss: 0.8576422929763794\n",
            "Saving model, epoch: 15860, train_loss: 0.8837674856185913, val_loss: 0.8576300144195557\n",
            "Saving model, epoch: 15862, train_loss: 0.8837591409683228, val_loss: 0.8576135635375977\n",
            "Saving model, epoch: 15864, train_loss: 0.8837506771087646, val_loss: 0.85759037733078\n",
            "Saving model, epoch: 15866, train_loss: 0.8837432265281677, val_loss: 0.8575572967529297\n",
            "Saving model, epoch: 15868, train_loss: 0.8837372064590454, val_loss: 0.8575122952461243\n",
            "Saving model, epoch: 15870, train_loss: 0.8837338089942932, val_loss: 0.8574544191360474\n",
            "Saving model, epoch: 15872, train_loss: 0.8837323784828186, val_loss: 0.8573992252349854\n",
            "Saving model, epoch: 15874, train_loss: 0.8837254643440247, val_loss: 0.8573853969573975\n",
            "epoch: 15901, train_loss: 0.8835956454277039, val_loss: 0.8576269149780273\n",
            "Saving model, epoch: 15990, train_loss: 0.8832264542579651, val_loss: 0.8573846817016602\n",
            "Saving model, epoch: 15991, train_loss: 0.8832219839096069, val_loss: 0.8573823571205139\n",
            "Saving model, epoch: 15992, train_loss: 0.8832181096076965, val_loss: 0.8573800921440125\n",
            "Saving model, epoch: 15993, train_loss: 0.8832137584686279, val_loss: 0.8573774695396423\n",
            "Saving model, epoch: 15994, train_loss: 0.8832095265388489, val_loss: 0.8573766350746155\n",
            "Saving model, epoch: 15995, train_loss: 0.8832055330276489, val_loss: 0.8573735952377319\n",
            "Saving model, epoch: 15996, train_loss: 0.8832012414932251, val_loss: 0.8573722839355469\n",
            "Saving model, epoch: 15997, train_loss: 0.8831970691680908, val_loss: 0.8573691248893738\n",
            "Saving model, epoch: 15998, train_loss: 0.8831928968429565, val_loss: 0.857367753982544\n",
            "Saving model, epoch: 15999, train_loss: 0.8831888437271118, val_loss: 0.8573654890060425\n",
            "Saving model, epoch: 16000, train_loss: 0.8831847310066223, val_loss: 0.8573638796806335\n",
            "epoch: 16001, train_loss: 0.8831804394721985, val_loss: 0.8573612570762634\n",
            "Saving model, epoch: 16001, train_loss: 0.8831804394721985, val_loss: 0.8573612570762634\n",
            "Saving model, epoch: 16002, train_loss: 0.8831762075424194, val_loss: 0.8573588132858276\n",
            "Saving model, epoch: 16003, train_loss: 0.8831720948219299, val_loss: 0.8573575615882874\n",
            "Saving model, epoch: 16004, train_loss: 0.8831678628921509, val_loss: 0.8573554158210754\n",
            "Saving model, epoch: 16005, train_loss: 0.8831639289855957, val_loss: 0.8573532700538635\n",
            "Saving model, epoch: 16006, train_loss: 0.8831595182418823, val_loss: 0.8573508858680725\n",
            "Saving model, epoch: 16007, train_loss: 0.8831555247306824, val_loss: 0.8573489785194397\n",
            "Saving model, epoch: 16008, train_loss: 0.8831510543823242, val_loss: 0.8573467135429382\n",
            "Saving model, epoch: 16009, train_loss: 0.8831470608711243, val_loss: 0.8573455810546875\n",
            "Saving model, epoch: 16010, train_loss: 0.8831428289413452, val_loss: 0.8573423624038696\n",
            "Saving model, epoch: 16011, train_loss: 0.8831385374069214, val_loss: 0.857341468334198\n",
            "Saving model, epoch: 16012, train_loss: 0.8831344246864319, val_loss: 0.8573377728462219\n",
            "Saving model, epoch: 16013, train_loss: 0.8831301927566528, val_loss: 0.8573376536369324\n",
            "Saving model, epoch: 16014, train_loss: 0.8831259608268738, val_loss: 0.8573338985443115\n",
            "Saving model, epoch: 16015, train_loss: 0.8831217288970947, val_loss: 0.8573336601257324\n",
            "Saving model, epoch: 16016, train_loss: 0.8831177353858948, val_loss: 0.8573294281959534\n",
            "Saving model, epoch: 16017, train_loss: 0.8831135034561157, val_loss: 0.8573291301727295\n",
            "Saving model, epoch: 16018, train_loss: 0.8831092715263367, val_loss: 0.85732501745224\n",
            "Saving model, epoch: 16019, train_loss: 0.8831049799919128, val_loss: 0.8573245406150818\n",
            "Saving model, epoch: 16020, train_loss: 0.8831009268760681, val_loss: 0.8573207855224609\n",
            "Saving model, epoch: 16022, train_loss: 0.8830924034118652, val_loss: 0.8573166131973267\n",
            "Saving model, epoch: 16024, train_loss: 0.8830839395523071, val_loss: 0.857312798500061\n",
            "Saving model, epoch: 16025, train_loss: 0.8830799460411072, val_loss: 0.8573123216629028\n",
            "Saving model, epoch: 16026, train_loss: 0.8830757141113281, val_loss: 0.8573082089424133\n",
            "Saving model, epoch: 16028, train_loss: 0.8830673098564148, val_loss: 0.8573036193847656\n",
            "Saving model, epoch: 16030, train_loss: 0.8830589652061462, val_loss: 0.8572999238967896\n",
            "Saving model, epoch: 16032, train_loss: 0.8830503821372986, val_loss: 0.857295036315918\n",
            "Saving model, epoch: 16034, train_loss: 0.8830420970916748, val_loss: 0.8572902083396912\n",
            "Saving model, epoch: 16036, train_loss: 0.8830335140228271, val_loss: 0.8572856187820435\n",
            "Saving model, epoch: 16038, train_loss: 0.8830251693725586, val_loss: 0.8572807312011719\n",
            "Saving model, epoch: 16040, train_loss: 0.88301682472229, val_loss: 0.8572754263877869\n",
            "Saving model, epoch: 16042, train_loss: 0.8830083012580872, val_loss: 0.8572701811790466\n",
            "Saving model, epoch: 16044, train_loss: 0.8830000758171082, val_loss: 0.8572647571563721\n",
            "Saving model, epoch: 16046, train_loss: 0.8829915523529053, val_loss: 0.8572588562965393\n",
            "Saving model, epoch: 16048, train_loss: 0.8829832077026367, val_loss: 0.8572524189949036\n",
            "Saving model, epoch: 16050, train_loss: 0.8829746842384338, val_loss: 0.8572439551353455\n",
            "Saving model, epoch: 16052, train_loss: 0.8829663991928101, val_loss: 0.8572337031364441\n",
            "Saving model, epoch: 16054, train_loss: 0.8829580545425415, val_loss: 0.8572211265563965\n",
            "Saving model, epoch: 16056, train_loss: 0.8829498291015625, val_loss: 0.8572049140930176\n",
            "Saving model, epoch: 16058, train_loss: 0.8829420208930969, val_loss: 0.8571827411651611\n",
            "Saving model, epoch: 16060, train_loss: 0.8829346895217896, val_loss: 0.8571528792381287\n",
            "Saving model, epoch: 16062, train_loss: 0.8829286694526672, val_loss: 0.8571128249168396\n",
            "Saving model, epoch: 16064, train_loss: 0.8829241991043091, val_loss: 0.8570650219917297\n",
            "Saving model, epoch: 16066, train_loss: 0.8829209804534912, val_loss: 0.8570200800895691\n",
            "Saving model, epoch: 16068, train_loss: 0.8829140663146973, val_loss: 0.8570064902305603\n",
            "epoch: 16101, train_loss: 0.8827670812606812, val_loss: 0.8571856021881104\n",
            "Saving model, epoch: 16175, train_loss: 0.882469117641449, val_loss: 0.8570052981376648\n",
            "Saving model, epoch: 16176, train_loss: 0.8824650645256042, val_loss: 0.857004702091217\n",
            "Saving model, epoch: 16177, train_loss: 0.8824608325958252, val_loss: 0.8570014834403992\n",
            "Saving model, epoch: 16178, train_loss: 0.8824568390846252, val_loss: 0.8570002913475037\n",
            "Saving model, epoch: 16179, train_loss: 0.8824528455734253, val_loss: 0.8569982051849365\n",
            "Saving model, epoch: 16180, train_loss: 0.8824487924575806, val_loss: 0.8569955825805664\n",
            "Saving model, epoch: 16181, train_loss: 0.8824445605278015, val_loss: 0.85699462890625\n",
            "Saving model, epoch: 16182, train_loss: 0.8824406862258911, val_loss: 0.8569910526275635\n",
            "Saving model, epoch: 16184, train_loss: 0.8824326395988464, val_loss: 0.8569863438606262\n",
            "Saving model, epoch: 16186, train_loss: 0.8824244141578674, val_loss: 0.8569824695587158\n",
            "Saving model, epoch: 16188, train_loss: 0.8824163675308228, val_loss: 0.8569779992103577\n",
            "Saving model, epoch: 16190, train_loss: 0.882408082485199, val_loss: 0.8569739460945129\n",
            "Saving model, epoch: 16192, train_loss: 0.8823999762535095, val_loss: 0.856969952583313\n",
            "Saving model, epoch: 16194, train_loss: 0.8823919296264648, val_loss: 0.8569653630256653\n",
            "Saving model, epoch: 16196, train_loss: 0.8823837041854858, val_loss: 0.8569605946540833\n",
            "Saving model, epoch: 16198, train_loss: 0.8823757767677307, val_loss: 0.8569567203521729\n",
            "Saving model, epoch: 16200, train_loss: 0.8823674917221069, val_loss: 0.8569522500038147\n",
            "epoch: 16201, train_loss: 0.8823633790016174, val_loss: 0.8569570779800415\n",
            "Saving model, epoch: 16202, train_loss: 0.8823593854904175, val_loss: 0.856947660446167\n",
            "Saving model, epoch: 16204, train_loss: 0.8823512196540833, val_loss: 0.856942355632782\n",
            "Saving model, epoch: 16206, train_loss: 0.8823432326316833, val_loss: 0.8569386005401611\n",
            "Saving model, epoch: 16208, train_loss: 0.8823350667953491, val_loss: 0.8569329380989075\n",
            "Saving model, epoch: 16210, train_loss: 0.8823268413543701, val_loss: 0.8569276928901672\n",
            "Saving model, epoch: 16212, train_loss: 0.8823187947273254, val_loss: 0.8569216132164001\n",
            "Saving model, epoch: 16214, train_loss: 0.8823106288909912, val_loss: 0.8569150567054749\n",
            "Saving model, epoch: 16216, train_loss: 0.8823023438453674, val_loss: 0.8569065928459167\n",
            "Saving model, epoch: 16218, train_loss: 0.8822944760322571, val_loss: 0.8568965196609497\n",
            "Saving model, epoch: 16220, train_loss: 0.8822864294052124, val_loss: 0.8568835854530334\n",
            "Saving model, epoch: 16222, train_loss: 0.8822783827781677, val_loss: 0.8568660020828247\n",
            "Saving model, epoch: 16224, train_loss: 0.8822709918022156, val_loss: 0.8568434119224548\n",
            "Saving model, epoch: 16226, train_loss: 0.8822639584541321, val_loss: 0.8568116426467896\n",
            "Saving model, epoch: 16228, train_loss: 0.8822582960128784, val_loss: 0.8567704558372498\n",
            "Saving model, epoch: 16230, train_loss: 0.8822544813156128, val_loss: 0.8567222356796265\n",
            "Saving model, epoch: 16232, train_loss: 0.882251501083374, val_loss: 0.8566804528236389\n",
            "Saving model, epoch: 16234, train_loss: 0.8822436332702637, val_loss: 0.8566768765449524\n",
            "epoch: 16301, train_loss: 0.8819692730903625, val_loss: 0.8567627668380737\n",
            "Saving model, epoch: 16342, train_loss: 0.8818089962005615, val_loss: 0.856676459312439\n",
            "Saving model, epoch: 16343, train_loss: 0.8818050622940063, val_loss: 0.8566743731498718\n",
            "Saving model, epoch: 16344, train_loss: 0.881801187992096, val_loss: 0.8566722273826599\n",
            "Saving model, epoch: 16345, train_loss: 0.8817973732948303, val_loss: 0.856671154499054\n",
            "Saving model, epoch: 16346, train_loss: 0.8817934393882751, val_loss: 0.8566673994064331\n",
            "Saving model, epoch: 16348, train_loss: 0.8817855715751648, val_loss: 0.856663167476654\n",
            "Saving model, epoch: 16350, train_loss: 0.8817777633666992, val_loss: 0.8566587567329407\n",
            "Saving model, epoch: 16352, train_loss: 0.8817698359489441, val_loss: 0.8566545844078064\n",
            "Saving model, epoch: 16354, train_loss: 0.8817620873451233, val_loss: 0.8566503524780273\n",
            "Saving model, epoch: 16356, train_loss: 0.8817542791366577, val_loss: 0.8566464185714722\n",
            "Saving model, epoch: 16358, train_loss: 0.881746232509613, val_loss: 0.8566423058509827\n",
            "Saving model, epoch: 16360, train_loss: 0.8817383646965027, val_loss: 0.8566381335258484\n",
            "Saving model, epoch: 16362, train_loss: 0.8817305564880371, val_loss: 0.856634259223938\n",
            "Saving model, epoch: 16364, train_loss: 0.8817226886749268, val_loss: 0.8566297292709351\n",
            "Saving model, epoch: 16366, train_loss: 0.8817147612571716, val_loss: 0.8566257357597351\n",
            "Saving model, epoch: 16368, train_loss: 0.881706953048706, val_loss: 0.8566213250160217\n",
            "Saving model, epoch: 16370, train_loss: 0.8816990852355957, val_loss: 0.8566169738769531\n",
            "Saving model, epoch: 16372, train_loss: 0.8816911578178406, val_loss: 0.8566120266914368\n",
            "Saving model, epoch: 16374, train_loss: 0.8816832304000854, val_loss: 0.8566058278083801\n",
            "Saving model, epoch: 16376, train_loss: 0.8816756010055542, val_loss: 0.8565996885299683\n",
            "Saving model, epoch: 16378, train_loss: 0.8816676735877991, val_loss: 0.8565926551818848\n",
            "Saving model, epoch: 16380, train_loss: 0.8816596865653992, val_loss: 0.8565835356712341\n",
            "Saving model, epoch: 16382, train_loss: 0.8816518783569336, val_loss: 0.856572687625885\n",
            "Saving model, epoch: 16384, train_loss: 0.8816442489624023, val_loss: 0.8565585613250732\n",
            "Saving model, epoch: 16386, train_loss: 0.8816366195678711, val_loss: 0.8565399646759033\n",
            "Saving model, epoch: 16388, train_loss: 0.881629467010498, val_loss: 0.8565148115158081\n",
            "Saving model, epoch: 16390, train_loss: 0.8816231489181519, val_loss: 0.8564813733100891\n",
            "Saving model, epoch: 16392, train_loss: 0.881618320941925, val_loss: 0.8564379811286926\n",
            "Saving model, epoch: 16394, train_loss: 0.8816149234771729, val_loss: 0.8563905954360962\n",
            "Saving model, epoch: 16396, train_loss: 0.8816112875938416, val_loss: 0.8563570380210876\n",
            "epoch: 16401, train_loss: 0.8815783262252808, val_loss: 0.8566119074821472\n",
            "epoch: 16501, train_loss: 0.8812010884284973, val_loss: 0.8563701510429382\n",
            "Saving model, epoch: 16510, train_loss: 0.8811668157577515, val_loss: 0.8563550114631653\n",
            "Saving model, epoch: 16511, train_loss: 0.8811630010604858, val_loss: 0.8563538789749146\n",
            "Saving model, epoch: 16512, train_loss: 0.8811591863632202, val_loss: 0.8563507199287415\n",
            "Saving model, epoch: 16514, train_loss: 0.8811516165733337, val_loss: 0.8563463687896729\n",
            "Saving model, epoch: 16516, train_loss: 0.881144106388092, val_loss: 0.8563420176506042\n",
            "Saving model, epoch: 16518, train_loss: 0.8811364769935608, val_loss: 0.8563377261161804\n",
            "Saving model, epoch: 16520, train_loss: 0.8811288475990295, val_loss: 0.8563334345817566\n",
            "Saving model, epoch: 16522, train_loss: 0.8811210989952087, val_loss: 0.8563293218612671\n",
            "Saving model, epoch: 16524, train_loss: 0.8811136484146118, val_loss: 0.8563248515129089\n",
            "Saving model, epoch: 16526, train_loss: 0.881105899810791, val_loss: 0.8563202023506165\n",
            "Saving model, epoch: 16528, train_loss: 0.8810982704162598, val_loss: 0.8563158512115479\n",
            "Saving model, epoch: 16530, train_loss: 0.8810907602310181, val_loss: 0.8563111424446106\n",
            "Saving model, epoch: 16532, train_loss: 0.8810830116271973, val_loss: 0.8563056588172913\n",
            "Saving model, epoch: 16534, train_loss: 0.881075382232666, val_loss: 0.8563002943992615\n",
            "Saving model, epoch: 16536, train_loss: 0.8810678720474243, val_loss: 0.8562934398651123\n",
            "Saving model, epoch: 16538, train_loss: 0.8810601234436035, val_loss: 0.856285572052002\n",
            "Saving model, epoch: 16540, train_loss: 0.881052553653717, val_loss: 0.8562759160995483\n",
            "Saving model, epoch: 16542, train_loss: 0.8810450434684753, val_loss: 0.8562642335891724\n",
            "Saving model, epoch: 16544, train_loss: 0.8810378313064575, val_loss: 0.8562490940093994\n",
            "Saving model, epoch: 16546, train_loss: 0.8810305595397949, val_loss: 0.8562284111976624\n",
            "Saving model, epoch: 16548, train_loss: 0.8810239434242249, val_loss: 0.8562015891075134\n",
            "Saving model, epoch: 16550, train_loss: 0.8810181021690369, val_loss: 0.8561663627624512\n",
            "Saving model, epoch: 16552, train_loss: 0.8810136914253235, val_loss: 0.8561233878135681\n",
            "Saving model, epoch: 16554, train_loss: 0.8810104727745056, val_loss: 0.8560817241668701\n",
            "Saving model, epoch: 16556, train_loss: 0.8810049295425415, val_loss: 0.8560624718666077\n",
            "epoch: 16601, train_loss: 0.8808274269104004, val_loss: 0.856170654296875\n",
            "Saving model, epoch: 16668, train_loss: 0.880581259727478, val_loss: 0.8560616970062256\n",
            "Saving model, epoch: 16670, train_loss: 0.8805740475654602, val_loss: 0.8560575842857361\n",
            "Saving model, epoch: 16672, train_loss: 0.8805665969848633, val_loss: 0.8560541868209839\n",
            "Saving model, epoch: 16674, train_loss: 0.8805592656135559, val_loss: 0.8560504913330078\n",
            "Saving model, epoch: 16676, train_loss: 0.880551815032959, val_loss: 0.8560460805892944\n",
            "Saving model, epoch: 16678, train_loss: 0.8805444240570068, val_loss: 0.856042206287384\n",
            "Saving model, epoch: 16680, train_loss: 0.8805370926856995, val_loss: 0.8560380935668945\n",
            "Saving model, epoch: 16682, train_loss: 0.8805296421051025, val_loss: 0.8560337424278259\n",
            "Saving model, epoch: 16684, train_loss: 0.8805221915245056, val_loss: 0.8560295104980469\n",
            "Saving model, epoch: 16686, train_loss: 0.8805147409439087, val_loss: 0.8560240268707275\n",
            "Saving model, epoch: 16688, train_loss: 0.8805073499679565, val_loss: 0.8560188412666321\n",
            "Saving model, epoch: 16690, train_loss: 0.8804998993873596, val_loss: 0.856012761592865\n",
            "Saving model, epoch: 16692, train_loss: 0.8804924488067627, val_loss: 0.8560048341751099\n",
            "Saving model, epoch: 16694, train_loss: 0.8804852366447449, val_loss: 0.8559970855712891\n",
            "Saving model, epoch: 16696, train_loss: 0.880477786064148, val_loss: 0.8559855818748474\n",
            "Saving model, epoch: 16698, train_loss: 0.8804707527160645, val_loss: 0.8559716939926147\n",
            "Saving model, epoch: 16700, train_loss: 0.880463719367981, val_loss: 0.8559514880180359\n",
            "epoch: 16701, train_loss: 0.8804603219032288, val_loss: 0.8560668230056763\n",
            "Saving model, epoch: 16702, train_loss: 0.8804571032524109, val_loss: 0.8559235334396362\n",
            "Saving model, epoch: 16704, train_loss: 0.8804517984390259, val_loss: 0.8558845520019531\n",
            "Saving model, epoch: 16706, train_loss: 0.8804481625556946, val_loss: 0.8558347225189209\n",
            "Saving model, epoch: 16708, train_loss: 0.88044673204422, val_loss: 0.8557813763618469\n",
            "Saving model, epoch: 16710, train_loss: 0.8804434537887573, val_loss: 0.8557522296905518\n",
            "epoch: 16801, train_loss: 0.8801062703132629, val_loss: 0.8558219075202942\n",
            "Saving model, epoch: 16843, train_loss: 0.8799574375152588, val_loss: 0.8557497262954712\n",
            "Saving model, epoch: 16844, train_loss: 0.879953920841217, val_loss: 0.855749249458313\n",
            "Saving model, epoch: 16845, train_loss: 0.8799503445625305, val_loss: 0.8557453155517578\n",
            "Saving model, epoch: 16847, train_loss: 0.8799431920051575, val_loss: 0.8557415008544922\n",
            "Saving model, epoch: 16849, train_loss: 0.8799362778663635, val_loss: 0.8557382225990295\n",
            "Saving model, epoch: 16851, train_loss: 0.8799288272857666, val_loss: 0.8557343482971191\n",
            "Saving model, epoch: 16853, train_loss: 0.8799217939376831, val_loss: 0.8557305932044983\n",
            "Saving model, epoch: 16855, train_loss: 0.8799147605895996, val_loss: 0.855726957321167\n",
            "Saving model, epoch: 16857, train_loss: 0.8799075484275818, val_loss: 0.8557227849960327\n",
            "Saving model, epoch: 16859, train_loss: 0.8799002766609192, val_loss: 0.855718731880188\n",
            "Saving model, epoch: 16861, train_loss: 0.8798933625221252, val_loss: 0.8557145595550537\n",
            "Saving model, epoch: 16863, train_loss: 0.8798860311508179, val_loss: 0.8557103276252747\n",
            "Saving model, epoch: 16865, train_loss: 0.8798789978027344, val_loss: 0.8557054996490479\n",
            "Saving model, epoch: 16867, train_loss: 0.8798719644546509, val_loss: 0.8557005524635315\n",
            "Saving model, epoch: 16869, train_loss: 0.8798647522926331, val_loss: 0.8556938171386719\n",
            "Saving model, epoch: 16871, train_loss: 0.8798574805259705, val_loss: 0.8556870222091675\n",
            "Saving model, epoch: 16873, train_loss: 0.879850447177887, val_loss: 0.855678379535675\n",
            "Saving model, epoch: 16875, train_loss: 0.8798434138298035, val_loss: 0.855667233467102\n",
            "Saving model, epoch: 16877, train_loss: 0.87983638048172, val_loss: 0.8556522727012634\n",
            "Saving model, epoch: 16879, train_loss: 0.8798297643661499, val_loss: 0.8556320071220398\n",
            "Saving model, epoch: 16881, train_loss: 0.8798234462738037, val_loss: 0.8556032776832581\n",
            "Saving model, epoch: 16883, train_loss: 0.879818320274353, val_loss: 0.8555643558502197\n",
            "Saving model, epoch: 16885, train_loss: 0.8798152804374695, val_loss: 0.855514407157898\n",
            "Saving model, epoch: 16887, train_loss: 0.879814088344574, val_loss: 0.8554624915122986\n",
            "Saving model, epoch: 16889, train_loss: 0.879810094833374, val_loss: 0.8554381132125854\n",
            "epoch: 16901, train_loss: 0.8797523975372314, val_loss: 0.8556726574897766\n",
            "epoch: 17001, train_loss: 0.8794131278991699, val_loss: 0.8554776310920715\n",
            "Saving model, epoch: 17025, train_loss: 0.8793307542800903, val_loss: 0.8554365634918213\n",
            "Saving model, epoch: 17026, train_loss: 0.879327118396759, val_loss: 0.8554349541664124\n",
            "Saving model, epoch: 17027, train_loss: 0.8793236017227173, val_loss: 0.85543292760849\n",
            "Saving model, epoch: 17028, train_loss: 0.8793202638626099, val_loss: 0.8554309010505676\n",
            "Saving model, epoch: 17029, train_loss: 0.8793167471885681, val_loss: 0.8554297089576721\n",
            "Saving model, epoch: 17030, train_loss: 0.8793133497238159, val_loss: 0.8554280400276184\n",
            "Saving model, epoch: 17031, train_loss: 0.8793099522590637, val_loss: 0.8554264903068542\n",
            "Saving model, epoch: 17032, train_loss: 0.8793064951896667, val_loss: 0.8554245233535767\n",
            "Saving model, epoch: 17033, train_loss: 0.8793030977249146, val_loss: 0.8554226160049438\n",
            "Saving model, epoch: 17034, train_loss: 0.8792997002601624, val_loss: 0.8554214239120483\n",
            "Saving model, epoch: 17035, train_loss: 0.8792962431907654, val_loss: 0.8554192781448364\n",
            "Saving model, epoch: 17036, train_loss: 0.8792927265167236, val_loss: 0.8554182052612305\n",
            "Saving model, epoch: 17037, train_loss: 0.879289448261261, val_loss: 0.8554154634475708\n",
            "Saving model, epoch: 17038, train_loss: 0.8792859315872192, val_loss: 0.8554150462150574\n",
            "Saving model, epoch: 17039, train_loss: 0.8792821764945984, val_loss: 0.8554112911224365\n",
            "Saving model, epoch: 17041, train_loss: 0.879275381565094, val_loss: 0.8554076552391052\n",
            "Saving model, epoch: 17043, train_loss: 0.8792688250541687, val_loss: 0.855402946472168\n",
            "Saving model, epoch: 17045, train_loss: 0.8792617321014404, val_loss: 0.8553982377052307\n",
            "Saving model, epoch: 17047, train_loss: 0.8792548775672913, val_loss: 0.855394184589386\n",
            "Saving model, epoch: 17049, train_loss: 0.8792479634284973, val_loss: 0.8553884029388428\n",
            "Saving model, epoch: 17051, train_loss: 0.8792409896850586, val_loss: 0.8553815484046936\n",
            "Saving model, epoch: 17053, train_loss: 0.8792341947555542, val_loss: 0.8553739786148071\n",
            "Saving model, epoch: 17055, train_loss: 0.879227340221405, val_loss: 0.855363130569458\n",
            "Saving model, epoch: 17057, train_loss: 0.8792206048965454, val_loss: 0.8553494811058044\n",
            "Saving model, epoch: 17059, train_loss: 0.8792138695716858, val_loss: 0.8553292155265808\n",
            "Saving model, epoch: 17061, train_loss: 0.879207968711853, val_loss: 0.8553006649017334\n",
            "Saving model, epoch: 17063, train_loss: 0.8792032599449158, val_loss: 0.8552592992782593\n",
            "Saving model, epoch: 17065, train_loss: 0.8792006373405457, val_loss: 0.85520339012146\n",
            "Saving model, epoch: 17067, train_loss: 0.8792014122009277, val_loss: 0.8551402688026428\n",
            "Saving model, epoch: 17069, train_loss: 0.8792002201080322, val_loss: 0.8551033139228821\n",
            "epoch: 17101, train_loss: 0.8790756464004517, val_loss: 0.8553329110145569\n",
            "epoch: 17201, train_loss: 0.8787504434585571, val_loss: 0.855145275592804\n",
            "Saving model, epoch: 17227, train_loss: 0.8786651492118835, val_loss: 0.8551031351089478\n",
            "Saving model, epoch: 17228, train_loss: 0.8786619305610657, val_loss: 0.8551011085510254\n",
            "Saving model, epoch: 17229, train_loss: 0.8786584734916687, val_loss: 0.8550994396209717\n",
            "Saving model, epoch: 17230, train_loss: 0.878655195236206, val_loss: 0.855097770690918\n",
            "Saving model, epoch: 17231, train_loss: 0.8786518573760986, val_loss: 0.8550968170166016\n",
            "Saving model, epoch: 17232, train_loss: 0.8786486387252808, val_loss: 0.8550944328308105\n",
            "Saving model, epoch: 17233, train_loss: 0.8786452412605286, val_loss: 0.8550934791564941\n",
            "Saving model, epoch: 17234, train_loss: 0.8786420226097107, val_loss: 0.8550909757614136\n",
            "Saving model, epoch: 17235, train_loss: 0.8786386251449585, val_loss: 0.8550904393196106\n",
            "Saving model, epoch: 17236, train_loss: 0.8786354064941406, val_loss: 0.8550875186920166\n",
            "Saving model, epoch: 17238, train_loss: 0.8786287307739258, val_loss: 0.855084240436554\n",
            "Saving model, epoch: 17240, train_loss: 0.8786221146583557, val_loss: 0.8550800085067749\n",
            "Saving model, epoch: 17242, train_loss: 0.8786154985427856, val_loss: 0.8550766110420227\n",
            "Saving model, epoch: 17244, train_loss: 0.8786088824272156, val_loss: 0.8550730347633362\n",
            "Saving model, epoch: 17246, train_loss: 0.8786022067070007, val_loss: 0.8550693392753601\n",
            "Saving model, epoch: 17248, train_loss: 0.878595769405365, val_loss: 0.8550649285316467\n",
            "Saving model, epoch: 17250, train_loss: 0.8785891532897949, val_loss: 0.8550612926483154\n",
            "Saving model, epoch: 17252, train_loss: 0.8785825371742249, val_loss: 0.8550567030906677\n",
            "Saving model, epoch: 17254, train_loss: 0.8785759210586548, val_loss: 0.8550511598587036\n",
            "Saving model, epoch: 17256, train_loss: 0.8785692453384399, val_loss: 0.8550447821617126\n",
            "Saving model, epoch: 17258, train_loss: 0.8785626292228699, val_loss: 0.8550366163253784\n",
            "Saving model, epoch: 17260, train_loss: 0.8785560131072998, val_loss: 0.8550259470939636\n",
            "Saving model, epoch: 17262, train_loss: 0.8785495758056641, val_loss: 0.8550123572349548\n",
            "Saving model, epoch: 17264, train_loss: 0.8785433769226074, val_loss: 0.8549931049346924\n",
            "Saving model, epoch: 17266, train_loss: 0.8785375356674194, val_loss: 0.8549655079841614\n",
            "Saving model, epoch: 17268, train_loss: 0.8785327076911926, val_loss: 0.8549258708953857\n",
            "Saving model, epoch: 17270, train_loss: 0.8785303831100464, val_loss: 0.8548725247383118\n",
            "Saving model, epoch: 17272, train_loss: 0.8785306215286255, val_loss: 0.8548122048377991\n",
            "Saving model, epoch: 17274, train_loss: 0.8785299062728882, val_loss: 0.8547734618186951\n",
            "epoch: 17301, train_loss: 0.8784260153770447, val_loss: 0.8550309538841248\n",
            "epoch: 17401, train_loss: 0.878113329410553, val_loss: 0.8548265695571899\n",
            "Saving model, epoch: 17436, train_loss: 0.8780028223991394, val_loss: 0.8547702431678772\n",
            "Saving model, epoch: 17438, train_loss: 0.8779963850975037, val_loss: 0.854767382144928\n",
            "Saving model, epoch: 17439, train_loss: 0.8779931664466858, val_loss: 0.854766845703125\n",
            "Saving model, epoch: 17440, train_loss: 0.877990186214447, val_loss: 0.8547642230987549\n",
            "Saving model, epoch: 17442, train_loss: 0.8779838681221008, val_loss: 0.8547604084014893\n",
            "Saving model, epoch: 17444, train_loss: 0.8779773116111755, val_loss: 0.8547570109367371\n",
            "Saving model, epoch: 17446, train_loss: 0.8779710531234741, val_loss: 0.8547529578208923\n",
            "Saving model, epoch: 17448, train_loss: 0.8779647350311279, val_loss: 0.8547489643096924\n",
            "Saving model, epoch: 17450, train_loss: 0.8779584169387817, val_loss: 0.8547446727752686\n",
            "Saving model, epoch: 17452, train_loss: 0.8779520988464355, val_loss: 0.8547393083572388\n",
            "Saving model, epoch: 17454, train_loss: 0.8779456615447998, val_loss: 0.8547326922416687\n",
            "Saving model, epoch: 17456, train_loss: 0.8779391050338745, val_loss: 0.8547257781028748\n",
            "Saving model, epoch: 17458, train_loss: 0.8779330849647522, val_loss: 0.8547160625457764\n",
            "Saving model, epoch: 17460, train_loss: 0.877926766872406, val_loss: 0.8547037839889526\n",
            "Saving model, epoch: 17462, train_loss: 0.8779206275939941, val_loss: 0.8546860218048096\n",
            "Saving model, epoch: 17464, train_loss: 0.8779151439666748, val_loss: 0.8546604514122009\n",
            "Saving model, epoch: 17466, train_loss: 0.877910315990448, val_loss: 0.8546237349510193\n",
            "Saving model, epoch: 17468, train_loss: 0.877907395362854, val_loss: 0.8545727729797363\n",
            "Saving model, epoch: 17470, train_loss: 0.8779078125953674, val_loss: 0.8545116782188416\n",
            "Saving model, epoch: 17472, train_loss: 0.8779084086418152, val_loss: 0.8544629216194153\n",
            "epoch: 17501, train_loss: 0.8778026700019836, val_loss: 0.8547307848930359\n",
            "epoch: 17601, train_loss: 0.8775022625923157, val_loss: 0.8545204401016235\n",
            "Saving model, epoch: 17639, train_loss: 0.8773869276046753, val_loss: 0.8544601798057556\n",
            "Saving model, epoch: 17641, train_loss: 0.8773807287216187, val_loss: 0.8544566631317139\n",
            "Saving model, epoch: 17643, train_loss: 0.8773747086524963, val_loss: 0.8544528484344482\n",
            "Saving model, epoch: 17645, train_loss: 0.8773684501647949, val_loss: 0.8544484972953796\n",
            "Saving model, epoch: 17647, train_loss: 0.8773624300956726, val_loss: 0.8544447422027588\n",
            "Saving model, epoch: 17649, train_loss: 0.8773564100265503, val_loss: 0.8544391989707947\n",
            "Saving model, epoch: 17651, train_loss: 0.877350389957428, val_loss: 0.8544329404830933\n",
            "Saving model, epoch: 17653, train_loss: 0.8773444294929504, val_loss: 0.8544255495071411\n",
            "Saving model, epoch: 17655, train_loss: 0.8773383498191833, val_loss: 0.8544161319732666\n",
            "Saving model, epoch: 17657, train_loss: 0.8773322105407715, val_loss: 0.8544034361839294\n",
            "Saving model, epoch: 17659, train_loss: 0.877326488494873, val_loss: 0.8543859124183655\n",
            "Saving model, epoch: 17661, train_loss: 0.8773209452629089, val_loss: 0.854360818862915\n",
            "Saving model, epoch: 17663, train_loss: 0.877316415309906, val_loss: 0.8543269038200378\n",
            "Saving model, epoch: 17665, train_loss: 0.8773133158683777, val_loss: 0.8542805314064026\n",
            "Saving model, epoch: 17667, train_loss: 0.8773127794265747, val_loss: 0.8542260527610779\n",
            "Saving model, epoch: 17669, train_loss: 0.877312421798706, val_loss: 0.85418301820755\n",
            "epoch: 17701, train_loss: 0.8772034645080566, val_loss: 0.854327917098999\n",
            "epoch: 17801, train_loss: 0.8769132494926453, val_loss: 0.8542249798774719\n",
            "Saving model, epoch: 17827, train_loss: 0.8768371939659119, val_loss: 0.8541792035102844\n",
            "Saving model, epoch: 17829, train_loss: 0.8768311738967896, val_loss: 0.8541738390922546\n",
            "Saving model, epoch: 17831, train_loss: 0.8768253326416016, val_loss: 0.854166567325592\n",
            "Saving model, epoch: 17833, train_loss: 0.8768194913864136, val_loss: 0.8541578650474548\n",
            "Saving model, epoch: 17835, train_loss: 0.8768138885498047, val_loss: 0.8541460633277893\n",
            "Saving model, epoch: 17837, train_loss: 0.8768081665039062, val_loss: 0.854131281375885\n",
            "Saving model, epoch: 17839, train_loss: 0.8768028020858765, val_loss: 0.8541088104248047\n",
            "Saving model, epoch: 17841, train_loss: 0.8767979145050049, val_loss: 0.8540793061256409\n",
            "Saving model, epoch: 17843, train_loss: 0.8767944574356079, val_loss: 0.854037344455719\n",
            "Saving model, epoch: 17845, train_loss: 0.8767930865287781, val_loss: 0.853985607624054\n",
            "Saving model, epoch: 17847, train_loss: 0.8767932057380676, val_loss: 0.8539367318153381\n",
            "Saving model, epoch: 17849, train_loss: 0.8767889738082886, val_loss: 0.8539232611656189\n",
            "epoch: 17901, train_loss: 0.8766282200813293, val_loss: 0.8540968298912048\n",
            "epoch: 18001, train_loss: 0.8763473033905029, val_loss: 0.8539437651634216\n",
            "Saving model, epoch: 18010, train_loss: 0.87632155418396, val_loss: 0.8539204001426697\n",
            "Saving model, epoch: 18012, train_loss: 0.8763159513473511, val_loss: 0.8539137840270996\n",
            "Saving model, epoch: 18014, train_loss: 0.8763103485107422, val_loss: 0.853904128074646\n",
            "Saving model, epoch: 18016, train_loss: 0.8763046860694885, val_loss: 0.8538919687271118\n",
            "Saving model, epoch: 18018, train_loss: 0.876299262046814, val_loss: 0.8538745045661926\n",
            "Saving model, epoch: 18020, train_loss: 0.8762942552566528, val_loss: 0.853848397731781\n",
            "Saving model, epoch: 18022, train_loss: 0.8762902021408081, val_loss: 0.8538107872009277\n",
            "Saving model, epoch: 18024, train_loss: 0.8762884140014648, val_loss: 0.8537575006484985\n",
            "Saving model, epoch: 18026, train_loss: 0.8762900233268738, val_loss: 0.853691816329956\n",
            "Saving model, epoch: 18028, train_loss: 0.8762922286987305, val_loss: 0.8536394238471985\n",
            "epoch: 18101, train_loss: 0.8760772347450256, val_loss: 0.8538026809692383\n",
            "epoch: 18201, train_loss: 0.8758091926574707, val_loss: 0.8536706566810608\n",
            "Saving model, epoch: 18223, train_loss: 0.875749409198761, val_loss: 0.8536390662193298\n",
            "Saving model, epoch: 18225, train_loss: 0.8757438659667969, val_loss: 0.8536347150802612\n",
            "Saving model, epoch: 18227, train_loss: 0.8757384419441223, val_loss: 0.8536294102668762\n",
            "Saving model, epoch: 18229, train_loss: 0.8757330179214478, val_loss: 0.8536235094070435\n",
            "Saving model, epoch: 18231, train_loss: 0.8757277131080627, val_loss: 0.8536155223846436\n",
            "Saving model, epoch: 18233, train_loss: 0.8757222890853882, val_loss: 0.853603720664978\n",
            "Saving model, epoch: 18235, train_loss: 0.875717043876648, val_loss: 0.8535869121551514\n",
            "Saving model, epoch: 18237, train_loss: 0.8757123351097107, val_loss: 0.8535615801811218\n",
            "Saving model, epoch: 18239, train_loss: 0.8757085204124451, val_loss: 0.8535218834877014\n",
            "Saving model, epoch: 18241, train_loss: 0.8757073283195496, val_loss: 0.8534648418426514\n",
            "Saving model, epoch: 18243, train_loss: 0.8757101893424988, val_loss: 0.8533895015716553\n",
            "Saving model, epoch: 18245, train_loss: 0.875715434551239, val_loss: 0.8533257842063904\n",
            "epoch: 18301, train_loss: 0.8755493760108948, val_loss: 0.8535583019256592\n",
            "epoch: 18401, train_loss: 0.8752964735031128, val_loss: 0.8534150123596191\n",
            "Saving model, epoch: 18457, train_loss: 0.8751528859138489, val_loss: 0.8533228635787964\n",
            "Saving model, epoch: 18459, train_loss: 0.8751477599143982, val_loss: 0.8533133864402771\n",
            "Saving model, epoch: 18461, train_loss: 0.8751426935195923, val_loss: 0.8533008694648743\n",
            "Saving model, epoch: 18463, train_loss: 0.8751380443572998, val_loss: 0.853283703327179\n",
            "Saving model, epoch: 18465, train_loss: 0.8751336336135864, val_loss: 0.8532608151435852\n",
            "Saving model, epoch: 18467, train_loss: 0.8751296997070312, val_loss: 0.8532291054725647\n",
            "Saving model, epoch: 18469, train_loss: 0.8751271963119507, val_loss: 0.8531888723373413\n",
            "Saving model, epoch: 18471, train_loss: 0.8751261830329895, val_loss: 0.8531431555747986\n",
            "Saving model, epoch: 18473, train_loss: 0.875124990940094, val_loss: 0.8531104326248169\n",
            "epoch: 18501, train_loss: 0.8750430941581726, val_loss: 0.8533316850662231\n",
            "epoch: 18601, train_loss: 0.8747950196266174, val_loss: 0.8531703352928162\n",
            "Saving model, epoch: 18614, train_loss: 0.8747628927230835, val_loss: 0.853104829788208\n",
            "Saving model, epoch: 18616, train_loss: 0.8747585415840149, val_loss: 0.8530840873718262\n",
            "Saving model, epoch: 18618, train_loss: 0.8747546672821045, val_loss: 0.8530552387237549\n",
            "Saving model, epoch: 18620, train_loss: 0.8747518658638, val_loss: 0.8530164957046509\n",
            "Saving model, epoch: 18622, train_loss: 0.8747508525848389, val_loss: 0.852968156337738\n",
            "Saving model, epoch: 18624, train_loss: 0.8747509121894836, val_loss: 0.8529217839241028\n",
            "Saving model, epoch: 18626, train_loss: 0.8747479319572449, val_loss: 0.8529066443443298\n",
            "epoch: 18701, train_loss: 0.8745546936988831, val_loss: 0.853038489818573\n",
            "Saving model, epoch: 18782, train_loss: 0.8743607997894287, val_loss: 0.8528885245323181\n",
            "Saving model, epoch: 18784, train_loss: 0.8743571043014526, val_loss: 0.8528592586517334\n",
            "Saving model, epoch: 18786, train_loss: 0.874354362487793, val_loss: 0.8528186678886414\n",
            "Saving model, epoch: 18788, train_loss: 0.8743540048599243, val_loss: 0.8527659177780151\n",
            "Saving model, epoch: 18790, train_loss: 0.8743553757667542, val_loss: 0.8527131080627441\n",
            "Saving model, epoch: 18792, train_loss: 0.8743535876274109, val_loss: 0.8526923656463623\n",
            "epoch: 18801, train_loss: 0.8743210434913635, val_loss: 0.8527801632881165\n",
            "epoch: 18901, train_loss: 0.8740869164466858, val_loss: 0.8528082966804504\n",
            "Saving model, epoch: 18958, train_loss: 0.8739555478096008, val_loss: 0.8526858687400818\n",
            "Saving model, epoch: 18960, train_loss: 0.87395179271698, val_loss: 0.852659285068512\n",
            "Saving model, epoch: 18962, train_loss: 0.8739491105079651, val_loss: 0.8526225686073303\n",
            "Saving model, epoch: 18964, train_loss: 0.8739479780197144, val_loss: 0.8525769710540771\n",
            "Saving model, epoch: 18966, train_loss: 0.8739482760429382, val_loss: 0.8525295257568359\n",
            "Saving model, epoch: 18968, train_loss: 0.8739467859268188, val_loss: 0.8525055050849915\n",
            "epoch: 19001, train_loss: 0.8738590478897095, val_loss: 0.8527363538742065\n",
            "epoch: 19101, train_loss: 0.8736362457275391, val_loss: 0.8525909781455994\n",
            "Saving model, epoch: 19118, train_loss: 0.873599648475647, val_loss: 0.8524835705757141\n",
            "Saving model, epoch: 19120, train_loss: 0.8735970258712769, val_loss: 0.8524488806724548\n",
            "Saving model, epoch: 19122, train_loss: 0.8735958337783813, val_loss: 0.8524045348167419\n",
            "Saving model, epoch: 19124, train_loss: 0.8735958337783813, val_loss: 0.8523605465888977\n",
            "Saving model, epoch: 19126, train_loss: 0.8735942244529724, val_loss: 0.852336585521698\n",
            "epoch: 19201, train_loss: 0.8734194040298462, val_loss: 0.8524775505065918\n",
            "Saving model, epoch: 19274, train_loss: 0.8732630610466003, val_loss: 0.8523111343383789\n",
            "Saving model, epoch: 19276, train_loss: 0.8732610940933228, val_loss: 0.8522711396217346\n",
            "Saving model, epoch: 19278, train_loss: 0.8732608556747437, val_loss: 0.8522217869758606\n",
            "Saving model, epoch: 19280, train_loss: 0.8732621073722839, val_loss: 0.8521730899810791\n",
            "Saving model, epoch: 19282, train_loss: 0.8732600808143616, val_loss: 0.8521544337272644\n",
            "epoch: 19301, train_loss: 0.8732048869132996, val_loss: 0.8523985743522644\n",
            "epoch: 19401, train_loss: 0.8729977607727051, val_loss: 0.8522669672966003\n",
            "Saving model, epoch: 19442, train_loss: 0.8729134202003479, val_loss: 0.8521432280540466\n",
            "Saving model, epoch: 19444, train_loss: 0.8729112148284912, val_loss: 0.8521054983139038\n",
            "Saving model, epoch: 19446, train_loss: 0.8729108572006226, val_loss: 0.8520572781562805\n",
            "Saving model, epoch: 19448, train_loss: 0.8729121685028076, val_loss: 0.8520046472549438\n",
            "Saving model, epoch: 19450, train_loss: 0.8729118704795837, val_loss: 0.8519756197929382\n",
            "epoch: 19501, train_loss: 0.872793972492218, val_loss: 0.8521587252616882\n",
            "epoch: 19601, train_loss: 0.8725932836532593, val_loss: 0.852066159248352\n",
            "Saving model, epoch: 19622, train_loss: 0.872552752494812, val_loss: 0.851954460144043\n",
            "Saving model, epoch: 19624, train_loss: 0.8725515604019165, val_loss: 0.8519046306610107\n",
            "Saving model, epoch: 19626, train_loss: 0.8725537657737732, val_loss: 0.8518380522727966\n",
            "Saving model, epoch: 19628, train_loss: 0.8725593686103821, val_loss: 0.8517714142799377\n",
            "Saving model, epoch: 19630, train_loss: 0.8725574016571045, val_loss: 0.851758599281311\n",
            "epoch: 19701, train_loss: 0.872401237487793, val_loss: 0.8519614338874817\n",
            "epoch: 19801, train_loss: 0.8722108602523804, val_loss: 0.851873517036438\n",
            "Saving model, epoch: 19841, train_loss: 0.872136116027832, val_loss: 0.8517379760742188\n",
            "Saving model, epoch: 19843, train_loss: 0.8721354007720947, val_loss: 0.8516887426376343\n",
            "Saving model, epoch: 19845, train_loss: 0.8721376061439514, val_loss: 0.8516256809234619\n",
            "Saving model, epoch: 19847, train_loss: 0.872141420841217, val_loss: 0.8515718579292297\n",
            "epoch: 19901, train_loss: 0.872024416923523, val_loss: 0.8517709374427795\n",
            "epoch: 20001, train_loss: 0.8718420267105103, val_loss: 0.851692795753479\n",
            "Saving model, epoch: 20036, train_loss: 0.8717805743217468, val_loss: 0.8515474796295166\n",
            "Saving model, epoch: 20038, train_loss: 0.8717799782752991, val_loss: 0.8515030741691589\n",
            "Saving model, epoch: 20040, train_loss: 0.8717813491821289, val_loss: 0.8514546751976013\n",
            "Saving model, epoch: 20042, train_loss: 0.8717813491821289, val_loss: 0.8514243364334106\n",
            "epoch: 20101, train_loss: 0.8716627359390259, val_loss: 0.8515916466712952\n",
            "epoch: 20201, train_loss: 0.8714849948883057, val_loss: 0.8515341281890869\n",
            "Saving model, epoch: 20208, train_loss: 0.8714747428894043, val_loss: 0.8514123558998108\n",
            "Saving model, epoch: 20210, train_loss: 0.8714747428894043, val_loss: 0.8513560891151428\n",
            "Saving model, epoch: 20212, train_loss: 0.8714792728424072, val_loss: 0.851280689239502\n",
            "Saving model, epoch: 20214, train_loss: 0.8714866638183594, val_loss: 0.851211428642273\n",
            "epoch: 20301, train_loss: 0.8713183999061584, val_loss: 0.8514341711997986\n",
            "epoch: 20401, train_loss: 0.8711510300636292, val_loss: 0.8513497710227966\n",
            "Saving model, epoch: 20448, train_loss: 0.8710753917694092, val_loss: 0.851183295249939\n",
            "Saving model, epoch: 20450, train_loss: 0.8710774779319763, val_loss: 0.8511192202568054\n",
            "Saving model, epoch: 20452, train_loss: 0.8710833191871643, val_loss: 0.851049542427063\n",
            "Saving model, epoch: 20454, train_loss: 0.8710849285125732, val_loss: 0.8510192632675171\n",
            "epoch: 20501, train_loss: 0.8709872364997864, val_loss: 0.8512553572654724\n",
            "epoch: 20601, train_loss: 0.8708283305168152, val_loss: 0.8511897325515747\n",
            "Saving model, epoch: 20662, train_loss: 0.8707339763641357, val_loss: 0.8510146141052246\n",
            "Saving model, epoch: 20664, train_loss: 0.8707351684570312, val_loss: 0.8509618043899536\n",
            "Saving model, epoch: 20666, train_loss: 0.8707380890846252, val_loss: 0.850908637046814\n",
            "Saving model, epoch: 20668, train_loss: 0.870737612247467, val_loss: 0.8508884906768799\n",
            "epoch: 20701, train_loss: 0.8706696629524231, val_loss: 0.8511188626289368\n",
            "epoch: 20801, train_loss: 0.8705160617828369, val_loss: 0.8510367274284363\n",
            "Saving model, epoch: 20838, train_loss: 0.8704650402069092, val_loss: 0.8508498072624207\n",
            "Saving model, epoch: 20840, train_loss: 0.870466947555542, val_loss: 0.8508005142211914\n",
            "Saving model, epoch: 20842, train_loss: 0.8704673051834106, val_loss: 0.8507707118988037\n",
            "epoch: 20901, train_loss: 0.8703649640083313, val_loss: 0.8509529829025269\n",
            "epoch: 21001, train_loss: 0.8702162504196167, val_loss: 0.850814938545227\n",
            "Saving model, epoch: 21005, train_loss: 0.870216429233551, val_loss: 0.8507195711135864\n",
            "Saving model, epoch: 21007, train_loss: 0.8702206611633301, val_loss: 0.8506542444229126\n",
            "Saving model, epoch: 21009, train_loss: 0.8702244758605957, val_loss: 0.8506096005439758\n",
            "epoch: 21101, train_loss: 0.8700735569000244, val_loss: 0.8508208990097046\n",
            "epoch: 21201, train_loss: 0.8699319958686829, val_loss: 0.8508308529853821\n",
            "Saving model, epoch: 21206, train_loss: 0.8699331283569336, val_loss: 0.8505547642707825\n",
            "Saving model, epoch: 21208, train_loss: 0.8699365258216858, val_loss: 0.850504994392395\n",
            "Saving model, epoch: 21210, train_loss: 0.8699343204498291, val_loss: 0.8504972457885742\n",
            "epoch: 21301, train_loss: 0.8697938919067383, val_loss: 0.8506826162338257\n",
            "Saving model, epoch: 21383, train_loss: 0.8696889877319336, val_loss: 0.850450336933136\n",
            "Saving model, epoch: 21385, train_loss: 0.8696920275688171, val_loss: 0.8503993153572083\n",
            "Saving model, epoch: 21387, train_loss: 0.869691789150238, val_loss: 0.8503799438476562\n",
            "epoch: 21401, train_loss: 0.8696582317352295, val_loss: 0.8505353331565857\n",
            "epoch: 21501, train_loss: 0.8695253133773804, val_loss: 0.8505523204803467\n",
            "Saving model, epoch: 21554, train_loss: 0.8694635033607483, val_loss: 0.8503357768058777\n",
            "Saving model, epoch: 21556, train_loss: 0.8694660663604736, val_loss: 0.8502905964851379\n",
            "Saving model, epoch: 21558, train_loss: 0.8694642186164856, val_loss: 0.8502827882766724\n",
            "epoch: 21601, train_loss: 0.869394838809967, val_loss: 0.8504898548126221\n",
            "epoch: 21701, train_loss: 0.869266927242279, val_loss: 0.8504318594932556\n",
            "Saving model, epoch: 21720, train_loss: 0.8692483305931091, val_loss: 0.8502642512321472\n",
            "Saving model, epoch: 21722, train_loss: 0.8692522644996643, val_loss: 0.850199818611145\n",
            "Saving model, epoch: 21724, train_loss: 0.8692570924758911, val_loss: 0.8501471877098083\n",
            "epoch: 21801, train_loss: 0.8691443204879761, val_loss: 0.8503653407096863\n",
            "epoch: 21901, train_loss: 0.8690218925476074, val_loss: 0.8503005504608154\n",
            "Saving model, epoch: 21917, train_loss: 0.8690091967582703, val_loss: 0.8501328229904175\n",
            "Saving model, epoch: 21919, train_loss: 0.8690129518508911, val_loss: 0.8500775694847107\n",
            "Saving model, epoch: 21921, train_loss: 0.8690146207809448, val_loss: 0.8500457406044006\n",
            "epoch: 22001, train_loss: 0.8689036965370178, val_loss: 0.8502497673034668\n",
            "Saving model, epoch: 22098, train_loss: 0.8688005208969116, val_loss: 0.8499883413314819\n",
            "Saving model, epoch: 22100, train_loss: 0.8688059449195862, val_loss: 0.8499318361282349\n",
            "epoch: 22101, train_loss: 0.8688062429428101, val_loss: 0.8505130410194397\n",
            "epoch: 22201, train_loss: 0.8686737418174744, val_loss: 0.8501434922218323\n",
            "Saving model, epoch: 22294, train_loss: 0.8685801029205322, val_loss: 0.8498826622962952\n",
            "Saving model, epoch: 22296, train_loss: 0.8685830235481262, val_loss: 0.849842369556427\n",
            "epoch: 22301, train_loss: 0.8685607314109802, val_loss: 0.8501308560371399\n",
            "epoch: 22401, train_loss: 0.8684524893760681, val_loss: 0.8500398993492126\n",
            "Saving model, epoch: 22471, train_loss: 0.8683843612670898, val_loss: 0.8498207330703735\n",
            "Saving model, epoch: 22473, train_loss: 0.8683897256851196, val_loss: 0.8497578501701355\n",
            "Saving model, epoch: 22475, train_loss: 0.8683921098709106, val_loss: 0.8497270345687866\n",
            "epoch: 22501, train_loss: 0.8683450818061829, val_loss: 0.8499460816383362\n",
            "epoch: 22601, train_loss: 0.8682414293289185, val_loss: 0.8499383926391602\n",
            "Saving model, epoch: 22665, train_loss: 0.8681857585906982, val_loss: 0.8496976494789124\n",
            "Saving model, epoch: 22667, train_loss: 0.8681882619857788, val_loss: 0.8496612906455994\n",
            "epoch: 22701, train_loss: 0.8681381344795227, val_loss: 0.8498696684837341\n",
            "epoch: 22801, train_loss: 0.868037760257721, val_loss: 0.8498430252075195\n",
            "Saving model, epoch: 22839, train_loss: 0.8680100440979004, val_loss: 0.8496197462081909\n",
            "Saving model, epoch: 22841, train_loss: 0.8680166006088257, val_loss: 0.8495563864707947\n",
            "Saving model, epoch: 22843, train_loss: 0.8680158853530884, val_loss: 0.8495480418205261\n",
            "epoch: 22901, train_loss: 0.8679400086402893, val_loss: 0.8498076796531677\n",
            "epoch: 23001, train_loss: 0.8678441643714905, val_loss: 0.8497549891471863\n",
            "Saving model, epoch: 23044, train_loss: 0.8678140044212341, val_loss: 0.8495230078697205\n",
            "Saving model, epoch: 23046, train_loss: 0.86781907081604, val_loss: 0.8494732975959778\n",
            "epoch: 23101, train_loss: 0.8677502274513245, val_loss: 0.8497118949890137\n",
            "epoch: 23201, train_loss: 0.8676577210426331, val_loss: 0.8496700525283813\n",
            "Saving model, epoch: 23232, train_loss: 0.8676429390907288, val_loss: 0.8494236469268799\n",
            "Saving model, epoch: 23234, train_loss: 0.8676461577415466, val_loss: 0.8493872880935669\n",
            "epoch: 23301, train_loss: 0.8675680756568909, val_loss: 0.8496183753013611\n",
            "epoch: 23401, train_loss: 0.8674787878990173, val_loss: 0.849581778049469\n",
            "Saving model, epoch: 23421, train_loss: 0.8674732446670532, val_loss: 0.8493618369102478\n",
            "Saving model, epoch: 23423, train_loss: 0.8674792647361755, val_loss: 0.8493043184280396\n",
            "epoch: 23501, train_loss: 0.867393434047699, val_loss: 0.849553644657135\n",
            "epoch: 23601, train_loss: 0.8673081398010254, val_loss: 0.8495110869407654\n",
            "Saving model, epoch: 23626, train_loss: 0.8672986030578613, val_loss: 0.8492847681045532\n",
            "Saving model, epoch: 23628, train_loss: 0.8673060536384583, val_loss: 0.8492190837860107\n",
            "Saving model, epoch: 23630, train_loss: 0.8673050403594971, val_loss: 0.8492140769958496\n",
            "epoch: 23701, train_loss: 0.8672263026237488, val_loss: 0.8494652509689331\n",
            "epoch: 23801, train_loss: 0.8671451807022095, val_loss: 0.8494378924369812\n",
            "Saving model, epoch: 23834, train_loss: 0.86713045835495, val_loss: 0.8492042422294617\n",
            "Saving model, epoch: 23836, train_loss: 0.86713045835495, val_loss: 0.8491914868354797\n",
            "epoch: 23901, train_loss: 0.8670653104782104, val_loss: 0.8494030833244324\n",
            "Saving model, epoch: 23992, train_loss: 0.8670018315315247, val_loss: 0.8491812944412231\n",
            "Saving model, epoch: 23994, train_loss: 0.8670080304145813, val_loss: 0.8491157293319702\n",
            "Saving model, epoch: 23996, train_loss: 0.8670112490653992, val_loss: 0.8490855097770691\n",
            "epoch: 24001, train_loss: 0.8669867515563965, val_loss: 0.8493089079856873\n",
            "epoch: 24101, train_loss: 0.8669115304946899, val_loss: 0.8493338227272034\n",
            "Saving model, epoch: 24197, train_loss: 0.8668575286865234, val_loss: 0.8490310907363892\n",
            "Saving model, epoch: 24199, train_loss: 0.8668575286865234, val_loss: 0.8490206599235535\n",
            "epoch: 24201, train_loss: 0.8668428659439087, val_loss: 0.8491241931915283\n",
            "epoch: 24301, train_loss: 0.866763710975647, val_loss: 0.8492665886878967\n",
            "Saving model, epoch: 24398, train_loss: 0.8667096495628357, val_loss: 0.8489842414855957\n",
            "epoch: 24401, train_loss: 0.8666988015174866, val_loss: 0.8494329452514648\n",
            "epoch: 24501, train_loss: 0.8666211366653442, val_loss: 0.8492075204849243\n",
            "Saving model, epoch: 24572, train_loss: 0.866582989692688, val_loss: 0.8489670753479004\n",
            "Saving model, epoch: 24574, train_loss: 0.8665894269943237, val_loss: 0.848909854888916\n",
            "epoch: 24601, train_loss: 0.8665521144866943, val_loss: 0.8492289781570435\n",
            "epoch: 24701, train_loss: 0.8664848208427429, val_loss: 0.849148154258728\n",
            "Saving model, epoch: 24770, train_loss: 0.8664520382881165, val_loss: 0.8488972783088684\n",
            "Saving model, epoch: 24772, train_loss: 0.8664544820785522, val_loss: 0.8488685488700867\n",
            "epoch: 24801, train_loss: 0.8664186000823975, val_loss: 0.8491743803024292\n",
            "epoch: 24901, train_loss: 0.8663533926010132, val_loss: 0.8490926027297974\n",
            "Saving model, epoch: 24958, train_loss: 0.866328775882721, val_loss: 0.8488532304763794\n",
            "Saving model, epoch: 24960, train_loss: 0.8663403391838074, val_loss: 0.8487673997879028\n",
            "Saving model, epoch: 24962, train_loss: 0.8663423657417297, val_loss: 0.8487497568130493\n",
            "epoch: 25001, train_loss: 0.8662900924682617, val_loss: 0.8490275144577026\n",
            "epoch: 25101, train_loss: 0.8662294149398804, val_loss: 0.8490421772003174\n",
            "epoch: 25201, train_loss: 0.8661676049232483, val_loss: 0.8489957451820374\n",
            "Saving model, epoch: 25215, train_loss: 0.8661820292472839, val_loss: 0.8487173318862915\n",
            "epoch: 25301, train_loss: 0.8661091327667236, val_loss: 0.8489955067634583\n",
            "epoch: 25401, train_loss: 0.8660497665405273, val_loss: 0.8489668965339661\n",
            "Saving model, epoch: 25431, train_loss: 0.866053581237793, val_loss: 0.8486751317977905\n",
            "epoch: 25501, train_loss: 0.8659928441047668, val_loss: 0.8489429354667664\n",
            "epoch: 25601, train_loss: 0.8659360408782959, val_loss: 0.848923921585083\n",
            "Saving model, epoch: 25642, train_loss: 0.8659301996231079, val_loss: 0.8486546277999878\n",
            "Saving model, epoch: 25644, train_loss: 0.8659334182739258, val_loss: 0.8486266136169434\n",
            "epoch: 25701, train_loss: 0.8658812046051025, val_loss: 0.8489037156105042\n",
            "epoch: 25801, train_loss: 0.8658267259597778, val_loss: 0.8488796353340149\n",
            "Saving model, epoch: 25843, train_loss: 0.8658205270767212, val_loss: 0.8486160635948181\n",
            "epoch: 25901, train_loss: 0.8657734990119934, val_loss: 0.8488532900810242\n",
            "epoch: 26001, train_loss: 0.8657208681106567, val_loss: 0.8488423228263855\n",
            "Saving model, epoch: 26026, train_loss: 0.8657241463661194, val_loss: 0.8485800623893738\n",
            "Saving model, epoch: 26028, train_loss: 0.8657296895980835, val_loss: 0.8485386967658997\n",
            "epoch: 26101, train_loss: 0.8656702041625977, val_loss: 0.8488141894340515\n",
            "epoch: 26201, train_loss: 0.8656197786331177, val_loss: 0.8487985134124756\n",
            "epoch: 26301, train_loss: 0.8655703663825989, val_loss: 0.8487783670425415\n",
            "finished training after 26329 epochs\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                    [-1, 1]              21\n",
            "================================================================\n",
            "Total params: 21\n",
            "Trainable params: 21\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "config: {'INPUT_DIM': 14, 'TRAIN_PATH': 'covid.train.csv', 'TEST_PATH': 'covid.test.csv', 'MODEL_PATH': 'models/model.pth', 'PRED_PATH': 'pred.csv', 'EPOCH_NUM': 30000, 'BATCH_SIZE': 4096, 'VAL_RATIO': 0.1, 'OPTIMIZER': 'Adam', 'OPTIM_PARAMS': {'lr': 0.05}, 'DECAY_RATE': 0.999, 'MIN_LR': 0.0001, 'EARLY_STOP': 300, 'MODEL_NUM': 20}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsFxYmKkF3eN",
        "outputId": "b1153fa4-7df5-491f-a138-91c931c0ccac"
      },
      "source": [
        "model = emssembler.model\n",
        "for param in model.parameters():\n",
        "  print(param.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.5831, -0.1135,  0.1467,  0.0612,  0.0638,  0.2162, -0.1018,  0.0530,\n",
            "          0.1253, -0.3116, -0.0709,  0.0207, -0.1050, -0.1909,  0.2175,  0.1779,\n",
            "          0.1721, -0.1760,  0.2850, -0.0524]], device='cuda:0')\n",
            "tensor([-0.0041], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}