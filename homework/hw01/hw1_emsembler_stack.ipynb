{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw1_emsembler_stack.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wS_4-77xHk44",
        "g0pdrhQAO41L",
        "aQikz3IPiyPf",
        "nfrVxqJanGpE",
        "9tmCwXgpot3t"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chuuuuu/machine_learning_2021/blob/main/homework/hw01/hw1_emsembler_stack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz0_QVkxCrX3"
      },
      "source": [
        "# **Homework 1: COVID-19 Cases Prediction (Regression)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMj55YDKG6ch",
        "outputId": "17a37224-9c7b-4503-c979-ac26854354c1"
      },
      "source": [
        "from os import path, makedirs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "WORKSPACE = 'drive/MyDrive/ColabNotebooks/HW1'\n",
        "\n",
        "if not path.exists(f'{WORKSPACE}/dataset'):\n",
        "  makedirs(f'{WORKSPACE}/dataset')\n",
        "  !gdown --id '19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF' --output '{WORKSPACE}/dataset/covid.train.csv'\n",
        "  !gdown --id '1CE240jLm2npU-tdz81-oVKEF3T2yfT1O' --output '{WORKSPACE}/dataset/covid.test.csv'\n",
        "\n",
        "DATA_PATH = f'{WORKSPACE}/dataset'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF\n",
            "To: /content/covid.train.csv\n",
            "100% 2.00M/2.00M [00:00<00:00, 7.58MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CE240jLm2npU-tdz81-oVKEF3T2yfT1O\n",
            "To: /content/covid.test.csv\n",
            "100% 651k/651k [00:00<00:00, 93.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF-QTQLkv5h8"
      },
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# set a random seed for reproducibility\n",
        "myseed = 42069\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)\n",
        "\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "config = {\n",
        "    'INPUT_DIM': 14,\n",
        "    'TRAIN_PATH': f'{DATA_PATH}/covid.train.csv',\n",
        "    'TEST_PATH': f'{DATA_PATH}/covid.test.csv',\n",
        "    'MODEL_PATH': 'models/model.pth',\n",
        "    'PRED_PATH': 'pred.csv',\n",
        "\n",
        "    'EPOCH_NUM': 30000,\n",
        "    'BATCH_SIZE': 4096,\n",
        "    'VAL_RATIO': 0.1,\n",
        "    'OPTIMIZER': 'Adam',\n",
        "    'OPTIM_PARAMS': {\n",
        "        'lr': 5e-2,\n",
        "        # 'weight_decay': 5e-3,\n",
        "    },\n",
        "    'DECAY_RATE': 0.999,\n",
        "    'MIN_LR': 1e-4,\n",
        "    'EARLY_STOP': 300,\n",
        "    'MODEL_NUM': 3,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaRGyPYGvvV5"
      },
      "source": [
        "class Drawer():\n",
        "    def plot_learning_curve(self, loss_record, title=''):\n",
        "        ''' Plot learning curve of your DNN (train & dev loss) '''\n",
        "        total_steps = len(loss_record['train'])\n",
        "        x_1 = range(total_steps)\n",
        "        x_2 = x_1[::len(loss_record['train']) // len(loss_record['val'])]\n",
        "        figure(figsize=(6, 4))\n",
        "        plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
        "        plt.plot(x_2, loss_record['val'], c='tab:cyan', label='val')\n",
        "        plt.ylim(0.0, 5.)\n",
        "        plt.xlabel('Training steps')\n",
        "        plt.ylabel('MSE loss')\n",
        "        plt.title('Learning curve of {}'.format(title))\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_pred(self, dv_set, model, device, lim=35., preds=None, targets=None):\n",
        "        ''' Plot prediction of your DNN '''\n",
        "        if preds is None or targets is None:\n",
        "            model.eval()\n",
        "            preds, targets = [], []\n",
        "            for x, y in dv_set:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                with torch.no_grad():\n",
        "                    pred = model(x)\n",
        "                    preds.append(pred.detach().cpu())\n",
        "                    targets.append(y.detach().cpu())\n",
        "            preds = torch.cat(preds, dim=0).numpy()\n",
        "            targets = torch.cat(targets, dim=0).numpy()\n",
        "\n",
        "        figure(figsize=(5, 5))\n",
        "        plt.scatter(targets, preds, c='r', alpha=0.5)\n",
        "        plt.plot([-0.2, lim], [-0.2, lim], c='b')\n",
        "        plt.xlim(-0.2, lim)\n",
        "        plt.ylim(-0.2, lim)\n",
        "        plt.xlabel('ground truth value')\n",
        "        plt.ylabel('predicted value')\n",
        "        plt.title('Ground Truth v.s. Prediction')\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uazKiUCwD9U"
      },
      "source": [
        "from sklearn.feature_selection import f_regression, SelectKBest, mutual_info_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "class DataManager():\n",
        "    def __init__(self):\n",
        "        print('init data manager...')\n",
        "        TRAIN_PATH = config['TRAIN_PATH']\n",
        "        INPUT_DIM = config['INPUT_DIM']\n",
        "\n",
        "        self.state = 1\n",
        "\n",
        "        with open(TRAIN_PATH, 'r') as f:\n",
        "            self.data = list(csv.reader(f))\n",
        "            self.data = np.array(self.data[1:])[:, 1:].astype(np.float32)\n",
        "\n",
        "        self.X = self.data[:, :-1]\n",
        "        self.y = self.data[:, -1]\n",
        "\n",
        "        selector = SelectKBest(f_regression, k=INPUT_DIM)\n",
        "        selector.fit(self.X, self.y)\n",
        "        self.cols = selector.get_support(indices=True)\n",
        "\n",
        "        self.X = self.X[:, self.cols]\n",
        "\n",
        "    def get_train_data(self):\n",
        "        print('getting train data...')\n",
        "        VAL_RATIO = config['VAL_RATIO']\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(self.X, self.y, test_size=VAL_RATIO, random_state=self.state)\n",
        "        self.state += 1\n",
        "\n",
        "        return X_train, X_val, y_train, y_val\n",
        "    \n",
        "    def get_test_data(self):\n",
        "        print('getting test data...')\n",
        "        TEST_PATH = config['TEST_PATH']\n",
        "        with open(TEST_PATH, 'r') as f:\n",
        "            data = list(csv.reader(f))\n",
        "            data = np.array(data[1:])[:, 1:].astype(np.float32)\n",
        "        X_test = data[:, self.cols]\n",
        "        return X_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3N7YzC72Ykp"
      },
      "source": [
        "class CovidDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        print('init dataset...')\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        if y is None:\n",
        "            self.y = None\n",
        "        else:\n",
        "            self.y = torch.from_numpy(y).float()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is None:\n",
        "            return self.X[idx]\n",
        "\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGJ7r5sQ4bTU"
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        print('init neural net...')\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        INPUT_DIM = config['INPUT_DIM']\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.BatchNorm1d(INPUT_DIM),\n",
        "            nn.Linear(INPUT_DIM, 64),\n",
        "            nn.ReLU(),\n",
        "            # nn.BatchNorm1d(64),\n",
        "            # nn.Linear(128, 128),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Linear(32, 16),\n",
        "            # nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "        )\n",
        "\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "    def get_loss(self, y_pred, y):\n",
        "        return self.criterion(y_pred, y)\n",
        "\n",
        "    def summary(self):\n",
        "        INPUT_DIM = config['INPUT_DIM']\n",
        "        summary(self, (INPUT_DIM, ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxNvo1sv5Qyb"
      },
      "source": [
        "class Trainer():\n",
        "    def __init__(self):\n",
        "        print('init trainer')\n",
        "        self.set_device()\n",
        "        self.set_model()\n",
        "        self.set_data_loader()\n",
        "        self.set_drawer()\n",
        "        self.set_optim()\n",
        "\n",
        "        self.loss_record = {'train': [], 'val': []}\n",
        "\n",
        "    def set_drawer(self):\n",
        "        self.drawer = Drawer()\n",
        "\n",
        "    def draw_learning_curve(self):\n",
        "        self.drawer.plot_learning_curve(self.loss_record, title='deep model')\n",
        "\n",
        "    def draw_val_results(self):\n",
        "        MODEL_PATH = config['MODEL_PATH']\n",
        "        del self.model\n",
        "        self.set_model()\n",
        "\n",
        "        ckpt = torch.load(MODEL_PATH, map_location='cpu')\n",
        "        self.model.load_state_dict(ckpt)\n",
        "        self.drawer.plot_pred(self.val_loader, self.model, self.device)\n",
        "\n",
        "    def pred_y_test(self):\n",
        "        print('predicting...')\n",
        "        BATCH_SIZE = config['BATCH_SIZE']\n",
        "        PRED_PATH = config['PRED_PATH']\n",
        "\n",
        "        X_test = self.dataManager.get_test_data()\n",
        "        test_loader = DataLoader(X_test, BATCH_SIZE, False, drop_last=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "        self.model.eval()\n",
        "        y_preds = []\n",
        "        for x in test_loader:\n",
        "            x = x.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                y_pred = self.model(x)\n",
        "                y_preds.append(y_pred.detach().cpu())\n",
        "        \n",
        "        y_preds = torch.cat(y_preds, dim=0).numpy()\n",
        "        return y_preds\n",
        "\n",
        "    def set_device(self):\n",
        "        ''' Get device (if GPU is available, use GPU) '''\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def set_data_loader(self):\n",
        "        BATCH_SIZE = config['BATCH_SIZE']\n",
        "        self.dataManager = DataManager()\n",
        "        X_train, X_val, y_train, y_val = self.dataManager.get_train_data()   \n",
        "\n",
        "        train_set = CovidDataset(X_train, y_train)\n",
        "        self.train_loader = DataLoader(train_set, BATCH_SIZE, True, drop_last=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "        val_set = CovidDataset(X_val, y_val)\n",
        "        self.val_loader = DataLoader(val_set, BATCH_SIZE, False, drop_last=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "    def set_model(self):\n",
        "        self.model = NeuralNet().to(self.device)\n",
        "\n",
        "    def set_optim(self):\n",
        "        OPTIMIZER = config['OPTIMIZER']\n",
        "        OPTIM_PARAMS = config['OPTIM_PARAMS']\n",
        "\n",
        "        self.optimizer = getattr(torch.optim, OPTIMIZER)(self.model.parameters(), **OPTIM_PARAMS)\n",
        "\n",
        "    def train(self):\n",
        "        EPOCH_NUM = config['EPOCH_NUM']\n",
        "        MODEL_PATH = config['MODEL_PATH']\n",
        "        EARLY_STOP = config['EARLY_STOP']\n",
        "\n",
        "        min_val_loss = float('inf')\n",
        "        early_stop_count = 0\n",
        "        for epoch in range(EPOCH_NUM):\n",
        "            self.model.train()\n",
        "            for x, y in self.train_loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                x, y = x.to(self.device), y.to(self.device)\n",
        "                y_pred = self.model(x)\n",
        "                loss = self.model.get_loss(y_pred, y)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            self.update_lr()\n",
        "            val_loss = self.get_loss(self.val_loader)\n",
        "            train_loss = self.get_loss(self.train_loader)\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'epoch: {epoch+1}, train_loss: {train_loss}, val_loss: {val_loss}')\n",
        "\n",
        "            if val_loss < min_val_loss:\n",
        "                min_val_loss = val_loss\n",
        "                print(f'Saving model, epoch: {epoch+1}, train_loss: {train_loss}, val_loss: {val_loss}')\n",
        "                torch.save(self.model.state_dict(), MODEL_PATH)\n",
        "                early_stop_cnt = 0\n",
        "\n",
        "            else:\n",
        "                early_stop_cnt += 1\n",
        "\n",
        "            self.loss_record['val'].append(val_loss)\n",
        "            self.loss_record['train'].append(train_loss)\n",
        "\n",
        "            if early_stop_cnt > EARLY_STOP:\n",
        "                break\n",
        "\n",
        "        # print(f'Saving model, epoch: {epoch+1}, train_loss: {train_loss}, val_loss: {val_loss}')\n",
        "        # torch.save(self.model.state_dict(), MODEL_PATH)\n",
        "        \n",
        "        print(f'finished training after {epoch+1} epochs')\n",
        "        self.model.summary()\n",
        "\n",
        "    \n",
        "    def update_lr(self):\n",
        "        DECAY_RATE = config['DECAY_RATE']\n",
        "        MIN_LR = config['MIN_LR']\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = param_group['lr'] * DECAY_RATE\n",
        "            param_group['lr'] = max(MIN_LR, param_group['lr'])\n",
        "\n",
        "    def get_loss(self, loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(self.device), y.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                y_pred = self.model(x)\n",
        "                loss = self.model.get_loss(y_pred, y)\n",
        "            \n",
        "            total_loss += loss.detach().cpu().item() * len(x)\n",
        "        total_loss /= len(loader.dataset)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def get_y_pred(self, loader):\n",
        "        self.model.eval()\n",
        "        y_preds = []\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(self.device), y.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                y_pred = self.model(x)\n",
        "                y_preds.append(y_pred.detach().cpu())\n",
        "\n",
        "        y_preds = torch.cat(y_preds, dim=0).numpy()\n",
        "        return y_preds\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikKiqeHNUTUK"
      },
      "source": [
        "class Emssembler(Trainer):\n",
        "    def __init__(self):\n",
        "        MODEL_NUM = config['MODEL_NUM']\n",
        "        self.trainers = []\n",
        "        for i in range(MODEL_NUM):\n",
        "            self.trainers.append(Trainer())\n",
        "        \n",
        "        self.trainModels()\n",
        "        super(Emssembler, self).__init__()\n",
        "\n",
        "    def set_model(self):\n",
        "        self.model = MetaLearner().to(self.device)\n",
        "\n",
        "    def set_data_loader(self):\n",
        "        BATCH_SIZE = config['BATCH_SIZE']\n",
        "        MODEL_NUM = config['MODEL_NUM']\n",
        "        self.dataManager = DataManager()\n",
        "        X_train, X_val, y_train, y_val = self.dataManager.get_train_data()   \n",
        "        train_set = CovidDataset(X_train, y_train)\n",
        "        train_loader = DataLoader(train_set, BATCH_SIZE, False, drop_last=False, num_workers=0, pin_memory=True)\n",
        "        val_set = CovidDataset(X_val, y_val)\n",
        "        val_loader = DataLoader(val_set, BATCH_SIZE, False, drop_last=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "        meta_X_train = None\n",
        "        meta_X_val = None\n",
        "        for i in range(MODEL_NUM):\n",
        "            train_y_pred = self.trainers[i].get_y_pred(train_loader)\n",
        "            train_y_pred = np.reshape(train_y_pred, (train_y_pred.shape[0], 1))\n",
        "            if meta_X_train is None:\n",
        "                meta_X_train = train_y_pred\n",
        "            else:\n",
        "                meta_X_train = np.concatenate((meta_X_train, train_y_pred), axis=1)\n",
        "\n",
        "            val_y_pred = self.trainers[i].get_y_pred(val_loader)\n",
        "            val_y_pred = np.reshape(val_y_pred, (val_y_pred.shape[0], 1))\n",
        "            if meta_X_val is None:\n",
        "                meta_X_val = val_y_pred\n",
        "            else:\n",
        "                meta_X_val = np.concatenate((meta_X_val, val_y_pred), axis=1)\n",
        "\n",
        "        meta_train_set = CovidDataset(meta_X_train, y_train)\n",
        "        meta_val_set = CovidDataset(meta_X_val, y_val)\n",
        "        self.train_loader = DataLoader(meta_train_set, BATCH_SIZE, True, drop_last=False, num_workers=0, pin_memory=True)\n",
        "        self.val_loader = DataLoader(meta_val_set, BATCH_SIZE, False, drop_last=False, num_workers=0, pin_memory=True)\n",
        "        \n",
        "    def trainModels(self):\n",
        "        MODEL_NUM = config['MODEL_NUM']\n",
        "        for trainer in self.trainers:\n",
        "            trainer.train()\n",
        "\n",
        "    def pred_y_test(self):\n",
        "        MODEL_NUM = config['MODEL_NUM']\n",
        "        PRED_PATH = config['PRED_PATH']\n",
        "        BATCH_SIZE = config['BATCH_SIZE']\n",
        "        \n",
        "        # y_preds = None\n",
        "        meta_X_test = None\n",
        "        for trainer in self.trainers:\n",
        "            test_y_pred = trainer.pred_y_test()\n",
        "            test_y_pred = np.reshape(test_y_pred, (test_y_pred.shape[0], 1))\n",
        "            if meta_X_test is None:\n",
        "                meta_X_test = test_y_pred\n",
        "            else:\n",
        "                meta_X_test = np.concatenate((meta_X_test, test_y_pred), axis=1)\n",
        "\n",
        "        test_loader = DataLoader(meta_X_test, BATCH_SIZE, False, drop_last=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "        self.model.eval()\n",
        "        meta_y_preds = []\n",
        "        for x in test_loader:\n",
        "            x = x.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                meta_y_pred = self.model(x)\n",
        "                meta_y_preds.append(meta_y_pred.detach().cpu())\n",
        "        \n",
        "        meta_y_preds = torch.cat(meta_y_preds, dim=0).numpy()\n",
        "\n",
        "        with open(PRED_PATH, 'w') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['id', 'tested_positive'])\n",
        "            for i, p in enumerate(meta_y_preds):\n",
        "                writer.writerow([i, p])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au2n-Y2SbYzN"
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "class MetaLearner(nn.Module):\n",
        "    def __init__(self):\n",
        "        print('init neural net...')\n",
        "        super(MetaLearner, self).__init__()\n",
        "\n",
        "        MODEL_NUM = config['MODEL_NUM']\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(MODEL_NUM, 1),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Linear(16, 1),\n",
        "        )\n",
        "\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "    def get_loss(self, y_pred, y):\n",
        "        return self.criterion(y_pred, y)\n",
        "\n",
        "    def summary(self):\n",
        "        MODEL_NUM = config['MODEL_NUM']\n",
        "        summary(self, (MODEL_NUM, ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMBMUOu0ng0Y",
        "outputId": "f4351af3-53fe-4dfe-a36a-aa46d12fb029"
      },
      "source": [
        "# trainer = Trainer()\n",
        "# trainer.train()\n",
        "# trainer.draw_learning_curve()\n",
        "# trainer.draw_val_results()\n",
        "# trainer.pred_y_test()\n",
        "emssembler = Emssembler()\n",
        "emssembler.train()\n",
        "emssembler.pred_y_test()\n",
        "print(f'config: {config}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Saving model, epoch: 16731, train_loss: 0.9022015929222107, val_loss: 0.8746574521064758\n",
            "Saving model, epoch: 16732, train_loss: 0.9022002816200256, val_loss: 0.8746563792228699\n",
            "Saving model, epoch: 16733, train_loss: 0.9021990895271301, val_loss: 0.8746554255485535\n",
            "Saving model, epoch: 16734, train_loss: 0.9021977782249451, val_loss: 0.8746544122695923\n",
            "Saving model, epoch: 16735, train_loss: 0.9021965861320496, val_loss: 0.8746535778045654\n",
            "Saving model, epoch: 16736, train_loss: 0.9021952748298645, val_loss: 0.8746525645256042\n",
            "Saving model, epoch: 16737, train_loss: 0.9021940231323242, val_loss: 0.8746516108512878\n",
            "Saving model, epoch: 16738, train_loss: 0.9021927714347839, val_loss: 0.8746504783630371\n",
            "Saving model, epoch: 16739, train_loss: 0.9021915197372437, val_loss: 0.874649703502655\n",
            "Saving model, epoch: 16740, train_loss: 0.9021902084350586, val_loss: 0.8746485710144043\n",
            "Saving model, epoch: 16741, train_loss: 0.9021891355514526, val_loss: 0.8746480345726013\n",
            "Saving model, epoch: 16742, train_loss: 0.9021878242492676, val_loss: 0.8746471405029297\n",
            "Saving model, epoch: 16743, train_loss: 0.9021865129470825, val_loss: 0.8746458292007446\n",
            "Saving model, epoch: 16744, train_loss: 0.9021852016448975, val_loss: 0.874644935131073\n",
            "Saving model, epoch: 16745, train_loss: 0.902184009552002, val_loss: 0.8746440410614014\n",
            "Saving model, epoch: 16746, train_loss: 0.9021826982498169, val_loss: 0.8746431469917297\n",
            "Saving model, epoch: 16747, train_loss: 0.9021815061569214, val_loss: 0.8746422529220581\n",
            "Saving model, epoch: 16748, train_loss: 0.9021801948547363, val_loss: 0.8746412992477417\n",
            "Saving model, epoch: 16749, train_loss: 0.9021790027618408, val_loss: 0.8746403455734253\n",
            "Saving model, epoch: 16750, train_loss: 0.9021776914596558, val_loss: 0.8746395111083984\n",
            "Saving model, epoch: 16751, train_loss: 0.9021764397621155, val_loss: 0.8746383190155029\n",
            "Saving model, epoch: 16752, train_loss: 0.9021751880645752, val_loss: 0.8746374845504761\n",
            "Saving model, epoch: 16753, train_loss: 0.9021738767623901, val_loss: 0.8746362328529358\n",
            "Saving model, epoch: 16754, train_loss: 0.9021727442741394, val_loss: 0.8746354579925537\n",
            "Saving model, epoch: 16755, train_loss: 0.9021714329719543, val_loss: 0.8746346235275269\n",
            "Saving model, epoch: 16756, train_loss: 0.9021701216697693, val_loss: 0.8746335506439209\n",
            "Saving model, epoch: 16757, train_loss: 0.9021689295768738, val_loss: 0.8746324777603149\n",
            "Saving model, epoch: 16758, train_loss: 0.9021674990653992, val_loss: 0.8746317625045776\n",
            "Saving model, epoch: 16759, train_loss: 0.9021663069725037, val_loss: 0.8746307492256165\n",
            "Saving model, epoch: 16760, train_loss: 0.9021649956703186, val_loss: 0.8746299147605896\n",
            "Saving model, epoch: 16761, train_loss: 0.9021639227867126, val_loss: 0.8746289610862732\n",
            "Saving model, epoch: 16762, train_loss: 0.9021626114845276, val_loss: 0.8746278882026672\n",
            "Saving model, epoch: 16763, train_loss: 0.9021613001823425, val_loss: 0.8746269941329956\n",
            "Saving model, epoch: 16764, train_loss: 0.902160108089447, val_loss: 0.8746260404586792\n",
            "Saving model, epoch: 16765, train_loss: 0.902158796787262, val_loss: 0.8746250867843628\n",
            "Saving model, epoch: 16766, train_loss: 0.9021574854850769, val_loss: 0.8746243715286255\n",
            "Saving model, epoch: 16767, train_loss: 0.9021562933921814, val_loss: 0.8746230602264404\n",
            "Saving model, epoch: 16768, train_loss: 0.9021548628807068, val_loss: 0.8746222853660583\n",
            "Saving model, epoch: 16769, train_loss: 0.9021536707878113, val_loss: 0.8746212720870972\n",
            "Saving model, epoch: 16770, train_loss: 0.9021524786949158, val_loss: 0.8746203184127808\n",
            "Saving model, epoch: 16771, train_loss: 0.9021512269973755, val_loss: 0.8746195435523987\n",
            "Saving model, epoch: 16772, train_loss: 0.9021498560905457, val_loss: 0.874618411064148\n",
            "Saving model, epoch: 16773, train_loss: 0.9021486639976501, val_loss: 0.8746173977851868\n",
            "Saving model, epoch: 16774, train_loss: 0.9021473526954651, val_loss: 0.8746166229248047\n",
            "Saving model, epoch: 16775, train_loss: 0.90214604139328, val_loss: 0.8746154308319092\n",
            "Saving model, epoch: 16776, train_loss: 0.9021447896957397, val_loss: 0.8746148943901062\n",
            "Saving model, epoch: 16777, train_loss: 0.9021435976028442, val_loss: 0.8746136426925659\n",
            "Saving model, epoch: 16778, train_loss: 0.9021422863006592, val_loss: 0.8746127486228943\n",
            "Saving model, epoch: 16779, train_loss: 0.9021409749984741, val_loss: 0.8746117353439331\n",
            "Saving model, epoch: 16780, train_loss: 0.9021397829055786, val_loss: 0.8746107816696167\n",
            "Saving model, epoch: 16781, train_loss: 0.9021385908126831, val_loss: 0.8746101260185242\n",
            "Saving model, epoch: 16782, train_loss: 0.9021371603012085, val_loss: 0.874609112739563\n",
            "Saving model, epoch: 16783, train_loss: 0.902135968208313, val_loss: 0.8746080994606018\n",
            "Saving model, epoch: 16784, train_loss: 0.9021346569061279, val_loss: 0.8746070861816406\n",
            "Saving model, epoch: 16785, train_loss: 0.9021334648132324, val_loss: 0.8746064305305481\n",
            "Saving model, epoch: 16786, train_loss: 0.9021321535110474, val_loss: 0.8746054172515869\n",
            "Saving model, epoch: 16787, train_loss: 0.9021309614181519, val_loss: 0.8746042847633362\n",
            "Saving model, epoch: 16788, train_loss: 0.9021296501159668, val_loss: 0.8746035099029541\n",
            "Saving model, epoch: 16789, train_loss: 0.9021283388137817, val_loss: 0.8746022582054138\n",
            "Saving model, epoch: 16790, train_loss: 0.9021271467208862, val_loss: 0.874601423740387\n",
            "Saving model, epoch: 16791, train_loss: 0.9021257162094116, val_loss: 0.8746006488800049\n",
            "Saving model, epoch: 16792, train_loss: 0.9021247029304504, val_loss: 0.8745993971824646\n",
            "Saving model, epoch: 16793, train_loss: 0.9021233320236206, val_loss: 0.8745986223220825\n",
            "Saving model, epoch: 16794, train_loss: 0.902121901512146, val_loss: 0.8745974898338318\n",
            "Saving model, epoch: 16795, train_loss: 0.9021207094192505, val_loss: 0.8745965361595154\n",
            "Saving model, epoch: 16796, train_loss: 0.902119517326355, val_loss: 0.8745957016944885\n",
            "Saving model, epoch: 16797, train_loss: 0.9021182060241699, val_loss: 0.8745946288108826\n",
            "Saving model, epoch: 16798, train_loss: 0.9021168947219849, val_loss: 0.8745936155319214\n",
            "Saving model, epoch: 16799, train_loss: 0.9021157026290894, val_loss: 0.8745927214622498\n",
            "Saving model, epoch: 16800, train_loss: 0.9021143913269043, val_loss: 0.8745917677879333\n",
            "epoch: 16801, train_loss: 0.9021130800247192, val_loss: 0.8745908141136169\n",
            "Saving model, epoch: 16801, train_loss: 0.9021130800247192, val_loss: 0.8745908141136169\n",
            "Saving model, epoch: 16802, train_loss: 0.9021118879318237, val_loss: 0.8745899200439453\n",
            "Saving model, epoch: 16803, train_loss: 0.9021104574203491, val_loss: 0.8745888471603394\n",
            "Saving model, epoch: 16804, train_loss: 0.9021092653274536, val_loss: 0.8745879530906677\n",
            "Saving model, epoch: 16805, train_loss: 0.9021080732345581, val_loss: 0.8745869994163513\n",
            "Saving model, epoch: 16806, train_loss: 0.9021068215370178, val_loss: 0.8745862245559692\n",
            "Saving model, epoch: 16807, train_loss: 0.902105450630188, val_loss: 0.8745852708816528\n",
            "Saving model, epoch: 16808, train_loss: 0.9021042585372925, val_loss: 0.8745842576026917\n",
            "Saving model, epoch: 16809, train_loss: 0.9021029472351074, val_loss: 0.8745831251144409\n",
            "Saving model, epoch: 16810, train_loss: 0.9021016955375671, val_loss: 0.8745822310447693\n",
            "Saving model, epoch: 16811, train_loss: 0.9021002054214478, val_loss: 0.8745811581611633\n",
            "Saving model, epoch: 16812, train_loss: 0.9020991325378418, val_loss: 0.8745803236961365\n",
            "Saving model, epoch: 16813, train_loss: 0.9020978212356567, val_loss: 0.8745793700218201\n",
            "Saving model, epoch: 16814, train_loss: 0.9020965099334717, val_loss: 0.874578595161438\n",
            "Saving model, epoch: 16815, train_loss: 0.902095377445221, val_loss: 0.8745774030685425\n",
            "Saving model, epoch: 16816, train_loss: 0.9020940661430359, val_loss: 0.8745763897895813\n",
            "Saving model, epoch: 16817, train_loss: 0.9020927548408508, val_loss: 0.8745755553245544\n",
            "Saving model, epoch: 16818, train_loss: 0.9020914435386658, val_loss: 0.8745746612548828\n",
            "Saving model, epoch: 16819, train_loss: 0.9020901918411255, val_loss: 0.8745738863945007\n",
            "Saving model, epoch: 16820, train_loss: 0.9020889401435852, val_loss: 0.8745726346969604\n",
            "Saving model, epoch: 16821, train_loss: 0.9020877480506897, val_loss: 0.8745716214179993\n",
            "Saving model, epoch: 16822, train_loss: 0.9020863175392151, val_loss: 0.8745707869529724\n",
            "Saving model, epoch: 16823, train_loss: 0.9020851254463196, val_loss: 0.8745698928833008\n",
            "Saving model, epoch: 16824, train_loss: 0.9020837545394897, val_loss: 0.8745686411857605\n",
            "Saving model, epoch: 16825, train_loss: 0.9020825028419495, val_loss: 0.874567985534668\n",
            "Saving model, epoch: 16826, train_loss: 0.9020812511444092, val_loss: 0.8745667934417725\n",
            "Saving model, epoch: 16827, train_loss: 0.9020799994468689, val_loss: 0.874565839767456\n",
            "Saving model, epoch: 16828, train_loss: 0.9020786881446838, val_loss: 0.8745648860931396\n",
            "Saving model, epoch: 16829, train_loss: 0.9020774960517883, val_loss: 0.8745640516281128\n",
            "Saving model, epoch: 16830, train_loss: 0.9020763039588928, val_loss: 0.8745632171630859\n",
            "Saving model, epoch: 16831, train_loss: 0.9020748734474182, val_loss: 0.8745622038841248\n",
            "Saving model, epoch: 16832, train_loss: 0.9020736813545227, val_loss: 0.8745611906051636\n",
            "Saving model, epoch: 16833, train_loss: 0.9020722508430481, val_loss: 0.8745603561401367\n",
            "Saving model, epoch: 16834, train_loss: 0.9020710587501526, val_loss: 0.8745591044425964\n",
            "Saving model, epoch: 16835, train_loss: 0.9020697474479675, val_loss: 0.8745582103729248\n",
            "Saving model, epoch: 16836, train_loss: 0.9020683765411377, val_loss: 0.8745574951171875\n",
            "Saving model, epoch: 16837, train_loss: 0.902067244052887, val_loss: 0.8745564818382263\n",
            "Saving model, epoch: 16838, train_loss: 0.9020659327507019, val_loss: 0.8745553493499756\n",
            "Saving model, epoch: 16839, train_loss: 0.9020647406578064, val_loss: 0.874554455280304\n",
            "Saving model, epoch: 16840, train_loss: 0.9020634293556213, val_loss: 0.874553382396698\n",
            "Saving model, epoch: 16841, train_loss: 0.9020621180534363, val_loss: 0.8745526671409607\n",
            "Saving model, epoch: 16842, train_loss: 0.9020608067512512, val_loss: 0.8745517134666443\n",
            "Saving model, epoch: 16843, train_loss: 0.9020596146583557, val_loss: 0.874550461769104\n",
            "Saving model, epoch: 16844, train_loss: 0.9020582437515259, val_loss: 0.8745496869087219\n",
            "Saving model, epoch: 16845, train_loss: 0.9020569920539856, val_loss: 0.8745484352111816\n",
            "Saving model, epoch: 16846, train_loss: 0.9020556807518005, val_loss: 0.8745477795600891\n",
            "Saving model, epoch: 16847, train_loss: 0.9020543694496155, val_loss: 0.8745468854904175\n",
            "Saving model, epoch: 16848, train_loss: 0.90205317735672, val_loss: 0.8745459914207458\n",
            "Saving model, epoch: 16849, train_loss: 0.9020518660545349, val_loss: 0.8745449185371399\n",
            "Saving model, epoch: 16850, train_loss: 0.9020505547523499, val_loss: 0.8745439052581787\n",
            "Saving model, epoch: 16851, train_loss: 0.9020492434501648, val_loss: 0.8745431900024414\n",
            "Saving model, epoch: 16852, train_loss: 0.9020480513572693, val_loss: 0.8745418787002563\n",
            "Saving model, epoch: 16853, train_loss: 0.9020467400550842, val_loss: 0.8745409846305847\n",
            "Saving model, epoch: 16854, train_loss: 0.9020454287528992, val_loss: 0.8745400905609131\n",
            "Saving model, epoch: 16855, train_loss: 0.9020441770553589, val_loss: 0.8745389580726624\n",
            "Saving model, epoch: 16856, train_loss: 0.9020428657531738, val_loss: 0.8745381236076355\n",
            "Saving model, epoch: 16857, train_loss: 0.9020416140556335, val_loss: 0.8745372891426086\n",
            "Saving model, epoch: 16858, train_loss: 0.9020403027534485, val_loss: 0.8745362162590027\n",
            "Saving model, epoch: 16859, train_loss: 0.9020390510559082, val_loss: 0.8745353817939758\n",
            "Saving model, epoch: 16860, train_loss: 0.9020377397537231, val_loss: 0.8745344877243042\n",
            "Saving model, epoch: 16861, train_loss: 0.9020364880561829, val_loss: 0.8745334148406982\n",
            "Saving model, epoch: 16862, train_loss: 0.9020352959632874, val_loss: 0.8745324015617371\n",
            "Saving model, epoch: 16863, train_loss: 0.9020339250564575, val_loss: 0.8745316863059998\n",
            "Saving model, epoch: 16864, train_loss: 0.9020326137542725, val_loss: 0.8745304942131042\n",
            "Saving model, epoch: 16865, train_loss: 0.9020313024520874, val_loss: 0.8745294809341431\n",
            "Saving model, epoch: 16866, train_loss: 0.9020301103591919, val_loss: 0.8745286464691162\n",
            "Saving model, epoch: 16867, train_loss: 0.9020287990570068, val_loss: 0.8745274543762207\n",
            "Saving model, epoch: 16868, train_loss: 0.9020275473594666, val_loss: 0.8745266199111938\n",
            "Saving model, epoch: 16869, train_loss: 0.9020262360572815, val_loss: 0.8745256066322327\n",
            "Saving model, epoch: 16870, train_loss: 0.9020248651504517, val_loss: 0.8745245933532715\n",
            "Saving model, epoch: 16871, train_loss: 0.9020236730575562, val_loss: 0.8745236396789551\n",
            "Saving model, epoch: 16872, train_loss: 0.9020223617553711, val_loss: 0.8745226860046387\n",
            "Saving model, epoch: 16873, train_loss: 0.902021050453186, val_loss: 0.874521791934967\n",
            "Saving model, epoch: 16874, train_loss: 0.9020198583602905, val_loss: 0.8745208382606506\n",
            "Saving model, epoch: 16875, train_loss: 0.9020184278488159, val_loss: 0.8745198845863342\n",
            "Saving model, epoch: 16876, train_loss: 0.9020172357559204, val_loss: 0.8745189905166626\n",
            "Saving model, epoch: 16877, train_loss: 0.9020159244537354, val_loss: 0.8745174407958984\n",
            "Saving model, epoch: 16878, train_loss: 0.9020146131515503, val_loss: 0.8745167851448059\n",
            "Saving model, epoch: 16879, train_loss: 0.9020134210586548, val_loss: 0.8745161890983582\n",
            "Saving model, epoch: 16880, train_loss: 0.9020119905471802, val_loss: 0.874515175819397\n",
            "Saving model, epoch: 16881, train_loss: 0.9020107984542847, val_loss: 0.8745138645172119\n",
            "Saving model, epoch: 16882, train_loss: 0.9020096063613892, val_loss: 0.8745128512382507\n",
            "Saving model, epoch: 16883, train_loss: 0.9020081758499146, val_loss: 0.8745121359825134\n",
            "Saving model, epoch: 16884, train_loss: 0.9020067453384399, val_loss: 0.8745111227035522\n",
            "Saving model, epoch: 16885, train_loss: 0.9020055532455444, val_loss: 0.8745100498199463\n",
            "Saving model, epoch: 16886, train_loss: 0.9020043611526489, val_loss: 0.8745090961456299\n",
            "Saving model, epoch: 16887, train_loss: 0.9020030498504639, val_loss: 0.8745080828666687\n",
            "Saving model, epoch: 16888, train_loss: 0.9020017385482788, val_loss: 0.8745072484016418\n",
            "Saving model, epoch: 16889, train_loss: 0.9020004272460938, val_loss: 0.8745062947273254\n",
            "Saving model, epoch: 16890, train_loss: 0.9019991159439087, val_loss: 0.8745052814483643\n",
            "Saving model, epoch: 16891, train_loss: 0.9019979238510132, val_loss: 0.8745041489601135\n",
            "Saving model, epoch: 16892, train_loss: 0.9019965529441833, val_loss: 0.8745032548904419\n",
            "Saving model, epoch: 16893, train_loss: 0.9019953012466431, val_loss: 0.8745023012161255\n",
            "Saving model, epoch: 16894, train_loss: 0.9019941091537476, val_loss: 0.8745012879371643\n",
            "Saving model, epoch: 16895, train_loss: 0.901992678642273, val_loss: 0.8745004534721375\n",
            "Saving model, epoch: 16896, train_loss: 0.9019914865493774, val_loss: 0.8744993805885315\n",
            "Saving model, epoch: 16897, train_loss: 0.9019901752471924, val_loss: 0.8744984865188599\n",
            "Saving model, epoch: 16898, train_loss: 0.9019888639450073, val_loss: 0.874497652053833\n",
            "Saving model, epoch: 16899, train_loss: 0.9019874930381775, val_loss: 0.8744964599609375\n",
            "Saving model, epoch: 16900, train_loss: 0.901986300945282, val_loss: 0.8744955658912659\n",
            "epoch: 16901, train_loss: 0.9019849896430969, val_loss: 0.874494731426239\n",
            "Saving model, epoch: 16901, train_loss: 0.9019849896430969, val_loss: 0.874494731426239\n",
            "Saving model, epoch: 16902, train_loss: 0.9019836783409119, val_loss: 0.8744935989379883\n",
            "Saving model, epoch: 16903, train_loss: 0.9019823670387268, val_loss: 0.8744926452636719\n",
            "Saving model, epoch: 16904, train_loss: 0.9019811749458313, val_loss: 0.8744917511940002\n",
            "Saving model, epoch: 16905, train_loss: 0.9019798636436462, val_loss: 0.8744908571243286\n",
            "Saving model, epoch: 16906, train_loss: 0.9019785523414612, val_loss: 0.8744898438453674\n",
            "Saving model, epoch: 16907, train_loss: 0.9019772410392761, val_loss: 0.874488890171051\n",
            "Saving model, epoch: 16908, train_loss: 0.9019760489463806, val_loss: 0.8744879961013794\n",
            "Saving model, epoch: 16909, train_loss: 0.901974618434906, val_loss: 0.8744868040084839\n",
            "Saving model, epoch: 16910, train_loss: 0.9019734263420105, val_loss: 0.8744857311248779\n",
            "Saving model, epoch: 16911, train_loss: 0.9019719958305359, val_loss: 0.8744848966598511\n",
            "Saving model, epoch: 16912, train_loss: 0.9019706845283508, val_loss: 0.874484121799469\n",
            "Saving model, epoch: 16913, train_loss: 0.9019693732261658, val_loss: 0.8744828701019287\n",
            "Saving model, epoch: 16914, train_loss: 0.9019681811332703, val_loss: 0.8744819760322571\n",
            "Saving model, epoch: 16915, train_loss: 0.9019668102264404, val_loss: 0.8744811415672302\n",
            "Saving model, epoch: 16916, train_loss: 0.9019655585289001, val_loss: 0.874480128288269\n",
            "Saving model, epoch: 16917, train_loss: 0.9019643664360046, val_loss: 0.8744791150093079\n",
            "Saving model, epoch: 16918, train_loss: 0.9019629955291748, val_loss: 0.8744780421257019\n",
            "Saving model, epoch: 16919, train_loss: 0.9019616842269897, val_loss: 0.8744770884513855\n",
            "Saving model, epoch: 16920, train_loss: 0.9019603729248047, val_loss: 0.8744761347770691\n",
            "Saving model, epoch: 16921, train_loss: 0.9019591808319092, val_loss: 0.8744753003120422\n",
            "Saving model, epoch: 16922, train_loss: 0.9019577503204346, val_loss: 0.874474287033081\n",
            "Saving model, epoch: 16923, train_loss: 0.9019565582275391, val_loss: 0.8744732737541199\n",
            "Saving model, epoch: 16924, train_loss: 0.901955246925354, val_loss: 0.8744723796844482\n",
            "Saving model, epoch: 16925, train_loss: 0.901953935623169, val_loss: 0.8744713664054871\n",
            "Saving model, epoch: 16926, train_loss: 0.9019527435302734, val_loss: 0.8744703531265259\n",
            "Saving model, epoch: 16927, train_loss: 0.9019513130187988, val_loss: 0.8744694590568542\n",
            "Saving model, epoch: 16928, train_loss: 0.9019501209259033, val_loss: 0.8744683861732483\n",
            "Saving model, epoch: 16929, train_loss: 0.9019486904144287, val_loss: 0.8744674921035767\n",
            "Saving model, epoch: 16930, train_loss: 0.9019474983215332, val_loss: 0.8744664788246155\n",
            "Saving model, epoch: 16931, train_loss: 0.9019463062286377, val_loss: 0.8744655847549438\n",
            "Saving model, epoch: 16932, train_loss: 0.9019448757171631, val_loss: 0.8744645714759827\n",
            "Saving model, epoch: 16933, train_loss: 0.901943564414978, val_loss: 0.8744635581970215\n",
            "Saving model, epoch: 16934, train_loss: 0.901942253112793, val_loss: 0.8744626641273499\n",
            "Saving model, epoch: 16935, train_loss: 0.9019409418106079, val_loss: 0.8744615912437439\n",
            "Saving model, epoch: 16936, train_loss: 0.9019397497177124, val_loss: 0.8744606971740723\n",
            "Saving model, epoch: 16937, train_loss: 0.9019384384155273, val_loss: 0.8744597434997559\n",
            "Saving model, epoch: 16938, train_loss: 0.9019371271133423, val_loss: 0.8744586110115051\n",
            "Saving model, epoch: 16939, train_loss: 0.9019358158111572, val_loss: 0.8744575381278992\n",
            "Saving model, epoch: 16940, train_loss: 0.9019344449043274, val_loss: 0.8744565844535828\n",
            "Saving model, epoch: 16941, train_loss: 0.9019332528114319, val_loss: 0.8744557499885559\n",
            "Saving model, epoch: 16942, train_loss: 0.9019319415092468, val_loss: 0.8744549751281738\n",
            "Saving model, epoch: 16943, train_loss: 0.9019305109977722, val_loss: 0.8744539022445679\n",
            "Saving model, epoch: 16944, train_loss: 0.9019293189048767, val_loss: 0.8744530081748962\n",
            "Saving model, epoch: 16945, train_loss: 0.9019280076026917, val_loss: 0.8744519352912903\n",
            "Saving model, epoch: 16946, train_loss: 0.9019266963005066, val_loss: 0.8744510412216187\n",
            "Saving model, epoch: 16947, train_loss: 0.9019253849983215, val_loss: 0.8744499683380127\n",
            "Saving model, epoch: 16948, train_loss: 0.9019240140914917, val_loss: 0.874448835849762\n",
            "Saving model, epoch: 16949, train_loss: 0.9019227623939514, val_loss: 0.8744478225708008\n",
            "Saving model, epoch: 16950, train_loss: 0.9019214510917664, val_loss: 0.8744469285011292\n",
            "Saving model, epoch: 16951, train_loss: 0.9019201397895813, val_loss: 0.8744460344314575\n",
            "Saving model, epoch: 16952, train_loss: 0.901918888092041, val_loss: 0.8744450211524963\n",
            "Saving model, epoch: 16953, train_loss: 0.901917576789856, val_loss: 0.8744440674781799\n",
            "Saving model, epoch: 16954, train_loss: 0.9019162654876709, val_loss: 0.8744431138038635\n",
            "Saving model, epoch: 16955, train_loss: 0.9019149541854858, val_loss: 0.8744422793388367\n",
            "Saving model, epoch: 16956, train_loss: 0.9019137620925903, val_loss: 0.8744411468505859\n",
            "Saving model, epoch: 16957, train_loss: 0.9019123315811157, val_loss: 0.8744400143623352\n",
            "Saving model, epoch: 16958, train_loss: 0.9019111394882202, val_loss: 0.8744391798973083\n",
            "Saving model, epoch: 16959, train_loss: 0.9019097089767456, val_loss: 0.8744382858276367\n",
            "Saving model, epoch: 16960, train_loss: 0.9019083976745605, val_loss: 0.8744370937347412\n",
            "Saving model, epoch: 16961, train_loss: 0.901907205581665, val_loss: 0.8744363188743591\n",
            "Saving model, epoch: 16962, train_loss: 0.90190589427948, val_loss: 0.8744354248046875\n",
            "Saving model, epoch: 16963, train_loss: 0.9019045233726501, val_loss: 0.8744344115257263\n",
            "Saving model, epoch: 16964, train_loss: 0.9019032716751099, val_loss: 0.8744335174560547\n",
            "Saving model, epoch: 16965, train_loss: 0.9019019603729248, val_loss: 0.8744322061538696\n",
            "Saving model, epoch: 16966, train_loss: 0.9019006490707397, val_loss: 0.8744315505027771\n",
            "Saving model, epoch: 16967, train_loss: 0.9018992781639099, val_loss: 0.8744304180145264\n",
            "Saving model, epoch: 16968, train_loss: 0.9018980860710144, val_loss: 0.8744294047355652\n",
            "Saving model, epoch: 16969, train_loss: 0.9018966555595398, val_loss: 0.8744286298751831\n",
            "Saving model, epoch: 16970, train_loss: 0.9018954634666443, val_loss: 0.8744273781776428\n",
            "Saving model, epoch: 16971, train_loss: 0.9018942713737488, val_loss: 0.8744268417358398\n",
            "Saving model, epoch: 16972, train_loss: 0.9018928408622742, val_loss: 0.8744257092475891\n",
            "Saving model, epoch: 16973, train_loss: 0.9018914103507996, val_loss: 0.8744245767593384\n",
            "Saving model, epoch: 16974, train_loss: 0.901890218257904, val_loss: 0.8744235634803772\n",
            "Saving model, epoch: 16975, train_loss: 0.9018890261650085, val_loss: 0.8744226694107056\n",
            "Saving model, epoch: 16976, train_loss: 0.9018875956535339, val_loss: 0.8744218349456787\n",
            "Saving model, epoch: 16977, train_loss: 0.9018862843513489, val_loss: 0.8744206428527832\n",
            "Saving model, epoch: 16978, train_loss: 0.9018850922584534, val_loss: 0.8744196891784668\n",
            "Saving model, epoch: 16979, train_loss: 0.9018837213516235, val_loss: 0.8744187951087952\n",
            "Saving model, epoch: 16980, train_loss: 0.9018824100494385, val_loss: 0.874417781829834\n",
            "Saving model, epoch: 16981, train_loss: 0.9018811583518982, val_loss: 0.8744167685508728\n",
            "Saving model, epoch: 16982, train_loss: 0.9018797874450684, val_loss: 0.8744156956672668\n",
            "Saving model, epoch: 16983, train_loss: 0.9018784761428833, val_loss: 0.8744147419929504\n",
            "Saving model, epoch: 16984, train_loss: 0.9018771648406982, val_loss: 0.8744137287139893\n",
            "Saving model, epoch: 16985, train_loss: 0.9018759727478027, val_loss: 0.8744127750396729\n",
            "Saving model, epoch: 16986, train_loss: 0.9018745422363281, val_loss: 0.8744117617607117\n",
            "Saving model, epoch: 16987, train_loss: 0.9018733501434326, val_loss: 0.8744107484817505\n",
            "Saving model, epoch: 16988, train_loss: 0.901871919631958, val_loss: 0.8744098544120789\n",
            "Saving model, epoch: 16989, train_loss: 0.9018705487251282, val_loss: 0.8744089603424072\n",
            "Saving model, epoch: 16990, train_loss: 0.9018693566322327, val_loss: 0.874407947063446\n",
            "Saving model, epoch: 16991, train_loss: 0.9018680453300476, val_loss: 0.8744069337844849\n",
            "Saving model, epoch: 16992, train_loss: 0.9018667340278625, val_loss: 0.8744059205055237\n",
            "Saving model, epoch: 16993, train_loss: 0.9018654227256775, val_loss: 0.8744051456451416\n",
            "Saving model, epoch: 16994, train_loss: 0.9018641114234924, val_loss: 0.8744039535522461\n",
            "Saving model, epoch: 16995, train_loss: 0.9018626809120178, val_loss: 0.8744030594825745\n",
            "Saving model, epoch: 16996, train_loss: 0.9018614888191223, val_loss: 0.8744020462036133\n",
            "Saving model, epoch: 16997, train_loss: 0.9018601775169373, val_loss: 0.8744010925292969\n",
            "Saving model, epoch: 16998, train_loss: 0.9018588662147522, val_loss: 0.8744003772735596\n",
            "Saving model, epoch: 16999, train_loss: 0.9018575549125671, val_loss: 0.8743992447853088\n",
            "Saving model, epoch: 17000, train_loss: 0.9018562436103821, val_loss: 0.8743981719017029\n",
            "epoch: 17001, train_loss: 0.9018549919128418, val_loss: 0.8743970394134521\n",
            "Saving model, epoch: 17001, train_loss: 0.9018549919128418, val_loss: 0.8743970394134521\n",
            "Saving model, epoch: 17002, train_loss: 0.9018536806106567, val_loss: 0.8743963241577148\n",
            "Saving model, epoch: 17003, train_loss: 0.9018522500991821, val_loss: 0.874395489692688\n",
            "Saving model, epoch: 17004, train_loss: 0.9018510580062866, val_loss: 0.8743943572044373\n",
            "Saving model, epoch: 17005, train_loss: 0.901849627494812, val_loss: 0.8743934035301208\n",
            "Saving model, epoch: 17006, train_loss: 0.9018484354019165, val_loss: 0.8743923902511597\n",
            "Saving model, epoch: 17007, train_loss: 0.9018471240997314, val_loss: 0.8743911981582642\n",
            "Saving model, epoch: 17008, train_loss: 0.9018456935882568, val_loss: 0.874390184879303\n",
            "Saving model, epoch: 17009, train_loss: 0.9018445014953613, val_loss: 0.8743894100189209\n",
            "Saving model, epoch: 17010, train_loss: 0.9018431901931763, val_loss: 0.8743883967399597\n",
            "Saving model, epoch: 17011, train_loss: 0.9018418192863464, val_loss: 0.874387264251709\n",
            "Saving model, epoch: 17012, train_loss: 0.9018406271934509, val_loss: 0.8743863105773926\n",
            "Saving model, epoch: 17013, train_loss: 0.9018391966819763, val_loss: 0.8743854761123657\n",
            "Saving model, epoch: 17014, train_loss: 0.9018380045890808, val_loss: 0.874384343624115\n",
            "Saving model, epoch: 17015, train_loss: 0.9018365740776062, val_loss: 0.8743835091590881\n",
            "Saving model, epoch: 17016, train_loss: 0.9018353819847107, val_loss: 0.8743823170661926\n",
            "Saving model, epoch: 17017, train_loss: 0.9018339514732361, val_loss: 0.874381422996521\n",
            "Saving model, epoch: 17018, train_loss: 0.901832640171051, val_loss: 0.8743805289268494\n",
            "Saving model, epoch: 17019, train_loss: 0.901831328868866, val_loss: 0.8743796348571777\n",
            "Saving model, epoch: 17020, train_loss: 0.9018300771713257, val_loss: 0.8743785619735718\n",
            "Saving model, epoch: 17021, train_loss: 0.9018287658691406, val_loss: 0.8743776082992554\n",
            "Saving model, epoch: 17022, train_loss: 0.9018275141716003, val_loss: 0.8743765950202942\n",
            "Saving model, epoch: 17023, train_loss: 0.901826024055481, val_loss: 0.8743756413459778\n",
            "Saving model, epoch: 17024, train_loss: 0.9018247127532959, val_loss: 0.8743746876716614\n",
            "Saving model, epoch: 17025, train_loss: 0.9018235206604004, val_loss: 0.874373733997345\n",
            "Saving model, epoch: 17026, train_loss: 0.9018220901489258, val_loss: 0.8743726015090942\n",
            "Saving model, epoch: 17027, train_loss: 0.9018207788467407, val_loss: 0.8743717670440674\n",
            "Saving model, epoch: 17028, train_loss: 0.9018195271492004, val_loss: 0.8743707537651062\n",
            "Saving model, epoch: 17029, train_loss: 0.9018180966377258, val_loss: 0.874369740486145\n",
            "Saving model, epoch: 17030, train_loss: 0.9018169045448303, val_loss: 0.8743689656257629\n",
            "Saving model, epoch: 17031, train_loss: 0.9018155932426453, val_loss: 0.8743677139282227\n",
            "Saving model, epoch: 17032, train_loss: 0.9018142819404602, val_loss: 0.874366819858551\n",
            "Saving model, epoch: 17033, train_loss: 0.9018130898475647, val_loss: 0.8743656873703003\n",
            "Saving model, epoch: 17034, train_loss: 0.9018117785453796, val_loss: 0.8743648529052734\n",
            "Saving model, epoch: 17035, train_loss: 0.901810348033905, val_loss: 0.8743637204170227\n",
            "Saving model, epoch: 17036, train_loss: 0.90180903673172, val_loss: 0.8743627667427063\n",
            "Saving model, epoch: 17037, train_loss: 0.9018076658248901, val_loss: 0.8743618726730347\n",
            "Saving model, epoch: 17038, train_loss: 0.9018063545227051, val_loss: 0.8743607997894287\n",
            "Saving model, epoch: 17039, train_loss: 0.9018051028251648, val_loss: 0.8743599057197571\n",
            "Saving model, epoch: 17040, train_loss: 0.9018038511276245, val_loss: 0.8743590116500854\n",
            "Saving model, epoch: 17041, train_loss: 0.9018024206161499, val_loss: 0.8743577599525452\n",
            "Saving model, epoch: 17042, train_loss: 0.9018011093139648, val_loss: 0.8743569850921631\n",
            "Saving model, epoch: 17043, train_loss: 0.9017997980117798, val_loss: 0.8743560910224915\n",
            "Saving model, epoch: 17044, train_loss: 0.9017986059188843, val_loss: 0.8743550181388855\n",
            "Saving model, epoch: 17045, train_loss: 0.9017971754074097, val_loss: 0.8743541240692139\n",
            "Saving model, epoch: 17046, train_loss: 0.9017958045005798, val_loss: 0.8743529915809631\n",
            "Saving model, epoch: 17047, train_loss: 0.9017945528030396, val_loss: 0.874351978302002\n",
            "Saving model, epoch: 17048, train_loss: 0.9017931818962097, val_loss: 0.8743509650230408\n",
            "Saving model, epoch: 17049, train_loss: 0.9017917513847351, val_loss: 0.8743498921394348\n",
            "Saving model, epoch: 17050, train_loss: 0.9017905592918396, val_loss: 0.8743489980697632\n",
            "Saving model, epoch: 17051, train_loss: 0.9017892479896545, val_loss: 0.8743480443954468\n",
            "Saving model, epoch: 17052, train_loss: 0.9017879366874695, val_loss: 0.8743470311164856\n",
            "Saving model, epoch: 17053, train_loss: 0.9017866253852844, val_loss: 0.8743460178375244\n",
            "Saving model, epoch: 17054, train_loss: 0.9017853140830994, val_loss: 0.8743451833724976\n",
            "Saving model, epoch: 17055, train_loss: 0.9017839431762695, val_loss: 0.8743440508842468\n",
            "Saving model, epoch: 17056, train_loss: 0.901782751083374, val_loss: 0.8743430376052856\n",
            "Saving model, epoch: 17057, train_loss: 0.9017813205718994, val_loss: 0.8743422627449036\n",
            "Saving model, epoch: 17058, train_loss: 0.9017801284790039, val_loss: 0.8743410110473633\n",
            "Saving model, epoch: 17059, train_loss: 0.9017786979675293, val_loss: 0.8743399977684021\n",
            "Saving model, epoch: 17060, train_loss: 0.9017773866653442, val_loss: 0.8743391036987305\n",
            "Saving model, epoch: 17061, train_loss: 0.9017760753631592, val_loss: 0.8743382096290588\n",
            "Saving model, epoch: 17062, train_loss: 0.9017747044563293, val_loss: 0.8743371963500977\n",
            "Saving model, epoch: 17063, train_loss: 0.9017735123634338, val_loss: 0.8743360638618469\n",
            "Saving model, epoch: 17064, train_loss: 0.9017720818519592, val_loss: 0.8743351697921753\n",
            "Saving model, epoch: 17065, train_loss: 0.9017707705497742, val_loss: 0.8743342757225037\n",
            "Saving model, epoch: 17066, train_loss: 0.9017694592475891, val_loss: 0.8743332028388977\n",
            "Saving model, epoch: 17067, train_loss: 0.9017682671546936, val_loss: 0.8743324875831604\n",
            "Saving model, epoch: 17068, train_loss: 0.901766836643219, val_loss: 0.8743312358856201\n",
            "Saving model, epoch: 17069, train_loss: 0.9017655253410339, val_loss: 0.8743302822113037\n",
            "Saving model, epoch: 17070, train_loss: 0.9017642140388489, val_loss: 0.874329149723053\n",
            "Saving model, epoch: 17071, train_loss: 0.9017629623413086, val_loss: 0.8743282556533813\n",
            "Saving model, epoch: 17072, train_loss: 0.901761531829834, val_loss: 0.8743273019790649\n",
            "Saving model, epoch: 17073, train_loss: 0.9017602205276489, val_loss: 0.8743262887001038\n",
            "Saving model, epoch: 17074, train_loss: 0.9017589092254639, val_loss: 0.8743254542350769\n",
            "Saving model, epoch: 17075, train_loss: 0.9017575979232788, val_loss: 0.8743243217468262\n",
            "Saving model, epoch: 17076, train_loss: 0.901756227016449, val_loss: 0.8743234276771545\n",
            "Saving model, epoch: 17077, train_loss: 0.9017549753189087, val_loss: 0.8743222951889038\n",
            "Saving model, epoch: 17078, train_loss: 0.9017536044120789, val_loss: 0.8743214011192322\n",
            "Saving model, epoch: 17079, train_loss: 0.9017524123191833, val_loss: 0.8743202686309814\n",
            "Saving model, epoch: 17080, train_loss: 0.9017509818077087, val_loss: 0.8743191957473755\n",
            "Saving model, epoch: 17081, train_loss: 0.9017497897148132, val_loss: 0.8743184208869934\n",
            "Saving model, epoch: 17082, train_loss: 0.9017483592033386, val_loss: 0.8743172883987427\n",
            "Saving model, epoch: 17083, train_loss: 0.9017469882965088, val_loss: 0.8743163347244263\n",
            "Saving model, epoch: 17084, train_loss: 0.9017456769943237, val_loss: 0.8743153810501099\n",
            "Saving model, epoch: 17085, train_loss: 0.9017443656921387, val_loss: 0.8743146061897278\n",
            "Saving model, epoch: 17086, train_loss: 0.9017431139945984, val_loss: 0.8743135929107666\n",
            "Saving model, epoch: 17087, train_loss: 0.901741623878479, val_loss: 0.8743123412132263\n",
            "Saving model, epoch: 17088, train_loss: 0.901740550994873, val_loss: 0.8743113279342651\n",
            "Saving model, epoch: 17089, train_loss: 0.9017390012741089, val_loss: 0.8743106126785278\n",
            "Saving model, epoch: 17090, train_loss: 0.9017378091812134, val_loss: 0.8743095397949219\n",
            "Saving model, epoch: 17091, train_loss: 0.9017364978790283, val_loss: 0.8743085861206055\n",
            "Saving model, epoch: 17092, train_loss: 0.9017351269721985, val_loss: 0.8743071556091309\n",
            "Saving model, epoch: 17093, train_loss: 0.9017338156700134, val_loss: 0.8743065595626831\n",
            "Saving model, epoch: 17094, train_loss: 0.9017325043678284, val_loss: 0.8743056058883667\n",
            "Saving model, epoch: 17095, train_loss: 0.9017310738563538, val_loss: 0.8743049502372742\n",
            "Saving model, epoch: 17096, train_loss: 0.9017298817634583, val_loss: 0.8743038177490234\n",
            "Saving model, epoch: 17097, train_loss: 0.9017284512519836, val_loss: 0.8743026256561279\n",
            "Saving model, epoch: 17098, train_loss: 0.9017272591590881, val_loss: 0.8743016123771667\n",
            "Saving model, epoch: 17099, train_loss: 0.9017258882522583, val_loss: 0.8743006587028503\n",
            "Saving model, epoch: 17100, train_loss: 0.9017245769500732, val_loss: 0.8742997646331787\n",
            "epoch: 17101, train_loss: 0.9017231464385986, val_loss: 0.8742987513542175\n",
            "Saving model, epoch: 17101, train_loss: 0.9017231464385986, val_loss: 0.8742987513542175\n",
            "Saving model, epoch: 17102, train_loss: 0.9017218351364136, val_loss: 0.8742977976799011\n",
            "Saving model, epoch: 17103, train_loss: 0.9017206430435181, val_loss: 0.8742966651916504\n",
            "Saving model, epoch: 17104, train_loss: 0.9017192125320435, val_loss: 0.8742958307266235\n",
            "Saving model, epoch: 17105, train_loss: 0.9017178416252136, val_loss: 0.8742949962615967\n",
            "Saving model, epoch: 17106, train_loss: 0.9017165899276733, val_loss: 0.8742938041687012\n",
            "Saving model, epoch: 17107, train_loss: 0.9017152190208435, val_loss: 0.8742929100990295\n",
            "Saving model, epoch: 17108, train_loss: 0.901714026927948, val_loss: 0.8742917776107788\n",
            "Saving model, epoch: 17109, train_loss: 0.9017125964164734, val_loss: 0.8742908835411072\n",
            "Saving model, epoch: 17110, train_loss: 0.9017112851142883, val_loss: 0.8742898106575012\n",
            "Saving model, epoch: 17111, train_loss: 0.9017099738121033, val_loss: 0.8742886185646057\n",
            "Saving model, epoch: 17112, train_loss: 0.9017086029052734, val_loss: 0.8742877840995789\n",
            "Saving model, epoch: 17113, train_loss: 0.9017072916030884, val_loss: 0.8742868304252625\n",
            "Saving model, epoch: 17114, train_loss: 0.9017059803009033, val_loss: 0.874285876750946\n",
            "Saving model, epoch: 17115, train_loss: 0.9017045497894287, val_loss: 0.8742845058441162\n",
            "Saving model, epoch: 17116, train_loss: 0.9017033576965332, val_loss: 0.8742836117744446\n",
            "Saving model, epoch: 17117, train_loss: 0.9017019271850586, val_loss: 0.8742827773094177\n",
            "Saving model, epoch: 17118, train_loss: 0.9017005562782288, val_loss: 0.8742820024490356\n",
            "Saving model, epoch: 17119, train_loss: 0.9016993641853333, val_loss: 0.8742809891700745\n",
            "Saving model, epoch: 17120, train_loss: 0.901698112487793, val_loss: 0.874279797077179\n",
            "Saving model, epoch: 17121, train_loss: 0.9016966223716736, val_loss: 0.8742789030075073\n",
            "Saving model, epoch: 17122, train_loss: 0.9016953110694885, val_loss: 0.8742779493331909\n",
            "Saving model, epoch: 17123, train_loss: 0.9016939401626587, val_loss: 0.874276876449585\n",
            "Saving model, epoch: 17124, train_loss: 0.9016925096511841, val_loss: 0.8742756843566895\n",
            "Saving model, epoch: 17125, train_loss: 0.9016913175582886, val_loss: 0.8742746710777283\n",
            "Saving model, epoch: 17126, train_loss: 0.9016900658607483, val_loss: 0.8742737770080566\n",
            "Saving model, epoch: 17127, train_loss: 0.9016886949539185, val_loss: 0.8742729425430298\n",
            "Saving model, epoch: 17128, train_loss: 0.9016872644424438, val_loss: 0.8742719888687134\n",
            "Saving model, epoch: 17129, train_loss: 0.9016860723495483, val_loss: 0.8742706775665283\n",
            "Saving model, epoch: 17130, train_loss: 0.9016846418380737, val_loss: 0.874269962310791\n",
            "Saving model, epoch: 17131, train_loss: 0.9016833901405334, val_loss: 0.8742690086364746\n",
            "Saving model, epoch: 17132, train_loss: 0.9016820788383484, val_loss: 0.8742679953575134\n",
            "Saving model, epoch: 17133, train_loss: 0.9016808271408081, val_loss: 0.8742668628692627\n",
            "Saving model, epoch: 17134, train_loss: 0.9016794562339783, val_loss: 0.874265730381012\n",
            "Saving model, epoch: 17135, train_loss: 0.9016780257225037, val_loss: 0.8742649555206299\n",
            "Saving model, epoch: 17136, train_loss: 0.9016766548156738, val_loss: 0.8742640614509583\n",
            "Saving model, epoch: 17137, train_loss: 0.9016754031181335, val_loss: 0.8742629289627075\n",
            "Saving model, epoch: 17138, train_loss: 0.9016740918159485, val_loss: 0.8742619156837463\n",
            "Saving model, epoch: 17139, train_loss: 0.9016727209091187, val_loss: 0.8742607831954956\n",
            "Saving model, epoch: 17140, train_loss: 0.9016714096069336, val_loss: 0.874259889125824\n",
            "Saving model, epoch: 17141, train_loss: 0.901669979095459, val_loss: 0.8742589950561523\n",
            "Saving model, epoch: 17142, train_loss: 0.9016687870025635, val_loss: 0.8742579817771912\n",
            "Saving model, epoch: 17143, train_loss: 0.9016674160957336, val_loss: 0.8742568492889404\n",
            "Saving model, epoch: 17144, train_loss: 0.901665985584259, val_loss: 0.8742561340332031\n",
            "Saving model, epoch: 17145, train_loss: 0.901664674282074, val_loss: 0.8742549419403076\n",
            "Saving model, epoch: 17146, train_loss: 0.9016633629798889, val_loss: 0.8742538690567017\n",
            "Saving model, epoch: 17147, train_loss: 0.9016620516777039, val_loss: 0.87425297498703\n",
            "Saving model, epoch: 17148, train_loss: 0.9016607403755188, val_loss: 0.874252200126648\n",
            "Saving model, epoch: 17149, train_loss: 0.9016594290733337, val_loss: 0.8742508292198181\n",
            "Saving model, epoch: 17150, train_loss: 0.9016581177711487, val_loss: 0.8742499947547913\n",
            "Saving model, epoch: 17151, train_loss: 0.9016567468643188, val_loss: 0.8742490410804749\n",
            "Saving model, epoch: 17152, train_loss: 0.9016553163528442, val_loss: 0.8742480874061584\n",
            "Saving model, epoch: 17153, train_loss: 0.9016540050506592, val_loss: 0.8742470145225525\n",
            "Saving model, epoch: 17154, train_loss: 0.9016526937484741, val_loss: 0.8742462396621704\n",
            "Saving model, epoch: 17155, train_loss: 0.9016513228416443, val_loss: 0.8742449283599854\n",
            "Saving model, epoch: 17156, train_loss: 0.9016500115394592, val_loss: 0.8742440938949585\n",
            "Saving model, epoch: 17157, train_loss: 0.9016487002372742, val_loss: 0.8742432594299316\n",
            "Saving model, epoch: 17158, train_loss: 0.9016473889350891, val_loss: 0.874241828918457\n",
            "Saving model, epoch: 17159, train_loss: 0.9016460180282593, val_loss: 0.8742411732673645\n",
            "Saving model, epoch: 17160, train_loss: 0.9016447067260742, val_loss: 0.8742401003837585\n",
            "Saving model, epoch: 17161, train_loss: 0.9016434550285339, val_loss: 0.8742390871047974\n",
            "Saving model, epoch: 17162, train_loss: 0.9016419649124146, val_loss: 0.8742381930351257\n",
            "Saving model, epoch: 17163, train_loss: 0.9016408920288086, val_loss: 0.8742371201515198\n",
            "Saving model, epoch: 17164, train_loss: 0.9016393423080444, val_loss: 0.8742361664772034\n",
            "Saving model, epoch: 17165, train_loss: 0.9016380310058594, val_loss: 0.8742350935935974\n",
            "Saving model, epoch: 17166, train_loss: 0.9016367197036743, val_loss: 0.8742340803146362\n",
            "Saving model, epoch: 17167, train_loss: 0.901635468006134, val_loss: 0.8742329478263855\n",
            "Saving model, epoch: 17168, train_loss: 0.9016340374946594, val_loss: 0.8742321729660034\n",
            "Saving model, epoch: 17169, train_loss: 0.9016326069831848, val_loss: 0.8742311596870422\n",
            "Saving model, epoch: 17170, train_loss: 0.9016314148902893, val_loss: 0.8742300868034363\n",
            "Saving model, epoch: 17171, train_loss: 0.9016299843788147, val_loss: 0.8742291331291199\n",
            "Saving model, epoch: 17172, train_loss: 0.9016287326812744, val_loss: 0.8742281794548035\n",
            "Saving model, epoch: 17173, train_loss: 0.9016271829605103, val_loss: 0.8742271661758423\n",
            "Saving model, epoch: 17174, train_loss: 0.9016259908676147, val_loss: 0.8742261528968811\n",
            "Saving model, epoch: 17175, train_loss: 0.9016245603561401, val_loss: 0.8742251396179199\n",
            "Saving model, epoch: 17176, train_loss: 0.9016233682632446, val_loss: 0.8742242455482483\n",
            "Saving model, epoch: 17177, train_loss: 0.9016219973564148, val_loss: 0.8742231130599976\n",
            "Saving model, epoch: 17178, train_loss: 0.9016207456588745, val_loss: 0.8742222785949707\n",
            "Saving model, epoch: 17179, train_loss: 0.9016193747520447, val_loss: 0.8742209672927856\n",
            "Saving model, epoch: 17180, train_loss: 0.9016179442405701, val_loss: 0.8742203116416931\n",
            "Saving model, epoch: 17181, train_loss: 0.901616632938385, val_loss: 0.8742192387580872\n",
            "Saving model, epoch: 17182, train_loss: 0.9016153216362, val_loss: 0.874218225479126\n",
            "Saving model, epoch: 17183, train_loss: 0.9016139507293701, val_loss: 0.87421715259552\n",
            "Saving model, epoch: 17184, train_loss: 0.9016126394271851, val_loss: 0.8742161393165588\n",
            "Saving model, epoch: 17185, train_loss: 0.901611328125, val_loss: 0.8742152452468872\n",
            "Saving model, epoch: 17186, train_loss: 0.9016100168228149, val_loss: 0.8742142915725708\n",
            "Saving model, epoch: 17187, train_loss: 0.9016085863113403, val_loss: 0.8742133378982544\n",
            "Saving model, epoch: 17188, train_loss: 0.9016073346138, val_loss: 0.8742119669914246\n",
            "Saving model, epoch: 17189, train_loss: 0.9016059041023254, val_loss: 0.8742109537124634\n",
            "Saving model, epoch: 17190, train_loss: 0.9016045928001404, val_loss: 0.8742101192474365\n",
            "Saving model, epoch: 17191, train_loss: 0.9016032814979553, val_loss: 0.8742091059684753\n",
            "Saving model, epoch: 17192, train_loss: 0.9016019105911255, val_loss: 0.8742079734802246\n",
            "Saving model, epoch: 17193, train_loss: 0.9016004800796509, val_loss: 0.8742069005966187\n",
            "Saving model, epoch: 17194, train_loss: 0.9015992879867554, val_loss: 0.874206006526947\n",
            "Saving model, epoch: 17195, train_loss: 0.9015979766845703, val_loss: 0.8742051124572754\n",
            "Saving model, epoch: 17196, train_loss: 0.9015966653823853, val_loss: 0.8742040395736694\n",
            "Saving model, epoch: 17197, train_loss: 0.9015952348709106, val_loss: 0.8742029070854187\n",
            "Saving model, epoch: 17198, train_loss: 0.9015938639640808, val_loss: 0.8742021322250366\n",
            "Saving model, epoch: 17199, train_loss: 0.9015925526618958, val_loss: 0.8742010593414307\n",
            "Saving model, epoch: 17200, train_loss: 0.9015912413597107, val_loss: 0.8742002248764038\n",
            "epoch: 17201, train_loss: 0.9015898108482361, val_loss: 0.8741990923881531\n",
            "Saving model, epoch: 17201, train_loss: 0.9015898108482361, val_loss: 0.8741990923881531\n",
            "Saving model, epoch: 17202, train_loss: 0.9015886187553406, val_loss: 0.8741980791091919\n",
            "Saving model, epoch: 17203, train_loss: 0.901587188243866, val_loss: 0.8741970062255859\n",
            "Saving model, epoch: 17204, train_loss: 0.9015858173370361, val_loss: 0.8741959929466248\n",
            "Saving model, epoch: 17205, train_loss: 0.9015845060348511, val_loss: 0.8741949796676636\n",
            "Saving model, epoch: 17206, train_loss: 0.901583194732666, val_loss: 0.8741940855979919\n",
            "Saving model, epoch: 17207, train_loss: 0.9015817642211914, val_loss: 0.8741932511329651\n",
            "Saving model, epoch: 17208, train_loss: 0.9015804529190063, val_loss: 0.8741922378540039\n",
            "Saving model, epoch: 17209, train_loss: 0.9015792012214661, val_loss: 0.8741910457611084\n",
            "Saving model, epoch: 17210, train_loss: 0.901577889919281, val_loss: 0.8741900324821472\n",
            "Saving model, epoch: 17211, train_loss: 0.9015763401985168, val_loss: 0.874189019203186\n",
            "Saving model, epoch: 17212, train_loss: 0.9015751481056213, val_loss: 0.8741880655288696\n",
            "Saving model, epoch: 17213, train_loss: 0.9015737771987915, val_loss: 0.8741870522499084\n",
            "Saving model, epoch: 17214, train_loss: 0.9015723466873169, val_loss: 0.8741859793663025\n",
            "Saving model, epoch: 17215, train_loss: 0.9015710353851318, val_loss: 0.8741849660873413\n",
            "Saving model, epoch: 17216, train_loss: 0.9015697240829468, val_loss: 0.8741840124130249\n",
            "Saving model, epoch: 17217, train_loss: 0.9015684127807617, val_loss: 0.8741833567619324\n",
            "Saving model, epoch: 17218, train_loss: 0.9015670418739319, val_loss: 0.8741820454597473\n",
            "Saving model, epoch: 17219, train_loss: 0.9015657305717468, val_loss: 0.8741809129714966\n",
            "Saving model, epoch: 17220, train_loss: 0.9015643000602722, val_loss: 0.8741800785064697\n",
            "Saving model, epoch: 17221, train_loss: 0.9015629887580872, val_loss: 0.8741791844367981\n",
            "Saving model, epoch: 17222, train_loss: 0.9015616774559021, val_loss: 0.8741781711578369\n",
            "Saving model, epoch: 17223, train_loss: 0.9015603065490723, val_loss: 0.874177098274231\n",
            "Saving model, epoch: 17224, train_loss: 0.9015589952468872, val_loss: 0.8741761445999146\n",
            "Saving model, epoch: 17225, train_loss: 0.9015576839447021, val_loss: 0.874174952507019\n",
            "Saving model, epoch: 17226, train_loss: 0.9015562534332275, val_loss: 0.8741740584373474\n",
            "Saving model, epoch: 17227, train_loss: 0.9015549421310425, val_loss: 0.8741728067398071\n",
            "Saving model, epoch: 17228, train_loss: 0.9015536308288574, val_loss: 0.874172031879425\n",
            "Saving model, epoch: 17229, train_loss: 0.9015522599220276, val_loss: 0.8741710186004639\n",
            "Saving model, epoch: 17230, train_loss: 0.9015510678291321, val_loss: 0.8741702437400818\n",
            "Saving model, epoch: 17231, train_loss: 0.9015495181083679, val_loss: 0.8741689324378967\n",
            "Saving model, epoch: 17232, train_loss: 0.9015482068061829, val_loss: 0.8741679787635803\n",
            "Saving model, epoch: 17233, train_loss: 0.901546835899353, val_loss: 0.8741669654846191\n",
            "Saving model, epoch: 17234, train_loss: 0.9015456438064575, val_loss: 0.8741658329963684\n",
            "Saving model, epoch: 17235, train_loss: 0.9015442132949829, val_loss: 0.8741649389266968\n",
            "Saving model, epoch: 17236, train_loss: 0.9015427827835083, val_loss: 0.8741639852523804\n",
            "Saving model, epoch: 17237, train_loss: 0.9015414714813232, val_loss: 0.8741629123687744\n",
            "Saving model, epoch: 17238, train_loss: 0.901540219783783, val_loss: 0.8741620779037476\n",
            "Saving model, epoch: 17239, train_loss: 0.9015387892723083, val_loss: 0.8741611242294312\n",
            "Saving model, epoch: 17240, train_loss: 0.9015374779701233, val_loss: 0.8741598725318909\n",
            "Saving model, epoch: 17241, train_loss: 0.9015361666679382, val_loss: 0.8741592168807983\n",
            "Saving model, epoch: 17242, train_loss: 0.9015347957611084, val_loss: 0.8741581439971924\n",
            "Saving model, epoch: 17243, train_loss: 0.9015334844589233, val_loss: 0.8741569519042969\n",
            "Saving model, epoch: 17244, train_loss: 0.9015321731567383, val_loss: 0.8741559982299805\n",
            "Saving model, epoch: 17245, train_loss: 0.9015307426452637, val_loss: 0.8741550445556641\n",
            "Saving model, epoch: 17246, train_loss: 0.9015294313430786, val_loss: 0.8741537928581238\n",
            "Saving model, epoch: 17247, train_loss: 0.9015280604362488, val_loss: 0.8741530179977417\n",
            "Saving model, epoch: 17248, train_loss: 0.9015267491340637, val_loss: 0.8741520047187805\n",
            "Saving model, epoch: 17249, train_loss: 0.9015253186225891, val_loss: 0.874150812625885\n",
            "Saving model, epoch: 17250, train_loss: 0.9015241265296936, val_loss: 0.8741500973701477\n",
            "Saving model, epoch: 17251, train_loss: 0.9015226364135742, val_loss: 0.8741490244865417\n",
            "Saving model, epoch: 17252, train_loss: 0.9015213251113892, val_loss: 0.874147891998291\n",
            "Saving model, epoch: 17253, train_loss: 0.9015198945999146, val_loss: 0.8741468787193298\n",
            "Saving model, epoch: 17254, train_loss: 0.9015185832977295, val_loss: 0.874146044254303\n",
            "Saving model, epoch: 17255, train_loss: 0.9015172719955444, val_loss: 0.8741450905799866\n",
            "Saving model, epoch: 17256, train_loss: 0.9015159010887146, val_loss: 0.8741440773010254\n",
            "Saving model, epoch: 17257, train_loss: 0.9015145897865295, val_loss: 0.8741428852081299\n",
            "Saving model, epoch: 17258, train_loss: 0.9015132784843445, val_loss: 0.8741419911384583\n",
            "Saving model, epoch: 17259, train_loss: 0.9015118479728699, val_loss: 0.8741408586502075\n",
            "Saving model, epoch: 17260, train_loss: 0.90151047706604, val_loss: 0.874140202999115\n",
            "Saving model, epoch: 17261, train_loss: 0.9015092253684998, val_loss: 0.8741390705108643\n",
            "Saving model, epoch: 17262, train_loss: 0.9015078544616699, val_loss: 0.8741381168365479\n",
            "Saving model, epoch: 17263, train_loss: 0.9015064239501953, val_loss: 0.8741368055343628\n",
            "Saving model, epoch: 17264, train_loss: 0.9015052318572998, val_loss: 0.8741359114646912\n",
            "Saving model, epoch: 17265, train_loss: 0.9015038013458252, val_loss: 0.87413489818573\n",
            "Saving model, epoch: 17266, train_loss: 0.9015024304389954, val_loss: 0.8741339445114136\n",
            "Saving model, epoch: 17267, train_loss: 0.9015012383460999, val_loss: 0.8741328120231628\n",
            "Saving model, epoch: 17268, train_loss: 0.9014996886253357, val_loss: 0.8741322755813599\n",
            "Saving model, epoch: 17269, train_loss: 0.9014983177185059, val_loss: 0.87413090467453\n",
            "Saving model, epoch: 17270, train_loss: 0.9014970064163208, val_loss: 0.8741298913955688\n",
            "Saving model, epoch: 17271, train_loss: 0.9014958143234253, val_loss: 0.8741289973258972\n",
            "Saving model, epoch: 17272, train_loss: 0.9014943838119507, val_loss: 0.8741279244422913\n",
            "Saving model, epoch: 17273, train_loss: 0.9014929533004761, val_loss: 0.8741267919540405\n",
            "Saving model, epoch: 17274, train_loss: 0.9014915823936462, val_loss: 0.8741258978843689\n",
            "Saving model, epoch: 17275, train_loss: 0.9014902710914612, val_loss: 0.8741247653961182\n",
            "Saving model, epoch: 17276, train_loss: 0.9014889597892761, val_loss: 0.8741238117218018\n",
            "Saving model, epoch: 17277, train_loss: 0.9014875292778015, val_loss: 0.8741229176521301\n",
            "Saving model, epoch: 17278, train_loss: 0.9014862179756165, val_loss: 0.874121904373169\n",
            "Saving model, epoch: 17279, train_loss: 0.9014849066734314, val_loss: 0.874120831489563\n",
            "Saving model, epoch: 17280, train_loss: 0.9014835357666016, val_loss: 0.8741196990013123\n",
            "Saving model, epoch: 17281, train_loss: 0.901482105255127, val_loss: 0.8741188049316406\n",
            "Saving model, epoch: 17282, train_loss: 0.9014807939529419, val_loss: 0.8741180300712585\n",
            "Saving model, epoch: 17283, train_loss: 0.9014794826507568, val_loss: 0.8741168975830078\n",
            "Saving model, epoch: 17284, train_loss: 0.901478111743927, val_loss: 0.8741157650947571\n",
            "Saving model, epoch: 17285, train_loss: 0.9014766812324524, val_loss: 0.8741147518157959\n",
            "Saving model, epoch: 17286, train_loss: 0.9014754891395569, val_loss: 0.8741138577461243\n",
            "Saving model, epoch: 17287, train_loss: 0.9014739990234375, val_loss: 0.8741127252578735\n",
            "Saving model, epoch: 17288, train_loss: 0.9014726877212524, val_loss: 0.8741115927696228\n",
            "Saving model, epoch: 17289, train_loss: 0.9014713764190674, val_loss: 0.874110758304596\n",
            "Saving model, epoch: 17290, train_loss: 0.9014700651168823, val_loss: 0.8741097450256348\n",
            "Saving model, epoch: 17291, train_loss: 0.9014686346054077, val_loss: 0.8741087913513184\n",
            "Saving model, epoch: 17292, train_loss: 0.9014672636985779, val_loss: 0.8741075992584229\n",
            "Saving model, epoch: 17293, train_loss: 0.9014659523963928, val_loss: 0.8741067051887512\n",
            "Saving model, epoch: 17294, train_loss: 0.9014645218849182, val_loss: 0.87410569190979\n",
            "Saving model, epoch: 17295, train_loss: 0.9014632105827332, val_loss: 0.8741047978401184\n",
            "Saving model, epoch: 17296, train_loss: 0.9014618992805481, val_loss: 0.8741036057472229\n",
            "Saving model, epoch: 17297, train_loss: 0.9014606475830078, val_loss: 0.8741028904914856\n",
            "Saving model, epoch: 17298, train_loss: 0.9014590978622437, val_loss: 0.8741018772125244\n",
            "Saving model, epoch: 17299, train_loss: 0.9014577865600586, val_loss: 0.8741006255149841\n",
            "Saving model, epoch: 17300, train_loss: 0.9014564156532288, val_loss: 0.8740996718406677\n",
            "epoch: 17301, train_loss: 0.9014549851417542, val_loss: 0.8740987181663513\n",
            "Saving model, epoch: 17301, train_loss: 0.9014549851417542, val_loss: 0.8740987181663513\n",
            "Saving model, epoch: 17302, train_loss: 0.9014537930488586, val_loss: 0.8740977048873901\n",
            "Saving model, epoch: 17303, train_loss: 0.901452362537384, val_loss: 0.8740965723991394\n",
            "Saving model, epoch: 17304, train_loss: 0.901451051235199, val_loss: 0.8740954399108887\n",
            "Saving model, epoch: 17305, train_loss: 0.9014497995376587, val_loss: 0.874094545841217\n",
            "Saving model, epoch: 17306, train_loss: 0.9014483690261841, val_loss: 0.8740938901901245\n",
            "Saving model, epoch: 17307, train_loss: 0.9014469385147095, val_loss: 0.874092698097229\n",
            "Saving model, epoch: 17308, train_loss: 0.9014456272125244, val_loss: 0.8740919232368469\n",
            "Saving model, epoch: 17309, train_loss: 0.9014443755149841, val_loss: 0.8740906715393066\n",
            "Saving model, epoch: 17310, train_loss: 0.9014429450035095, val_loss: 0.8740895390510559\n",
            "Saving model, epoch: 17311, train_loss: 0.9014416337013245, val_loss: 0.8740885257720947\n",
            "Saving model, epoch: 17312, train_loss: 0.9014402031898499, val_loss: 0.8740876317024231\n",
            "Saving model, epoch: 17313, train_loss: 0.9014388918876648, val_loss: 0.8740864992141724\n",
            "Saving model, epoch: 17314, train_loss: 0.901437520980835, val_loss: 0.874085545539856\n",
            "Saving model, epoch: 17315, train_loss: 0.9014362096786499, val_loss: 0.8740846514701843\n",
            "Saving model, epoch: 17316, train_loss: 0.9014347791671753, val_loss: 0.8740836381912231\n",
            "Saving model, epoch: 17317, train_loss: 0.9014334678649902, val_loss: 0.8740825653076172\n",
            "Saving model, epoch: 17318, train_loss: 0.9014319777488708, val_loss: 0.8740814328193665\n",
            "Saving model, epoch: 17319, train_loss: 0.9014306664466858, val_loss: 0.87408047914505\n",
            "Saving model, epoch: 17320, train_loss: 0.9014294743537903, val_loss: 0.8740795254707336\n",
            "Saving model, epoch: 17321, train_loss: 0.9014280438423157, val_loss: 0.8740786910057068\n",
            "Saving model, epoch: 17322, train_loss: 0.9014266729354858, val_loss: 0.8740776181221008\n",
            "Saving model, epoch: 17323, train_loss: 0.9014253616333008, val_loss: 0.8740766644477844\n",
            "Saving model, epoch: 17324, train_loss: 0.9014240503311157, val_loss: 0.8740755319595337\n",
            "Saving model, epoch: 17325, train_loss: 0.9014226198196411, val_loss: 0.874074399471283\n",
            "Saving model, epoch: 17326, train_loss: 0.9014212489128113, val_loss: 0.874073326587677\n",
            "Saving model, epoch: 17327, train_loss: 0.9014198184013367, val_loss: 0.874072253704071\n",
            "Saving model, epoch: 17328, train_loss: 0.9014186263084412, val_loss: 0.8740715384483337\n",
            "Saving model, epoch: 17329, train_loss: 0.9014171957969666, val_loss: 0.8740705847740173\n",
            "Saving model, epoch: 17330, train_loss: 0.9014158248901367, val_loss: 0.8740695118904114\n",
            "Saving model, epoch: 17331, train_loss: 0.9014143943786621, val_loss: 0.8740684986114502\n",
            "Saving model, epoch: 17332, train_loss: 0.9014132022857666, val_loss: 0.8740673065185547\n",
            "Saving model, epoch: 17333, train_loss: 0.9014117121696472, val_loss: 0.8740664720535278\n",
            "Saving model, epoch: 17334, train_loss: 0.9014104008674622, val_loss: 0.8740654587745667\n",
            "Saving model, epoch: 17335, train_loss: 0.9014089703559875, val_loss: 0.8740644454956055\n",
            "Saving model, epoch: 17336, train_loss: 0.9014075994491577, val_loss: 0.8740633130073547\n",
            "Saving model, epoch: 17337, train_loss: 0.9014062881469727, val_loss: 0.8740623593330383\n",
            "Saving model, epoch: 17338, train_loss: 0.9014049768447876, val_loss: 0.8740612864494324\n",
            "Saving model, epoch: 17339, train_loss: 0.901403546333313, val_loss: 0.8740602731704712\n",
            "Saving model, epoch: 17340, train_loss: 0.9014022350311279, val_loss: 0.8740593791007996\n",
            "Saving model, epoch: 17341, train_loss: 0.9014008641242981, val_loss: 0.8740582466125488\n",
            "Saving model, epoch: 17342, train_loss: 0.901399552822113, val_loss: 0.8740573525428772\n",
            "Saving model, epoch: 17343, train_loss: 0.901398241519928, val_loss: 0.8740563988685608\n",
            "Saving model, epoch: 17344, train_loss: 0.9013967514038086, val_loss: 0.8740550875663757\n",
            "Saving model, epoch: 17345, train_loss: 0.901395320892334, val_loss: 0.8740540146827698\n",
            "Saving model, epoch: 17346, train_loss: 0.9013941287994385, val_loss: 0.8740531206130981\n",
            "Saving model, epoch: 17347, train_loss: 0.9013926982879639, val_loss: 0.8740522265434265\n",
            "Saving model, epoch: 17348, train_loss: 0.901391327381134, val_loss: 0.8740512132644653\n",
            "Saving model, epoch: 17349, train_loss: 0.901390016078949, val_loss: 0.8740500807762146\n",
            "Saving model, epoch: 17350, train_loss: 0.9013884663581848, val_loss: 0.8740490674972534\n",
            "Saving model, epoch: 17351, train_loss: 0.901387095451355, val_loss: 0.8740483522415161\n",
            "Saving model, epoch: 17352, train_loss: 0.9013858437538147, val_loss: 0.8740472197532654\n",
            "Saving model, epoch: 17353, train_loss: 0.9013844728469849, val_loss: 0.8740461468696594\n",
            "Saving model, epoch: 17354, train_loss: 0.9013831615447998, val_loss: 0.8740451335906982\n",
            "Saving model, epoch: 17355, train_loss: 0.9013817310333252, val_loss: 0.8740440011024475\n",
            "Saving model, epoch: 17356, train_loss: 0.9013804197311401, val_loss: 0.8740432262420654\n",
            "Saving model, epoch: 17357, train_loss: 0.9013790488243103, val_loss: 0.8740419745445251\n",
            "Saving model, epoch: 17358, train_loss: 0.9013776183128357, val_loss: 0.8740408420562744\n",
            "Saving model, epoch: 17359, train_loss: 0.9013763070106506, val_loss: 0.8740401864051819\n",
            "Saving model, epoch: 17360, train_loss: 0.9013749957084656, val_loss: 0.8740392327308655\n",
            "Saving model, epoch: 17361, train_loss: 0.9013736248016357, val_loss: 0.87403804063797\n",
            "Saving model, epoch: 17362, train_loss: 0.9013723134994507, val_loss: 0.8740372657775879\n",
            "Saving model, epoch: 17363, train_loss: 0.9013708829879761, val_loss: 0.8740361928939819\n",
            "Saving model, epoch: 17364, train_loss: 0.9013695120811462, val_loss: 0.8740352392196655\n",
            "Saving model, epoch: 17365, train_loss: 0.9013682007789612, val_loss: 0.8740339279174805\n",
            "Saving model, epoch: 17366, train_loss: 0.9013668894767761, val_loss: 0.8740330338478088\n",
            "Saving model, epoch: 17367, train_loss: 0.9013654589653015, val_loss: 0.8740322589874268\n",
            "Saving model, epoch: 17368, train_loss: 0.9013641476631165, val_loss: 0.8740310072898865\n",
            "Saving model, epoch: 17369, train_loss: 0.9013627767562866, val_loss: 0.8740299940109253\n",
            "Saving model, epoch: 17370, train_loss: 0.901361346244812, val_loss: 0.874029278755188\n",
            "Saving model, epoch: 17371, train_loss: 0.901360034942627, val_loss: 0.8740281462669373\n",
            "Saving model, epoch: 17372, train_loss: 0.9013586640357971, val_loss: 0.8740269541740417\n",
            "Saving model, epoch: 17373, train_loss: 0.9013573527336121, val_loss: 0.8740261197090149\n",
            "Saving model, epoch: 17374, train_loss: 0.9013559222221375, val_loss: 0.8740248680114746\n",
            "Saving model, epoch: 17375, train_loss: 0.9013545513153076, val_loss: 0.8740240335464478\n",
            "Saving model, epoch: 17376, train_loss: 0.9013532996177673, val_loss: 0.8740231990814209\n",
            "Saving model, epoch: 17377, train_loss: 0.9013519287109375, val_loss: 0.8740219473838806\n",
            "Saving model, epoch: 17378, train_loss: 0.9013504981994629, val_loss: 0.8740208148956299\n",
            "Saving model, epoch: 17379, train_loss: 0.9013491272926331, val_loss: 0.874019980430603\n",
            "Saving model, epoch: 17380, train_loss: 0.9013476967811584, val_loss: 0.8740190267562866\n",
            "Saving model, epoch: 17381, train_loss: 0.9013463854789734, val_loss: 0.8740179538726807\n",
            "Saving model, epoch: 17382, train_loss: 0.9013450741767883, val_loss: 0.8740166425704956\n",
            "Saving model, epoch: 17383, train_loss: 0.9013437032699585, val_loss: 0.8740158677101135\n",
            "Saving model, epoch: 17384, train_loss: 0.9013422727584839, val_loss: 0.8740150332450867\n",
            "Saving model, epoch: 17385, train_loss: 0.9013409614562988, val_loss: 0.8740138411521912\n",
            "Saving model, epoch: 17386, train_loss: 0.9013396501541138, val_loss: 0.8740127682685852\n",
            "Saving model, epoch: 17387, train_loss: 0.9013382792472839, val_loss: 0.8740116953849792\n",
            "Saving model, epoch: 17388, train_loss: 0.9013368487358093, val_loss: 0.8740110993385315\n",
            "Saving model, epoch: 17389, train_loss: 0.9013354182243347, val_loss: 0.8740097880363464\n",
            "Saving model, epoch: 17390, train_loss: 0.9013342261314392, val_loss: 0.8740086555480957\n",
            "Saving model, epoch: 17391, train_loss: 0.9013327956199646, val_loss: 0.8740077018737793\n",
            "Saving model, epoch: 17392, train_loss: 0.9013314247131348, val_loss: 0.8740068674087524\n",
            "Saving model, epoch: 17393, train_loss: 0.9013301134109497, val_loss: 0.8740057349205017\n",
            "Saving model, epoch: 17394, train_loss: 0.9013286232948303, val_loss: 0.8740045428276062\n",
            "Saving model, epoch: 17395, train_loss: 0.9013271927833557, val_loss: 0.8740036487579346\n",
            "Saving model, epoch: 17396, train_loss: 0.9013260006904602, val_loss: 0.8740026354789734\n",
            "Saving model, epoch: 17397, train_loss: 0.9013245701789856, val_loss: 0.8740018010139465\n",
            "Saving model, epoch: 17398, train_loss: 0.9013231992721558, val_loss: 0.874000608921051\n",
            "Saving model, epoch: 17399, train_loss: 0.9013218879699707, val_loss: 0.8739997744560242\n",
            "Saving model, epoch: 17400, train_loss: 0.9013203382492065, val_loss: 0.8739985823631287\n",
            "epoch: 17401, train_loss: 0.9013190865516663, val_loss: 0.8739974498748779\n",
            "Saving model, epoch: 17401, train_loss: 0.9013190865516663, val_loss: 0.8739974498748779\n",
            "Saving model, epoch: 17402, train_loss: 0.9013177752494812, val_loss: 0.8739966750144958\n",
            "Saving model, epoch: 17403, train_loss: 0.9013163447380066, val_loss: 0.8739957213401794\n",
            "Saving model, epoch: 17404, train_loss: 0.9013150334358215, val_loss: 0.8739947080612183\n",
            "Saving model, epoch: 17405, train_loss: 0.9013137221336365, val_loss: 0.873993456363678\n",
            "Saving model, epoch: 17406, train_loss: 0.9013123512268066, val_loss: 0.8739925622940063\n",
            "Saving model, epoch: 17407, train_loss: 0.901310920715332, val_loss: 0.8739914894104004\n",
            "Saving model, epoch: 17408, train_loss: 0.9013097286224365, val_loss: 0.8739904165267944\n",
            "Saving model, epoch: 17409, train_loss: 0.9013082981109619, val_loss: 0.8739897608757019\n",
            "Saving model, epoch: 17410, train_loss: 0.9013068079948425, val_loss: 0.8739885091781616\n",
            "Saving model, epoch: 17411, train_loss: 0.9013054966926575, val_loss: 0.8739874958992004\n",
            "Saving model, epoch: 17412, train_loss: 0.9013040661811829, val_loss: 0.8739863634109497\n",
            "Saving model, epoch: 17413, train_loss: 0.901302695274353, val_loss: 0.8739854097366333\n",
            "Saving model, epoch: 17414, train_loss: 0.901301383972168, val_loss: 0.8739843964576721\n",
            "Saving model, epoch: 17415, train_loss: 0.9012999534606934, val_loss: 0.8739833831787109\n",
            "Saving model, epoch: 17416, train_loss: 0.9012986421585083, val_loss: 0.873982310295105\n",
            "Saving model, epoch: 17417, train_loss: 0.9012972712516785, val_loss: 0.8739813566207886\n",
            "Saving model, epoch: 17418, train_loss: 0.9012958407402039, val_loss: 0.8739803433418274\n",
            "Saving model, epoch: 17419, train_loss: 0.901294469833374, val_loss: 0.8739795088768005\n",
            "Saving model, epoch: 17420, train_loss: 0.901293158531189, val_loss: 0.873978316783905\n",
            "Saving model, epoch: 17421, train_loss: 0.9012918472290039, val_loss: 0.8739773035049438\n",
            "Saving model, epoch: 17422, train_loss: 0.9012904167175293, val_loss: 0.8739761710166931\n",
            "Saving model, epoch: 17423, train_loss: 0.9012890458106995, val_loss: 0.8739752769470215\n",
            "Saving model, epoch: 17424, train_loss: 0.9012876152992249, val_loss: 0.873974084854126\n",
            "Saving model, epoch: 17425, train_loss: 0.9012864232063293, val_loss: 0.8739733695983887\n",
            "Saving model, epoch: 17426, train_loss: 0.9012849926948547, val_loss: 0.8739722371101379\n",
            "Saving model, epoch: 17427, train_loss: 0.9012836217880249, val_loss: 0.8739713430404663\n",
            "Saving model, epoch: 17428, train_loss: 0.9012821912765503, val_loss: 0.8739701509475708\n",
            "Saving model, epoch: 17429, train_loss: 0.9012808799743652, val_loss: 0.873969316482544\n",
            "Saving model, epoch: 17430, train_loss: 0.9012794494628906, val_loss: 0.8739681243896484\n",
            "Saving model, epoch: 17431, train_loss: 0.9012781977653503, val_loss: 0.8739672303199768\n",
            "Saving model, epoch: 17432, train_loss: 0.9012767672538757, val_loss: 0.8739662170410156\n",
            "Saving model, epoch: 17433, train_loss: 0.9012753367424011, val_loss: 0.8739648461341858\n",
            "Saving model, epoch: 17434, train_loss: 0.9012739658355713, val_loss: 0.8739643096923828\n",
            "Saving model, epoch: 17435, train_loss: 0.9012726545333862, val_loss: 0.8739632964134216\n",
            "Saving model, epoch: 17436, train_loss: 0.9012713432312012, val_loss: 0.8739620447158813\n",
            "Saving model, epoch: 17437, train_loss: 0.9012699127197266, val_loss: 0.8739609122276306\n",
            "Saving model, epoch: 17438, train_loss: 0.9012685418128967, val_loss: 0.8739603757858276\n",
            "Saving model, epoch: 17439, train_loss: 0.9012671113014221, val_loss: 0.8739590048789978\n",
            "Saving model, epoch: 17440, train_loss: 0.9012657403945923, val_loss: 0.8739579319953918\n",
            "Saving model, epoch: 17441, train_loss: 0.9012644290924072, val_loss: 0.8739569187164307\n",
            "Saving model, epoch: 17442, train_loss: 0.9012631177902222, val_loss: 0.8739558458328247\n",
            "Saving model, epoch: 17443, train_loss: 0.9012616872787476, val_loss: 0.8739550709724426\n",
            "Saving model, epoch: 17444, train_loss: 0.901260256767273, val_loss: 0.8739538788795471\n",
            "Saving model, epoch: 17445, train_loss: 0.9012588858604431, val_loss: 0.8739528656005859\n",
            "Saving model, epoch: 17446, train_loss: 0.9012575745582581, val_loss: 0.8739518523216248\n",
            "Saving model, epoch: 17447, train_loss: 0.901256263256073, val_loss: 0.8739510774612427\n",
            "Saving model, epoch: 17448, train_loss: 0.9012548327445984, val_loss: 0.8739499449729919\n",
            "Saving model, epoch: 17449, train_loss: 0.9012534618377686, val_loss: 0.8739487528800964\n",
            "Saving model, epoch: 17450, train_loss: 0.901252031326294, val_loss: 0.8739477396011353\n",
            "Saving model, epoch: 17451, train_loss: 0.9012507200241089, val_loss: 0.8739469647407532\n",
            "Saving model, epoch: 17452, train_loss: 0.9012494087219238, val_loss: 0.8739458322525024\n",
            "Saving model, epoch: 17453, train_loss: 0.901248037815094, val_loss: 0.8739449381828308\n",
            "Saving model, epoch: 17454, train_loss: 0.9012466073036194, val_loss: 0.8739437460899353\n",
            "Saving model, epoch: 17455, train_loss: 0.9012452363967896, val_loss: 0.873943030834198\n",
            "Saving model, epoch: 17456, train_loss: 0.9012438058853149, val_loss: 0.8739420771598816\n",
            "Saving model, epoch: 17457, train_loss: 0.9012424945831299, val_loss: 0.8739405870437622\n",
            "Saving model, epoch: 17458, train_loss: 0.9012411236763, val_loss: 0.8739396929740906\n",
            "Saving model, epoch: 17459, train_loss: 0.9012396931648254, val_loss: 0.873938798904419\n",
            "Saving model, epoch: 17460, train_loss: 0.9012383818626404, val_loss: 0.873937726020813\n",
            "Saving model, epoch: 17461, train_loss: 0.9012369513511658, val_loss: 0.8739367127418518\n",
            "Saving model, epoch: 17462, train_loss: 0.9012356996536255, val_loss: 0.8739354610443115\n",
            "Saving model, epoch: 17463, train_loss: 0.9012342691421509, val_loss: 0.8739345669746399\n",
            "Saving model, epoch: 17464, train_loss: 0.9012329578399658, val_loss: 0.8739337921142578\n",
            "Saving model, epoch: 17465, train_loss: 0.9012315273284912, val_loss: 0.8739325404167175\n",
            "Saving model, epoch: 17466, train_loss: 0.9012300372123718, val_loss: 0.8739314675331116\n",
            "Saving model, epoch: 17467, train_loss: 0.9012287259101868, val_loss: 0.8739305138587952\n",
            "Saving model, epoch: 17468, train_loss: 0.9012273550033569, val_loss: 0.8739296793937683\n",
            "Saving model, epoch: 17469, train_loss: 0.9012260437011719, val_loss: 0.8739286065101624\n",
            "Saving model, epoch: 17470, train_loss: 0.9012246131896973, val_loss: 0.8739273548126221\n",
            "Saving model, epoch: 17471, train_loss: 0.9012232422828674, val_loss: 0.8739264607429504\n",
            "Saving model, epoch: 17472, train_loss: 0.9012219309806824, val_loss: 0.8739254474639893\n",
            "Saving model, epoch: 17473, train_loss: 0.9012205004692078, val_loss: 0.8739244937896729\n",
            "Saving model, epoch: 17474, train_loss: 0.9012191891670227, val_loss: 0.8739233613014221\n",
            "Saving model, epoch: 17475, train_loss: 0.9012178778648376, val_loss: 0.8739222884178162\n",
            "Saving model, epoch: 17476, train_loss: 0.9012163877487183, val_loss: 0.873921275138855\n",
            "Saving model, epoch: 17477, train_loss: 0.9012150764465332, val_loss: 0.8739203214645386\n",
            "Saving model, epoch: 17478, train_loss: 0.9012136459350586, val_loss: 0.8739194273948669\n",
            "Saving model, epoch: 17479, train_loss: 0.9012122750282288, val_loss: 0.8739181160926819\n",
            "Saving model, epoch: 17480, train_loss: 0.9012109637260437, val_loss: 0.8739175200462341\n",
            "Saving model, epoch: 17481, train_loss: 0.9012095332145691, val_loss: 0.8739162087440491\n",
            "Saving model, epoch: 17482, train_loss: 0.9012080430984497, val_loss: 0.8739151358604431\n",
            "Saving model, epoch: 17483, train_loss: 0.9012068510055542, val_loss: 0.8739142417907715\n",
            "Saving model, epoch: 17484, train_loss: 0.9012054204940796, val_loss: 0.8739132285118103\n",
            "Saving model, epoch: 17485, train_loss: 0.9012040495872498, val_loss: 0.8739122152328491\n",
            "Saving model, epoch: 17486, train_loss: 0.9012026190757751, val_loss: 0.8739112019538879\n",
            "Saving model, epoch: 17487, train_loss: 0.9012011885643005, val_loss: 0.8739100694656372\n",
            "Saving model, epoch: 17488, train_loss: 0.9011998772621155, val_loss: 0.8739091157913208\n",
            "Saving model, epoch: 17489, train_loss: 0.9011985063552856, val_loss: 0.8739081621170044\n",
            "Saving model, epoch: 17490, train_loss: 0.9011971950531006, val_loss: 0.8739073872566223\n",
            "Saving model, epoch: 17491, train_loss: 0.901195764541626, val_loss: 0.8739060163497925\n",
            "Saving model, epoch: 17492, train_loss: 0.9011943936347961, val_loss: 0.8739050030708313\n",
            "Saving model, epoch: 17493, train_loss: 0.9011932015419006, val_loss: 0.8739038109779358\n",
            "Saving model, epoch: 17494, train_loss: 0.901191771030426, val_loss: 0.8739031553268433\n",
            "Saving model, epoch: 17495, train_loss: 0.9011903405189514, val_loss: 0.8739019632339478\n",
            "Saving model, epoch: 17496, train_loss: 0.9011889696121216, val_loss: 0.8739010691642761\n",
            "Saving model, epoch: 17497, train_loss: 0.9011876583099365, val_loss: 0.8738996386528015\n",
            "Saving model, epoch: 17498, train_loss: 0.9011861681938171, val_loss: 0.873898983001709\n",
            "Saving model, epoch: 17499, train_loss: 0.9011847376823425, val_loss: 0.8738978505134583\n",
            "Saving model, epoch: 17500, train_loss: 0.9011834263801575, val_loss: 0.8738967776298523\n",
            "epoch: 17501, train_loss: 0.9011821150779724, val_loss: 0.8738958239555359\n",
            "Saving model, epoch: 17501, train_loss: 0.9011821150779724, val_loss: 0.8738958239555359\n",
            "Saving model, epoch: 17502, train_loss: 0.9011807441711426, val_loss: 0.8738948106765747\n",
            "Saving model, epoch: 17503, train_loss: 0.901179313659668, val_loss: 0.8738939166069031\n",
            "Saving model, epoch: 17504, train_loss: 0.9011778831481934, val_loss: 0.8738926649093628\n",
            "Saving model, epoch: 17505, train_loss: 0.9011765122413635, val_loss: 0.8738918304443359\n",
            "Saving model, epoch: 17506, train_loss: 0.9011752605438232, val_loss: 0.8738906383514404\n",
            "Saving model, epoch: 17507, train_loss: 0.9011738896369934, val_loss: 0.8738900423049927\n",
            "Saving model, epoch: 17508, train_loss: 0.9011724591255188, val_loss: 0.8738885521888733\n",
            "Saving model, epoch: 17509, train_loss: 0.901171088218689, val_loss: 0.8738877177238464\n",
            "Saving model, epoch: 17510, train_loss: 0.9011696577072144, val_loss: 0.8738866448402405\n",
            "Saving model, epoch: 17511, train_loss: 0.9011682271957397, val_loss: 0.8738857507705688\n",
            "Saving model, epoch: 17512, train_loss: 0.9011668562889099, val_loss: 0.8738845586776733\n",
            "Saving model, epoch: 17513, train_loss: 0.9011654257774353, val_loss: 0.8738836646080017\n",
            "Saving model, epoch: 17514, train_loss: 0.9011642336845398, val_loss: 0.8738828301429749\n",
            "Saving model, epoch: 17515, train_loss: 0.9011627435684204, val_loss: 0.8738817572593689\n",
            "Saving model, epoch: 17516, train_loss: 0.9011614322662354, val_loss: 0.8738805651664734\n",
            "Saving model, epoch: 17517, train_loss: 0.9011600017547607, val_loss: 0.873879611492157\n",
            "Saving model, epoch: 17518, train_loss: 0.9011586308479309, val_loss: 0.8738784790039062\n",
            "Saving model, epoch: 17519, train_loss: 0.9011573195457458, val_loss: 0.8738775849342346\n",
            "Saving model, epoch: 17520, train_loss: 0.9011560082435608, val_loss: 0.8738765120506287\n",
            "Saving model, epoch: 17521, train_loss: 0.9011545181274414, val_loss: 0.8738753795623779\n",
            "Saving model, epoch: 17522, train_loss: 0.9011532068252563, val_loss: 0.8738745450973511\n",
            "Saving model, epoch: 17523, train_loss: 0.9011517763137817, val_loss: 0.8738734126091003\n",
            "Saving model, epoch: 17524, train_loss: 0.9011503458023071, val_loss: 0.8738722801208496\n",
            "Saving model, epoch: 17525, train_loss: 0.9011489748954773, val_loss: 0.873871386051178\n",
            "Saving model, epoch: 17526, train_loss: 0.9011475443840027, val_loss: 0.8738702535629272\n",
            "Saving model, epoch: 17527, train_loss: 0.9011462330818176, val_loss: 0.8738693594932556\n",
            "Saving model, epoch: 17528, train_loss: 0.9011448621749878, val_loss: 0.8738682270050049\n",
            "Saving model, epoch: 17529, train_loss: 0.9011435508728027, val_loss: 0.8738672137260437\n",
            "Saving model, epoch: 17530, train_loss: 0.9011421203613281, val_loss: 0.8738663196563721\n",
            "Saving model, epoch: 17531, train_loss: 0.9011407494544983, val_loss: 0.8738653063774109\n",
            "Saving model, epoch: 17532, train_loss: 0.9011393189430237, val_loss: 0.8738644123077393\n",
            "Saving model, epoch: 17533, train_loss: 0.9011378884315491, val_loss: 0.8738633990287781\n",
            "Saving model, epoch: 17534, train_loss: 0.9011366367340088, val_loss: 0.8738621473312378\n",
            "Saving model, epoch: 17535, train_loss: 0.9011353254318237, val_loss: 0.8738609552383423\n",
            "Saving model, epoch: 17536, train_loss: 0.9011337757110596, val_loss: 0.8738602995872498\n",
            "Saving model, epoch: 17537, train_loss: 0.9011324644088745, val_loss: 0.8738591074943542\n",
            "Saving model, epoch: 17538, train_loss: 0.9011310935020447, val_loss: 0.8738580346107483\n",
            "Saving model, epoch: 17539, train_loss: 0.9011296629905701, val_loss: 0.8738568425178528\n",
            "Saving model, epoch: 17540, train_loss: 0.901128351688385, val_loss: 0.873856246471405\n",
            "Saving model, epoch: 17541, train_loss: 0.9011269807815552, val_loss: 0.8738551735877991\n",
            "Saving model, epoch: 17542, train_loss: 0.9011255502700806, val_loss: 0.873853862285614\n",
            "Saving model, epoch: 17543, train_loss: 0.9011242389678955, val_loss: 0.8738530278205872\n",
            "Saving model, epoch: 17544, train_loss: 0.9011228680610657, val_loss: 0.873852014541626\n",
            "Saving model, epoch: 17545, train_loss: 0.9011214375495911, val_loss: 0.87385094165802\n",
            "Saving model, epoch: 17546, train_loss: 0.9011200070381165, val_loss: 0.8738500475883484\n",
            "Saving model, epoch: 17547, train_loss: 0.9011186957359314, val_loss: 0.8738489151000977\n",
            "Saving model, epoch: 17548, train_loss: 0.901117205619812, val_loss: 0.8738481402397156\n",
            "Saving model, epoch: 17549, train_loss: 0.901115894317627, val_loss: 0.8738468885421753\n",
            "Saving model, epoch: 17550, train_loss: 0.9011145830154419, val_loss: 0.8738459944725037\n",
            "Saving model, epoch: 17551, train_loss: 0.9011132121086121, val_loss: 0.8738447427749634\n",
            "Saving model, epoch: 17552, train_loss: 0.9011117815971375, val_loss: 0.8738438487052917\n",
            "Saving model, epoch: 17553, train_loss: 0.9011104106903076, val_loss: 0.8738427758216858\n",
            "Saving model, epoch: 17554, train_loss: 0.901108980178833, val_loss: 0.8738417625427246\n",
            "Saving model, epoch: 17555, train_loss: 0.9011075496673584, val_loss: 0.8738408088684082\n",
            "Saving model, epoch: 17556, train_loss: 0.9011062383651733, val_loss: 0.873839795589447\n",
            "Saving model, epoch: 17557, train_loss: 0.9011049866676331, val_loss: 0.8738387227058411\n",
            "Saving model, epoch: 17558, train_loss: 0.9011034369468689, val_loss: 0.873837411403656\n",
            "Saving model, epoch: 17559, train_loss: 0.9011021256446838, val_loss: 0.8738364577293396\n",
            "Saving model, epoch: 17560, train_loss: 0.901100754737854, val_loss: 0.8738357424736023\n",
            "Saving model, epoch: 17561, train_loss: 0.9010993242263794, val_loss: 0.8738346099853516\n",
            "Saving model, epoch: 17562, train_loss: 0.9010979533195496, val_loss: 0.8738334774971008\n",
            "Saving model, epoch: 17563, train_loss: 0.9010966420173645, val_loss: 0.8738327026367188\n",
            "Saving model, epoch: 17564, train_loss: 0.9010953307151794, val_loss: 0.8738316297531128\n",
            "Saving model, epoch: 17565, train_loss: 0.9010939002037048, val_loss: 0.8738307356834412\n",
            "Saving model, epoch: 17566, train_loss: 0.901092529296875, val_loss: 0.8738295435905457\n",
            "Saving model, epoch: 17567, train_loss: 0.9010910987854004, val_loss: 0.8738284707069397\n",
            "Saving model, epoch: 17568, train_loss: 0.9010896682739258, val_loss: 0.8738275170326233\n",
            "Saving model, epoch: 17569, train_loss: 0.901088297367096, val_loss: 0.8738265633583069\n",
            "Saving model, epoch: 17570, train_loss: 0.9010868668556213, val_loss: 0.8738253712654114\n",
            "Saving model, epoch: 17571, train_loss: 0.9010854959487915, val_loss: 0.873824417591095\n",
            "Saving model, epoch: 17572, train_loss: 0.9010841846466064, val_loss: 0.873823344707489\n",
            "Saving model, epoch: 17573, train_loss: 0.9010828733444214, val_loss: 0.8738223910331726\n",
            "Saving model, epoch: 17574, train_loss: 0.9010814428329468, val_loss: 0.8738212585449219\n",
            "Saving model, epoch: 17575, train_loss: 0.9010800719261169, val_loss: 0.8738201856613159\n",
            "Saving model, epoch: 17576, train_loss: 0.9010786414146423, val_loss: 0.8738192915916443\n",
            "Saving model, epoch: 17577, train_loss: 0.9010772109031677, val_loss: 0.8738180994987488\n",
            "Saving model, epoch: 17578, train_loss: 0.9010758399963379, val_loss: 0.8738172650337219\n",
            "Saving model, epoch: 17579, train_loss: 0.9010744094848633, val_loss: 0.8738160729408264\n",
            "Saving model, epoch: 17580, train_loss: 0.9010730385780334, val_loss: 0.8738152980804443\n",
            "Saving model, epoch: 17581, train_loss: 0.9010717272758484, val_loss: 0.8738144040107727\n",
            "Saving model, epoch: 17582, train_loss: 0.9010704159736633, val_loss: 0.8738132119178772\n",
            "Saving model, epoch: 17583, train_loss: 0.9010689854621887, val_loss: 0.8738122582435608\n",
            "Saving model, epoch: 17584, train_loss: 0.9010674953460693, val_loss: 0.8738110661506653\n",
            "Saving model, epoch: 17585, train_loss: 0.9010661840438843, val_loss: 0.8738099336624146\n",
            "Saving model, epoch: 17586, train_loss: 0.9010647535324097, val_loss: 0.8738090395927429\n",
            "Saving model, epoch: 17587, train_loss: 0.9010634422302246, val_loss: 0.8738080859184265\n",
            "Saving model, epoch: 17588, train_loss: 0.9010619521141052, val_loss: 0.8738071918487549\n",
            "Saving model, epoch: 17589, train_loss: 0.9010606408119202, val_loss: 0.8738059997558594\n",
            "Saving model, epoch: 17590, train_loss: 0.9010592699050903, val_loss: 0.873805046081543\n",
            "Saving model, epoch: 17591, train_loss: 0.9010579586029053, val_loss: 0.8738037943840027\n",
            "Saving model, epoch: 17592, train_loss: 0.9010565280914307, val_loss: 0.8738030195236206\n",
            "Saving model, epoch: 17593, train_loss: 0.9010551571846008, val_loss: 0.8738019466400146\n",
            "Saving model, epoch: 17594, train_loss: 0.9010537266731262, val_loss: 0.8738009929656982\n",
            "Saving model, epoch: 17595, train_loss: 0.9010522961616516, val_loss: 0.873799741268158\n",
            "Saving model, epoch: 17596, train_loss: 0.9010509252548218, val_loss: 0.8737987279891968\n",
            "Saving model, epoch: 17597, train_loss: 0.9010494947433472, val_loss: 0.8737978339195251\n",
            "Saving model, epoch: 17598, train_loss: 0.9010483026504517, val_loss: 0.8737968802452087\n",
            "Saving model, epoch: 17599, train_loss: 0.901046872138977, val_loss: 0.8737958669662476\n",
            "Saving model, epoch: 17600, train_loss: 0.9010453820228577, val_loss: 0.873794674873352\n",
            "epoch: 17601, train_loss: 0.9010440707206726, val_loss: 0.8737937211990356\n",
            "Saving model, epoch: 17601, train_loss: 0.9010440707206726, val_loss: 0.8737937211990356\n",
            "Saving model, epoch: 17602, train_loss: 0.901042640209198, val_loss: 0.8737926483154297\n",
            "Saving model, epoch: 17603, train_loss: 0.9010412693023682, val_loss: 0.8737918138504028\n",
            "Saving model, epoch: 17604, train_loss: 0.9010398387908936, val_loss: 0.8737905025482178\n",
            "Saving model, epoch: 17605, train_loss: 0.9010384678840637, val_loss: 0.8737897872924805\n",
            "Saving model, epoch: 17606, train_loss: 0.9010370373725891, val_loss: 0.8737887740135193\n",
            "Saving model, epoch: 17607, train_loss: 0.9010358452796936, val_loss: 0.8737878799438477\n",
            "Saving model, epoch: 17608, train_loss: 0.9010343551635742, val_loss: 0.8737863898277283\n",
            "Saving model, epoch: 17609, train_loss: 0.9010329246520996, val_loss: 0.8737854361534119\n",
            "Saving model, epoch: 17610, train_loss: 0.9010316133499146, val_loss: 0.8737844824790955\n",
            "Saving model, epoch: 17611, train_loss: 0.9010301828384399, val_loss: 0.8737835884094238\n",
            "Saving model, epoch: 17612, train_loss: 0.9010288119316101, val_loss: 0.8737825155258179\n",
            "Saving model, epoch: 17613, train_loss: 0.9010273814201355, val_loss: 0.8737813234329224\n",
            "Saving model, epoch: 17614, train_loss: 0.9010260701179504, val_loss: 0.8737804293632507\n",
            "Saving model, epoch: 17615, train_loss: 0.901024580001831, val_loss: 0.8737794160842896\n",
            "Saving model, epoch: 17616, train_loss: 0.901023268699646, val_loss: 0.8737784028053284\n",
            "Saving model, epoch: 17617, train_loss: 0.9010219573974609, val_loss: 0.873777449131012\n",
            "Saving model, epoch: 17618, train_loss: 0.9010205864906311, val_loss: 0.8737763166427612\n",
            "Saving model, epoch: 17619, train_loss: 0.9010191559791565, val_loss: 0.8737751245498657\n",
            "Saving model, epoch: 17620, train_loss: 0.9010177254676819, val_loss: 0.8737741708755493\n",
            "Saving model, epoch: 17621, train_loss: 0.901016354560852, val_loss: 0.8737733960151672\n",
            "Saving model, epoch: 17622, train_loss: 0.9010149240493774, val_loss: 0.8737720847129822\n",
            "Saving model, epoch: 17623, train_loss: 0.9010135531425476, val_loss: 0.8737711906433105\n",
            "Saving model, epoch: 17624, train_loss: 0.9010122418403625, val_loss: 0.8737700581550598\n",
            "Saving model, epoch: 17625, train_loss: 0.9010108113288879, val_loss: 0.873769223690033\n",
            "Saving model, epoch: 17626, train_loss: 0.9010094404220581, val_loss: 0.8737683296203613\n",
            "Saving model, epoch: 17627, train_loss: 0.901008129119873, val_loss: 0.8737668395042419\n",
            "Saving model, epoch: 17628, train_loss: 0.9010065793991089, val_loss: 0.8737660646438599\n",
            "Saving model, epoch: 17629, train_loss: 0.9010052680969238, val_loss: 0.8737649321556091\n",
            "Saving model, epoch: 17630, train_loss: 0.901003897190094, val_loss: 0.8737640380859375\n",
            "Saving model, epoch: 17631, train_loss: 0.9010024666786194, val_loss: 0.8737629055976868\n",
            "Saving model, epoch: 17632, train_loss: 0.9010010957717896, val_loss: 0.8737618327140808\n",
            "Saving model, epoch: 17633, train_loss: 0.9009996652603149, val_loss: 0.8737608790397644\n",
            "Saving model, epoch: 17634, train_loss: 0.9009983539581299, val_loss: 0.8737598061561584\n",
            "Saving model, epoch: 17635, train_loss: 0.9009969830513, val_loss: 0.8737586736679077\n",
            "Saving model, epoch: 17636, train_loss: 0.9009954333305359, val_loss: 0.8737577795982361\n",
            "Saving model, epoch: 17637, train_loss: 0.9009942412376404, val_loss: 0.8737567067146301\n",
            "Saving model, epoch: 17638, train_loss: 0.9009928107261658, val_loss: 0.8737558722496033\n",
            "Saving model, epoch: 17639, train_loss: 0.9009914398193359, val_loss: 0.873754620552063\n",
            "Saving model, epoch: 17640, train_loss: 0.9009900093078613, val_loss: 0.8737536668777466\n",
            "Saving model, epoch: 17641, train_loss: 0.9009886384010315, val_loss: 0.8737527132034302\n",
            "Saving model, epoch: 17642, train_loss: 0.9009873270988464, val_loss: 0.873751699924469\n",
            "Saving model, epoch: 17643, train_loss: 0.9009857773780823, val_loss: 0.8737505674362183\n",
            "Saving model, epoch: 17644, train_loss: 0.9009844064712524, val_loss: 0.8737493753433228\n",
            "Saving model, epoch: 17645, train_loss: 0.9009829759597778, val_loss: 0.8737485408782959\n",
            "Saving model, epoch: 17646, train_loss: 0.900981605052948, val_loss: 0.8737474679946899\n",
            "Saving model, epoch: 17647, train_loss: 0.9009802937507629, val_loss: 0.8737467527389526\n",
            "Saving model, epoch: 17648, train_loss: 0.9009789824485779, val_loss: 0.8737455010414124\n",
            "Saving model, epoch: 17649, train_loss: 0.9009774923324585, val_loss: 0.8737444877624512\n",
            "Saving model, epoch: 17650, train_loss: 0.9009761810302734, val_loss: 0.8737435340881348\n",
            "Saving model, epoch: 17651, train_loss: 0.9009747505187988, val_loss: 0.8737425208091736\n",
            "Saving model, epoch: 17652, train_loss: 0.9009733200073242, val_loss: 0.8737413883209229\n",
            "Saving model, epoch: 17653, train_loss: 0.9009720683097839, val_loss: 0.8737403750419617\n",
            "Saving model, epoch: 17654, train_loss: 0.9009707570075989, val_loss: 0.8737393617630005\n",
            "Saving model, epoch: 17655, train_loss: 0.9009692072868347, val_loss: 0.8737384676933289\n",
            "Saving model, epoch: 17656, train_loss: 0.9009678363800049, val_loss: 0.8737373352050781\n",
            "Saving model, epoch: 17657, train_loss: 0.9009662866592407, val_loss: 0.8737363219261169\n",
            "Saving model, epoch: 17658, train_loss: 0.9009650945663452, val_loss: 0.8737353086471558\n",
            "Saving model, epoch: 17659, train_loss: 0.9009637236595154, val_loss: 0.873734176158905\n",
            "Saving model, epoch: 17660, train_loss: 0.9009622931480408, val_loss: 0.873733401298523\n",
            "Saving model, epoch: 17661, train_loss: 0.9009608626365662, val_loss: 0.873732328414917\n",
            "Saving model, epoch: 17662, train_loss: 0.9009594917297363, val_loss: 0.8737313151359558\n",
            "Saving model, epoch: 17663, train_loss: 0.9009580612182617, val_loss: 0.8737301230430603\n",
            "Saving model, epoch: 17664, train_loss: 0.9009566903114319, val_loss: 0.8737289905548096\n",
            "Saving model, epoch: 17665, train_loss: 0.9009553790092468, val_loss: 0.8737282156944275\n",
            "Saving model, epoch: 17666, train_loss: 0.9009538292884827, val_loss: 0.8737273216247559\n",
            "Saving model, epoch: 17667, train_loss: 0.9009525775909424, val_loss: 0.8737260699272156\n",
            "Saving model, epoch: 17668, train_loss: 0.9009511470794678, val_loss: 0.873725175857544\n",
            "Saving model, epoch: 17669, train_loss: 0.9009497165679932, val_loss: 0.8737239837646484\n",
            "Saving model, epoch: 17670, train_loss: 0.9009483456611633, val_loss: 0.873723030090332\n",
            "Saving model, epoch: 17671, train_loss: 0.9009470343589783, val_loss: 0.8737220764160156\n",
            "Saving model, epoch: 17672, train_loss: 0.9009456038475037, val_loss: 0.8737210631370544\n",
            "Saving model, epoch: 17673, train_loss: 0.9009442329406738, val_loss: 0.8737198710441589\n",
            "Saving model, epoch: 17674, train_loss: 0.9009429216384888, val_loss: 0.8737188577651978\n",
            "Saving model, epoch: 17675, train_loss: 0.9009413719177246, val_loss: 0.8737179040908813\n",
            "Saving model, epoch: 17676, train_loss: 0.9009400010108948, val_loss: 0.8737170696258545\n",
            "Saving model, epoch: 17677, train_loss: 0.9009385704994202, val_loss: 0.8737158179283142\n",
            "Saving model, epoch: 17678, train_loss: 0.9009373784065247, val_loss: 0.8737149834632874\n",
            "Saving model, epoch: 17679, train_loss: 0.90093594789505, val_loss: 0.8737139701843262\n",
            "Saving model, epoch: 17680, train_loss: 0.9009345769882202, val_loss: 0.873712956905365\n",
            "Saving model, epoch: 17681, train_loss: 0.9009331464767456, val_loss: 0.8737119436264038\n",
            "Saving model, epoch: 17682, train_loss: 0.9009317755699158, val_loss: 0.8737108707427979\n",
            "Saving model, epoch: 17683, train_loss: 0.9009303450584412, val_loss: 0.8737097978591919\n",
            "Saving model, epoch: 17684, train_loss: 0.9009289145469666, val_loss: 0.8737087845802307\n",
            "Saving model, epoch: 17685, train_loss: 0.9009276032447815, val_loss: 0.8737075924873352\n",
            "Saving model, epoch: 17686, train_loss: 0.9009261131286621, val_loss: 0.8737066984176636\n",
            "Saving model, epoch: 17687, train_loss: 0.900924801826477, val_loss: 0.8737056851387024\n",
            "Saving model, epoch: 17688, train_loss: 0.9009233117103577, val_loss: 0.8737046718597412\n",
            "Saving model, epoch: 17689, train_loss: 0.9009220004081726, val_loss: 0.8737033605575562\n",
            "Saving model, epoch: 17690, train_loss: 0.900920569896698, val_loss: 0.8737024068832397\n",
            "Saving model, epoch: 17691, train_loss: 0.9009193181991577, val_loss: 0.8737016916275024\n",
            "Saving model, epoch: 17692, train_loss: 0.9009178876876831, val_loss: 0.873700737953186\n",
            "Saving model, epoch: 17693, train_loss: 0.9009164571762085, val_loss: 0.8736993670463562\n",
            "Saving model, epoch: 17694, train_loss: 0.9009150862693787, val_loss: 0.8736984729766846\n",
            "Saving model, epoch: 17695, train_loss: 0.900913655757904, val_loss: 0.8736975193023682\n",
            "Saving model, epoch: 17696, train_loss: 0.9009122848510742, val_loss: 0.873696506023407\n",
            "Saving model, epoch: 17697, train_loss: 0.9009108543395996, val_loss: 0.8736951351165771\n",
            "Saving model, epoch: 17698, train_loss: 0.900909423828125, val_loss: 0.8736943006515503\n",
            "Saving model, epoch: 17699, train_loss: 0.9009081125259399, val_loss: 0.873693585395813\n",
            "Saving model, epoch: 17700, train_loss: 0.9009067416191101, val_loss: 0.873692512512207\n",
            "epoch: 17701, train_loss: 0.9009052515029907, val_loss: 0.8736912608146667\n",
            "Saving model, epoch: 17701, train_loss: 0.9009052515029907, val_loss: 0.8736912608146667\n",
            "Saving model, epoch: 17702, train_loss: 0.9009039402008057, val_loss: 0.8736902475357056\n",
            "Saving model, epoch: 17703, train_loss: 0.9009023904800415, val_loss: 0.8736891746520996\n",
            "Saving model, epoch: 17704, train_loss: 0.9009010791778564, val_loss: 0.8736880421638489\n",
            "Saving model, epoch: 17705, train_loss: 0.9008997082710266, val_loss: 0.8736870884895325\n",
            "Saving model, epoch: 17706, train_loss: 0.9008983969688416, val_loss: 0.8736860156059265\n",
            "Saving model, epoch: 17707, train_loss: 0.9008969664573669, val_loss: 0.8736851811408997\n",
            "Saving model, epoch: 17708, train_loss: 0.9008955955505371, val_loss: 0.8736839890480042\n",
            "Saving model, epoch: 17709, train_loss: 0.9008941650390625, val_loss: 0.8736831545829773\n",
            "Saving model, epoch: 17710, train_loss: 0.9008927941322327, val_loss: 0.8736820220947266\n",
            "Saving model, epoch: 17711, train_loss: 0.9008913636207581, val_loss: 0.8736809492111206\n",
            "Saving model, epoch: 17712, train_loss: 0.9008899331092834, val_loss: 0.873680055141449\n",
            "Saving model, epoch: 17713, train_loss: 0.9008885622024536, val_loss: 0.8736789226531982\n",
            "Saving model, epoch: 17714, train_loss: 0.9008872509002686, val_loss: 0.8736778497695923\n",
            "Saving model, epoch: 17715, train_loss: 0.9008857607841492, val_loss: 0.8736770749092102\n",
            "Saving model, epoch: 17716, train_loss: 0.9008845090866089, val_loss: 0.8736759424209595\n",
            "Saving model, epoch: 17717, train_loss: 0.900883138179779, val_loss: 0.8736746311187744\n",
            "Saving model, epoch: 17718, train_loss: 0.9008817076683044, val_loss: 0.8736737966537476\n",
            "Saving model, epoch: 17719, train_loss: 0.9008802175521851, val_loss: 0.8736729025840759\n",
            "Saving model, epoch: 17720, train_loss: 0.90087890625, val_loss: 0.87367182970047\n",
            "Saving model, epoch: 17721, train_loss: 0.9008774757385254, val_loss: 0.8736705780029297\n",
            "Saving model, epoch: 17722, train_loss: 0.9008761048316956, val_loss: 0.8736696839332581\n",
            "Saving model, epoch: 17723, train_loss: 0.900874674320221, val_loss: 0.8736687898635864\n",
            "Saving model, epoch: 17724, train_loss: 0.9008733034133911, val_loss: 0.8736675977706909\n",
            "Saving model, epoch: 17725, train_loss: 0.900871992111206, val_loss: 0.8736667037010193\n",
            "Saving model, epoch: 17726, train_loss: 0.9008705615997314, val_loss: 0.8736655116081238\n",
            "Saving model, epoch: 17727, train_loss: 0.9008691310882568, val_loss: 0.8736646771430969\n",
            "Saving model, epoch: 17728, train_loss: 0.900867760181427, val_loss: 0.8736639022827148\n",
            "Saving model, epoch: 17729, train_loss: 0.9008663296699524, val_loss: 0.8736622929573059\n",
            "Saving model, epoch: 17730, train_loss: 0.9008650183677673, val_loss: 0.8736613988876343\n",
            "Saving model, epoch: 17731, train_loss: 0.9008636474609375, val_loss: 0.8736605048179626\n",
            "Saving model, epoch: 17732, train_loss: 0.9008622169494629, val_loss: 0.8736594319343567\n",
            "Saving model, epoch: 17733, train_loss: 0.9008608460426331, val_loss: 0.8736584186553955\n",
            "Saving model, epoch: 17734, train_loss: 0.9008594155311584, val_loss: 0.8736573457717896\n",
            "Saving model, epoch: 17735, train_loss: 0.9008579850196838, val_loss: 0.8736563920974731\n",
            "Saving model, epoch: 17736, train_loss: 0.900856614112854, val_loss: 0.8736553192138672\n",
            "Saving model, epoch: 17737, train_loss: 0.9008551836013794, val_loss: 0.8736541867256165\n",
            "Saving model, epoch: 17738, train_loss: 0.9008538126945496, val_loss: 0.8736534714698792\n",
            "Saving model, epoch: 17739, train_loss: 0.900852382183075, val_loss: 0.8736522197723389\n",
            "Saving model, epoch: 17740, train_loss: 0.9008509516716003, val_loss: 0.8736512064933777\n",
            "Saving model, epoch: 17741, train_loss: 0.9008497595787048, val_loss: 0.873650312423706\n",
            "Saving model, epoch: 17742, train_loss: 0.9008482694625854, val_loss: 0.8736492991447449\n",
            "Saving model, epoch: 17743, train_loss: 0.9008467793464661, val_loss: 0.8736480474472046\n",
            "Saving model, epoch: 17744, train_loss: 0.900845468044281, val_loss: 0.873647153377533\n",
            "Saving model, epoch: 17745, train_loss: 0.900844156742096, val_loss: 0.873646080493927\n",
            "Saving model, epoch: 17746, train_loss: 0.9008427262306213, val_loss: 0.8736453056335449\n",
            "Saving model, epoch: 17747, train_loss: 0.9008413553237915, val_loss: 0.8736441731452942\n",
            "Saving model, epoch: 17748, train_loss: 0.9008399248123169, val_loss: 0.8736430406570435\n",
            "Saving model, epoch: 17749, train_loss: 0.9008384943008423, val_loss: 0.8736422657966614\n",
            "Saving model, epoch: 17750, train_loss: 0.9008371233940125, val_loss: 0.8736408352851868\n",
            "Saving model, epoch: 17751, train_loss: 0.9008356928825378, val_loss: 0.8736398220062256\n",
            "Saving model, epoch: 17752, train_loss: 0.900834321975708, val_loss: 0.8736391067504883\n",
            "Saving model, epoch: 17753, train_loss: 0.9008328914642334, val_loss: 0.8736377954483032\n",
            "Saving model, epoch: 17754, train_loss: 0.9008314609527588, val_loss: 0.8736366629600525\n",
            "Saving model, epoch: 17755, train_loss: 0.900830090045929, val_loss: 0.8736358284950256\n",
            "Saving model, epoch: 17756, train_loss: 0.9008286595344543, val_loss: 0.8736348152160645\n",
            "Saving model, epoch: 17757, train_loss: 0.9008274674415588, val_loss: 0.8736339211463928\n",
            "Saving model, epoch: 17758, train_loss: 0.9008259773254395, val_loss: 0.8736327886581421\n",
            "Saving model, epoch: 17759, train_loss: 0.9008244276046753, val_loss: 0.8736315369606018\n",
            "Saving model, epoch: 17760, train_loss: 0.9008232355117798, val_loss: 0.8736308813095093\n",
            "Saving model, epoch: 17761, train_loss: 0.9008217453956604, val_loss: 0.8736298084259033\n",
            "Saving model, epoch: 17762, train_loss: 0.9008203148841858, val_loss: 0.8736286759376526\n",
            "Saving model, epoch: 17763, train_loss: 0.9008190035820007, val_loss: 0.8736276626586914\n",
            "Saving model, epoch: 17764, train_loss: 0.9008176326751709, val_loss: 0.8736265301704407\n",
            "Saving model, epoch: 17765, train_loss: 0.9008162021636963, val_loss: 0.8736254572868347\n",
            "Saving model, epoch: 17766, train_loss: 0.9008148312568665, val_loss: 0.873624324798584\n",
            "Saving model, epoch: 17767, train_loss: 0.9008134007453918, val_loss: 0.8736235499382019\n",
            "Saving model, epoch: 17768, train_loss: 0.9008119702339172, val_loss: 0.8736225366592407\n",
            "Saving model, epoch: 17769, train_loss: 0.9008105993270874, val_loss: 0.8736215233802795\n",
            "Saving model, epoch: 17770, train_loss: 0.9008091688156128, val_loss: 0.873620331287384\n",
            "Saving model, epoch: 17771, train_loss: 0.900807797908783, val_loss: 0.8736192584037781\n",
            "Saving model, epoch: 17772, train_loss: 0.9008063673973083, val_loss: 0.8736183643341064\n",
            "Saving model, epoch: 17773, train_loss: 0.9008049368858337, val_loss: 0.8736173510551453\n",
            "Saving model, epoch: 17774, train_loss: 0.9008035659790039, val_loss: 0.8736162781715393\n",
            "Saving model, epoch: 17775, train_loss: 0.9008023738861084, val_loss: 0.8736152052879333\n",
            "Saving model, epoch: 17776, train_loss: 0.9008008241653442, val_loss: 0.8736143708229065\n",
            "Saving model, epoch: 17777, train_loss: 0.9007995128631592, val_loss: 0.8736132383346558\n",
            "Saving model, epoch: 17778, train_loss: 0.9007981419563293, val_loss: 0.8736120462417603\n",
            "Saving model, epoch: 17779, train_loss: 0.9007965326309204, val_loss: 0.8736110329627991\n",
            "Saving model, epoch: 17780, train_loss: 0.9007953405380249, val_loss: 0.8736101984977722\n",
            "Saving model, epoch: 17781, train_loss: 0.9007939100265503, val_loss: 0.873609185218811\n",
            "Saving model, epoch: 17782, train_loss: 0.9007925987243652, val_loss: 0.8736080527305603\n",
            "Saving model, epoch: 17783, train_loss: 0.9007911086082458, val_loss: 0.8736069202423096\n",
            "Saving model, epoch: 17784, train_loss: 0.9007896780967712, val_loss: 0.873606264591217\n",
            "Saving model, epoch: 17785, train_loss: 0.9007883071899414, val_loss: 0.8736050128936768\n",
            "Saving model, epoch: 17786, train_loss: 0.9007868766784668, val_loss: 0.8736039996147156\n",
            "Saving model, epoch: 17787, train_loss: 0.9007855653762817, val_loss: 0.8736029863357544\n",
            "Saving model, epoch: 17788, train_loss: 0.9007840752601624, val_loss: 0.8736017346382141\n",
            "Saving model, epoch: 17789, train_loss: 0.9007826447486877, val_loss: 0.8736010193824768\n",
            "Saving model, epoch: 17790, train_loss: 0.9007812738418579, val_loss: 0.8735997080802917\n",
            "Saving model, epoch: 17791, train_loss: 0.9007799625396729, val_loss: 0.8735986948013306\n",
            "Saving model, epoch: 17792, train_loss: 0.9007785320281982, val_loss: 0.8735977411270142\n",
            "Saving model, epoch: 17793, train_loss: 0.9007770419120789, val_loss: 0.8735969066619873\n",
            "Saving model, epoch: 17794, train_loss: 0.9007757306098938, val_loss: 0.873595654964447\n",
            "Saving model, epoch: 17795, train_loss: 0.9007744193077087, val_loss: 0.8735947608947754\n",
            "Saving model, epoch: 17796, train_loss: 0.9007729887962341, val_loss: 0.8735936284065247\n",
            "Saving model, epoch: 17797, train_loss: 0.9007716178894043, val_loss: 0.8735927939414978\n",
            "Saving model, epoch: 17798, train_loss: 0.9007701873779297, val_loss: 0.873591423034668\n",
            "Saving model, epoch: 17799, train_loss: 0.9007688164710999, val_loss: 0.8735906481742859\n",
            "Saving model, epoch: 17800, train_loss: 0.9007673859596252, val_loss: 0.8735897541046143\n",
            "epoch: 17801, train_loss: 0.9007658958435059, val_loss: 0.8735886216163635\n",
            "Saving model, epoch: 17801, train_loss: 0.9007658958435059, val_loss: 0.8735886216163635\n",
            "Saving model, epoch: 17802, train_loss: 0.9007645845413208, val_loss: 0.8735874891281128\n",
            "Saving model, epoch: 17803, train_loss: 0.9007631540298462, val_loss: 0.8735864162445068\n",
            "Saving model, epoch: 17804, train_loss: 0.9007618427276611, val_loss: 0.8735855221748352\n",
            "Saving model, epoch: 17805, train_loss: 0.9007603526115417, val_loss: 0.8735846281051636\n",
            "Saving model, epoch: 17806, train_loss: 0.9007590413093567, val_loss: 0.8735833764076233\n",
            "Saving model, epoch: 17807, train_loss: 0.9007576107978821, val_loss: 0.8735822439193726\n",
            "Saving model, epoch: 17808, train_loss: 0.9007561206817627, val_loss: 0.8735814094543457\n",
            "Saving model, epoch: 17809, train_loss: 0.9007547497749329, val_loss: 0.8735802173614502\n",
            "Saving model, epoch: 17810, train_loss: 0.9007533192634583, val_loss: 0.8735790848731995\n",
            "Saving model, epoch: 17811, train_loss: 0.9007520079612732, val_loss: 0.8735783100128174\n",
            "Saving model, epoch: 17812, train_loss: 0.9007505774497986, val_loss: 0.8735772967338562\n",
            "Saving model, epoch: 17813, train_loss: 0.9007492065429688, val_loss: 0.873576283454895\n",
            "Saving model, epoch: 17814, train_loss: 0.9007478952407837, val_loss: 0.8735751509666443\n",
            "Saving model, epoch: 17815, train_loss: 0.9007464647293091, val_loss: 0.8735743761062622\n",
            "Saving model, epoch: 17816, train_loss: 0.9007449746131897, val_loss: 0.8735731244087219\n",
            "Saving model, epoch: 17817, train_loss: 0.9007436633110046, val_loss: 0.8735722303390503\n",
            "Saving model, epoch: 17818, train_loss: 0.9007422924041748, val_loss: 0.8735712170600891\n",
            "Saving model, epoch: 17819, train_loss: 0.9007408618927002, val_loss: 0.8735700845718384\n",
            "Saving model, epoch: 17820, train_loss: 0.9007393717765808, val_loss: 0.873569130897522\n",
            "Saving model, epoch: 17821, train_loss: 0.9007380604743958, val_loss: 0.873568058013916\n",
            "Saving model, epoch: 17822, train_loss: 0.9007366299629211, val_loss: 0.8735669851303101\n",
            "Saving model, epoch: 17823, train_loss: 0.9007352590560913, val_loss: 0.8735659122467041\n",
            "Saving model, epoch: 17824, train_loss: 0.9007339477539062, val_loss: 0.8735648989677429\n",
            "Saving model, epoch: 17825, train_loss: 0.9007323980331421, val_loss: 0.8735640048980713\n",
            "Saving model, epoch: 17826, train_loss: 0.9007310271263123, val_loss: 0.8735629320144653\n",
            "Saving model, epoch: 17827, train_loss: 0.9007295966148376, val_loss: 0.8735617995262146\n",
            "Saving model, epoch: 17828, train_loss: 0.9007284045219421, val_loss: 0.8735606074333191\n",
            "Saving model, epoch: 17829, train_loss: 0.9007267951965332, val_loss: 0.8735597133636475\n",
            "Saving model, epoch: 17830, train_loss: 0.9007256031036377, val_loss: 0.8735587000846863\n",
            "Saving model, epoch: 17831, train_loss: 0.9007240533828735, val_loss: 0.8735578656196594\n",
            "Saving model, epoch: 17832, train_loss: 0.9007226824760437, val_loss: 0.8735567331314087\n",
            "Saving model, epoch: 17833, train_loss: 0.9007213711738586, val_loss: 0.8735555410385132\n",
            "Saving model, epoch: 17834, train_loss: 0.900719940662384, val_loss: 0.873554527759552\n",
            "Saving model, epoch: 17835, train_loss: 0.9007185697555542, val_loss: 0.8735536336898804\n",
            "Saving model, epoch: 17836, train_loss: 0.9007171392440796, val_loss: 0.8735527992248535\n",
            "Saving model, epoch: 17837, train_loss: 0.9007157683372498, val_loss: 0.8735516667366028\n",
            "Saving model, epoch: 17838, train_loss: 0.9007143378257751, val_loss: 0.8735505938529968\n",
            "Saving model, epoch: 17839, train_loss: 0.9007129073143005, val_loss: 0.8735495805740356\n",
            "Saving model, epoch: 17840, train_loss: 0.9007115364074707, val_loss: 0.8735483884811401\n",
            "Saving model, epoch: 17841, train_loss: 0.9007101058959961, val_loss: 0.8735474348068237\n",
            "Saving model, epoch: 17842, train_loss: 0.9007087349891663, val_loss: 0.873546302318573\n",
            "Saving model, epoch: 17843, train_loss: 0.9007073044776917, val_loss: 0.8735454082489014\n",
            "Saving model, epoch: 17844, train_loss: 0.900705873966217, val_loss: 0.8735442757606506\n",
            "Saving model, epoch: 17845, train_loss: 0.9007045030593872, val_loss: 0.8735433220863342\n",
            "Saving model, epoch: 17846, train_loss: 0.9007030725479126, val_loss: 0.8735420107841492\n",
            "Saving model, epoch: 17847, train_loss: 0.9007017612457275, val_loss: 0.8735411167144775\n",
            "Saving model, epoch: 17848, train_loss: 0.9007004499435425, val_loss: 0.8735403418540955\n",
            "Saving model, epoch: 17849, train_loss: 0.9006988406181335, val_loss: 0.8735390901565552\n",
            "Saving model, epoch: 17850, train_loss: 0.9006974697113037, val_loss: 0.8735382556915283\n",
            "Saving model, epoch: 17851, train_loss: 0.9006960391998291, val_loss: 0.8735372424125671\n",
            "Saving model, epoch: 17852, train_loss: 0.900694727897644, val_loss: 0.8735362887382507\n",
            "Saving model, epoch: 17853, train_loss: 0.9006933569908142, val_loss: 0.8735350370407104\n",
            "Saving model, epoch: 17854, train_loss: 0.9006919264793396, val_loss: 0.8735341429710388\n",
            "Saving model, epoch: 17855, train_loss: 0.9006906151771545, val_loss: 0.8735332489013672\n",
            "Saving model, epoch: 17856, train_loss: 0.9006890058517456, val_loss: 0.8735321164131165\n",
            "Saving model, epoch: 17857, train_loss: 0.9006878137588501, val_loss: 0.8735309839248657\n",
            "Saving model, epoch: 17858, train_loss: 0.9006863236427307, val_loss: 0.8735302090644836\n",
            "Saving model, epoch: 17859, train_loss: 0.9006848931312561, val_loss: 0.8735291361808777\n",
            "Saving model, epoch: 17860, train_loss: 0.9006834626197815, val_loss: 0.8735277652740479\n",
            "Saving model, epoch: 17861, train_loss: 0.9006822109222412, val_loss: 0.8735268712043762\n",
            "Saving model, epoch: 17862, train_loss: 0.9006807804107666, val_loss: 0.8735259771347046\n",
            "Saving model, epoch: 17863, train_loss: 0.900679349899292, val_loss: 0.8735249638557434\n",
            "Saving model, epoch: 17864, train_loss: 0.9006779789924622, val_loss: 0.873523473739624\n",
            "Saving model, epoch: 17865, train_loss: 0.9006765484809875, val_loss: 0.8735226988792419\n",
            "Saving model, epoch: 17866, train_loss: 0.9006751775741577, val_loss: 0.8735216856002808\n",
            "Saving model, epoch: 17867, train_loss: 0.9006737470626831, val_loss: 0.8735206723213196\n",
            "Saving model, epoch: 17868, train_loss: 0.9006723165512085, val_loss: 0.8735196590423584\n",
            "Saving model, epoch: 17869, train_loss: 0.9006709456443787, val_loss: 0.8735185861587524\n",
            "Saving model, epoch: 17870, train_loss: 0.900669515132904, val_loss: 0.873517632484436\n",
            "Saving model, epoch: 17871, train_loss: 0.9006681442260742, val_loss: 0.8735167980194092\n",
            "Saving model, epoch: 17872, train_loss: 0.9006668329238892, val_loss: 0.8735157251358032\n",
            "Saving model, epoch: 17873, train_loss: 0.9006654024124146, val_loss: 0.8735144734382629\n",
            "Saving model, epoch: 17874, train_loss: 0.9006639719009399, val_loss: 0.873513400554657\n",
            "Saving model, epoch: 17875, train_loss: 0.9006626009941101, val_loss: 0.8735125660896301\n",
            "Saving model, epoch: 17876, train_loss: 0.9006611108779907, val_loss: 0.8735114336013794\n",
            "Saving model, epoch: 17877, train_loss: 0.9006598591804504, val_loss: 0.8735103607177734\n",
            "Saving model, epoch: 17878, train_loss: 0.9006584882736206, val_loss: 0.8735092282295227\n",
            "Saving model, epoch: 17879, train_loss: 0.9006568789482117, val_loss: 0.8735082149505615\n",
            "Saving model, epoch: 17880, train_loss: 0.9006555676460266, val_loss: 0.8735072016716003\n",
            "Saving model, epoch: 17881, train_loss: 0.900654137134552, val_loss: 0.8735064268112183\n",
            "Saving model, epoch: 17882, train_loss: 0.9006528258323669, val_loss: 0.873505175113678\n",
            "Saving model, epoch: 17883, train_loss: 0.9006514549255371, val_loss: 0.8735044002532959\n",
            "Saving model, epoch: 17884, train_loss: 0.9006500244140625, val_loss: 0.8735035061836243\n",
            "Saving model, epoch: 17885, train_loss: 0.9006486535072327, val_loss: 0.8735021352767944\n",
            "Saving model, epoch: 17886, train_loss: 0.9006472229957581, val_loss: 0.8735010623931885\n",
            "Saving model, epoch: 17887, train_loss: 0.9006457924842834, val_loss: 0.8735002279281616\n",
            "Saving model, epoch: 17888, train_loss: 0.9006444215774536, val_loss: 0.8734991550445557\n",
            "Saving model, epoch: 17889, train_loss: 0.900642991065979, val_loss: 0.8734980225563049\n",
            "Saving model, epoch: 17890, train_loss: 0.9006416201591492, val_loss: 0.8734971284866333\n",
            "Saving model, epoch: 17891, train_loss: 0.9006401896476746, val_loss: 0.8734961748123169\n",
            "Saving model, epoch: 17892, train_loss: 0.9006387591362, val_loss: 0.8734949231147766\n",
            "Saving model, epoch: 17893, train_loss: 0.9006373882293701, val_loss: 0.8734939694404602\n",
            "Saving model, epoch: 17894, train_loss: 0.9006360769271851, val_loss: 0.8734933137893677\n",
            "Saving model, epoch: 17895, train_loss: 0.9006345868110657, val_loss: 0.8734919428825378\n",
            "Saving model, epoch: 17896, train_loss: 0.9006333351135254, val_loss: 0.8734908103942871\n",
            "Saving model, epoch: 17897, train_loss: 0.9006319642066956, val_loss: 0.8734899759292603\n",
            "Saving model, epoch: 17898, train_loss: 0.9006304144859314, val_loss: 0.8734890222549438\n",
            "Saving model, epoch: 17899, train_loss: 0.900628924369812, val_loss: 0.8734880089759827\n",
            "Saving model, epoch: 17900, train_loss: 0.9006275534629822, val_loss: 0.8734867572784424\n",
            "epoch: 17901, train_loss: 0.9006261229515076, val_loss: 0.8734859228134155\n",
            "Saving model, epoch: 17901, train_loss: 0.9006261229515076, val_loss: 0.8734859228134155\n",
            "Saving model, epoch: 17902, train_loss: 0.9006248116493225, val_loss: 0.8734848499298096\n",
            "Saving model, epoch: 17903, train_loss: 0.9006235003471375, val_loss: 0.8734836578369141\n",
            "Saving model, epoch: 17904, train_loss: 0.9006221294403076, val_loss: 0.8734825849533081\n",
            "Saving model, epoch: 17905, train_loss: 0.900620698928833, val_loss: 0.8734819293022156\n",
            "Saving model, epoch: 17906, train_loss: 0.9006192088127136, val_loss: 0.8734807372093201\n",
            "Saving model, epoch: 17907, train_loss: 0.900617778301239, val_loss: 0.8734795451164246\n",
            "Saving model, epoch: 17908, train_loss: 0.9006164073944092, val_loss: 0.8734785914421082\n",
            "Saving model, epoch: 17909, train_loss: 0.9006150960922241, val_loss: 0.8734774589538574\n",
            "Saving model, epoch: 17910, train_loss: 0.9006136655807495, val_loss: 0.8734765648841858\n",
            "Saving model, epoch: 17911, train_loss: 0.9006122350692749, val_loss: 0.8734754920005798\n",
            "Saving model, epoch: 17912, train_loss: 0.9006108641624451, val_loss: 0.8734744787216187\n",
            "Saving model, epoch: 17913, train_loss: 0.9006094336509705, val_loss: 0.8734734058380127\n",
            "Saving model, epoch: 17914, train_loss: 0.9006080627441406, val_loss: 0.8734723329544067\n",
            "Saving model, epoch: 17915, train_loss: 0.900606632232666, val_loss: 0.8734712600708008\n",
            "Saving model, epoch: 17916, train_loss: 0.9006052017211914, val_loss: 0.8734704852104187\n",
            "Saving model, epoch: 17917, train_loss: 0.900603711605072, val_loss: 0.873469352722168\n",
            "Saving model, epoch: 17918, train_loss: 0.900602400302887, val_loss: 0.8734685182571411\n",
            "Saving model, epoch: 17919, train_loss: 0.9006010293960571, val_loss: 0.8734673261642456\n",
            "Saving model, epoch: 17920, train_loss: 0.9005995988845825, val_loss: 0.8734661936759949\n",
            "Saving model, epoch: 17921, train_loss: 0.9005981683731079, val_loss: 0.8734654784202576\n",
            "Saving model, epoch: 17922, train_loss: 0.9005969166755676, val_loss: 0.8734643459320068\n",
            "Saving model, epoch: 17923, train_loss: 0.9005953669548035, val_loss: 0.8734631538391113\n",
            "Saving model, epoch: 17924, train_loss: 0.9005940556526184, val_loss: 0.8734621405601501\n",
            "Saving model, epoch: 17925, train_loss: 0.900592565536499, val_loss: 0.8734610080718994\n",
            "Saving model, epoch: 17926, train_loss: 0.9005911350250244, val_loss: 0.873460054397583\n",
            "Saving model, epoch: 17927, train_loss: 0.9005898833274841, val_loss: 0.8734591603279114\n",
            "Saving model, epoch: 17928, train_loss: 0.9005884528160095, val_loss: 0.873458206653595\n",
            "Saving model, epoch: 17929, train_loss: 0.9005870223045349, val_loss: 0.8734570145606995\n",
            "Saving model, epoch: 17930, train_loss: 0.9005856513977051, val_loss: 0.8734558820724487\n",
            "Saving model, epoch: 17931, train_loss: 0.9005842208862305, val_loss: 0.8734549880027771\n",
            "Saving model, epoch: 17932, train_loss: 0.9005828499794006, val_loss: 0.8734537959098816\n",
            "Saving model, epoch: 17933, train_loss: 0.9005813002586365, val_loss: 0.8734527826309204\n",
            "Saving model, epoch: 17934, train_loss: 0.900580108165741, val_loss: 0.873451828956604\n",
            "Saving model, epoch: 17935, train_loss: 0.9005786180496216, val_loss: 0.8734509348869324\n",
            "Saving model, epoch: 17936, train_loss: 0.900577187538147, val_loss: 0.8734499216079712\n",
            "Saving model, epoch: 17937, train_loss: 0.9005758762359619, val_loss: 0.8734487295150757\n",
            "Saving model, epoch: 17938, train_loss: 0.9005743861198425, val_loss: 0.8734479546546936\n",
            "Saving model, epoch: 17939, train_loss: 0.9005730748176575, val_loss: 0.8734467029571533\n",
            "Saving model, epoch: 17940, train_loss: 0.9005715847015381, val_loss: 0.8734457492828369\n",
            "Saving model, epoch: 17941, train_loss: 0.900570273399353, val_loss: 0.8734444975852966\n",
            "Saving model, epoch: 17942, train_loss: 0.9005688428878784, val_loss: 0.8734438419342041\n",
            "Saving model, epoch: 17943, train_loss: 0.9005674719810486, val_loss: 0.8734426498413086\n",
            "Saving model, epoch: 17944, train_loss: 0.900566041469574, val_loss: 0.8734414577484131\n",
            "Saving model, epoch: 17945, train_loss: 0.9005646109580994, val_loss: 0.8734404444694519\n",
            "Saving model, epoch: 17946, train_loss: 0.9005632400512695, val_loss: 0.873439610004425\n",
            "Saving model, epoch: 17947, train_loss: 0.9005618095397949, val_loss: 0.8734386563301086\n",
            "Saving model, epoch: 17948, train_loss: 0.9005604386329651, val_loss: 0.8734372854232788\n",
            "Saving model, epoch: 17949, train_loss: 0.9005590081214905, val_loss: 0.8734362721443176\n",
            "Saving model, epoch: 17950, train_loss: 0.9005575776100159, val_loss: 0.8734355568885803\n",
            "Saving model, epoch: 17951, train_loss: 0.900556206703186, val_loss: 0.8734343647956848\n",
            "Saving model, epoch: 17952, train_loss: 0.9005547761917114, val_loss: 0.8734331727027893\n",
            "Saving model, epoch: 17953, train_loss: 0.9005534052848816, val_loss: 0.8734323978424072\n",
            "Saving model, epoch: 17954, train_loss: 0.900551974773407, val_loss: 0.8734315037727356\n",
            "Saving model, epoch: 17955, train_loss: 0.9005505442619324, val_loss: 0.8734303116798401\n",
            "Saving model, epoch: 17956, train_loss: 0.9005491733551025, val_loss: 0.8734291791915894\n",
            "Saving model, epoch: 17957, train_loss: 0.9005477428436279, val_loss: 0.8734281659126282\n",
            "Saving model, epoch: 17958, train_loss: 0.9005463719367981, val_loss: 0.8734273314476013\n",
            "Saving model, epoch: 17959, train_loss: 0.9005451202392578, val_loss: 0.8734261989593506\n",
            "Saving model, epoch: 17960, train_loss: 0.9005435109138489, val_loss: 0.8734249472618103\n",
            "Saving model, epoch: 17961, train_loss: 0.9005422592163086, val_loss: 0.8734241724014282\n",
            "Saving model, epoch: 17962, train_loss: 0.9005409479141235, val_loss: 0.8734230399131775\n",
            "Saving model, epoch: 17963, train_loss: 0.9005393981933594, val_loss: 0.8734219670295715\n",
            "Saving model, epoch: 17964, train_loss: 0.90053790807724, val_loss: 0.8734208941459656\n",
            "Saving model, epoch: 17965, train_loss: 0.9005365967750549, val_loss: 0.873420000076294\n",
            "Saving model, epoch: 17966, train_loss: 0.9005352854728699, val_loss: 0.8734190464019775\n",
            "Saving model, epoch: 17967, train_loss: 0.9005336761474609, val_loss: 0.8734177947044373\n",
            "Saving model, epoch: 17968, train_loss: 0.9005324840545654, val_loss: 0.8734170198440552\n",
            "Saving model, epoch: 17969, train_loss: 0.900530993938446, val_loss: 0.873416006565094\n",
            "Saving model, epoch: 17970, train_loss: 0.9005295634269714, val_loss: 0.8734147548675537\n",
            "Saving model, epoch: 17971, train_loss: 0.9005281925201416, val_loss: 0.8734138011932373\n",
            "Saving model, epoch: 17972, train_loss: 0.9005268812179565, val_loss: 0.8734127283096313\n",
            "Saving model, epoch: 17973, train_loss: 0.9005253314971924, val_loss: 0.8734118938446045\n",
            "Saving model, epoch: 17974, train_loss: 0.9005240201950073, val_loss: 0.873410701751709\n",
            "Saving model, epoch: 17975, train_loss: 0.9005225300788879, val_loss: 0.8734095692634583\n",
            "Saving model, epoch: 17976, train_loss: 0.9005211591720581, val_loss: 0.8734087944030762\n",
            "Saving model, epoch: 17977, train_loss: 0.900519847869873, val_loss: 0.8734079003334045\n",
            "Saving model, epoch: 17978, train_loss: 0.9005184173583984, val_loss: 0.8734067678451538\n",
            "Saving model, epoch: 17979, train_loss: 0.9005169868469238, val_loss: 0.8734056353569031\n",
            "Saving model, epoch: 17980, train_loss: 0.900515615940094, val_loss: 0.8734046816825867\n",
            "Saving model, epoch: 17981, train_loss: 0.9005141854286194, val_loss: 0.8734035491943359\n",
            "Saving model, epoch: 17982, train_loss: 0.9005128145217896, val_loss: 0.8734024167060852\n",
            "Saving model, epoch: 17983, train_loss: 0.9005113840103149, val_loss: 0.8734012246131897\n",
            "Saving model, epoch: 17984, train_loss: 0.9005099534988403, val_loss: 0.8734006285667419\n",
            "Saving model, epoch: 17985, train_loss: 0.9005085825920105, val_loss: 0.8733996152877808\n",
            "Saving model, epoch: 17986, train_loss: 0.9005071520805359, val_loss: 0.87339848279953\n",
            "Saving model, epoch: 17987, train_loss: 0.9005058407783508, val_loss: 0.8733973503112793\n",
            "Saving model, epoch: 17988, train_loss: 0.900504469871521, val_loss: 0.8733965158462524\n",
            "Saving model, epoch: 17989, train_loss: 0.9005029201507568, val_loss: 0.8733954429626465\n",
            "Saving model, epoch: 17990, train_loss: 0.900501549243927, val_loss: 0.8733944296836853\n",
            "Saving model, epoch: 17991, train_loss: 0.9005002379417419, val_loss: 0.8733934164047241\n",
            "Saving model, epoch: 17992, train_loss: 0.9004988074302673, val_loss: 0.8733922839164734\n",
            "Saving model, epoch: 17993, train_loss: 0.9004974365234375, val_loss: 0.8733910918235779\n",
            "Saving model, epoch: 17994, train_loss: 0.9004961252212524, val_loss: 0.8733903765678406\n",
            "Saving model, epoch: 17995, train_loss: 0.9004946351051331, val_loss: 0.8733891844749451\n",
            "Saving model, epoch: 17996, train_loss: 0.9004930853843689, val_loss: 0.8733881115913391\n",
            "Saving model, epoch: 17997, train_loss: 0.9004917740821838, val_loss: 0.8733872175216675\n",
            "Saving model, epoch: 17998, train_loss: 0.900490403175354, val_loss: 0.8733860850334167\n",
            "Saving model, epoch: 17999, train_loss: 0.9004888534545898, val_loss: 0.8733850121498108\n",
            "Saving model, epoch: 18000, train_loss: 0.9004876613616943, val_loss: 0.8733841180801392\n",
            "epoch: 18001, train_loss: 0.9004860520362854, val_loss: 0.8733829855918884\n",
            "Saving model, epoch: 18001, train_loss: 0.9004860520362854, val_loss: 0.8733829855918884\n",
            "Saving model, epoch: 18002, train_loss: 0.9004847407341003, val_loss: 0.8733817934989929\n",
            "Saving model, epoch: 18003, train_loss: 0.9004833698272705, val_loss: 0.8733811378479004\n",
            "Saving model, epoch: 18004, train_loss: 0.9004819393157959, val_loss: 0.8733798265457153\n",
            "Saving model, epoch: 18005, train_loss: 0.9004806280136108, val_loss: 0.8733789324760437\n",
            "Saving model, epoch: 18006, train_loss: 0.900479257106781, val_loss: 0.8733776807785034\n",
            "Saving model, epoch: 18007, train_loss: 0.9004778265953064, val_loss: 0.8733769655227661\n",
            "Saving model, epoch: 18008, train_loss: 0.900476336479187, val_loss: 0.8733758926391602\n",
            "Saving model, epoch: 18009, train_loss: 0.900475025177002, val_loss: 0.8733751773834229\n",
            "Saving model, epoch: 18010, train_loss: 0.9004735946655273, val_loss: 0.8733739256858826\n",
            "Saving model, epoch: 18011, train_loss: 0.9004722237586975, val_loss: 0.8733729720115662\n",
            "Saving model, epoch: 18012, train_loss: 0.9004707932472229, val_loss: 0.8733718991279602\n",
            "Saving model, epoch: 18013, train_loss: 0.9004693627357483, val_loss: 0.8733708262443542\n",
            "Saving model, epoch: 18014, train_loss: 0.9004679918289185, val_loss: 0.8733699321746826\n",
            "Saving model, epoch: 18015, train_loss: 0.9004665613174438, val_loss: 0.8733689188957214\n",
            "Saving model, epoch: 18016, train_loss: 0.9004650712013245, val_loss: 0.8733678460121155\n",
            "Saving model, epoch: 18017, train_loss: 0.9004637598991394, val_loss: 0.8733665943145752\n",
            "Saving model, epoch: 18018, train_loss: 0.9004623293876648, val_loss: 0.8733657002449036\n",
            "Saving model, epoch: 18019, train_loss: 0.900460958480835, val_loss: 0.8733646273612976\n",
            "Saving model, epoch: 18020, train_loss: 0.9004595279693604, val_loss: 0.8733635544776917\n",
            "Saving model, epoch: 18021, train_loss: 0.9004581570625305, val_loss: 0.87336266040802\n",
            "Saving model, epoch: 18022, train_loss: 0.9004567265510559, val_loss: 0.8733615875244141\n",
            "Saving model, epoch: 18023, train_loss: 0.9004552960395813, val_loss: 0.8733605742454529\n",
            "Saving model, epoch: 18024, train_loss: 0.9004539251327515, val_loss: 0.873359203338623\n",
            "Saving model, epoch: 18025, train_loss: 0.9004524946212769, val_loss: 0.8733584880828857\n",
            "Saving model, epoch: 18026, train_loss: 0.900451123714447, val_loss: 0.8733577132225037\n",
            "Saving model, epoch: 18027, train_loss: 0.9004496932029724, val_loss: 0.8733564019203186\n",
            "Saving model, epoch: 18028, train_loss: 0.9004483819007874, val_loss: 0.8733554482460022\n",
            "Saving model, epoch: 18029, train_loss: 0.900446891784668, val_loss: 0.8733543753623962\n",
            "Saving model, epoch: 18030, train_loss: 0.9004454612731934, val_loss: 0.8733533620834351\n",
            "Saving model, epoch: 18031, train_loss: 0.9004440903663635, val_loss: 0.8733523488044739\n",
            "Saving model, epoch: 18032, train_loss: 0.9004427790641785, val_loss: 0.8733512163162231\n",
            "Saving model, epoch: 18033, train_loss: 0.9004414677619934, val_loss: 0.8733502626419067\n",
            "Saving model, epoch: 18034, train_loss: 0.900439977645874, val_loss: 0.8733493685722351\n",
            "Saving model, epoch: 18035, train_loss: 0.9004384279251099, val_loss: 0.8733482956886292\n",
            "Saving model, epoch: 18036, train_loss: 0.9004372358322144, val_loss: 0.8733469843864441\n",
            "Saving model, epoch: 18037, train_loss: 0.9004356265068054, val_loss: 0.8733460307121277\n",
            "Saving model, epoch: 18038, train_loss: 0.9004344344139099, val_loss: 0.873345136642456\n",
            "Saving model, epoch: 18039, train_loss: 0.900432825088501, val_loss: 0.8733441829681396\n",
            "Saving model, epoch: 18040, train_loss: 0.9004315137863159, val_loss: 0.8733429908752441\n",
            "Saving model, epoch: 18041, train_loss: 0.9004300832748413, val_loss: 0.873341977596283\n",
            "Saving model, epoch: 18042, train_loss: 0.9004287719726562, val_loss: 0.8733409643173218\n",
            "Saving model, epoch: 18043, train_loss: 0.9004272818565369, val_loss: 0.8733399510383606\n",
            "Saving model, epoch: 18044, train_loss: 0.900425910949707, val_loss: 0.8733389377593994\n",
            "Saving model, epoch: 18045, train_loss: 0.900424599647522, val_loss: 0.873337984085083\n",
            "Saving model, epoch: 18046, train_loss: 0.9004230499267578, val_loss: 0.8733370304107666\n",
            "Saving model, epoch: 18047, train_loss: 0.900421679019928, val_loss: 0.8733359575271606\n",
            "Saving model, epoch: 18048, train_loss: 0.9004203677177429, val_loss: 0.8733349442481995\n",
            "Saving model, epoch: 18049, train_loss: 0.9004189372062683, val_loss: 0.8733338713645935\n",
            "Saving model, epoch: 18050, train_loss: 0.9004174470901489, val_loss: 0.8733328580856323\n",
            "Saving model, epoch: 18051, train_loss: 0.9004161357879639, val_loss: 0.8733320832252502\n",
            "Saving model, epoch: 18052, train_loss: 0.900414764881134, val_loss: 0.87333083152771\n",
            "Saving model, epoch: 18053, train_loss: 0.9004133343696594, val_loss: 0.873329758644104\n",
            "Saving model, epoch: 18054, train_loss: 0.9004119038581848, val_loss: 0.8733288049697876\n",
            "Saving model, epoch: 18055, train_loss: 0.900410532951355, val_loss: 0.8733277320861816\n",
            "Saving model, epoch: 18056, train_loss: 0.9004091024398804, val_loss: 0.8733267784118652\n",
            "Saving model, epoch: 18057, train_loss: 0.9004077315330505, val_loss: 0.8733258247375488\n",
            "Saving model, epoch: 18058, train_loss: 0.9004063010215759, val_loss: 0.8733246326446533\n",
            "Saving model, epoch: 18059, train_loss: 0.9004049897193909, val_loss: 0.8733237981796265\n",
            "Saving model, epoch: 18060, train_loss: 0.9004034996032715, val_loss: 0.873322606086731\n",
            "Saving model, epoch: 18061, train_loss: 0.9004020690917969, val_loss: 0.8733217716217041\n",
            "Saving model, epoch: 18062, train_loss: 0.900400698184967, val_loss: 0.8733206391334534\n",
            "Saving model, epoch: 18063, train_loss: 0.9003992676734924, val_loss: 0.8733193874359131\n",
            "Saving model, epoch: 18064, train_loss: 0.9003978371620178, val_loss: 0.8733185529708862\n",
            "Saving model, epoch: 18065, train_loss: 0.900396466255188, val_loss: 0.8733174204826355\n",
            "Saving model, epoch: 18066, train_loss: 0.9003950357437134, val_loss: 0.8733165264129639\n",
            "Saving model, epoch: 18067, train_loss: 0.9003937244415283, val_loss: 0.8733152747154236\n",
            "Saving model, epoch: 18068, train_loss: 0.9003922343254089, val_loss: 0.873314380645752\n",
            "Saving model, epoch: 18069, train_loss: 0.9003909230232239, val_loss: 0.8733131885528564\n",
            "Saving model, epoch: 18070, train_loss: 0.9003894329071045, val_loss: 0.8733121752738953\n",
            "Saving model, epoch: 18071, train_loss: 0.9003881216049194, val_loss: 0.873311460018158\n",
            "Saving model, epoch: 18072, train_loss: 0.9003866314888, val_loss: 0.8733104467391968\n",
            "Saving model, epoch: 18073, train_loss: 0.900385320186615, val_loss: 0.8733091950416565\n",
            "Saving model, epoch: 18074, train_loss: 0.9003838896751404, val_loss: 0.8733082413673401\n",
            "Saving model, epoch: 18075, train_loss: 0.9003824591636658, val_loss: 0.8733072876930237\n",
            "Saving model, epoch: 18076, train_loss: 0.9003812074661255, val_loss: 0.873306155204773\n",
            "Saving model, epoch: 18077, train_loss: 0.9003796577453613, val_loss: 0.8733052611351013\n",
            "Saving model, epoch: 18078, train_loss: 0.9003782868385315, val_loss: 0.8733041882514954\n",
            "Saving model, epoch: 18079, train_loss: 0.9003769755363464, val_loss: 0.873303234577179\n",
            "Saving model, epoch: 18080, train_loss: 0.9003755450248718, val_loss: 0.873302161693573\n",
            "Saving model, epoch: 18081, train_loss: 0.9003739356994629, val_loss: 0.8733011484146118\n",
            "Saving model, epoch: 18082, train_loss: 0.9003726243972778, val_loss: 0.8732999563217163\n",
            "Saving model, epoch: 18083, train_loss: 0.900371253490448, val_loss: 0.8732989430427551\n",
            "Saving model, epoch: 18084, train_loss: 0.9003699421882629, val_loss: 0.873298168182373\n",
            "Saving model, epoch: 18085, train_loss: 0.9003685116767883, val_loss: 0.8732969760894775\n",
            "Saving model, epoch: 18086, train_loss: 0.900367021560669, val_loss: 0.8732958436012268\n",
            "Saving model, epoch: 18087, train_loss: 0.9003655314445496, val_loss: 0.8732948899269104\n",
            "Saving model, epoch: 18088, train_loss: 0.9003642797470093, val_loss: 0.8732940554618835\n",
            "Saving model, epoch: 18089, train_loss: 0.9003629088401794, val_loss: 0.8732928037643433\n",
            "Saving model, epoch: 18090, train_loss: 0.9003613591194153, val_loss: 0.8732916712760925\n",
            "Saving model, epoch: 18091, train_loss: 0.9003599882125854, val_loss: 0.8732911348342896\n",
            "Saving model, epoch: 18092, train_loss: 0.9003586769104004, val_loss: 0.8732898831367493\n",
            "Saving model, epoch: 18093, train_loss: 0.9003572463989258, val_loss: 0.8732886910438538\n",
            "Saving model, epoch: 18094, train_loss: 0.900355875492096, val_loss: 0.8732876777648926\n",
            "Saving model, epoch: 18095, train_loss: 0.9003544449806213, val_loss: 0.8732868432998657\n",
            "Saving model, epoch: 18096, train_loss: 0.9003530740737915, val_loss: 0.8732856512069702\n",
            "Saving model, epoch: 18097, train_loss: 0.9003516435623169, val_loss: 0.873284637928009\n",
            "Saving model, epoch: 18098, train_loss: 0.9003502130508423, val_loss: 0.8732838034629822\n",
            "Saving model, epoch: 18099, train_loss: 0.9003488421440125, val_loss: 0.8732828497886658\n",
            "Saving model, epoch: 18100, train_loss: 0.9003474116325378, val_loss: 0.8732815384864807\n",
            "epoch: 18101, train_loss: 0.900346040725708, val_loss: 0.8732803463935852\n",
            "Saving model, epoch: 18101, train_loss: 0.900346040725708, val_loss: 0.8732803463935852\n",
            "Saving model, epoch: 18102, train_loss: 0.900344729423523, val_loss: 0.8732796907424927\n",
            "Saving model, epoch: 18103, train_loss: 0.9003431797027588, val_loss: 0.8732786178588867\n",
            "Saving model, epoch: 18104, train_loss: 0.9003418684005737, val_loss: 0.8732774257659912\n",
            "Saving model, epoch: 18105, train_loss: 0.9003403782844543, val_loss: 0.8732765316963196\n",
            "Saving model, epoch: 18106, train_loss: 0.9003390073776245, val_loss: 0.8732754588127136\n",
            "Saving model, epoch: 18107, train_loss: 0.9003375768661499, val_loss: 0.873274564743042\n",
            "Saving model, epoch: 18108, train_loss: 0.9003361463546753, val_loss: 0.8732733726501465\n",
            "Saving model, epoch: 18109, train_loss: 0.9003347754478455, val_loss: 0.8732722401618958\n",
            "Saving model, epoch: 18110, train_loss: 0.9003333449363708, val_loss: 0.873271644115448\n",
            "Saving model, epoch: 18111, train_loss: 0.900331974029541, val_loss: 0.8732704520225525\n",
            "Saving model, epoch: 18112, train_loss: 0.9003305435180664, val_loss: 0.8732693791389465\n",
            "Saving model, epoch: 18113, train_loss: 0.9003291130065918, val_loss: 0.8732684850692749\n",
            "Saving model, epoch: 18114, train_loss: 0.900327742099762, val_loss: 0.8732675313949585\n",
            "Saving model, epoch: 18115, train_loss: 0.9003263115882874, val_loss: 0.8732661008834839\n",
            "Saving model, epoch: 18116, train_loss: 0.9003250002861023, val_loss: 0.873265266418457\n",
            "Saving model, epoch: 18117, train_loss: 0.9003236293792725, val_loss: 0.8732645511627197\n",
            "Saving model, epoch: 18118, train_loss: 0.9003220796585083, val_loss: 0.8732630610466003\n",
            "Saving model, epoch: 18119, train_loss: 0.9003207683563232, val_loss: 0.8732619285583496\n",
            "Saving model, epoch: 18120, train_loss: 0.9003192782402039, val_loss: 0.873261034488678\n",
            "Saving model, epoch: 18121, train_loss: 0.900317907333374, val_loss: 0.873260498046875\n",
            "Saving model, epoch: 18122, train_loss: 0.900316596031189, val_loss: 0.8732590675354004\n",
            "Saving model, epoch: 18123, train_loss: 0.9003151655197144, val_loss: 0.873258113861084\n",
            "Saving model, epoch: 18124, train_loss: 0.9003137350082397, val_loss: 0.8732572793960571\n",
            "Saving model, epoch: 18125, train_loss: 0.9003123641014099, val_loss: 0.873256266117096\n",
            "Saving model, epoch: 18126, train_loss: 0.9003109335899353, val_loss: 0.8732548952102661\n",
            "Saving model, epoch: 18127, train_loss: 0.9003095626831055, val_loss: 0.8732538223266602\n",
            "Saving model, epoch: 18128, train_loss: 0.9003082513809204, val_loss: 0.8732532262802124\n",
            "Saving model, epoch: 18129, train_loss: 0.9003067016601562, val_loss: 0.8732519149780273\n",
            "Saving model, epoch: 18130, train_loss: 0.9003053307533264, val_loss: 0.8732506632804871\n",
            "Saving model, epoch: 18131, train_loss: 0.9003039002418518, val_loss: 0.8732498288154602\n",
            "Saving model, epoch: 18132, train_loss: 0.9003025889396667, val_loss: 0.8732489347457886\n",
            "Saving model, epoch: 18133, train_loss: 0.9003010988235474, val_loss: 0.8732478022575378\n",
            "Saving model, epoch: 18134, train_loss: 0.9002997279167175, val_loss: 0.8732467889785767\n",
            "Saving model, epoch: 18135, train_loss: 0.9002984166145325, val_loss: 0.8732457756996155\n",
            "Saving model, epoch: 18136, train_loss: 0.9002969861030579, val_loss: 0.8732447624206543\n",
            "Saving model, epoch: 18137, train_loss: 0.9002955555915833, val_loss: 0.873243510723114\n",
            "Saving model, epoch: 18138, train_loss: 0.9002941846847534, val_loss: 0.8732426166534424\n",
            "Saving model, epoch: 18139, train_loss: 0.9002927541732788, val_loss: 0.8732419013977051\n",
            "Saving model, epoch: 18140, train_loss: 0.9002912640571594, val_loss: 0.8732407093048096\n",
            "Saving model, epoch: 18141, train_loss: 0.9002899527549744, val_loss: 0.8732394576072693\n",
            "Saving model, epoch: 18142, train_loss: 0.9002885222434998, val_loss: 0.8732386231422424\n",
            "Saving model, epoch: 18143, train_loss: 0.9002871513366699, val_loss: 0.8732376098632812\n",
            "Saving model, epoch: 18144, train_loss: 0.9002857208251953, val_loss: 0.8732364177703857\n",
            "Saving model, epoch: 18145, train_loss: 0.9002843499183655, val_loss: 0.8732354044914246\n",
            "Saving model, epoch: 18146, train_loss: 0.9002829194068909, val_loss: 0.8732343912124634\n",
            "Saving model, epoch: 18147, train_loss: 0.9002816081047058, val_loss: 0.8732334971427917\n",
            "Saving model, epoch: 18148, train_loss: 0.9002801179885864, val_loss: 0.873232364654541\n",
            "Saving model, epoch: 18149, train_loss: 0.9002786874771118, val_loss: 0.8732314109802246\n",
            "Saving model, epoch: 18150, train_loss: 0.900277316570282, val_loss: 0.8732306361198425\n",
            "Saving model, epoch: 18151, train_loss: 0.9002758860588074, val_loss: 0.8732295036315918\n",
            "Saving model, epoch: 18152, train_loss: 0.9002745747566223, val_loss: 0.8732281923294067\n",
            "Saving model, epoch: 18153, train_loss: 0.9002732634544373, val_loss: 0.8732271194458008\n",
            "Saving model, epoch: 18154, train_loss: 0.9002716541290283, val_loss: 0.8732262849807739\n",
            "Saving model, epoch: 18155, train_loss: 0.9002702832221985, val_loss: 0.8732255697250366\n",
            "Saving model, epoch: 18156, train_loss: 0.9002688527107239, val_loss: 0.8732240200042725\n",
            "Saving model, epoch: 18157, train_loss: 0.9002675414085388, val_loss: 0.8732232451438904\n",
            "Saving model, epoch: 18158, train_loss: 0.9002660512924194, val_loss: 0.8732225894927979\n",
            "Saving model, epoch: 18159, train_loss: 0.9002646207809448, val_loss: 0.8732211589813232\n",
            "Saving model, epoch: 18160, train_loss: 0.9002633094787598, val_loss: 0.8732202649116516\n",
            "Saving model, epoch: 18161, train_loss: 0.9002619385719299, val_loss: 0.87321937084198\n",
            "Saving model, epoch: 18162, train_loss: 0.9002606272697449, val_loss: 0.8732179999351501\n",
            "Saving model, epoch: 18163, train_loss: 0.9002591371536255, val_loss: 0.8732168674468994\n",
            "Saving model, epoch: 18164, train_loss: 0.9002578258514404, val_loss: 0.8732160329818726\n",
            "Saving model, epoch: 18165, train_loss: 0.9002563953399658, val_loss: 0.8732152581214905\n",
            "Saving model, epoch: 18166, train_loss: 0.9002547860145569, val_loss: 0.8732138872146606\n",
            "Saving model, epoch: 18167, train_loss: 0.9002534747123718, val_loss: 0.8732128739356995\n",
            "Saving model, epoch: 18168, train_loss: 0.9002521634101868, val_loss: 0.8732122182846069\n",
            "Saving model, epoch: 18169, train_loss: 0.9002506732940674, val_loss: 0.8732109665870667\n",
            "Saving model, epoch: 18170, train_loss: 0.9002492427825928, val_loss: 0.8732098340988159\n",
            "Saving model, epoch: 18171, train_loss: 0.9002479314804077, val_loss: 0.8732089400291443\n",
            "Saving model, epoch: 18172, train_loss: 0.9002465605735779, val_loss: 0.8732078671455383\n",
            "Saving model, epoch: 18173, train_loss: 0.9002451300621033, val_loss: 0.8732070326805115\n",
            "Saving model, epoch: 18174, train_loss: 0.9002437591552734, val_loss: 0.8732057809829712\n",
            "Saving model, epoch: 18175, train_loss: 0.9002423286437988, val_loss: 0.8732051253318787\n",
            "Saving model, epoch: 18176, train_loss: 0.9002408981323242, val_loss: 0.8732038736343384\n",
            "Saving model, epoch: 18177, train_loss: 0.9002395272254944, val_loss: 0.8732026219367981\n",
            "Saving model, epoch: 18178, train_loss: 0.9002380967140198, val_loss: 0.8732016086578369\n",
            "Saving model, epoch: 18179, train_loss: 0.9002367258071899, val_loss: 0.8732008934020996\n",
            "Saving model, epoch: 18180, train_loss: 0.9002352952957153, val_loss: 0.8731995820999146\n",
            "Saving model, epoch: 18181, train_loss: 0.9002338647842407, val_loss: 0.8731988072395325\n",
            "Saving model, epoch: 18182, train_loss: 0.9002324938774109, val_loss: 0.8731974959373474\n",
            "Saving model, epoch: 18183, train_loss: 0.9002310633659363, val_loss: 0.873196542263031\n",
            "Saving model, epoch: 18184, train_loss: 0.9002296924591064, val_loss: 0.8731955289840698\n",
            "Saving model, epoch: 18185, train_loss: 0.9002282619476318, val_loss: 0.8731945753097534\n",
            "Saving model, epoch: 18186, train_loss: 0.9002268314361572, val_loss: 0.873193621635437\n",
            "Saving model, epoch: 18187, train_loss: 0.9002254605293274, val_loss: 0.8731926679611206\n",
            "Saving model, epoch: 18188, train_loss: 0.9002240300178528, val_loss: 0.8731915950775146\n",
            "Saving model, epoch: 18189, train_loss: 0.900222659111023, val_loss: 0.8731906414031982\n",
            "Saving model, epoch: 18190, train_loss: 0.9002214074134827, val_loss: 0.8731896281242371\n",
            "Saving model, epoch: 18191, train_loss: 0.9002197980880737, val_loss: 0.8731886744499207\n",
            "Saving model, epoch: 18192, train_loss: 0.9002184271812439, val_loss: 0.8731874227523804\n",
            "Saving model, epoch: 18193, train_loss: 0.9002171158790588, val_loss: 0.8731865286827087\n",
            "Saving model, epoch: 18194, train_loss: 0.9002156853675842, val_loss: 0.8731855154037476\n",
            "Saving model, epoch: 18195, train_loss: 0.9002143144607544, val_loss: 0.873184323310852\n",
            "Saving model, epoch: 18196, train_loss: 0.9002128839492798, val_loss: 0.8731833100318909\n",
            "Saving model, epoch: 18197, train_loss: 0.9002115726470947, val_loss: 0.8731825947761536\n",
            "Saving model, epoch: 18198, train_loss: 0.9002100825309753, val_loss: 0.8731813430786133\n",
            "Saving model, epoch: 18199, train_loss: 0.900208592414856, val_loss: 0.8731802105903625\n",
            "Saving model, epoch: 18200, train_loss: 0.9002072811126709, val_loss: 0.8731791973114014\n",
            "epoch: 18201, train_loss: 0.9002059698104858, val_loss: 0.8731783628463745\n",
            "Saving model, epoch: 18201, train_loss: 0.9002059698104858, val_loss: 0.8731783628463745\n",
            "Saving model, epoch: 18202, train_loss: 0.9002045392990112, val_loss: 0.8731772899627686\n",
            "Saving model, epoch: 18203, train_loss: 0.9002030491828918, val_loss: 0.8731759786605835\n",
            "Saving model, epoch: 18204, train_loss: 0.9002017378807068, val_loss: 0.873175323009491\n",
            "Saving model, epoch: 18205, train_loss: 0.9002001285552979, val_loss: 0.8731744289398193\n",
            "Saving model, epoch: 18206, train_loss: 0.9001988172531128, val_loss: 0.8731731176376343\n",
            "Saving model, epoch: 18207, train_loss: 0.9001975059509277, val_loss: 0.8731721639633179\n",
            "Saving model, epoch: 18208, train_loss: 0.9001960158348083, val_loss: 0.873171329498291\n",
            "Saving model, epoch: 18209, train_loss: 0.9001945853233337, val_loss: 0.8731702566146851\n",
            "Saving model, epoch: 18210, train_loss: 0.9001932740211487, val_loss: 0.8731690645217896\n",
            "Saving model, epoch: 18211, train_loss: 0.9001919031143188, val_loss: 0.8731681108474731\n",
            "Saving model, epoch: 18212, train_loss: 0.9001904726028442, val_loss: 0.873167097568512\n",
            "Saving model, epoch: 18213, train_loss: 0.9001889824867249, val_loss: 0.8731660842895508\n",
            "Saving model, epoch: 18214, train_loss: 0.9001876711845398, val_loss: 0.8731650114059448\n",
            "Saving model, epoch: 18215, train_loss: 0.9001862406730652, val_loss: 0.8731640577316284\n",
            "Saving model, epoch: 18216, train_loss: 0.9001848697662354, val_loss: 0.8731630444526672\n",
            "Saving model, epoch: 18217, train_loss: 0.9001834392547607, val_loss: 0.873162031173706\n",
            "Saving model, epoch: 18218, train_loss: 0.9001820683479309, val_loss: 0.8731610178947449\n",
            "Saving model, epoch: 18219, train_loss: 0.9001806378364563, val_loss: 0.8731600046157837\n",
            "Saving model, epoch: 18220, train_loss: 0.9001792073249817, val_loss: 0.8731589317321777\n",
            "Saving model, epoch: 18221, train_loss: 0.9001778364181519, val_loss: 0.8731578588485718\n",
            "Saving model, epoch: 18222, train_loss: 0.9001765251159668, val_loss: 0.873156726360321\n",
            "Saving model, epoch: 18223, train_loss: 0.9001750349998474, val_loss: 0.873155951499939\n",
            "Saving model, epoch: 18224, train_loss: 0.9001736044883728, val_loss: 0.8731548190116882\n",
            "Saving model, epoch: 18225, train_loss: 0.9001721739768982, val_loss: 0.873153805732727\n",
            "Saving model, epoch: 18226, train_loss: 0.9001708030700684, val_loss: 0.8731528520584106\n",
            "Saving model, epoch: 18227, train_loss: 0.9001693725585938, val_loss: 0.8731517791748047\n",
            "Saving model, epoch: 18228, train_loss: 0.9001681804656982, val_loss: 0.873150646686554\n",
            "Saving model, epoch: 18229, train_loss: 0.9001666903495789, val_loss: 0.8731497526168823\n",
            "Saving model, epoch: 18230, train_loss: 0.9001651406288147, val_loss: 0.8731487989425659\n",
            "Saving model, epoch: 18231, train_loss: 0.9001638889312744, val_loss: 0.8731475472450256\n",
            "Saving model, epoch: 18232, train_loss: 0.9001623392105103, val_loss: 0.8731465339660645\n",
            "Saving model, epoch: 18233, train_loss: 0.9001609683036804, val_loss: 0.8731456995010376\n",
            "Saving model, epoch: 18234, train_loss: 0.9001596570014954, val_loss: 0.8731447458267212\n",
            "Saving model, epoch: 18235, train_loss: 0.9001582264900208, val_loss: 0.8731434941291809\n",
            "Saving model, epoch: 18236, train_loss: 0.9001568555831909, val_loss: 0.8731426000595093\n",
            "Saving model, epoch: 18237, train_loss: 0.9001555442810059, val_loss: 0.8731415867805481\n",
            "Saving model, epoch: 18238, train_loss: 0.9001539945602417, val_loss: 0.8731407523155212\n",
            "Saving model, epoch: 18239, train_loss: 0.9001526832580566, val_loss: 0.8731393814086914\n",
            "Saving model, epoch: 18240, train_loss: 0.9001513123512268, val_loss: 0.873138427734375\n",
            "Saving model, epoch: 18241, train_loss: 0.9001498222351074, val_loss: 0.8731375932693481\n",
            "Saving model, epoch: 18242, train_loss: 0.9001485109329224, val_loss: 0.8731362819671631\n",
            "Saving model, epoch: 18243, train_loss: 0.9001470804214478, val_loss: 0.8731353878974915\n",
            "Saving model, epoch: 18244, train_loss: 0.9001456499099731, val_loss: 0.8731344938278198\n",
            "Saving model, epoch: 18245, train_loss: 0.9001441597938538, val_loss: 0.8731334209442139\n",
            "Saving model, epoch: 18246, train_loss: 0.9001428484916687, val_loss: 0.8731323480606079\n",
            "Saving model, epoch: 18247, train_loss: 0.9001414775848389, val_loss: 0.8731315732002258\n",
            "Saving model, epoch: 18248, train_loss: 0.9001400470733643, val_loss: 0.8731305599212646\n",
            "Saving model, epoch: 18249, train_loss: 0.9001386165618896, val_loss: 0.8731293678283691\n",
            "Saving model, epoch: 18250, train_loss: 0.9001372456550598, val_loss: 0.873128354549408\n",
            "Saving model, epoch: 18251, train_loss: 0.9001358151435852, val_loss: 0.873127281665802\n",
            "Saving model, epoch: 18252, train_loss: 0.9001345038414001, val_loss: 0.8731263875961304\n",
            "Saving model, epoch: 18253, train_loss: 0.9001330137252808, val_loss: 0.8731253147125244\n",
            "Saving model, epoch: 18254, train_loss: 0.9001317024230957, val_loss: 0.8731241822242737\n",
            "Saving model, epoch: 18255, train_loss: 0.9001302123069763, val_loss: 0.873123288154602\n",
            "Saving model, epoch: 18256, train_loss: 0.9001287817955017, val_loss: 0.8731222748756409\n",
            "Saving model, epoch: 18257, train_loss: 0.9001274704933167, val_loss: 0.8731211423873901\n",
            "Saving model, epoch: 18258, train_loss: 0.9001259803771973, val_loss: 0.8731202483177185\n",
            "Saving model, epoch: 18259, train_loss: 0.9001246690750122, val_loss: 0.8731193542480469\n",
            "Saving model, epoch: 18260, train_loss: 0.9001232981681824, val_loss: 0.8731182217597961\n",
            "Saving model, epoch: 18261, train_loss: 0.9001217484474182, val_loss: 0.8731172680854797\n",
            "Saving model, epoch: 18262, train_loss: 0.9001203775405884, val_loss: 0.8731162548065186\n",
            "Saving model, epoch: 18263, train_loss: 0.9001191258430481, val_loss: 0.8731151223182678\n",
            "Saving model, epoch: 18264, train_loss: 0.9001176357269287, val_loss: 0.8731140494346619\n",
            "Saving model, epoch: 18265, train_loss: 0.9001162648200989, val_loss: 0.873113214969635\n",
            "Saving model, epoch: 18266, train_loss: 0.9001148343086243, val_loss: 0.8731122016906738\n",
            "Saving model, epoch: 18267, train_loss: 0.9001134037971497, val_loss: 0.8731110095977783\n",
            "Saving model, epoch: 18268, train_loss: 0.9001120924949646, val_loss: 0.8731102347373962\n",
            "Saving model, epoch: 18269, train_loss: 0.9001106023788452, val_loss: 0.873108983039856\n",
            "Saving model, epoch: 18270, train_loss: 0.9001092910766602, val_loss: 0.8731080293655396\n",
            "Saving model, epoch: 18271, train_loss: 0.9001079201698303, val_loss: 0.8731068968772888\n",
            "Saving model, epoch: 18272, train_loss: 0.9001064896583557, val_loss: 0.8731057643890381\n",
            "Saving model, epoch: 18273, train_loss: 0.9001051187515259, val_loss: 0.8731050491333008\n",
            "Saving model, epoch: 18274, train_loss: 0.9001035690307617, val_loss: 0.87310391664505\n",
            "Saving model, epoch: 18275, train_loss: 0.9001022577285767, val_loss: 0.8731029033660889\n",
            "Saving model, epoch: 18276, train_loss: 0.9001008868217468, val_loss: 0.873102068901062\n",
            "Saving model, epoch: 18277, train_loss: 0.9000994563102722, val_loss: 0.8731008768081665\n",
            "Saving model, epoch: 18278, train_loss: 0.9000980854034424, val_loss: 0.8730995059013367\n",
            "Saving model, epoch: 18279, train_loss: 0.9000966548919678, val_loss: 0.8730987310409546\n",
            "Saving model, epoch: 18280, train_loss: 0.9000952243804932, val_loss: 0.873097836971283\n",
            "Saving model, epoch: 18281, train_loss: 0.9000938534736633, val_loss: 0.8730967044830322\n",
            "Saving model, epoch: 18282, train_loss: 0.9000925421714783, val_loss: 0.8730959296226501\n",
            "Saving model, epoch: 18283, train_loss: 0.9000911116600037, val_loss: 0.8730947375297546\n",
            "Saving model, epoch: 18284, train_loss: 0.9000896215438843, val_loss: 0.8730937242507935\n",
            "Saving model, epoch: 18285, train_loss: 0.9000881910324097, val_loss: 0.8730928301811218\n",
            "Saving model, epoch: 18286, train_loss: 0.9000868797302246, val_loss: 0.8730917572975159\n",
            "Saving model, epoch: 18287, train_loss: 0.9000853896141052, val_loss: 0.8730907440185547\n",
            "Saving model, epoch: 18288, train_loss: 0.9000841975212097, val_loss: 0.873089611530304\n",
            "Saving model, epoch: 18289, train_loss: 0.9000827074050903, val_loss: 0.8730885982513428\n",
            "Saving model, epoch: 18290, train_loss: 0.9000811576843262, val_loss: 0.8730876445770264\n",
            "Saving model, epoch: 18291, train_loss: 0.9000798463821411, val_loss: 0.8730866312980652\n",
            "Saving model, epoch: 18292, train_loss: 0.9000784754753113, val_loss: 0.8730853199958801\n",
            "Saving model, epoch: 18293, train_loss: 0.9000771641731262, val_loss: 0.8730844855308533\n",
            "Saving model, epoch: 18294, train_loss: 0.9000756740570068, val_loss: 0.8730835318565369\n",
            "Saving model, epoch: 18295, train_loss: 0.9000743627548218, val_loss: 0.8730825185775757\n",
            "Saving model, epoch: 18296, train_loss: 0.9000730514526367, val_loss: 0.8730816841125488\n",
            "Saving model, epoch: 18297, train_loss: 0.9000715613365173, val_loss: 0.8730803728103638\n",
            "Saving model, epoch: 18298, train_loss: 0.9000701308250427, val_loss: 0.8730796575546265\n",
            "Saving model, epoch: 18299, train_loss: 0.9000687003135681, val_loss: 0.873078465461731\n",
            "Saving model, epoch: 18300, train_loss: 0.9000673294067383, val_loss: 0.8730775713920593\n",
            "epoch: 18301, train_loss: 0.9000658988952637, val_loss: 0.8730764985084534\n",
            "Saving model, epoch: 18301, train_loss: 0.9000658988952637, val_loss: 0.8730764985084534\n",
            "Saving model, epoch: 18302, train_loss: 0.9000645279884338, val_loss: 0.8730753660202026\n",
            "Saving model, epoch: 18303, train_loss: 0.9000630974769592, val_loss: 0.8730743527412415\n",
            "Saving model, epoch: 18304, train_loss: 0.9000616669654846, val_loss: 0.873073399066925\n",
            "Saving model, epoch: 18305, train_loss: 0.9000603556632996, val_loss: 0.8730723261833191\n",
            "Saving model, epoch: 18306, train_loss: 0.9000589847564697, val_loss: 0.8730711936950684\n",
            "Saving model, epoch: 18307, train_loss: 0.9000574946403503, val_loss: 0.8730701804161072\n",
            "Saving model, epoch: 18308, train_loss: 0.9000560641288757, val_loss: 0.8730693459510803\n",
            "Saving model, epoch: 18309, train_loss: 0.9000546336174011, val_loss: 0.8730683326721191\n",
            "Saving model, epoch: 18310, train_loss: 0.9000533223152161, val_loss: 0.8730673789978027\n",
            "Saving model, epoch: 18311, train_loss: 0.9000519514083862, val_loss: 0.8730664253234863\n",
            "Saving model, epoch: 18312, train_loss: 0.9000506401062012, val_loss: 0.873065173625946\n",
            "Saving model, epoch: 18313, train_loss: 0.9000491499900818, val_loss: 0.8730641007423401\n",
            "Saving model, epoch: 18314, train_loss: 0.9000477194786072, val_loss: 0.8730632662773132\n",
            "Saving model, epoch: 18315, train_loss: 0.9000464081764221, val_loss: 0.873062252998352\n",
            "Saving model, epoch: 18316, train_loss: 0.9000450372695923, val_loss: 0.8730612993240356\n",
            "Saving model, epoch: 18317, train_loss: 0.9000436067581177, val_loss: 0.8730602860450745\n",
            "Saving model, epoch: 18318, train_loss: 0.9000421762466431, val_loss: 0.873059093952179\n",
            "Saving model, epoch: 18319, train_loss: 0.9000408053398132, val_loss: 0.873058021068573\n",
            "Saving model, epoch: 18320, train_loss: 0.9000393748283386, val_loss: 0.8730571269989014\n",
            "Saving model, epoch: 18321, train_loss: 0.9000380039215088, val_loss: 0.8730559945106506\n",
            "Saving model, epoch: 18322, train_loss: 0.9000365734100342, val_loss: 0.8730552792549133\n",
            "Saving model, epoch: 18323, train_loss: 0.9000351428985596, val_loss: 0.8730542659759521\n",
            "Saving model, epoch: 18324, train_loss: 0.9000337719917297, val_loss: 0.8730530142784119\n",
            "Saving model, epoch: 18325, train_loss: 0.9000323414802551, val_loss: 0.8730519413948059\n",
            "Saving model, epoch: 18326, train_loss: 0.9000309705734253, val_loss: 0.8730512261390686\n",
            "Saving model, epoch: 18327, train_loss: 0.9000295400619507, val_loss: 0.8730502724647522\n",
            "Saving model, epoch: 18328, train_loss: 0.9000281095504761, val_loss: 0.8730489015579224\n",
            "Saving model, epoch: 18329, train_loss: 0.9000267386436462, val_loss: 0.8730477690696716\n",
            "Saving model, epoch: 18330, train_loss: 0.9000253081321716, val_loss: 0.8730471134185791\n",
            "Saving model, epoch: 18331, train_loss: 0.9000239372253418, val_loss: 0.8730460405349731\n",
            "Saving model, epoch: 18332, train_loss: 0.9000225067138672, val_loss: 0.873045027256012\n",
            "Saving model, epoch: 18333, train_loss: 0.9000213146209717, val_loss: 0.8730440139770508\n",
            "Saving model, epoch: 18334, train_loss: 0.9000197649002075, val_loss: 0.8730428814888\n",
            "Saving model, epoch: 18335, train_loss: 0.9000182747840881, val_loss: 0.8730418086051941\n",
            "Saving model, epoch: 18336, train_loss: 0.9000169634819031, val_loss: 0.8730407953262329\n",
            "Saving model, epoch: 18337, train_loss: 0.9000155925750732, val_loss: 0.8730401396751404\n",
            "Saving model, epoch: 18338, train_loss: 0.9000141620635986, val_loss: 0.8730387091636658\n",
            "Saving model, epoch: 18339, train_loss: 0.9000128507614136, val_loss: 0.8730379343032837\n",
            "Saving model, epoch: 18340, train_loss: 0.9000114798545837, val_loss: 0.8730369806289673\n",
            "Saving model, epoch: 18341, train_loss: 0.9000100493431091, val_loss: 0.873035728931427\n",
            "Saving model, epoch: 18342, train_loss: 0.9000086188316345, val_loss: 0.873034656047821\n",
            "Saving model, epoch: 18343, train_loss: 0.9000072479248047, val_loss: 0.8730339407920837\n",
            "Saving model, epoch: 18344, train_loss: 0.9000058174133301, val_loss: 0.8730329275131226\n",
            "Saving model, epoch: 18345, train_loss: 0.900004506111145, val_loss: 0.8730317950248718\n",
            "Saving model, epoch: 18346, train_loss: 0.9000030159950256, val_loss: 0.873030960559845\n",
            "Saving model, epoch: 18347, train_loss: 0.900001585483551, val_loss: 0.8730298280715942\n",
            "Saving model, epoch: 18348, train_loss: 0.9000002145767212, val_loss: 0.8730286359786987\n",
            "Saving model, epoch: 18349, train_loss: 0.8999987840652466, val_loss: 0.8730276226997375\n",
            "Saving model, epoch: 18350, train_loss: 0.8999974131584167, val_loss: 0.873026430606842\n",
            "Saving model, epoch: 18351, train_loss: 0.8999959826469421, val_loss: 0.8730258941650391\n",
            "Saving model, epoch: 18352, train_loss: 0.8999945521354675, val_loss: 0.8730245232582092\n",
            "Saving model, epoch: 18353, train_loss: 0.8999931812286377, val_loss: 0.8730236887931824\n",
            "Saving model, epoch: 18354, train_loss: 0.8999918699264526, val_loss: 0.873022735118866\n",
            "Saving model, epoch: 18355, train_loss: 0.899990439414978, val_loss: 0.8730216026306152\n",
            "Saving model, epoch: 18356, train_loss: 0.8999890685081482, val_loss: 0.873020589351654\n",
            "Saving model, epoch: 18357, train_loss: 0.8999876379966736, val_loss: 0.873019814491272\n",
            "Saving model, epoch: 18358, train_loss: 0.8999863266944885, val_loss: 0.8730185031890869\n",
            "Saving model, epoch: 18359, train_loss: 0.8999848365783691, val_loss: 0.8730176687240601\n",
            "Saving model, epoch: 18360, train_loss: 0.8999833464622498, val_loss: 0.8730165958404541\n",
            "Saving model, epoch: 18361, train_loss: 0.8999820947647095, val_loss: 0.8730155229568481\n",
            "Saving model, epoch: 18362, train_loss: 0.8999807238578796, val_loss: 0.8730145692825317\n",
            "Saving model, epoch: 18363, train_loss: 0.8999791741371155, val_loss: 0.8730137348175049\n",
            "Saving model, epoch: 18364, train_loss: 0.8999778032302856, val_loss: 0.8730124831199646\n",
            "Saving model, epoch: 18365, train_loss: 0.8999764919281006, val_loss: 0.873011589050293\n",
            "Saving model, epoch: 18366, train_loss: 0.899975061416626, val_loss: 0.8730103969573975\n",
            "Saving model, epoch: 18367, train_loss: 0.8999736905097961, val_loss: 0.873009443283081\n",
            "Saving model, epoch: 18368, train_loss: 0.8999722599983215, val_loss: 0.8730085492134094\n",
            "Saving model, epoch: 18369, train_loss: 0.8999708890914917, val_loss: 0.8730073571205139\n",
            "Saving model, epoch: 18370, train_loss: 0.8999695777893066, val_loss: 0.8730064034461975\n",
            "Saving model, epoch: 18371, train_loss: 0.8999680280685425, val_loss: 0.8730054497718811\n",
            "Saving model, epoch: 18372, train_loss: 0.8999666571617126, val_loss: 0.8730043768882751\n",
            "Saving model, epoch: 18373, train_loss: 0.899965226650238, val_loss: 0.873003363609314\n",
            "Saving model, epoch: 18374, train_loss: 0.8999640345573425, val_loss: 0.8730025291442871\n",
            "Saving model, epoch: 18375, train_loss: 0.8999625444412231, val_loss: 0.8730012774467468\n",
            "Saving model, epoch: 18376, train_loss: 0.899960994720459, val_loss: 0.8730003833770752\n",
            "Saving model, epoch: 18377, train_loss: 0.8999598026275635, val_loss: 0.8729996085166931\n",
            "Saving model, epoch: 18378, train_loss: 0.8999583125114441, val_loss: 0.8729984760284424\n",
            "Saving model, epoch: 18379, train_loss: 0.8999568819999695, val_loss: 0.8729971647262573\n",
            "Saving model, epoch: 18380, train_loss: 0.8999555110931396, val_loss: 0.8729963302612305\n",
            "Saving model, epoch: 18381, train_loss: 0.8999541997909546, val_loss: 0.8729953765869141\n",
            "Saving model, epoch: 18382, train_loss: 0.89995276927948, val_loss: 0.8729941844940186\n",
            "Saving model, epoch: 18383, train_loss: 0.8999512791633606, val_loss: 0.8729933500289917\n",
            "Saving model, epoch: 18384, train_loss: 0.8999499678611755, val_loss: 0.8729925155639648\n",
            "Saving model, epoch: 18385, train_loss: 0.8999485373497009, val_loss: 0.8729912042617798\n",
            "Saving model, epoch: 18386, train_loss: 0.8999471664428711, val_loss: 0.8729901313781738\n",
            "Saving model, epoch: 18387, train_loss: 0.8999457359313965, val_loss: 0.8729894757270813\n",
            "Saving model, epoch: 18388, train_loss: 0.8999443650245667, val_loss: 0.8729883432388306\n",
            "Saving model, epoch: 18389, train_loss: 0.899942934513092, val_loss: 0.8729872107505798\n",
            "Saving model, epoch: 18390, train_loss: 0.8999415040016174, val_loss: 0.8729860782623291\n",
            "Saving model, epoch: 18391, train_loss: 0.8999401926994324, val_loss: 0.8729854226112366\n",
            "Saving model, epoch: 18392, train_loss: 0.8999388217926025, val_loss: 0.8729840517044067\n",
            "Saving model, epoch: 18393, train_loss: 0.8999373912811279, val_loss: 0.8729832768440247\n",
            "Saving model, epoch: 18394, train_loss: 0.8999360203742981, val_loss: 0.8729822635650635\n",
            "Saving model, epoch: 18395, train_loss: 0.8999345898628235, val_loss: 0.8729812502861023\n",
            "Saving model, epoch: 18396, train_loss: 0.8999332785606384, val_loss: 0.8729798793792725\n",
            "Saving model, epoch: 18397, train_loss: 0.8999319076538086, val_loss: 0.8729790449142456\n",
            "Saving model, epoch: 18398, train_loss: 0.899930477142334, val_loss: 0.8729780316352844\n",
            "Saving model, epoch: 18399, train_loss: 0.8999290466308594, val_loss: 0.8729771375656128\n",
            "Saving model, epoch: 18400, train_loss: 0.8999276757240295, val_loss: 0.8729761242866516\n",
            "epoch: 18401, train_loss: 0.8999262452125549, val_loss: 0.87297523021698\n",
            "Saving model, epoch: 18401, train_loss: 0.8999262452125549, val_loss: 0.87297523021698\n",
            "Saving model, epoch: 18402, train_loss: 0.8999248743057251, val_loss: 0.872974157333374\n",
            "Saving model, epoch: 18403, train_loss: 0.8999234437942505, val_loss: 0.8729729652404785\n",
            "Saving model, epoch: 18404, train_loss: 0.8999220132827759, val_loss: 0.8729720115661621\n",
            "Saving model, epoch: 18405, train_loss: 0.899920642375946, val_loss: 0.8729710578918457\n",
            "Saving model, epoch: 18406, train_loss: 0.8999192118644714, val_loss: 0.8729698061943054\n",
            "Saving model, epoch: 18407, train_loss: 0.8999178409576416, val_loss: 0.8729690909385681\n",
            "Saving model, epoch: 18408, train_loss: 0.899916410446167, val_loss: 0.8729678392410278\n",
            "Saving model, epoch: 18409, train_loss: 0.8999149799346924, val_loss: 0.8729670643806458\n",
            "Saving model, epoch: 18410, train_loss: 0.8999137878417969, val_loss: 0.8729659914970398\n",
            "Saving model, epoch: 18411, train_loss: 0.8999121785163879, val_loss: 0.8729650974273682\n",
            "Saving model, epoch: 18412, train_loss: 0.8999109864234924, val_loss: 0.8729640245437622\n",
            "Saving model, epoch: 18413, train_loss: 0.899909496307373, val_loss: 0.8729628920555115\n",
            "Saving model, epoch: 18414, train_loss: 0.899908185005188, val_loss: 0.8729619383811951\n",
            "Saving model, epoch: 18415, train_loss: 0.8999067544937134, val_loss: 0.8729609847068787\n",
            "Saving model, epoch: 18416, train_loss: 0.8999053835868835, val_loss: 0.8729599714279175\n",
            "Saving model, epoch: 18417, train_loss: 0.8999039530754089, val_loss: 0.8729588985443115\n",
            "Saving model, epoch: 18418, train_loss: 0.8999025225639343, val_loss: 0.8729580640792847\n",
            "Saving model, epoch: 18419, train_loss: 0.8999011516571045, val_loss: 0.872957170009613\n",
            "Saving model, epoch: 18420, train_loss: 0.8998997211456299, val_loss: 0.8729556202888489\n",
            "Saving model, epoch: 18421, train_loss: 0.8998983502388, val_loss: 0.8729549050331116\n",
            "Saving model, epoch: 18422, train_loss: 0.8998969197273254, val_loss: 0.8729540705680847\n",
            "Saving model, epoch: 18423, train_loss: 0.8998954892158508, val_loss: 0.8729526400566101\n",
            "Saving model, epoch: 18424, train_loss: 0.899894118309021, val_loss: 0.8729519248008728\n",
            "Saving model, epoch: 18425, train_loss: 0.8998926877975464, val_loss: 0.872951090335846\n",
            "Saving model, epoch: 18426, train_loss: 0.8998913764953613, val_loss: 0.8729497790336609\n",
            "Saving model, epoch: 18427, train_loss: 0.8998900055885315, val_loss: 0.8729490637779236\n",
            "Saving model, epoch: 18428, train_loss: 0.8998884558677673, val_loss: 0.8729479908943176\n",
            "Saving model, epoch: 18429, train_loss: 0.8998870849609375, val_loss: 0.8729467391967773\n",
            "Saving model, epoch: 18430, train_loss: 0.899885892868042, val_loss: 0.8729457259178162\n",
            "Saving model, epoch: 18431, train_loss: 0.8998843431472778, val_loss: 0.8729448914527893\n",
            "Saving model, epoch: 18432, train_loss: 0.8998830318450928, val_loss: 0.8729437589645386\n",
            "Saving model, epoch: 18433, train_loss: 0.8998816609382629, val_loss: 0.8729427456855774\n",
            "Saving model, epoch: 18434, train_loss: 0.8998802304267883, val_loss: 0.8729417324066162\n",
            "Saving model, epoch: 18435, train_loss: 0.8998788595199585, val_loss: 0.8729409575462341\n",
            "Saving model, epoch: 18436, train_loss: 0.8998774290084839, val_loss: 0.8729398250579834\n",
            "Saving model, epoch: 18437, train_loss: 0.8998759984970093, val_loss: 0.8729388117790222\n",
            "Saving model, epoch: 18438, train_loss: 0.8998746275901794, val_loss: 0.8729376792907715\n",
            "Saving model, epoch: 18439, train_loss: 0.8998731970787048, val_loss: 0.8729365468025208\n",
            "Saving model, epoch: 18440, train_loss: 0.899871826171875, val_loss: 0.8729355335235596\n",
            "Saving model, epoch: 18441, train_loss: 0.8998705148696899, val_loss: 0.8729345798492432\n",
            "Saving model, epoch: 18442, train_loss: 0.8998689651489258, val_loss: 0.8729338645935059\n",
            "Saving model, epoch: 18443, train_loss: 0.899867594242096, val_loss: 0.8729323148727417\n",
            "Saving model, epoch: 18444, train_loss: 0.8998664021492004, val_loss: 0.8729313611984253\n",
            "Saving model, epoch: 18445, train_loss: 0.8998649716377258, val_loss: 0.8729307651519775\n",
            "Saving model, epoch: 18446, train_loss: 0.8998634815216064, val_loss: 0.8729295134544373\n",
            "Saving model, epoch: 18447, train_loss: 0.8998620510101318, val_loss: 0.8729283213615417\n",
            "Saving model, epoch: 18448, train_loss: 0.8998607397079468, val_loss: 0.8729276657104492\n",
            "Saving model, epoch: 18449, train_loss: 0.8998593688011169, val_loss: 0.8729267716407776\n",
            "Saving model, epoch: 18450, train_loss: 0.8998579382896423, val_loss: 0.872925341129303\n",
            "Saving model, epoch: 18451, train_loss: 0.8998565077781677, val_loss: 0.8729243874549866\n",
            "Saving model, epoch: 18452, train_loss: 0.8998551368713379, val_loss: 0.8729236721992493\n",
            "Saving model, epoch: 18453, train_loss: 0.8998537063598633, val_loss: 0.8729227185249329\n",
            "Saving model, epoch: 18454, train_loss: 0.8998523950576782, val_loss: 0.8729212880134583\n",
            "Saving model, epoch: 18455, train_loss: 0.8998509049415588, val_loss: 0.8729206919670105\n",
            "Saving model, epoch: 18456, train_loss: 0.8998495936393738, val_loss: 0.8729197978973389\n",
            "Saving model, epoch: 18457, train_loss: 0.8998481631278992, val_loss: 0.8729184865951538\n",
            "Saving model, epoch: 18458, train_loss: 0.8998469114303589, val_loss: 0.8729174137115479\n",
            "Saving model, epoch: 18459, train_loss: 0.8998454809188843, val_loss: 0.8729168772697449\n",
            "Saving model, epoch: 18460, train_loss: 0.8998440504074097, val_loss: 0.872915506362915\n",
            "Saving model, epoch: 18461, train_loss: 0.8998425602912903, val_loss: 0.8729144930839539\n",
            "Saving model, epoch: 18462, train_loss: 0.8998412489891052, val_loss: 0.8729137182235718\n",
            "Saving model, epoch: 18463, train_loss: 0.8998398780822754, val_loss: 0.8729125261306763\n",
            "Saving model, epoch: 18464, train_loss: 0.8998384475708008, val_loss: 0.8729113340377808\n",
            "Saving model, epoch: 18465, train_loss: 0.8998370170593262, val_loss: 0.8729104399681091\n",
            "Saving model, epoch: 18466, train_loss: 0.8998356461524963, val_loss: 0.8729097843170166\n",
            "Saving model, epoch: 18467, train_loss: 0.8998342156410217, val_loss: 0.8729081749916077\n",
            "Saving model, epoch: 18468, train_loss: 0.8998328447341919, val_loss: 0.8729075193405151\n",
            "Saving model, epoch: 18469, train_loss: 0.8998315930366516, val_loss: 0.872906506061554\n",
            "Saving model, epoch: 18470, train_loss: 0.8998301029205322, val_loss: 0.872905433177948\n",
            "Saving model, epoch: 18471, train_loss: 0.8998286128044128, val_loss: 0.8729043006896973\n",
            "Saving model, epoch: 18472, train_loss: 0.8998271822929382, val_loss: 0.8729035258293152\n",
            "Saving model, epoch: 18473, train_loss: 0.8998258709907532, val_loss: 0.8729023337364197\n",
            "Saving model, epoch: 18474, train_loss: 0.8998245000839233, val_loss: 0.8729013800621033\n",
            "Saving model, epoch: 18475, train_loss: 0.8998231887817383, val_loss: 0.8729004859924316\n",
            "Saving model, epoch: 18476, train_loss: 0.8998217582702637, val_loss: 0.8728994727134705\n",
            "Saving model, epoch: 18477, train_loss: 0.8998203873634338, val_loss: 0.8728983402252197\n",
            "Saving model, epoch: 18478, train_loss: 0.8998189568519592, val_loss: 0.8728974461555481\n",
            "Saving model, epoch: 18479, train_loss: 0.8998176455497742, val_loss: 0.8728965520858765\n",
            "Saving model, epoch: 18480, train_loss: 0.8998162150382996, val_loss: 0.872895359992981\n",
            "Saving model, epoch: 18481, train_loss: 0.8998147249221802, val_loss: 0.872894287109375\n",
            "Saving model, epoch: 18482, train_loss: 0.8998134136199951, val_loss: 0.8728935718536377\n",
            "Saving model, epoch: 18483, train_loss: 0.8998121023178101, val_loss: 0.8728923201560974\n",
            "Saving model, epoch: 18484, train_loss: 0.8998104929924011, val_loss: 0.8728911876678467\n",
            "Saving model, epoch: 18485, train_loss: 0.8998093008995056, val_loss: 0.8728903532028198\n",
            "Saving model, epoch: 18486, train_loss: 0.8998079299926758, val_loss: 0.872889518737793\n",
            "Saving model, epoch: 18487, train_loss: 0.8998064994812012, val_loss: 0.8728882670402527\n",
            "Saving model, epoch: 18488, train_loss: 0.8998050689697266, val_loss: 0.872887134552002\n",
            "Saving model, epoch: 18489, train_loss: 0.8998036980628967, val_loss: 0.8728865385055542\n",
            "Saving model, epoch: 18490, train_loss: 0.8998022675514221, val_loss: 0.8728852868080139\n",
            "Saving model, epoch: 18491, train_loss: 0.8998008966445923, val_loss: 0.8728843331336975\n",
            "Saving model, epoch: 18492, train_loss: 0.8997994661331177, val_loss: 0.8728834390640259\n",
            "Saving model, epoch: 18493, train_loss: 0.8997981548309326, val_loss: 0.8728821873664856\n",
            "Saving model, epoch: 18494, train_loss: 0.8997966647148132, val_loss: 0.872881293296814\n",
            "Saving model, epoch: 18495, train_loss: 0.8997953534126282, val_loss: 0.8728803396224976\n",
            "Saving model, epoch: 18496, train_loss: 0.8997940421104431, val_loss: 0.8728792667388916\n",
            "Saving model, epoch: 18497, train_loss: 0.8997926115989685, val_loss: 0.8728782534599304\n",
            "Saving model, epoch: 18498, train_loss: 0.8997912406921387, val_loss: 0.8728772401809692\n",
            "Saving model, epoch: 18499, train_loss: 0.8997898101806641, val_loss: 0.8728766441345215\n",
            "Saving model, epoch: 18500, train_loss: 0.8997884392738342, val_loss: 0.8728751540184021\n",
            "epoch: 18501, train_loss: 0.8997870087623596, val_loss: 0.8728741407394409\n",
            "Saving model, epoch: 18501, train_loss: 0.8997870087623596, val_loss: 0.8728741407394409\n",
            "Saving model, epoch: 18502, train_loss: 0.899785578250885, val_loss: 0.8728736042976379\n",
            "Saving model, epoch: 18503, train_loss: 0.8997842073440552, val_loss: 0.8728722333908081\n",
            "Saving model, epoch: 18504, train_loss: 0.8997827768325806, val_loss: 0.8728710412979126\n",
            "Saving model, epoch: 18505, train_loss: 0.8997814059257507, val_loss: 0.8728703260421753\n",
            "Saving model, epoch: 18506, train_loss: 0.8997800946235657, val_loss: 0.8728694319725037\n",
            "Saving model, epoch: 18507, train_loss: 0.8997785449028015, val_loss: 0.8728680610656738\n",
            "Saving model, epoch: 18508, train_loss: 0.8997771739959717, val_loss: 0.8728672862052917\n",
            "Saving model, epoch: 18509, train_loss: 0.8997757434844971, val_loss: 0.8728666305541992\n",
            "Saving model, epoch: 18510, train_loss: 0.8997745513916016, val_loss: 0.8728651404380798\n",
            "Saving model, epoch: 18511, train_loss: 0.8997730612754822, val_loss: 0.8728641271591187\n",
            "Saving model, epoch: 18512, train_loss: 0.8997716307640076, val_loss: 0.8728634119033813\n",
            "Saving model, epoch: 18513, train_loss: 0.8997703194618225, val_loss: 0.8728623390197754\n",
            "Saving model, epoch: 18514, train_loss: 0.8997688889503479, val_loss: 0.8728610277175903\n",
            "Saving model, epoch: 18515, train_loss: 0.8997675180435181, val_loss: 0.8728601336479187\n",
            "Saving model, epoch: 18516, train_loss: 0.8997660875320435, val_loss: 0.8728593587875366\n",
            "Saving model, epoch: 18517, train_loss: 0.8997647762298584, val_loss: 0.8728582262992859\n",
            "Saving model, epoch: 18518, train_loss: 0.899763286113739, val_loss: 0.8728571534156799\n",
            "Saving model, epoch: 18519, train_loss: 0.899761974811554, val_loss: 0.8728562593460083\n",
            "Saving model, epoch: 18520, train_loss: 0.8997606039047241, val_loss: 0.8728553056716919\n",
            "Saving model, epoch: 18521, train_loss: 0.8997591733932495, val_loss: 0.8728542327880859\n",
            "Saving model, epoch: 18522, train_loss: 0.8997577428817749, val_loss: 0.8728530406951904\n",
            "Saving model, epoch: 18523, train_loss: 0.8997564315795898, val_loss: 0.8728523850440979\n",
            "Saving model, epoch: 18524, train_loss: 0.89975506067276, val_loss: 0.8728510737419128\n",
            "Saving model, epoch: 18525, train_loss: 0.8997536301612854, val_loss: 0.8728500008583069\n",
            "Saving model, epoch: 18526, train_loss: 0.8997522592544556, val_loss: 0.8728494048118591\n",
            "Saving model, epoch: 18527, train_loss: 0.899750828742981, val_loss: 0.8728481531143188\n",
            "Saving model, epoch: 18528, train_loss: 0.8997493982315063, val_loss: 0.8728470802307129\n",
            "Saving model, epoch: 18529, train_loss: 0.8997480273246765, val_loss: 0.8728463053703308\n",
            "Saving model, epoch: 18530, train_loss: 0.8997465968132019, val_loss: 0.8728452324867249\n",
            "Saving model, epoch: 18531, train_loss: 0.8997452259063721, val_loss: 0.8728440999984741\n",
            "Saving model, epoch: 18532, train_loss: 0.899743914604187, val_loss: 0.8728431463241577\n",
            "Saving model, epoch: 18533, train_loss: 0.8997424840927124, val_loss: 0.8728422522544861\n",
            "Saving model, epoch: 18534, train_loss: 0.8997411727905273, val_loss: 0.8728411197662354\n",
            "Saving model, epoch: 18535, train_loss: 0.8997398018836975, val_loss: 0.8728401064872742\n",
            "Saving model, epoch: 18536, train_loss: 0.8997382521629333, val_loss: 0.8728392720222473\n",
            "Saving model, epoch: 18537, train_loss: 0.8997369408607483, val_loss: 0.8728382587432861\n",
            "Saving model, epoch: 18538, train_loss: 0.8997355699539185, val_loss: 0.8728370666503906\n",
            "Saving model, epoch: 18539, train_loss: 0.8997341394424438, val_loss: 0.8728362917900085\n",
            "Saving model, epoch: 18540, train_loss: 0.8997328281402588, val_loss: 0.8728353381156921\n",
            "Saving model, epoch: 18541, train_loss: 0.899731457233429, val_loss: 0.8728341460227966\n",
            "Saving model, epoch: 18542, train_loss: 0.8997301459312439, val_loss: 0.8728330135345459\n",
            "Saving model, epoch: 18543, train_loss: 0.899728536605835, val_loss: 0.8728323578834534\n",
            "Saving model, epoch: 18544, train_loss: 0.8997273445129395, val_loss: 0.8728312849998474\n",
            "Saving model, epoch: 18545, train_loss: 0.8997259140014648, val_loss: 0.8728299736976624\n",
            "Saving model, epoch: 18546, train_loss: 0.8997244834899902, val_loss: 0.872829258441925\n",
            "Saving model, epoch: 18547, train_loss: 0.8997231125831604, val_loss: 0.8728281855583191\n",
            "Saving model, epoch: 18548, train_loss: 0.8997218012809753, val_loss: 0.8728269338607788\n",
            "Saving model, epoch: 18549, train_loss: 0.899720311164856, val_loss: 0.8728259801864624\n",
            "Saving model, epoch: 18550, train_loss: 0.8997189998626709, val_loss: 0.8728252649307251\n",
            "Saving model, epoch: 18551, train_loss: 0.8997175693511963, val_loss: 0.8728241324424744\n",
            "Saving model, epoch: 18552, train_loss: 0.8997160792350769, val_loss: 0.8728230595588684\n",
            "Saving model, epoch: 18553, train_loss: 0.8997146487236023, val_loss: 0.8728222846984863\n",
            "Saving model, epoch: 18554, train_loss: 0.8997132778167725, val_loss: 0.8728213906288147\n",
            "Saving model, epoch: 18555, train_loss: 0.8997120261192322, val_loss: 0.8728200793266296\n",
            "Saving model, epoch: 18556, train_loss: 0.8997105360031128, val_loss: 0.8728193640708923\n",
            "Saving model, epoch: 18557, train_loss: 0.8997092247009277, val_loss: 0.8728183507919312\n",
            "Saving model, epoch: 18558, train_loss: 0.8997078537940979, val_loss: 0.8728170990943909\n",
            "Saving model, epoch: 18559, train_loss: 0.8997064232826233, val_loss: 0.8728160858154297\n",
            "Saving model, epoch: 18560, train_loss: 0.8997049927711487, val_loss: 0.8728151917457581\n",
            "Saving model, epoch: 18561, train_loss: 0.8997037410736084, val_loss: 0.8728139400482178\n",
            "Saving model, epoch: 18562, train_loss: 0.8997021913528442, val_loss: 0.8728131651878357\n",
            "Saving model, epoch: 18563, train_loss: 0.8997008800506592, val_loss: 0.8728123307228088\n",
            "Saving model, epoch: 18564, train_loss: 0.8996995687484741, val_loss: 0.8728112578392029\n",
            "Saving model, epoch: 18565, train_loss: 0.8996981978416443, val_loss: 0.8728100657463074\n",
            "Saving model, epoch: 18566, train_loss: 0.8996967673301697, val_loss: 0.8728089928627014\n",
            "Saving model, epoch: 18567, train_loss: 0.8996953964233398, val_loss: 0.8728083968162537\n",
            "Saving model, epoch: 18568, train_loss: 0.8996939659118652, val_loss: 0.872806966304779\n",
            "Saving model, epoch: 18569, train_loss: 0.8996925354003906, val_loss: 0.8728061318397522\n",
            "Saving model, epoch: 18570, train_loss: 0.8996911644935608, val_loss: 0.8728052973747253\n",
            "Saving model, epoch: 18571, train_loss: 0.8996897339820862, val_loss: 0.8728041648864746\n",
            "Saving model, epoch: 18572, train_loss: 0.8996884226799011, val_loss: 0.8728031516075134\n",
            "Saving model, epoch: 18573, train_loss: 0.8996869325637817, val_loss: 0.8728020191192627\n",
            "Saving model, epoch: 18574, train_loss: 0.8996857404708862, val_loss: 0.8728011250495911\n",
            "Saving model, epoch: 18575, train_loss: 0.8996843099594116, val_loss: 0.8728001117706299\n",
            "Saving model, epoch: 18576, train_loss: 0.8996829390525818, val_loss: 0.8727990984916687\n",
            "Saving model, epoch: 18577, train_loss: 0.8996815085411072, val_loss: 0.8727981448173523\n",
            "Saving model, epoch: 18578, train_loss: 0.8996800780296326, val_loss: 0.8727973103523254\n",
            "Saving model, epoch: 18579, train_loss: 0.8996787071228027, val_loss: 0.8727961778640747\n",
            "Saving model, epoch: 18580, train_loss: 0.8996772766113281, val_loss: 0.8727953433990479\n",
            "Saving model, epoch: 18581, train_loss: 0.8996759057044983, val_loss: 0.8727943301200867\n",
            "Saving model, epoch: 18582, train_loss: 0.8996745944023132, val_loss: 0.8727931976318359\n",
            "Saving model, epoch: 18583, train_loss: 0.8996731638908386, val_loss: 0.8727922439575195\n",
            "Saving model, epoch: 18584, train_loss: 0.8996717929840088, val_loss: 0.8727911710739136\n",
            "Saving model, epoch: 18585, train_loss: 0.8996704816818237, val_loss: 0.8727900385856628\n",
            "Saving model, epoch: 18586, train_loss: 0.8996689319610596, val_loss: 0.8727891445159912\n",
            "Saving model, epoch: 18587, train_loss: 0.8996675610542297, val_loss: 0.8727882504463196\n",
            "Saving model, epoch: 18588, train_loss: 0.8996662497520447, val_loss: 0.8727871179580688\n",
            "Saving model, epoch: 18589, train_loss: 0.8996648192405701, val_loss: 0.8727861642837524\n",
            "Saving model, epoch: 18590, train_loss: 0.899663507938385, val_loss: 0.872785210609436\n",
            "Saving model, epoch: 18591, train_loss: 0.8996621370315552, val_loss: 0.8727841377258301\n",
            "Saving model, epoch: 18592, train_loss: 0.8996607065200806, val_loss: 0.8727831840515137\n",
            "Saving model, epoch: 18593, train_loss: 0.8996593952178955, val_loss: 0.8727822303771973\n",
            "Saving model, epoch: 18594, train_loss: 0.8996580243110657, val_loss: 0.8727813959121704\n",
            "Saving model, epoch: 18595, train_loss: 0.8996565937995911, val_loss: 0.8727800250053406\n",
            "Saving model, epoch: 18596, train_loss: 0.8996551632881165, val_loss: 0.8727790713310242\n",
            "Saving model, epoch: 18597, train_loss: 0.8996537923812866, val_loss: 0.8727784156799316\n",
            "Saving model, epoch: 18598, train_loss: 0.899652361869812, val_loss: 0.8727772831916809\n",
            "Saving model, epoch: 18599, train_loss: 0.8996509909629822, val_loss: 0.872776210308075\n",
            "Saving model, epoch: 18600, train_loss: 0.8996496796607971, val_loss: 0.8727753162384033\n",
            "epoch: 18601, train_loss: 0.8996483683586121, val_loss: 0.8727744817733765\n",
            "Saving model, epoch: 18601, train_loss: 0.8996483683586121, val_loss: 0.8727744817733765\n",
            "Saving model, epoch: 18602, train_loss: 0.8996469378471375, val_loss: 0.8727729916572571\n",
            "Saving model, epoch: 18603, train_loss: 0.8996455669403076, val_loss: 0.872772216796875\n",
            "Saving model, epoch: 18604, train_loss: 0.899644136428833, val_loss: 0.8727712631225586\n",
            "Saving model, epoch: 18605, train_loss: 0.8996427059173584, val_loss: 0.8727703094482422\n",
            "Saving model, epoch: 18606, train_loss: 0.8996413350105286, val_loss: 0.8727691173553467\n",
            "Saving model, epoch: 18607, train_loss: 0.899639904499054, val_loss: 0.8727682828903198\n",
            "Saving model, epoch: 18608, train_loss: 0.8996387124061584, val_loss: 0.8727673888206482\n",
            "Saving model, epoch: 18609, train_loss: 0.8996371030807495, val_loss: 0.8727661371231079\n",
            "Saving model, epoch: 18610, train_loss: 0.899635910987854, val_loss: 0.8727651834487915\n",
            "Saving model, epoch: 18611, train_loss: 0.8996343612670898, val_loss: 0.8727642893791199\n",
            "Saving model, epoch: 18612, train_loss: 0.8996331095695496, val_loss: 0.8727633357048035\n",
            "Saving model, epoch: 18613, train_loss: 0.899631679058075, val_loss: 0.8727620840072632\n",
            "Saving model, epoch: 18614, train_loss: 0.8996303677558899, val_loss: 0.872761070728302\n",
            "Saving model, epoch: 18615, train_loss: 0.8996288776397705, val_loss: 0.8727602362632751\n",
            "Saving model, epoch: 18616, train_loss: 0.8996274471282959, val_loss: 0.8727591037750244\n",
            "Saving model, epoch: 18617, train_loss: 0.8996261358261108, val_loss: 0.8727582693099976\n",
            "Saving model, epoch: 18618, train_loss: 0.899624764919281, val_loss: 0.8727573752403259\n",
            "Saving model, epoch: 18619, train_loss: 0.8996232151985168, val_loss: 0.8727561235427856\n",
            "Saving model, epoch: 18620, train_loss: 0.8996220231056213, val_loss: 0.8727552890777588\n",
            "Saving model, epoch: 18621, train_loss: 0.8996206521987915, val_loss: 0.8727543354034424\n",
            "Saving model, epoch: 18622, train_loss: 0.8996191024780273, val_loss: 0.8727533221244812\n",
            "Saving model, epoch: 18623, train_loss: 0.8996177911758423, val_loss: 0.8727520704269409\n",
            "Saving model, epoch: 18624, train_loss: 0.8996164202690125, val_loss: 0.8727514147758484\n",
            "Saving model, epoch: 18625, train_loss: 0.8996151089668274, val_loss: 0.8727501630783081\n",
            "Saving model, epoch: 18626, train_loss: 0.8996136784553528, val_loss: 0.8727490901947021\n",
            "Saving model, epoch: 18627, train_loss: 0.899612307548523, val_loss: 0.8727485537528992\n",
            "Saving model, epoch: 18628, train_loss: 0.8996109962463379, val_loss: 0.8727473616600037\n",
            "Saving model, epoch: 18629, train_loss: 0.8996095657348633, val_loss: 0.8727461099624634\n",
            "Saving model, epoch: 18630, train_loss: 0.8996081948280334, val_loss: 0.8727455735206604\n",
            "Saving model, epoch: 18631, train_loss: 0.8996067643165588, val_loss: 0.8727445006370544\n",
            "Saving model, epoch: 18632, train_loss: 0.8996053338050842, val_loss: 0.8727432489395142\n",
            "Saving model, epoch: 18633, train_loss: 0.8996039628982544, val_loss: 0.8727422952651978\n",
            "Saving model, epoch: 18634, train_loss: 0.8996026515960693, val_loss: 0.8727415800094604\n",
            "Saving model, epoch: 18635, train_loss: 0.8996012210845947, val_loss: 0.8727400302886963\n",
            "Saving model, epoch: 18636, train_loss: 0.8995998501777649, val_loss: 0.8727391362190247\n",
            "Saving model, epoch: 18637, train_loss: 0.8995985388755798, val_loss: 0.8727385401725769\n",
            "Saving model, epoch: 18638, train_loss: 0.8995971083641052, val_loss: 0.8727371692657471\n",
            "Saving model, epoch: 18639, train_loss: 0.8995957374572754, val_loss: 0.8727363348007202\n",
            "Saving model, epoch: 18640, train_loss: 0.8995943069458008, val_loss: 0.8727355003356934\n",
            "Saving model, epoch: 18641, train_loss: 0.8995929956436157, val_loss: 0.8727344274520874\n",
            "Saving model, epoch: 18642, train_loss: 0.8995916843414307, val_loss: 0.8727332353591919\n",
            "Saving model, epoch: 18643, train_loss: 0.8995903134346008, val_loss: 0.8727322816848755\n",
            "Saving model, epoch: 18644, train_loss: 0.8995888829231262, val_loss: 0.8727315068244934\n",
            "Saving model, epoch: 18645, train_loss: 0.8995874524116516, val_loss: 0.8727301359176636\n",
            "Saving model, epoch: 18646, train_loss: 0.8995860815048218, val_loss: 0.8727293014526367\n",
            "Saving model, epoch: 18647, train_loss: 0.8995846509933472, val_loss: 0.8727285861968994\n",
            "Saving model, epoch: 18648, train_loss: 0.8995832800865173, val_loss: 0.8727272748947144\n",
            "Saving model, epoch: 18649, train_loss: 0.8995818495750427, val_loss: 0.8727265000343323\n",
            "Saving model, epoch: 18650, train_loss: 0.8995805382728577, val_loss: 0.872725784778595\n",
            "Saving model, epoch: 18651, train_loss: 0.8995792269706726, val_loss: 0.8727242946624756\n",
            "Saving model, epoch: 18652, train_loss: 0.899577796459198, val_loss: 0.8727234601974487\n",
            "Saving model, epoch: 18653, train_loss: 0.8995764255523682, val_loss: 0.8727226853370667\n",
            "Saving model, epoch: 18654, train_loss: 0.8995749950408936, val_loss: 0.8727214336395264\n",
            "Saving model, epoch: 18655, train_loss: 0.899573802947998, val_loss: 0.8727204203605652\n",
            "Saving model, epoch: 18656, train_loss: 0.8995723724365234, val_loss: 0.872719407081604\n",
            "Saving model, epoch: 18657, train_loss: 0.899570882320404, val_loss: 0.8727186322212219\n",
            "Saving model, epoch: 18658, train_loss: 0.8995695114135742, val_loss: 0.872717559337616\n",
            "Saving model, epoch: 18659, train_loss: 0.8995682001113892, val_loss: 0.8727167248725891\n",
            "Saving model, epoch: 18660, train_loss: 0.899566650390625, val_loss: 0.8727154731750488\n",
            "Saving model, epoch: 18661, train_loss: 0.8995653390884399, val_loss: 0.8727145195007324\n",
            "Saving model, epoch: 18662, train_loss: 0.8995639681816101, val_loss: 0.872713565826416\n",
            "Saving model, epoch: 18663, train_loss: 0.899562656879425, val_loss: 0.8727126717567444\n",
            "Saving model, epoch: 18664, train_loss: 0.89956134557724, val_loss: 0.8727115392684937\n",
            "Saving model, epoch: 18665, train_loss: 0.8995598554611206, val_loss: 0.872710645198822\n",
            "Saving model, epoch: 18666, train_loss: 0.899558424949646, val_loss: 0.8727096915245056\n",
            "Saving model, epoch: 18667, train_loss: 0.8995570540428162, val_loss: 0.8727085590362549\n",
            "Saving model, epoch: 18668, train_loss: 0.8995557427406311, val_loss: 0.8727075457572937\n",
            "Saving model, epoch: 18669, train_loss: 0.8995543122291565, val_loss: 0.8727067112922668\n",
            "Saving model, epoch: 18670, train_loss: 0.8995528817176819, val_loss: 0.8727056980133057\n",
            "Saving model, epoch: 18671, train_loss: 0.8995516300201416, val_loss: 0.8727045059204102\n",
            "Saving model, epoch: 18672, train_loss: 0.8995500802993774, val_loss: 0.8727036118507385\n",
            "Saving model, epoch: 18673, train_loss: 0.8995487689971924, val_loss: 0.8727025985717773\n",
            "Saving model, epoch: 18674, train_loss: 0.8995474576950073, val_loss: 0.8727015256881714\n",
            "Saving model, epoch: 18675, train_loss: 0.8995460867881775, val_loss: 0.8727005124092102\n",
            "Saving model, epoch: 18676, train_loss: 0.8995446562767029, val_loss: 0.8726996183395386\n",
            "Saving model, epoch: 18677, train_loss: 0.899543285369873, val_loss: 0.8726988434791565\n",
            "Saving model, epoch: 18678, train_loss: 0.899541974067688, val_loss: 0.872697651386261\n",
            "Saving model, epoch: 18679, train_loss: 0.8995405435562134, val_loss: 0.8726966381072998\n",
            "Saving model, epoch: 18680, train_loss: 0.8995391726493835, val_loss: 0.872695803642273\n",
            "Saving model, epoch: 18681, train_loss: 0.8995377421379089, val_loss: 0.8726945519447327\n",
            "Saving model, epoch: 18682, train_loss: 0.8995364308357239, val_loss: 0.872693657875061\n",
            "Saving model, epoch: 18683, train_loss: 0.8995350003242493, val_loss: 0.8726927042007446\n",
            "Saving model, epoch: 18684, train_loss: 0.8995336294174194, val_loss: 0.8726916313171387\n",
            "Saving model, epoch: 18685, train_loss: 0.8995323181152344, val_loss: 0.8726907968521118\n",
            "Saving model, epoch: 18686, train_loss: 0.899530827999115, val_loss: 0.8726897835731506\n",
            "Saving model, epoch: 18687, train_loss: 0.8995295166969299, val_loss: 0.8726886510848999\n",
            "Saving model, epoch: 18688, train_loss: 0.8995282053947449, val_loss: 0.8726876378059387\n",
            "Saving model, epoch: 18689, train_loss: 0.8995267748832703, val_loss: 0.8726869225502014\n",
            "Saving model, epoch: 18690, train_loss: 0.8995254039764404, val_loss: 0.8726858496665955\n",
            "Saving model, epoch: 18691, train_loss: 0.8995239734649658, val_loss: 0.8726845979690552\n",
            "Saving model, epoch: 18692, train_loss: 0.8995226621627808, val_loss: 0.8726837038993835\n",
            "Saving model, epoch: 18693, train_loss: 0.8995212912559509, val_loss: 0.8726828694343567\n",
            "Saving model, epoch: 18694, train_loss: 0.8995198607444763, val_loss: 0.8726818561553955\n",
            "Saving model, epoch: 18695, train_loss: 0.8995185494422913, val_loss: 0.8726807236671448\n",
            "Saving model, epoch: 18696, train_loss: 0.8995171189308167, val_loss: 0.8726798295974731\n",
            "Saving model, epoch: 18697, train_loss: 0.8995157480239868, val_loss: 0.8726789951324463\n",
            "Saving model, epoch: 18698, train_loss: 0.8995143175125122, val_loss: 0.8726776242256165\n",
            "Saving model, epoch: 18699, train_loss: 0.8995129466056824, val_loss: 0.8726770281791687\n",
            "Saving model, epoch: 18700, train_loss: 0.8995115160942078, val_loss: 0.8726759552955627\n",
            "epoch: 18701, train_loss: 0.8995102047920227, val_loss: 0.8726747632026672\n",
            "Saving model, epoch: 18701, train_loss: 0.8995102047920227, val_loss: 0.8726747632026672\n",
            "Saving model, epoch: 18702, train_loss: 0.8995088934898376, val_loss: 0.8726741075515747\n",
            "Saving model, epoch: 18703, train_loss: 0.8995075225830078, val_loss: 0.8726728558540344\n",
            "Saving model, epoch: 18704, train_loss: 0.8995060920715332, val_loss: 0.8726718425750732\n",
            "Saving model, epoch: 18705, train_loss: 0.8995047807693481, val_loss: 0.8726709485054016\n",
            "Saving model, epoch: 18706, train_loss: 0.8995033502578735, val_loss: 0.87267005443573\n",
            "Saving model, epoch: 18707, train_loss: 0.8995018601417542, val_loss: 0.8726689219474792\n",
            "Saving model, epoch: 18708, train_loss: 0.8995005488395691, val_loss: 0.8726679086685181\n",
            "Saving model, epoch: 18709, train_loss: 0.899499237537384, val_loss: 0.8726669549942017\n",
            "Saving model, epoch: 18710, train_loss: 0.8994978666305542, val_loss: 0.8726657032966614\n",
            "Saving model, epoch: 18711, train_loss: 0.8994964361190796, val_loss: 0.8726651668548584\n",
            "Saving model, epoch: 18712, train_loss: 0.8994951248168945, val_loss: 0.8726639151573181\n",
            "Saving model, epoch: 18713, train_loss: 0.8994936347007751, val_loss: 0.8726629018783569\n",
            "Saving model, epoch: 18714, train_loss: 0.8994924426078796, val_loss: 0.8726620078086853\n",
            "Saving model, epoch: 18715, train_loss: 0.8994908928871155, val_loss: 0.8726611137390137\n",
            "Saving model, epoch: 18716, train_loss: 0.8994896411895752, val_loss: 0.8726601004600525\n",
            "Saving model, epoch: 18717, train_loss: 0.8994882106781006, val_loss: 0.8726587891578674\n",
            "Saving model, epoch: 18718, train_loss: 0.899486780166626, val_loss: 0.8726582527160645\n",
            "Saving model, epoch: 18719, train_loss: 0.8994854688644409, val_loss: 0.8726570010185242\n",
            "Saving model, epoch: 18720, train_loss: 0.8994840979576111, val_loss: 0.8726561665534973\n",
            "Saving model, epoch: 18721, train_loss: 0.8994826674461365, val_loss: 0.8726552724838257\n",
            "Saving model, epoch: 18722, train_loss: 0.8994813561439514, val_loss: 0.8726540803909302\n",
            "Saving model, epoch: 18723, train_loss: 0.8994799852371216, val_loss: 0.8726531267166138\n",
            "Saving model, epoch: 18724, train_loss: 0.899478554725647, val_loss: 0.8726521730422974\n",
            "Saving model, epoch: 18725, train_loss: 0.8994771838188171, val_loss: 0.8726509809494019\n",
            "Saving model, epoch: 18726, train_loss: 0.8994757533073425, val_loss: 0.8726500868797302\n",
            "Saving model, epoch: 18727, train_loss: 0.8994744420051575, val_loss: 0.8726491332054138\n",
            "Saving model, epoch: 18728, train_loss: 0.8994730114936829, val_loss: 0.8726482391357422\n",
            "Saving model, epoch: 18729, train_loss: 0.8994717597961426, val_loss: 0.8726471662521362\n",
            "Saving model, epoch: 18730, train_loss: 0.899470329284668, val_loss: 0.8726463913917542\n",
            "Saving model, epoch: 18731, train_loss: 0.8994688987731934, val_loss: 0.8726452589035034\n",
            "Saving model, epoch: 18732, train_loss: 0.8994675874710083, val_loss: 0.8726440072059631\n",
            "Saving model, epoch: 18733, train_loss: 0.8994660973548889, val_loss: 0.8726435303688049\n",
            "Saving model, epoch: 18734, train_loss: 0.8994647860527039, val_loss: 0.8726423382759094\n",
            "Saving model, epoch: 18735, train_loss: 0.8994634747505188, val_loss: 0.8726413249969482\n",
            "Saving model, epoch: 18736, train_loss: 0.899462103843689, val_loss: 0.8726404309272766\n",
            "Saving model, epoch: 18737, train_loss: 0.8994606733322144, val_loss: 0.8726394176483154\n",
            "Saving model, epoch: 18738, train_loss: 0.8994593620300293, val_loss: 0.8726383447647095\n",
            "Saving model, epoch: 18739, train_loss: 0.8994579911231995, val_loss: 0.8726372718811035\n",
            "Saving model, epoch: 18740, train_loss: 0.8994565606117249, val_loss: 0.8726364970207214\n",
            "Saving model, epoch: 18741, train_loss: 0.8994552493095398, val_loss: 0.8726354241371155\n",
            "Saving model, epoch: 18742, train_loss: 0.8994537591934204, val_loss: 0.87263423204422\n",
            "Saving model, epoch: 18743, train_loss: 0.8994524478912354, val_loss: 0.8726335167884827\n",
            "Saving model, epoch: 18744, train_loss: 0.8994510173797607, val_loss: 0.8726323843002319\n",
            "Saving model, epoch: 18745, train_loss: 0.8994496464729309, val_loss: 0.8726314902305603\n",
            "Saving model, epoch: 18746, train_loss: 0.8994483947753906, val_loss: 0.8726305365562439\n",
            "Saving model, epoch: 18747, train_loss: 0.8994470238685608, val_loss: 0.8726295828819275\n",
            "Saving model, epoch: 18748, train_loss: 0.8994455933570862, val_loss: 0.8726285099983215\n",
            "Saving model, epoch: 18749, train_loss: 0.8994442820549011, val_loss: 0.8726276159286499\n",
            "Saving model, epoch: 18750, train_loss: 0.8994429111480713, val_loss: 0.8726264238357544\n",
            "Saving model, epoch: 18751, train_loss: 0.8994414806365967, val_loss: 0.8726253509521484\n",
            "Saving model, epoch: 18752, train_loss: 0.8994401693344116, val_loss: 0.8726245164871216\n",
            "Saving model, epoch: 18753, train_loss: 0.8994387984275818, val_loss: 0.8726235032081604\n",
            "Saving model, epoch: 18754, train_loss: 0.8994373679161072, val_loss: 0.8726224303245544\n",
            "Saving model, epoch: 18755, train_loss: 0.8994359374046326, val_loss: 0.8726217150688171\n",
            "Saving model, epoch: 18756, train_loss: 0.8994345664978027, val_loss: 0.8726206421852112\n",
            "Saving model, epoch: 18757, train_loss: 0.8994332551956177, val_loss: 0.8726197481155396\n",
            "Saving model, epoch: 18758, train_loss: 0.8994318246841431, val_loss: 0.8726187944412231\n",
            "Saving model, epoch: 18759, train_loss: 0.899430513381958, val_loss: 0.8726177215576172\n",
            "Saving model, epoch: 18760, train_loss: 0.8994291424751282, val_loss: 0.872616708278656\n",
            "Saving model, epoch: 18761, train_loss: 0.8994277119636536, val_loss: 0.87261563539505\n",
            "Saving model, epoch: 18762, train_loss: 0.8994263410568237, val_loss: 0.8726146817207336\n",
            "Saving model, epoch: 18763, train_loss: 0.8994250297546387, val_loss: 0.8726139068603516\n",
            "Saving model, epoch: 18764, train_loss: 0.8994235992431641, val_loss: 0.8726127743721008\n",
            "Saving model, epoch: 18765, train_loss: 0.899422287940979, val_loss: 0.8726118206977844\n",
            "Saving model, epoch: 18766, train_loss: 0.8994209170341492, val_loss: 0.8726109862327576\n",
            "Saving model, epoch: 18767, train_loss: 0.8994194865226746, val_loss: 0.8726097941398621\n",
            "Saving model, epoch: 18768, train_loss: 0.8994180560112, val_loss: 0.8726085424423218\n",
            "Saving model, epoch: 18769, train_loss: 0.8994167447090149, val_loss: 0.8726078867912292\n",
            "Saving model, epoch: 18770, train_loss: 0.8994154930114746, val_loss: 0.8726068139076233\n",
            "Saving model, epoch: 18771, train_loss: 0.8994140625, val_loss: 0.8726059198379517\n",
            "Saving model, epoch: 18772, train_loss: 0.8994126319885254, val_loss: 0.8726051449775696\n",
            "Saving model, epoch: 18773, train_loss: 0.8994112610816956, val_loss: 0.8726037740707397\n",
            "Saving model, epoch: 18774, train_loss: 0.8994099497795105, val_loss: 0.8726028800010681\n",
            "Saving model, epoch: 18775, train_loss: 0.8994086384773254, val_loss: 0.8726019859313965\n",
            "Saving model, epoch: 18776, train_loss: 0.8994072079658508, val_loss: 0.8726009726524353\n",
            "Saving model, epoch: 18777, train_loss: 0.899405837059021, val_loss: 0.8725998401641846\n",
            "Saving model, epoch: 18778, train_loss: 0.8994044065475464, val_loss: 0.8725990653038025\n",
            "Saving model, epoch: 18779, train_loss: 0.8994030356407166, val_loss: 0.8725979328155518\n",
            "Saving model, epoch: 18780, train_loss: 0.8994017839431763, val_loss: 0.8725969791412354\n",
            "Saving model, epoch: 18781, train_loss: 0.8994004130363464, val_loss: 0.8725959658622742\n",
            "Saving model, epoch: 18782, train_loss: 0.8993991017341614, val_loss: 0.8725953102111816\n",
            "Saving model, epoch: 18783, train_loss: 0.899397611618042, val_loss: 0.8725940585136414\n",
            "Saving model, epoch: 18784, train_loss: 0.8993961811065674, val_loss: 0.8725931644439697\n",
            "Saving model, epoch: 18785, train_loss: 0.8993948698043823, val_loss: 0.8725922107696533\n",
            "Saving model, epoch: 18786, train_loss: 0.8993934392929077, val_loss: 0.8725910186767578\n",
            "Saving model, epoch: 18787, train_loss: 0.8993921875953674, val_loss: 0.8725901246070862\n",
            "Saving model, epoch: 18788, train_loss: 0.8993907570838928, val_loss: 0.8725892901420593\n",
            "Saving model, epoch: 18789, train_loss: 0.8993893265724182, val_loss: 0.8725879788398743\n",
            "Saving model, epoch: 18790, train_loss: 0.8993880152702332, val_loss: 0.8725874423980713\n",
            "Saving model, epoch: 18791, train_loss: 0.8993866443634033, val_loss: 0.8725863099098206\n",
            "Saving model, epoch: 18792, train_loss: 0.8993852138519287, val_loss: 0.872585117816925\n",
            "Saving model, epoch: 18793, train_loss: 0.8993839025497437, val_loss: 0.872584342956543\n",
            "Saving model, epoch: 18794, train_loss: 0.8993825316429138, val_loss: 0.8725835680961609\n",
            "Saving model, epoch: 18795, train_loss: 0.8993811011314392, val_loss: 0.872582197189331\n",
            "Saving model, epoch: 18796, train_loss: 0.8993797898292542, val_loss: 0.8725810050964355\n",
            "Saving model, epoch: 18797, train_loss: 0.8993784785270691, val_loss: 0.8725806474685669\n",
            "Saving model, epoch: 18798, train_loss: 0.8993771076202393, val_loss: 0.8725795149803162\n",
            "Saving model, epoch: 18799, train_loss: 0.8993756771087646, val_loss: 0.8725781440734863\n",
            "Saving model, epoch: 18800, train_loss: 0.89937424659729, val_loss: 0.8725773692131042\n",
            "epoch: 18801, train_loss: 0.8993729948997498, val_loss: 0.8725764155387878\n",
            "Saving model, epoch: 18801, train_loss: 0.8993729948997498, val_loss: 0.8725764155387878\n",
            "Saving model, epoch: 18802, train_loss: 0.8993716835975647, val_loss: 0.8725751042366028\n",
            "Saving model, epoch: 18803, train_loss: 0.8993702530860901, val_loss: 0.8725743889808655\n",
            "Saving model, epoch: 18804, train_loss: 0.8993688225746155, val_loss: 0.8725734353065491\n",
            "Saving model, epoch: 18805, train_loss: 0.8993674516677856, val_loss: 0.8725723624229431\n",
            "Saving model, epoch: 18806, train_loss: 0.899366021156311, val_loss: 0.8725714683532715\n",
            "Saving model, epoch: 18807, train_loss: 0.8993648290634155, val_loss: 0.8725705742835999\n",
            "Saving model, epoch: 18808, train_loss: 0.8993633985519409, val_loss: 0.8725690841674805\n",
            "Saving model, epoch: 18809, train_loss: 0.8993620276451111, val_loss: 0.8725684285163879\n",
            "Saving model, epoch: 18810, train_loss: 0.8993605971336365, val_loss: 0.8725677132606506\n",
            "Saving model, epoch: 18811, train_loss: 0.8993592262268066, val_loss: 0.8725666403770447\n",
            "Saving model, epoch: 18812, train_loss: 0.8993579149246216, val_loss: 0.8725656867027283\n",
            "Saving model, epoch: 18813, train_loss: 0.8993563652038574, val_loss: 0.8725647330284119\n",
            "Saving model, epoch: 18814, train_loss: 0.8993551731109619, val_loss: 0.8725635409355164\n",
            "Saving model, epoch: 18815, train_loss: 0.8993538022041321, val_loss: 0.8725626468658447\n",
            "Saving model, epoch: 18816, train_loss: 0.899352490901947, val_loss: 0.8725618720054626\n",
            "Saving model, epoch: 18817, train_loss: 0.899351179599762, val_loss: 0.8725607991218567\n",
            "Saving model, epoch: 18818, train_loss: 0.8993497490882874, val_loss: 0.8725597262382507\n",
            "Saving model, epoch: 18819, train_loss: 0.8993483781814575, val_loss: 0.8725590705871582\n",
            "Saving model, epoch: 18820, train_loss: 0.8993469476699829, val_loss: 0.8725577592849731\n",
            "Saving model, epoch: 18821, train_loss: 0.8993455171585083, val_loss: 0.872556746006012\n",
            "Saving model, epoch: 18822, train_loss: 0.8993442058563232, val_loss: 0.8725558519363403\n",
            "Saving model, epoch: 18823, train_loss: 0.899342954158783, val_loss: 0.8725546598434448\n",
            "Saving model, epoch: 18824, train_loss: 0.8993415236473083, val_loss: 0.872553825378418\n",
            "Saving model, epoch: 18825, train_loss: 0.8993400931358337, val_loss: 0.8725529313087463\n",
            "Saving model, epoch: 18826, train_loss: 0.8993387222290039, val_loss: 0.8725519180297852\n",
            "Saving model, epoch: 18827, train_loss: 0.8993375301361084, val_loss: 0.8725509643554688\n",
            "Saving model, epoch: 18828, train_loss: 0.8993360996246338, val_loss: 0.8725500106811523\n",
            "Saving model, epoch: 18829, train_loss: 0.8993346691131592, val_loss: 0.8725490570068359\n",
            "Saving model, epoch: 18830, train_loss: 0.8993332982063293, val_loss: 0.8725481629371643\n",
            "Saving model, epoch: 18831, train_loss: 0.8993318676948547, val_loss: 0.8725470304489136\n",
            "Saving model, epoch: 18832, train_loss: 0.8993305563926697, val_loss: 0.8725460171699524\n",
            "Saving model, epoch: 18833, train_loss: 0.8993292450904846, val_loss: 0.8725452423095703\n",
            "Saving model, epoch: 18834, train_loss: 0.8993277549743652, val_loss: 0.8725441098213196\n",
            "Saving model, epoch: 18835, train_loss: 0.8993264436721802, val_loss: 0.8725433349609375\n",
            "Saving model, epoch: 18836, train_loss: 0.8993250727653503, val_loss: 0.872542142868042\n",
            "Saving model, epoch: 18837, train_loss: 0.8993237614631653, val_loss: 0.872541069984436\n",
            "Saving model, epoch: 18838, train_loss: 0.8993223309516907, val_loss: 0.8725401163101196\n",
            "Saving model, epoch: 18839, train_loss: 0.8993210196495056, val_loss: 0.8725391626358032\n",
            "Saving model, epoch: 18840, train_loss: 0.8993196487426758, val_loss: 0.8725384473800659\n",
            "Saving model, epoch: 18841, train_loss: 0.8993182182312012, val_loss: 0.8725370168685913\n",
            "Saving model, epoch: 18842, train_loss: 0.8993167877197266, val_loss: 0.8725362420082092\n",
            "Saving model, epoch: 18843, train_loss: 0.8993154764175415, val_loss: 0.8725354671478271\n",
            "Saving model, epoch: 18844, train_loss: 0.8993141651153564, val_loss: 0.8725340962409973\n",
            "Saving model, epoch: 18845, train_loss: 0.8993127942085266, val_loss: 0.8725334405899048\n",
            "Saving model, epoch: 18846, train_loss: 0.899311363697052, val_loss: 0.8725323677062988\n",
            "Saving model, epoch: 18847, train_loss: 0.8993100523948669, val_loss: 0.8725312948226929\n",
            "Saving model, epoch: 18848, train_loss: 0.8993087410926819, val_loss: 0.8725306391716003\n",
            "Saving model, epoch: 18849, train_loss: 0.899307370185852, val_loss: 0.8725293874740601\n",
            "Saving model, epoch: 18850, train_loss: 0.8993059396743774, val_loss: 0.8725283145904541\n",
            "Saving model, epoch: 18851, train_loss: 0.8993046283721924, val_loss: 0.8725274801254272\n",
            "Saving model, epoch: 18852, train_loss: 0.899303138256073, val_loss: 0.8725266456604004\n",
            "Saving model, epoch: 18853, train_loss: 0.8993019461631775, val_loss: 0.8725255727767944\n",
            "Saving model, epoch: 18854, train_loss: 0.8993005156517029, val_loss: 0.8725244402885437\n",
            "Saving model, epoch: 18855, train_loss: 0.899299144744873, val_loss: 0.8725237846374512\n",
            "Saving model, epoch: 18856, train_loss: 0.8992977142333984, val_loss: 0.8725228309631348\n",
            "Saving model, epoch: 18857, train_loss: 0.8992962837219238, val_loss: 0.8725214600563049\n",
            "Saving model, epoch: 18858, train_loss: 0.8992950320243835, val_loss: 0.8725208044052124\n",
            "Saving model, epoch: 18859, train_loss: 0.8992937207221985, val_loss: 0.8725197911262512\n",
            "Saving model, epoch: 18860, train_loss: 0.8992924094200134, val_loss: 0.8725185990333557\n",
            "Saving model, epoch: 18861, train_loss: 0.8992909789085388, val_loss: 0.872518002986908\n",
            "Saving model, epoch: 18862, train_loss: 0.8992896676063538, val_loss: 0.8725168108940125\n",
            "Saving model, epoch: 18863, train_loss: 0.8992882966995239, val_loss: 0.8725156784057617\n",
            "Saving model, epoch: 18864, train_loss: 0.8992868661880493, val_loss: 0.8725149035453796\n",
            "Saving model, epoch: 18865, train_loss: 0.8992855548858643, val_loss: 0.8725141286849976\n",
            "Saving model, epoch: 18866, train_loss: 0.8992842435836792, val_loss: 0.8725128173828125\n",
            "Saving model, epoch: 18867, train_loss: 0.8992828726768494, val_loss: 0.8725119829177856\n",
            "Saving model, epoch: 18868, train_loss: 0.8992814421653748, val_loss: 0.8725109696388245\n",
            "Saving model, epoch: 18869, train_loss: 0.8992800116539001, val_loss: 0.8725098371505737\n",
            "Saving model, epoch: 18870, train_loss: 0.8992787003517151, val_loss: 0.8725091814994812\n",
            "Saving model, epoch: 18871, train_loss: 0.8992773294448853, val_loss: 0.8725082278251648\n",
            "Saving model, epoch: 18872, train_loss: 0.8992760181427002, val_loss: 0.8725071549415588\n",
            "Saving model, epoch: 18873, train_loss: 0.8992745876312256, val_loss: 0.8725060820579529\n",
            "Saving model, epoch: 18874, train_loss: 0.8992732763290405, val_loss: 0.8725053668022156\n",
            "Saving model, epoch: 18875, train_loss: 0.8992720246315002, val_loss: 0.8725038766860962\n",
            "Saving model, epoch: 18876, train_loss: 0.8992705941200256, val_loss: 0.8725031614303589\n",
            "Saving model, epoch: 18877, train_loss: 0.899269163608551, val_loss: 0.8725022673606873\n",
            "Saving model, epoch: 18878, train_loss: 0.8992677927017212, val_loss: 0.8725011348724365\n",
            "Saving model, epoch: 18879, train_loss: 0.8992663621902466, val_loss: 0.8725001215934753\n",
            "Saving model, epoch: 18880, train_loss: 0.8992651700973511, val_loss: 0.8724991083145142\n",
            "Saving model, epoch: 18881, train_loss: 0.8992637395858765, val_loss: 0.8724983930587769\n",
            "Saving model, epoch: 18882, train_loss: 0.8992623686790466, val_loss: 0.8724973797798157\n",
            "Saving model, epoch: 18883, train_loss: 0.8992611169815063, val_loss: 0.8724964261054993\n",
            "Saving model, epoch: 18884, train_loss: 0.899259626865387, val_loss: 0.8724955916404724\n",
            "Saving model, epoch: 18885, train_loss: 0.8992583155632019, val_loss: 0.8724945187568665\n",
            "Saving model, epoch: 18886, train_loss: 0.8992569446563721, val_loss: 0.8724934458732605\n",
            "Saving model, epoch: 18887, train_loss: 0.8992555141448975, val_loss: 0.8724925518035889\n",
            "Saving model, epoch: 18888, train_loss: 0.8992542028427124, val_loss: 0.8724914193153381\n",
            "Saving model, epoch: 18889, train_loss: 0.8992528915405273, val_loss: 0.8724905252456665\n",
            "Saving model, epoch: 18890, train_loss: 0.8992515206336975, val_loss: 0.8724895119667053\n",
            "Saving model, epoch: 18891, train_loss: 0.8992499709129333, val_loss: 0.8724886178970337\n",
            "Saving model, epoch: 18892, train_loss: 0.8992487788200378, val_loss: 0.8724879622459412\n",
            "Saving model, epoch: 18893, train_loss: 0.8992472887039185, val_loss: 0.8724865913391113\n",
            "Saving model, epoch: 18894, train_loss: 0.899246096611023, val_loss: 0.8724856376647949\n",
            "Saving model, epoch: 18895, train_loss: 0.8992446660995483, val_loss: 0.8724846839904785\n",
            "Saving model, epoch: 18896, train_loss: 0.8992432355880737, val_loss: 0.8724837303161621\n",
            "Saving model, epoch: 18897, train_loss: 0.8992420434951782, val_loss: 0.8724827170372009\n",
            "Saving model, epoch: 18898, train_loss: 0.8992405533790588, val_loss: 0.8724818825721741\n",
            "Saving model, epoch: 18899, train_loss: 0.8992392420768738, val_loss: 0.8724806904792786\n",
            "Saving model, epoch: 18900, train_loss: 0.8992378115653992, val_loss: 0.8724796772003174\n",
            "epoch: 18901, train_loss: 0.8992364406585693, val_loss: 0.8724790215492249\n",
            "Saving model, epoch: 18901, train_loss: 0.8992364406585693, val_loss: 0.8724790215492249\n",
            "Saving model, epoch: 18902, train_loss: 0.8992352485656738, val_loss: 0.8724778294563293\n",
            "Saving model, epoch: 18903, train_loss: 0.8992338180541992, val_loss: 0.8724768757820129\n",
            "Saving model, epoch: 18904, train_loss: 0.8992323875427246, val_loss: 0.8724760413169861\n",
            "Saving model, epoch: 18905, train_loss: 0.8992311954498291, val_loss: 0.8724749684333801\n",
            "Saving model, epoch: 18906, train_loss: 0.8992298245429993, val_loss: 0.8724740147590637\n",
            "Saving model, epoch: 18907, train_loss: 0.8992283940315247, val_loss: 0.8724732398986816\n",
            "Saving model, epoch: 18908, train_loss: 0.89922696352005, val_loss: 0.8724721074104309\n",
            "Saving model, epoch: 18909, train_loss: 0.8992255926132202, val_loss: 0.8724711537361145\n",
            "Saving model, epoch: 18910, train_loss: 0.8992242813110352, val_loss: 0.8724703192710876\n",
            "Saving model, epoch: 18911, train_loss: 0.8992228507995605, val_loss: 0.8724690079689026\n",
            "Saving model, epoch: 18912, train_loss: 0.8992215394973755, val_loss: 0.8724681735038757\n",
            "Saving model, epoch: 18913, train_loss: 0.8992201685905457, val_loss: 0.8724672794342041\n",
            "Saving model, epoch: 18914, train_loss: 0.899218738079071, val_loss: 0.8724663257598877\n",
            "Saving model, epoch: 18915, train_loss: 0.899217426776886, val_loss: 0.872465193271637\n",
            "Saving model, epoch: 18916, train_loss: 0.8992161154747009, val_loss: 0.8724643588066101\n",
            "Saving model, epoch: 18917, train_loss: 0.8992147445678711, val_loss: 0.8724632859230042\n",
            "Saving model, epoch: 18918, train_loss: 0.899213433265686, val_loss: 0.8724623918533325\n",
            "Saving model, epoch: 18919, train_loss: 0.899212121963501, val_loss: 0.8724614977836609\n",
            "Saving model, epoch: 18920, train_loss: 0.8992106914520264, val_loss: 0.8724607229232788\n",
            "Saving model, epoch: 18921, train_loss: 0.8992093801498413, val_loss: 0.872459352016449\n",
            "Saving model, epoch: 18922, train_loss: 0.8992078900337219, val_loss: 0.8724584579467773\n",
            "Saving model, epoch: 18923, train_loss: 0.8992066979408264, val_loss: 0.8724575042724609\n",
            "Saving model, epoch: 18924, train_loss: 0.8992052674293518, val_loss: 0.8724566102027893\n",
            "Saving model, epoch: 18925, train_loss: 0.899203896522522, val_loss: 0.8724557161331177\n",
            "Saving model, epoch: 18926, train_loss: 0.8992024660110474, val_loss: 0.8724547028541565\n",
            "Saving model, epoch: 18927, train_loss: 0.8992012739181519, val_loss: 0.8724535703659058\n",
            "Saving model, epoch: 18928, train_loss: 0.8991998434066772, val_loss: 0.8724526762962341\n",
            "Saving model, epoch: 18929, train_loss: 0.8991984724998474, val_loss: 0.8724518418312073\n",
            "Saving model, epoch: 18930, train_loss: 0.8991972208023071, val_loss: 0.8724507689476013\n",
            "Saving model, epoch: 18931, train_loss: 0.8991957306861877, val_loss: 0.8724498152732849\n",
            "Saving model, epoch: 18932, train_loss: 0.8991944193840027, val_loss: 0.8724488019943237\n",
            "Saving model, epoch: 18933, train_loss: 0.8991930484771729, val_loss: 0.872447669506073\n",
            "Saving model, epoch: 18934, train_loss: 0.8991916179656982, val_loss: 0.8724469542503357\n",
            "Saving model, epoch: 18935, train_loss: 0.8991904258728027, val_loss: 0.8724460005760193\n",
            "Saving model, epoch: 18936, train_loss: 0.8991889953613281, val_loss: 0.8724448680877686\n",
            "Saving model, epoch: 18937, train_loss: 0.8991876244544983, val_loss: 0.872444212436676\n",
            "Saving model, epoch: 18938, train_loss: 0.8991863131523132, val_loss: 0.8724429607391357\n",
            "Saving model, epoch: 18939, train_loss: 0.8991848826408386, val_loss: 0.8724419474601746\n",
            "Saving model, epoch: 18940, train_loss: 0.8991835713386536, val_loss: 0.8724413514137268\n",
            "Saving model, epoch: 18941, train_loss: 0.8991822004318237, val_loss: 0.8724398016929626\n",
            "Saving model, epoch: 18942, train_loss: 0.8991807699203491, val_loss: 0.8724391460418701\n",
            "Saving model, epoch: 18943, train_loss: 0.8991795778274536, val_loss: 0.8724380731582642\n",
            "Saving model, epoch: 18944, train_loss: 0.899178147315979, val_loss: 0.8724371194839478\n",
            "Saving model, epoch: 18945, train_loss: 0.8991767764091492, val_loss: 0.8724360466003418\n",
            "Saving model, epoch: 18946, train_loss: 0.8991753458976746, val_loss: 0.8724353909492493\n",
            "Saving model, epoch: 18947, train_loss: 0.899174153804779, val_loss: 0.8724344372749329\n",
            "Saving model, epoch: 18948, train_loss: 0.8991727232933044, val_loss: 0.8724334836006165\n",
            "Saving model, epoch: 18949, train_loss: 0.8991713523864746, val_loss: 0.8724325895309448\n",
            "Saving model, epoch: 18950, train_loss: 0.899169921875, val_loss: 0.8724314570426941\n",
            "Saving model, epoch: 18951, train_loss: 0.8991686105728149, val_loss: 0.8724305629730225\n",
            "Saving model, epoch: 18952, train_loss: 0.8991672992706299, val_loss: 0.8724295496940613\n",
            "Saving model, epoch: 18953, train_loss: 0.8991659283638, val_loss: 0.872428297996521\n",
            "Saving model, epoch: 18954, train_loss: 0.8991646766662598, val_loss: 0.8724277019500732\n",
            "Saving model, epoch: 18955, train_loss: 0.8991633057594299, val_loss: 0.8724266290664673\n",
            "Saving model, epoch: 18956, train_loss: 0.8991618752479553, val_loss: 0.872425377368927\n",
            "Saving model, epoch: 18957, train_loss: 0.8991605639457703, val_loss: 0.8724247813224792\n",
            "Saving model, epoch: 18958, train_loss: 0.8991590738296509, val_loss: 0.8724237084388733\n",
            "Saving model, epoch: 18959, train_loss: 0.8991578817367554, val_loss: 0.8724225759506226\n",
            "Saving model, epoch: 18960, train_loss: 0.8991564512252808, val_loss: 0.87242192029953\n",
            "Saving model, epoch: 18961, train_loss: 0.8991550803184509, val_loss: 0.8724207282066345\n",
            "Saving model, epoch: 18962, train_loss: 0.8991538286209106, val_loss: 0.8724197149276733\n",
            "Saving model, epoch: 18963, train_loss: 0.8991523385047913, val_loss: 0.8724189400672913\n",
            "Saving model, epoch: 18964, train_loss: 0.8991510272026062, val_loss: 0.8724179267883301\n",
            "Saving model, epoch: 18965, train_loss: 0.8991495966911316, val_loss: 0.8724170327186584\n",
            "Saving model, epoch: 18966, train_loss: 0.8991484045982361, val_loss: 0.8724161386489868\n",
            "Saving model, epoch: 18967, train_loss: 0.8991470336914062, val_loss: 0.8724149465560913\n",
            "Saving model, epoch: 18968, train_loss: 0.8991456031799316, val_loss: 0.8724140524864197\n",
            "Saving model, epoch: 18969, train_loss: 0.8991444110870361, val_loss: 0.8724130988121033\n",
            "Saving model, epoch: 18970, train_loss: 0.8991429805755615, val_loss: 0.872411847114563\n",
            "Saving model, epoch: 18971, train_loss: 0.8991416096687317, val_loss: 0.8724111914634705\n",
            "Saving model, epoch: 18972, train_loss: 0.8991402983665466, val_loss: 0.8724101781845093\n",
            "Saving model, epoch: 18973, train_loss: 0.899138867855072, val_loss: 0.8724094033241272\n",
            "Saving model, epoch: 18974, train_loss: 0.899137556552887, val_loss: 0.8724082708358765\n",
            "Saving model, epoch: 18975, train_loss: 0.8991361856460571, val_loss: 0.8724074363708496\n",
            "Saving model, epoch: 18976, train_loss: 0.8991347551345825, val_loss: 0.8724063038825989\n",
            "Saving model, epoch: 18977, train_loss: 0.899133563041687, val_loss: 0.8724056482315063\n",
            "Saving model, epoch: 18978, train_loss: 0.8991321325302124, val_loss: 0.8724043369293213\n",
            "Saving model, epoch: 18979, train_loss: 0.8991307616233826, val_loss: 0.872403621673584\n",
            "Saving model, epoch: 18980, train_loss: 0.8991295099258423, val_loss: 0.8724024891853333\n",
            "Saving model, epoch: 18981, train_loss: 0.8991281390190125, val_loss: 0.8724017143249512\n",
            "Saving model, epoch: 18982, train_loss: 0.8991267085075378, val_loss: 0.8724004626274109\n",
            "Saving model, epoch: 18983, train_loss: 0.899125337600708, val_loss: 0.8723998069763184\n",
            "Saving model, epoch: 18984, train_loss: 0.8991239070892334, val_loss: 0.8723986148834229\n",
            "Saving model, epoch: 18985, train_loss: 0.8991225957870483, val_loss: 0.8723977208137512\n",
            "Saving model, epoch: 18986, train_loss: 0.8991212844848633, val_loss: 0.8723968267440796\n",
            "Saving model, epoch: 18987, train_loss: 0.8991199731826782, val_loss: 0.8723955750465393\n",
            "Saving model, epoch: 18988, train_loss: 0.8991186022758484, val_loss: 0.8723949193954468\n",
            "Saving model, epoch: 18989, train_loss: 0.8991172909736633, val_loss: 0.872393786907196\n",
            "Saving model, epoch: 18990, train_loss: 0.8991158604621887, val_loss: 0.8723927736282349\n",
            "Saving model, epoch: 18991, train_loss: 0.8991145491600037, val_loss: 0.8723919987678528\n",
            "Saving model, epoch: 18992, train_loss: 0.8991132378578186, val_loss: 0.8723910450935364\n",
            "Saving model, epoch: 18993, train_loss: 0.8991118669509888, val_loss: 0.8723898530006409\n",
            "Saving model, epoch: 18994, train_loss: 0.8991104364395142, val_loss: 0.8723893761634827\n",
            "Saving model, epoch: 18995, train_loss: 0.8991091251373291, val_loss: 0.8723878860473633\n",
            "Saving model, epoch: 18996, train_loss: 0.899107813835144, val_loss: 0.872387170791626\n",
            "Saving model, epoch: 18997, train_loss: 0.8991064429283142, val_loss: 0.8723863363265991\n",
            "Saving model, epoch: 18998, train_loss: 0.8991051316261292, val_loss: 0.8723853230476379\n",
            "Saving model, epoch: 18999, train_loss: 0.8991038203239441, val_loss: 0.8723841905593872\n",
            "Saving model, epoch: 19000, train_loss: 0.8991023898124695, val_loss: 0.8723833560943604\n",
            "epoch: 19001, train_loss: 0.8991010189056396, val_loss: 0.8723822832107544\n",
            "Saving model, epoch: 19001, train_loss: 0.8991010189056396, val_loss: 0.8723822832107544\n",
            "Saving model, epoch: 19002, train_loss: 0.899099588394165, val_loss: 0.872381329536438\n",
            "Saving model, epoch: 19003, train_loss: 0.8990983963012695, val_loss: 0.872380793094635\n",
            "Saving model, epoch: 19004, train_loss: 0.8990969657897949, val_loss: 0.8723794221878052\n",
            "Saving model, epoch: 19005, train_loss: 0.8990955948829651, val_loss: 0.8723784685134888\n",
            "Saving model, epoch: 19006, train_loss: 0.8990943431854248, val_loss: 0.8723777532577515\n",
            "Saving model, epoch: 19007, train_loss: 0.899092972278595, val_loss: 0.8723765015602112\n",
            "Saving model, epoch: 19008, train_loss: 0.8990915417671204, val_loss: 0.87237548828125\n",
            "Saving model, epoch: 19009, train_loss: 0.8990902304649353, val_loss: 0.8723747730255127\n",
            "Saving model, epoch: 19010, train_loss: 0.8990889191627502, val_loss: 0.8723737001419067\n",
            "Saving model, epoch: 19011, train_loss: 0.8990875482559204, val_loss: 0.8723728656768799\n",
            "Saving model, epoch: 19012, train_loss: 0.8990862369537354, val_loss: 0.872372031211853\n",
            "Saving model, epoch: 19013, train_loss: 0.8990849256515503, val_loss: 0.8723707795143127\n",
            "Saving model, epoch: 19014, train_loss: 0.8990834951400757, val_loss: 0.8723699450492859\n",
            "Saving model, epoch: 19015, train_loss: 0.8990821242332458, val_loss: 0.8723691701889038\n",
            "Saving model, epoch: 19016, train_loss: 0.8990808129310608, val_loss: 0.8723676800727844\n",
            "Saving model, epoch: 19017, train_loss: 0.8990795016288757, val_loss: 0.872367262840271\n",
            "Saving model, epoch: 19018, train_loss: 0.8990780711174011, val_loss: 0.8723660707473755\n",
            "Saving model, epoch: 19019, train_loss: 0.8990767002105713, val_loss: 0.8723645210266113\n",
            "Saving model, epoch: 19020, train_loss: 0.8990753889083862, val_loss: 0.8723642826080322\n",
            "Saving model, epoch: 19021, train_loss: 0.8990740776062012, val_loss: 0.8723630309104919\n",
            "Saving model, epoch: 19022, train_loss: 0.8990727663040161, val_loss: 0.8723620772361755\n",
            "Saving model, epoch: 19023, train_loss: 0.899071455001831, val_loss: 0.8723613619804382\n",
            "Saving model, epoch: 19024, train_loss: 0.8990700244903564, val_loss: 0.8723602294921875\n",
            "Saving model, epoch: 19025, train_loss: 0.8990686535835266, val_loss: 0.8723593354225159\n",
            "Saving model, epoch: 19026, train_loss: 0.8990674614906311, val_loss: 0.872358500957489\n",
            "Saving model, epoch: 19027, train_loss: 0.8990660309791565, val_loss: 0.8723574876785278\n",
            "Saving model, epoch: 19028, train_loss: 0.8990646004676819, val_loss: 0.8723563551902771\n",
            "Saving model, epoch: 19029, train_loss: 0.8990633487701416, val_loss: 0.8723553419113159\n",
            "Saving model, epoch: 19030, train_loss: 0.8990620374679565, val_loss: 0.8723546862602234\n",
            "Saving model, epoch: 19031, train_loss: 0.8990606069564819, val_loss: 0.8723534345626831\n",
            "Saving model, epoch: 19032, train_loss: 0.8990591764450073, val_loss: 0.872352659702301\n",
            "Saving model, epoch: 19033, train_loss: 0.8990578055381775, val_loss: 0.8723518252372742\n",
            "Saving model, epoch: 19034, train_loss: 0.899056613445282, val_loss: 0.8723508715629578\n",
            "Saving model, epoch: 19035, train_loss: 0.8990551829338074, val_loss: 0.872349739074707\n",
            "Saving model, epoch: 19036, train_loss: 0.8990538716316223, val_loss: 0.8723485469818115\n",
            "Saving model, epoch: 19037, train_loss: 0.8990525603294373, val_loss: 0.872347891330719\n",
            "Saving model, epoch: 19038, train_loss: 0.8990511894226074, val_loss: 0.8723471164703369\n",
            "Saving model, epoch: 19039, train_loss: 0.8990499377250671, val_loss: 0.8723455667495728\n",
            "Saving model, epoch: 19040, train_loss: 0.8990485668182373, val_loss: 0.8723448514938354\n",
            "Saving model, epoch: 19041, train_loss: 0.8990471363067627, val_loss: 0.8723440766334534\n",
            "Saving model, epoch: 19042, train_loss: 0.8990458250045776, val_loss: 0.8723429441452026\n",
            "Saving model, epoch: 19043, train_loss: 0.8990444540977478, val_loss: 0.8723422288894653\n",
            "Saving model, epoch: 19044, train_loss: 0.8990431427955627, val_loss: 0.8723410367965698\n",
            "Saving model, epoch: 19045, train_loss: 0.8990417122840881, val_loss: 0.8723401427268982\n",
            "Saving model, epoch: 19046, train_loss: 0.8990405201911926, val_loss: 0.8723393678665161\n",
            "Saving model, epoch: 19047, train_loss: 0.899039089679718, val_loss: 0.8723381757736206\n",
            "Saving model, epoch: 19048, train_loss: 0.899037778377533, val_loss: 0.872337281703949\n",
            "Saving model, epoch: 19049, train_loss: 0.8990364074707031, val_loss: 0.8723365664482117\n",
            "Saving model, epoch: 19050, train_loss: 0.8990350961685181, val_loss: 0.8723353743553162\n",
            "Saving model, epoch: 19051, train_loss: 0.8990336656570435, val_loss: 0.8723344206809998\n",
            "Saving model, epoch: 19052, train_loss: 0.8990322947502136, val_loss: 0.8723337054252625\n",
            "Saving model, epoch: 19053, train_loss: 0.8990310430526733, val_loss: 0.8723323941230774\n",
            "Saving model, epoch: 19054, train_loss: 0.8990296721458435, val_loss: 0.8723315596580505\n",
            "Saving model, epoch: 19055, train_loss: 0.899028480052948, val_loss: 0.8723308444023132\n",
            "Saving model, epoch: 19056, train_loss: 0.8990270495414734, val_loss: 0.8723295331001282\n",
            "Saving model, epoch: 19057, train_loss: 0.8990256190299988, val_loss: 0.8723286986351013\n",
            "Saving model, epoch: 19058, train_loss: 0.899024248123169, val_loss: 0.8723279237747192\n",
            "Saving model, epoch: 19059, train_loss: 0.8990230560302734, val_loss: 0.8723265528678894\n",
            "Saving model, epoch: 19060, train_loss: 0.8990215063095093, val_loss: 0.8723258972167969\n",
            "Saving model, epoch: 19061, train_loss: 0.8990201950073242, val_loss: 0.8723250031471252\n",
            "Saving model, epoch: 19062, train_loss: 0.8990188241004944, val_loss: 0.8723240494728088\n",
            "Saving model, epoch: 19063, train_loss: 0.8990176320075989, val_loss: 0.8723228573799133\n",
            "Saving model, epoch: 19064, train_loss: 0.8990162014961243, val_loss: 0.8723219633102417\n",
            "Saving model, epoch: 19065, train_loss: 0.8990148901939392, val_loss: 0.8723211884498596\n",
            "Saving model, epoch: 19066, train_loss: 0.8990135788917542, val_loss: 0.8723201751708984\n",
            "Saving model, epoch: 19067, train_loss: 0.8990122079849243, val_loss: 0.8723192811012268\n",
            "Saving model, epoch: 19068, train_loss: 0.8990107774734497, val_loss: 0.8723181486129761\n",
            "Saving model, epoch: 19069, train_loss: 0.8990095853805542, val_loss: 0.8723171353340149\n",
            "Saving model, epoch: 19070, train_loss: 0.89900803565979, val_loss: 0.8723165392875671\n",
            "Saving model, epoch: 19071, train_loss: 0.8990068435668945, val_loss: 0.8723153471946716\n",
            "Saving model, epoch: 19072, train_loss: 0.8990055322647095, val_loss: 0.8723142743110657\n",
            "Saving model, epoch: 19073, train_loss: 0.8990041613578796, val_loss: 0.8723135590553284\n",
            "Saving model, epoch: 19074, train_loss: 0.899002730846405, val_loss: 0.8723124861717224\n",
            "Saving model, epoch: 19075, train_loss: 0.8990015387535095, val_loss: 0.8723113536834717\n",
            "Saving model, epoch: 19076, train_loss: 0.8990001082420349, val_loss: 0.8723106980323792\n",
            "Saving model, epoch: 19077, train_loss: 0.8989987969398499, val_loss: 0.872309684753418\n",
            "Saving model, epoch: 19078, train_loss: 0.89899742603302, val_loss: 0.8723085522651672\n",
            "Saving model, epoch: 19079, train_loss: 0.898996114730835, val_loss: 0.8723076581954956\n",
            "Saving model, epoch: 19080, train_loss: 0.8989946842193604, val_loss: 0.8723070621490479\n",
            "Saving model, epoch: 19081, train_loss: 0.8989933729171753, val_loss: 0.8723053932189941\n",
            "Saving model, epoch: 19082, train_loss: 0.8989920020103455, val_loss: 0.8723052740097046\n",
            "Saving model, epoch: 19083, train_loss: 0.89899080991745, val_loss: 0.87230384349823\n",
            "Saving model, epoch: 19084, train_loss: 0.8989893794059753, val_loss: 0.8723028302192688\n",
            "Saving model, epoch: 19085, train_loss: 0.8989880681037903, val_loss: 0.8723021745681763\n",
            "Saving model, epoch: 19086, train_loss: 0.8989866375923157, val_loss: 0.8723008632659912\n",
            "Saving model, epoch: 19087, train_loss: 0.8989854454994202, val_loss: 0.8723004460334778\n",
            "Saving model, epoch: 19088, train_loss: 0.8989840745925903, val_loss: 0.8722991347312927\n",
            "Saving model, epoch: 19089, train_loss: 0.8989826440811157, val_loss: 0.8722983598709106\n",
            "Saving model, epoch: 19090, train_loss: 0.8989813327789307, val_loss: 0.8722973465919495\n",
            "Saving model, epoch: 19091, train_loss: 0.8989799618721008, val_loss: 0.8722962737083435\n",
            "Saving model, epoch: 19092, train_loss: 0.8989787101745605, val_loss: 0.8722952604293823\n",
            "Saving model, epoch: 19093, train_loss: 0.8989773392677307, val_loss: 0.872294545173645\n",
            "Saving model, epoch: 19094, train_loss: 0.8989760279655457, val_loss: 0.8722937703132629\n",
            "Saving model, epoch: 19095, train_loss: 0.8989747166633606, val_loss: 0.8722925186157227\n",
            "Saving model, epoch: 19096, train_loss: 0.898973286151886, val_loss: 0.8722918033599854\n",
            "Saving model, epoch: 19097, train_loss: 0.8989719748497009, val_loss: 0.8722906708717346\n",
            "Saving model, epoch: 19098, train_loss: 0.8989706635475159, val_loss: 0.8722897171974182\n",
            "Saving model, epoch: 19099, train_loss: 0.8989694118499756, val_loss: 0.8722888827323914\n",
            "Saving model, epoch: 19100, train_loss: 0.898967981338501, val_loss: 0.8722878098487854\n",
            "epoch: 19101, train_loss: 0.8989665508270264, val_loss: 0.8722870349884033\n",
            "Saving model, epoch: 19101, train_loss: 0.8989665508270264, val_loss: 0.8722870349884033\n",
            "Saving model, epoch: 19102, train_loss: 0.8989652395248413, val_loss: 0.8722860813140869\n",
            "Saving model, epoch: 19103, train_loss: 0.8989639282226562, val_loss: 0.8722850680351257\n",
            "Saving model, epoch: 19104, train_loss: 0.8989625573158264, val_loss: 0.872283935546875\n",
            "Saving model, epoch: 19105, train_loss: 0.8989612460136414, val_loss: 0.8722829222679138\n",
            "Saving model, epoch: 19106, train_loss: 0.8989599347114563, val_loss: 0.8722822666168213\n",
            "Saving model, epoch: 19107, train_loss: 0.8989586234092712, val_loss: 0.8722810745239258\n",
            "Saving model, epoch: 19108, train_loss: 0.8989573121070862, val_loss: 0.8722801208496094\n",
            "Saving model, epoch: 19109, train_loss: 0.8989559412002563, val_loss: 0.8722794651985168\n",
            "Saving model, epoch: 19110, train_loss: 0.8989546298980713, val_loss: 0.8722782731056213\n",
            "Saving model, epoch: 19111, train_loss: 0.8989533185958862, val_loss: 0.8722772598266602\n",
            "Saving model, epoch: 19112, train_loss: 0.8989518880844116, val_loss: 0.872276246547699\n",
            "Saving model, epoch: 19113, train_loss: 0.8989505171775818, val_loss: 0.8722755312919617\n",
            "Saving model, epoch: 19114, train_loss: 0.8989492654800415, val_loss: 0.8722745180130005\n",
            "Saving model, epoch: 19115, train_loss: 0.8989478945732117, val_loss: 0.8722735643386841\n",
            "Saving model, epoch: 19116, train_loss: 0.8989465832710266, val_loss: 0.8722723126411438\n",
            "Saving model, epoch: 19117, train_loss: 0.8989452719688416, val_loss: 0.8722720742225647\n",
            "Saving model, epoch: 19118, train_loss: 0.8989438414573669, val_loss: 0.8722705841064453\n",
            "Saving model, epoch: 19119, train_loss: 0.8989426493644714, val_loss: 0.8722697496414185\n",
            "Saving model, epoch: 19120, train_loss: 0.898941159248352, val_loss: 0.8722689151763916\n",
            "Saving model, epoch: 19121, train_loss: 0.898939847946167, val_loss: 0.87226802110672\n",
            "Saving model, epoch: 19122, train_loss: 0.8989384174346924, val_loss: 0.872266948223114\n",
            "Saving model, epoch: 19123, train_loss: 0.8989372253417969, val_loss: 0.8722661733627319\n",
            "Saving model, epoch: 19124, train_loss: 0.898935854434967, val_loss: 0.8722652196884155\n",
            "Saving model, epoch: 19125, train_loss: 0.8989344239234924, val_loss: 0.8722639083862305\n",
            "Saving model, epoch: 19126, train_loss: 0.8989332318305969, val_loss: 0.8722634315490723\n",
            "Saving model, epoch: 19127, train_loss: 0.8989318013191223, val_loss: 0.8722622990608215\n",
            "Saving model, epoch: 19128, train_loss: 0.8989304900169373, val_loss: 0.8722608685493469\n",
            "Saving model, epoch: 19129, train_loss: 0.8989291787147522, val_loss: 0.872260570526123\n",
            "Saving model, epoch: 19130, train_loss: 0.8989278078079224, val_loss: 0.8722593188285828\n",
            "Saving model, epoch: 19131, train_loss: 0.8989264965057373, val_loss: 0.8722581267356873\n",
            "Saving model, epoch: 19132, train_loss: 0.8989251852035522, val_loss: 0.872257649898529\n",
            "Saving model, epoch: 19133, train_loss: 0.8989238739013672, val_loss: 0.8722565174102783\n",
            "Saving model, epoch: 19134, train_loss: 0.8989225625991821, val_loss: 0.8722556829452515\n",
            "Saving model, epoch: 19135, train_loss: 0.8989211320877075, val_loss: 0.8722546100616455\n",
            "Saving model, epoch: 19136, train_loss: 0.8989198803901672, val_loss: 0.8722537159919739\n",
            "Saving model, epoch: 19137, train_loss: 0.8989185690879822, val_loss: 0.8722526431083679\n",
            "Saving model, epoch: 19138, train_loss: 0.8989171385765076, val_loss: 0.8722518682479858\n",
            "Saving model, epoch: 19139, train_loss: 0.8989158272743225, val_loss: 0.8722507357597351\n",
            "Saving model, epoch: 19140, train_loss: 0.8989143967628479, val_loss: 0.8722501993179321\n",
            "Saving model, epoch: 19141, train_loss: 0.8989131450653076, val_loss: 0.8722490072250366\n",
            "Saving model, epoch: 19142, train_loss: 0.8989118933677673, val_loss: 0.8722480535507202\n",
            "Saving model, epoch: 19143, train_loss: 0.8989105224609375, val_loss: 0.8722472190856934\n",
            "Saving model, epoch: 19144, train_loss: 0.8989092111587524, val_loss: 0.8722457885742188\n",
            "Saving model, epoch: 19145, train_loss: 0.8989078998565674, val_loss: 0.8722450137138367\n",
            "Saving model, epoch: 19146, train_loss: 0.8989064693450928, val_loss: 0.8722444772720337\n",
            "Saving model, epoch: 19147, train_loss: 0.8989051580429077, val_loss: 0.8722431063652039\n",
            "Saving model, epoch: 19148, train_loss: 0.8989039063453674, val_loss: 0.8722426295280457\n",
            "Saving model, epoch: 19149, train_loss: 0.8989024758338928, val_loss: 0.8722414970397949\n",
            "Saving model, epoch: 19150, train_loss: 0.8989012837409973, val_loss: 0.8722406029701233\n",
            "Saving model, epoch: 19151, train_loss: 0.8988998532295227, val_loss: 0.8722394704818726\n",
            "Saving model, epoch: 19152, train_loss: 0.8988984227180481, val_loss: 0.87223881483078\n",
            "Saving model, epoch: 19153, train_loss: 0.8988970518112183, val_loss: 0.8722375631332397\n",
            "Saving model, epoch: 19154, train_loss: 0.8988958597183228, val_loss: 0.8722366690635681\n",
            "Saving model, epoch: 19155, train_loss: 0.8988946080207825, val_loss: 0.8722360730171204\n",
            "Saving model, epoch: 19156, train_loss: 0.8988929986953735, val_loss: 0.8722346425056458\n",
            "Saving model, epoch: 19157, train_loss: 0.898891806602478, val_loss: 0.8722338676452637\n",
            "Saving model, epoch: 19158, train_loss: 0.898890495300293, val_loss: 0.872232973575592\n",
            "Saving model, epoch: 19159, train_loss: 0.8988891839981079, val_loss: 0.8722320199012756\n",
            "Saving model, epoch: 19160, train_loss: 0.8988879323005676, val_loss: 0.8722313046455383\n",
            "Saving model, epoch: 19161, train_loss: 0.8988863825798035, val_loss: 0.8722298741340637\n",
            "Saving model, epoch: 19162, train_loss: 0.898885190486908, val_loss: 0.8722292184829712\n",
            "Saving model, epoch: 19163, train_loss: 0.8988837599754333, val_loss: 0.8722283840179443\n",
            "Saving model, epoch: 19164, train_loss: 0.8988825678825378, val_loss: 0.8722273111343384\n",
            "Saving model, epoch: 19165, train_loss: 0.898881196975708, val_loss: 0.8722261786460876\n",
            "Saving model, epoch: 19166, train_loss: 0.898879885673523, val_loss: 0.872225821018219\n",
            "Saving model, epoch: 19167, train_loss: 0.8988785743713379, val_loss: 0.8722242116928101\n",
            "Saving model, epoch: 19168, train_loss: 0.8988771438598633, val_loss: 0.8722236156463623\n",
            "Saving model, epoch: 19169, train_loss: 0.8988759517669678, val_loss: 0.8722226619720459\n",
            "Saving model, epoch: 19170, train_loss: 0.8988745212554932, val_loss: 0.8722214698791504\n",
            "Saving model, epoch: 19171, train_loss: 0.8988732099533081, val_loss: 0.8722206354141235\n",
            "Saving model, epoch: 19172, train_loss: 0.8988718390464783, val_loss: 0.8722197413444519\n",
            "Saving model, epoch: 19173, train_loss: 0.8988705277442932, val_loss: 0.8722187876701355\n",
            "Saving model, epoch: 19174, train_loss: 0.8988693356513977, val_loss: 0.872218132019043\n",
            "Saving model, epoch: 19175, train_loss: 0.8988677859306335, val_loss: 0.8722169399261475\n",
            "Saving model, epoch: 19176, train_loss: 0.8988664746284485, val_loss: 0.872215986251831\n",
            "Saving model, epoch: 19177, train_loss: 0.898865282535553, val_loss: 0.8722150921821594\n",
            "Saving model, epoch: 19178, train_loss: 0.8988639116287231, val_loss: 0.8722140789031982\n",
            "Saving model, epoch: 19179, train_loss: 0.8988626003265381, val_loss: 0.8722133636474609\n",
            "Saving model, epoch: 19180, train_loss: 0.8988611698150635, val_loss: 0.8722123503684998\n",
            "Saving model, epoch: 19181, train_loss: 0.8988598585128784, val_loss: 0.8722111582756042\n",
            "Saving model, epoch: 19182, train_loss: 0.8988585472106934, val_loss: 0.8722105622291565\n",
            "Saving model, epoch: 19183, train_loss: 0.8988572359085083, val_loss: 0.872209370136261\n",
            "Saving model, epoch: 19184, train_loss: 0.8988559246063232, val_loss: 0.872208297252655\n",
            "Saving model, epoch: 19185, train_loss: 0.898854672908783, val_loss: 0.8722076416015625\n",
            "Saving model, epoch: 19186, train_loss: 0.8988532423973083, val_loss: 0.8722066879272461\n",
            "Saving model, epoch: 19187, train_loss: 0.8988519310951233, val_loss: 0.8722057342529297\n",
            "Saving model, epoch: 19188, train_loss: 0.8988506197929382, val_loss: 0.8722048997879028\n",
            "Saving model, epoch: 19189, train_loss: 0.8988493084907532, val_loss: 0.8722037672996521\n",
            "Saving model, epoch: 19190, train_loss: 0.8988479971885681, val_loss: 0.8722027540206909\n",
            "Saving model, epoch: 19191, train_loss: 0.8988466262817383, val_loss: 0.8722021579742432\n",
            "Saving model, epoch: 19192, train_loss: 0.8988453149795532, val_loss: 0.8722007274627686\n",
            "Saving model, epoch: 19193, train_loss: 0.8988440036773682, val_loss: 0.8722001910209656\n",
            "Saving model, epoch: 19194, train_loss: 0.8988426923751831, val_loss: 0.8721991777420044\n",
            "Saving model, epoch: 19195, train_loss: 0.8988412022590637, val_loss: 0.8721980452537537\n",
            "Saving model, epoch: 19196, train_loss: 0.8988399505615234, val_loss: 0.8721974492073059\n",
            "Saving model, epoch: 19197, train_loss: 0.8988386988639832, val_loss: 0.8721961975097656\n",
            "Saving model, epoch: 19198, train_loss: 0.8988373875617981, val_loss: 0.872195303440094\n",
            "Saving model, epoch: 19199, train_loss: 0.8988359570503235, val_loss: 0.8721944093704224\n",
            "Saving model, epoch: 19200, train_loss: 0.8988345265388489, val_loss: 0.8721933364868164\n",
            "epoch: 19201, train_loss: 0.8988333344459534, val_loss: 0.8721926212310791\n",
            "Saving model, epoch: 19201, train_loss: 0.8988333344459534, val_loss: 0.8721926212310791\n",
            "Saving model, epoch: 19202, train_loss: 0.8988320231437683, val_loss: 0.8721914291381836\n",
            "Saving model, epoch: 19203, train_loss: 0.8988307118415833, val_loss: 0.8721907734870911\n",
            "Saving model, epoch: 19204, train_loss: 0.8988292217254639, val_loss: 0.8721895217895508\n",
            "Saving model, epoch: 19205, train_loss: 0.8988280296325684, val_loss: 0.8721887469291687\n",
            "Saving model, epoch: 19206, train_loss: 0.8988267183303833, val_loss: 0.8721879124641418\n",
            "Saving model, epoch: 19207, train_loss: 0.8988254070281982, val_loss: 0.872187077999115\n",
            "Saving model, epoch: 19208, train_loss: 0.8988240957260132, val_loss: 0.8721858263015747\n",
            "Saving model, epoch: 19209, train_loss: 0.8988227248191833, val_loss: 0.8721849918365479\n",
            "Saving model, epoch: 19210, train_loss: 0.8988214135169983, val_loss: 0.872184157371521\n",
            "Saving model, epoch: 19211, train_loss: 0.8988199830055237, val_loss: 0.872183084487915\n",
            "Saving model, epoch: 19212, train_loss: 0.8988187909126282, val_loss: 0.8721822500228882\n",
            "Saving model, epoch: 19213, train_loss: 0.8988173007965088, val_loss: 0.8721810579299927\n",
            "Saving model, epoch: 19214, train_loss: 0.8988160490989685, val_loss: 0.8721802830696106\n",
            "Saving model, epoch: 19215, train_loss: 0.8988146781921387, val_loss: 0.8721795678138733\n",
            "Saving model, epoch: 19216, train_loss: 0.8988134264945984, val_loss: 0.8721781373023987\n",
            "Saving model, epoch: 19217, train_loss: 0.8988120555877686, val_loss: 0.8721776604652405\n",
            "Saving model, epoch: 19218, train_loss: 0.898810863494873, val_loss: 0.8721765279769897\n",
            "Saving model, epoch: 19219, train_loss: 0.8988094329833984, val_loss: 0.8721754550933838\n",
            "Saving model, epoch: 19220, train_loss: 0.8988080024719238, val_loss: 0.8721749782562256\n",
            "Saving model, epoch: 19221, train_loss: 0.8988067507743835, val_loss: 0.8721736073493958\n",
            "Saving model, epoch: 19222, train_loss: 0.8988054990768433, val_loss: 0.8721727132797241\n",
            "Saving model, epoch: 19223, train_loss: 0.8988041877746582, val_loss: 0.8721719980239868\n",
            "Saving model, epoch: 19224, train_loss: 0.8988028168678284, val_loss: 0.8721709251403809\n",
            "Saving model, epoch: 19225, train_loss: 0.8988013863563538, val_loss: 0.872170090675354\n",
            "Saving model, epoch: 19226, train_loss: 0.8988000750541687, val_loss: 0.8721692562103271\n",
            "Saving model, epoch: 19227, train_loss: 0.8987990021705627, val_loss: 0.8721681833267212\n",
            "Saving model, epoch: 19228, train_loss: 0.8987975716590881, val_loss: 0.8721674680709839\n",
            "Saving model, epoch: 19229, train_loss: 0.8987962007522583, val_loss: 0.8721662759780884\n",
            "Saving model, epoch: 19230, train_loss: 0.8987948894500732, val_loss: 0.8721652030944824\n",
            "Saving model, epoch: 19231, train_loss: 0.8987935781478882, val_loss: 0.8721644282341003\n",
            "Saving model, epoch: 19232, train_loss: 0.8987921476364136, val_loss: 0.8721635341644287\n",
            "Saving model, epoch: 19233, train_loss: 0.8987909555435181, val_loss: 0.8721627593040466\n",
            "Saving model, epoch: 19234, train_loss: 0.8987895250320435, val_loss: 0.8721616268157959\n",
            "Saving model, epoch: 19235, train_loss: 0.898788332939148, val_loss: 0.872160792350769\n",
            "Saving model, epoch: 19236, train_loss: 0.8987869024276733, val_loss: 0.8721599578857422\n",
            "Saving model, epoch: 19237, train_loss: 0.8987857103347778, val_loss: 0.8721588253974915\n",
            "Saving model, epoch: 19238, train_loss: 0.898784339427948, val_loss: 0.8721578121185303\n",
            "Saving model, epoch: 19239, train_loss: 0.8987830281257629, val_loss: 0.8721573352813721\n",
            "Saving model, epoch: 19240, train_loss: 0.8987817168235779, val_loss: 0.8721559643745422\n",
            "Saving model, epoch: 19241, train_loss: 0.8987802863121033, val_loss: 0.8721550703048706\n",
            "Saving model, epoch: 19242, train_loss: 0.8987789750099182, val_loss: 0.8721544146537781\n",
            "Saving model, epoch: 19243, train_loss: 0.8987776637077332, val_loss: 0.8721531629562378\n",
            "Saving model, epoch: 19244, train_loss: 0.8987762928009033, val_loss: 0.8721524477005005\n",
            "Saving model, epoch: 19245, train_loss: 0.8987749814987183, val_loss: 0.8721513152122498\n",
            "Saving model, epoch: 19246, train_loss: 0.8987736701965332, val_loss: 0.8721503019332886\n",
            "Saving model, epoch: 19247, train_loss: 0.8987724781036377, val_loss: 0.8721494078636169\n",
            "Saving model, epoch: 19248, train_loss: 0.8987710475921631, val_loss: 0.8721487522125244\n",
            "Saving model, epoch: 19249, train_loss: 0.8987696766853333, val_loss: 0.8721473813056946\n",
            "Saving model, epoch: 19250, train_loss: 0.898768424987793, val_loss: 0.8721469044685364\n",
            "Saving model, epoch: 19251, train_loss: 0.8987671136856079, val_loss: 0.8721457123756409\n",
            "Saving model, epoch: 19252, train_loss: 0.8987658619880676, val_loss: 0.8721448183059692\n",
            "Saving model, epoch: 19253, train_loss: 0.898764431476593, val_loss: 0.8721440434455872\n",
            "Saving model, epoch: 19254, train_loss: 0.8987630009651184, val_loss: 0.8721427321434021\n",
            "Saving model, epoch: 19255, train_loss: 0.8987618088722229, val_loss: 0.8721423745155334\n",
            "Saving model, epoch: 19256, train_loss: 0.8987604975700378, val_loss: 0.872141420841217\n",
            "Saving model, epoch: 19257, train_loss: 0.8987591862678528, val_loss: 0.8721401691436768\n",
            "Saving model, epoch: 19258, train_loss: 0.8987578749656677, val_loss: 0.872139573097229\n",
            "Saving model, epoch: 19259, train_loss: 0.8987565636634827, val_loss: 0.872138261795044\n",
            "Saving model, epoch: 19260, train_loss: 0.8987551927566528, val_loss: 0.8721373081207275\n",
            "Saving model, epoch: 19261, train_loss: 0.8987537622451782, val_loss: 0.8721365332603455\n",
            "Saving model, epoch: 19262, train_loss: 0.8987525701522827, val_loss: 0.87213534116745\n",
            "Saving model, epoch: 19263, train_loss: 0.8987512588500977, val_loss: 0.8721344470977783\n",
            "Saving model, epoch: 19264, train_loss: 0.8987499475479126, val_loss: 0.872133731842041\n",
            "Saving model, epoch: 19265, train_loss: 0.8987486362457275, val_loss: 0.8721326589584351\n",
            "Saving model, epoch: 19266, train_loss: 0.8987472653388977, val_loss: 0.8721316456794739\n",
            "Saving model, epoch: 19267, train_loss: 0.8987459540367126, val_loss: 0.8721310496330261\n",
            "Saving model, epoch: 19268, train_loss: 0.8987447619438171, val_loss: 0.8721299767494202\n",
            "Saving model, epoch: 19269, train_loss: 0.8987433314323425, val_loss: 0.8721288442611694\n",
            "Saving model, epoch: 19270, train_loss: 0.898742139339447, val_loss: 0.8721282482147217\n",
            "Saving model, epoch: 19271, train_loss: 0.8987407088279724, val_loss: 0.8721270561218262\n",
            "Saving model, epoch: 19272, train_loss: 0.8987393975257874, val_loss: 0.8721264004707336\n",
            "Saving model, epoch: 19273, train_loss: 0.8987380266189575, val_loss: 0.8721253275871277\n",
            "Saving model, epoch: 19274, train_loss: 0.8987367749214172, val_loss: 0.872124195098877\n",
            "Saving model, epoch: 19275, train_loss: 0.8987352848052979, val_loss: 0.8721235990524292\n",
            "Saving model, epoch: 19276, train_loss: 0.8987340927124023, val_loss: 0.872122585773468\n",
            "Saving model, epoch: 19277, train_loss: 0.8987327814102173, val_loss: 0.8721213936805725\n",
            "Saving model, epoch: 19278, train_loss: 0.8987314701080322, val_loss: 0.8721210360527039\n",
            "Saving model, epoch: 19279, train_loss: 0.8987301588058472, val_loss: 0.8721196055412292\n",
            "Saving model, epoch: 19280, train_loss: 0.8987288475036621, val_loss: 0.8721188902854919\n",
            "Saving model, epoch: 19281, train_loss: 0.8987274765968323, val_loss: 0.8721181750297546\n",
            "Saving model, epoch: 19282, train_loss: 0.898726224899292, val_loss: 0.8721166253089905\n",
            "Saving model, epoch: 19283, train_loss: 0.8987248539924622, val_loss: 0.872116208076477\n",
            "Saving model, epoch: 19284, train_loss: 0.8987236618995667, val_loss: 0.8721150755882263\n",
            "Saving model, epoch: 19285, train_loss: 0.898722231388092, val_loss: 0.8721140623092651\n",
            "Saving model, epoch: 19286, train_loss: 0.8987208008766174, val_loss: 0.8721133470535278\n",
            "Saving model, epoch: 19287, train_loss: 0.8987196087837219, val_loss: 0.8721123933792114\n",
            "Saving model, epoch: 19288, train_loss: 0.8987182974815369, val_loss: 0.8721112012863159\n",
            "Saving model, epoch: 19289, train_loss: 0.8987169861793518, val_loss: 0.8721105456352234\n",
            "Saving model, epoch: 19290, train_loss: 0.898715615272522, val_loss: 0.8721093535423279\n",
            "Saving model, epoch: 19291, train_loss: 0.8987143635749817, val_loss: 0.8721085786819458\n",
            "Saving model, epoch: 19292, train_loss: 0.8987131118774414, val_loss: 0.8721076846122742\n",
            "Saving model, epoch: 19293, train_loss: 0.8987118005752563, val_loss: 0.8721065521240234\n",
            "Saving model, epoch: 19294, train_loss: 0.8987103700637817, val_loss: 0.8721063137054443\n",
            "Saving model, epoch: 19295, train_loss: 0.8987091779708862, val_loss: 0.8721045255661011\n",
            "Saving model, epoch: 19296, train_loss: 0.8987077474594116, val_loss: 0.8721044659614563\n",
            "Saving model, epoch: 19297, train_loss: 0.8987064361572266, val_loss: 0.8721029162406921\n",
            "Saving model, epoch: 19298, train_loss: 0.8987051248550415, val_loss: 0.8721019625663757\n",
            "Saving model, epoch: 19299, train_loss: 0.8987038135528564, val_loss: 0.872101366519928\n",
            "Saving model, epoch: 19300, train_loss: 0.8987025618553162, val_loss: 0.8721000552177429\n",
            "epoch: 19301, train_loss: 0.8987011313438416, val_loss: 0.8720994591712952\n",
            "Saving model, epoch: 19301, train_loss: 0.8987011313438416, val_loss: 0.8720994591712952\n",
            "Saving model, epoch: 19302, train_loss: 0.898699939250946, val_loss: 0.8720985651016235\n",
            "Saving model, epoch: 19303, train_loss: 0.8986985087394714, val_loss: 0.8720974922180176\n",
            "Saving model, epoch: 19304, train_loss: 0.8986973166465759, val_loss: 0.8720965385437012\n",
            "Saving model, epoch: 19305, train_loss: 0.8986958861351013, val_loss: 0.8720957040786743\n",
            "Saving model, epoch: 19306, train_loss: 0.8986946940422058, val_loss: 0.8720946311950684\n",
            "Saving model, epoch: 19307, train_loss: 0.8986932635307312, val_loss: 0.8720936179161072\n",
            "Saving model, epoch: 19308, train_loss: 0.8986918926239014, val_loss: 0.8720929026603699\n",
            "Saving model, epoch: 19309, train_loss: 0.8986907005310059, val_loss: 0.8720917701721191\n",
            "Saving model, epoch: 19310, train_loss: 0.8986892700195312, val_loss: 0.8720912933349609\n",
            "Saving model, epoch: 19311, train_loss: 0.8986880779266357, val_loss: 0.8720901608467102\n",
            "Saving model, epoch: 19312, train_loss: 0.8986868858337402, val_loss: 0.8720890283584595\n",
            "Saving model, epoch: 19313, train_loss: 0.8986853361129761, val_loss: 0.8720884919166565\n",
            "Saving model, epoch: 19314, train_loss: 0.8986841440200806, val_loss: 0.8720871210098267\n",
            "Saving model, epoch: 19315, train_loss: 0.8986828327178955, val_loss: 0.8720864653587341\n",
            "Saving model, epoch: 19316, train_loss: 0.8986814618110657, val_loss: 0.8720857501029968\n",
            "Saving model, epoch: 19317, train_loss: 0.8986802101135254, val_loss: 0.8720844388008118\n",
            "Saving model, epoch: 19318, train_loss: 0.8986788392066956, val_loss: 0.8720837235450745\n",
            "Saving model, epoch: 19319, train_loss: 0.8986775279045105, val_loss: 0.8720825910568237\n",
            "Saving model, epoch: 19320, train_loss: 0.8986762166023254, val_loss: 0.8720816969871521\n",
            "Saving model, epoch: 19321, train_loss: 0.8986749053001404, val_loss: 0.8720808029174805\n",
            "Saving model, epoch: 19322, train_loss: 0.8986735939979553, val_loss: 0.8720798492431641\n",
            "Saving model, epoch: 19323, train_loss: 0.8986724019050598, val_loss: 0.8720791339874268\n",
            "Saving model, epoch: 19324, train_loss: 0.8986709713935852, val_loss: 0.8720781803131104\n",
            "Saving model, epoch: 19325, train_loss: 0.8986696004867554, val_loss: 0.8720772862434387\n",
            "Saving model, epoch: 19326, train_loss: 0.8986683487892151, val_loss: 0.8720763921737671\n",
            "Saving model, epoch: 19327, train_loss: 0.8986671566963196, val_loss: 0.8720753788948059\n",
            "Saving model, epoch: 19328, train_loss: 0.8986657857894897, val_loss: 0.8720743656158447\n",
            "Saving model, epoch: 19329, train_loss: 0.8986643552780151, val_loss: 0.8720733523368835\n",
            "Saving model, epoch: 19330, train_loss: 0.8986631631851196, val_loss: 0.8720727562904358\n",
            "Saving model, epoch: 19331, train_loss: 0.898661732673645, val_loss: 0.8720717430114746\n",
            "Saving model, epoch: 19332, train_loss: 0.8986605405807495, val_loss: 0.8720707297325134\n",
            "Saving model, epoch: 19333, train_loss: 0.8986591100692749, val_loss: 0.8720697164535522\n",
            "Saving model, epoch: 19334, train_loss: 0.8986579179763794, val_loss: 0.8720689415931702\n",
            "Saving model, epoch: 19335, train_loss: 0.8986565470695496, val_loss: 0.8720679879188538\n",
            "Saving model, epoch: 19336, train_loss: 0.8986552357673645, val_loss: 0.8720670342445374\n",
            "Saving model, epoch: 19337, train_loss: 0.8986539244651794, val_loss: 0.8720661997795105\n",
            "Saving model, epoch: 19338, train_loss: 0.8986527323722839, val_loss: 0.8720653057098389\n",
            "Saving model, epoch: 19339, train_loss: 0.8986513018608093, val_loss: 0.8720642924308777\n",
            "Saving model, epoch: 19340, train_loss: 0.8986499905586243, val_loss: 0.872063398361206\n",
            "Saving model, epoch: 19341, train_loss: 0.8986486792564392, val_loss: 0.8720625042915344\n",
            "Saving model, epoch: 19342, train_loss: 0.8986473679542542, val_loss: 0.8720613121986389\n",
            "Saving model, epoch: 19343, train_loss: 0.8986460566520691, val_loss: 0.8720607757568359\n",
            "Saving model, epoch: 19344, train_loss: 0.898644745349884, val_loss: 0.8720596432685852\n",
            "Saving model, epoch: 19345, train_loss: 0.898643434047699, val_loss: 0.8720585703849792\n",
            "Saving model, epoch: 19346, train_loss: 0.8986421823501587, val_loss: 0.872058093547821\n",
            "Saving model, epoch: 19347, train_loss: 0.8986408710479736, val_loss: 0.8720565438270569\n",
            "Saving model, epoch: 19348, train_loss: 0.898639440536499, val_loss: 0.8720561265945435\n",
            "Saving model, epoch: 19349, train_loss: 0.8986382484436035, val_loss: 0.8720552921295166\n",
            "Saving model, epoch: 19350, train_loss: 0.8986369371414185, val_loss: 0.8720536828041077\n",
            "Saving model, epoch: 19351, train_loss: 0.8986356258392334, val_loss: 0.8720533847808838\n",
            "Saving model, epoch: 19352, train_loss: 0.8986344337463379, val_loss: 0.8720521926879883\n",
            "Saving model, epoch: 19353, train_loss: 0.8986330032348633, val_loss: 0.8720510601997375\n",
            "Saving model, epoch: 19354, train_loss: 0.8986316323280334, val_loss: 0.8720508813858032\n",
            "Saving model, epoch: 19355, train_loss: 0.8986303210258484, val_loss: 0.8720491528511047\n",
            "Saving model, epoch: 19356, train_loss: 0.8986290693283081, val_loss: 0.8720487356185913\n",
            "Saving model, epoch: 19357, train_loss: 0.898627758026123, val_loss: 0.8720477223396301\n",
            "Saving model, epoch: 19358, train_loss: 0.8986263871192932, val_loss: 0.8720463514328003\n",
            "Saving model, epoch: 19359, train_loss: 0.8986251950263977, val_loss: 0.8720459938049316\n",
            "Saving model, epoch: 19360, train_loss: 0.8986237645149231, val_loss: 0.8720447421073914\n",
            "Saving model, epoch: 19361, train_loss: 0.8986225724220276, val_loss: 0.8720439672470093\n",
            "Saving model, epoch: 19362, train_loss: 0.898621141910553, val_loss: 0.8720433712005615\n",
            "Saving model, epoch: 19363, train_loss: 0.8986199498176575, val_loss: 0.8720418214797974\n",
            "Saving model, epoch: 19364, train_loss: 0.8986185193061829, val_loss: 0.8720414638519287\n",
            "Saving model, epoch: 19365, train_loss: 0.8986173272132874, val_loss: 0.8720400333404541\n",
            "Saving model, epoch: 19366, train_loss: 0.8986160159111023, val_loss: 0.8720394372940063\n",
            "Saving model, epoch: 19367, train_loss: 0.8986146450042725, val_loss: 0.872038722038269\n",
            "Saving model, epoch: 19368, train_loss: 0.8986133337020874, val_loss: 0.8720373511314392\n",
            "Saving model, epoch: 19369, train_loss: 0.8986120223999023, val_loss: 0.8720367550849915\n",
            "Saving model, epoch: 19370, train_loss: 0.8986108303070068, val_loss: 0.8720356822013855\n",
            "Saving model, epoch: 19371, train_loss: 0.8986095190048218, val_loss: 0.8720346093177795\n",
            "Saving model, epoch: 19372, train_loss: 0.8986082077026367, val_loss: 0.8720340132713318\n",
            "Saving model, epoch: 19373, train_loss: 0.8986068964004517, val_loss: 0.8720327615737915\n",
            "Saving model, epoch: 19374, train_loss: 0.898605465888977, val_loss: 0.8720320463180542\n",
            "Saving model, epoch: 19375, train_loss: 0.898604154586792, val_loss: 0.8720310926437378\n",
            "Saving model, epoch: 19376, train_loss: 0.8986028432846069, val_loss: 0.8720300793647766\n",
            "Saving model, epoch: 19377, train_loss: 0.8986016511917114, val_loss: 0.8720294237136841\n",
            "Saving model, epoch: 19378, train_loss: 0.8986003398895264, val_loss: 0.8720284700393677\n",
            "Saving model, epoch: 19379, train_loss: 0.8985990285873413, val_loss: 0.8720274567604065\n",
            "Saving model, epoch: 19380, train_loss: 0.8985976576805115, val_loss: 0.8720265626907349\n",
            "Saving model, epoch: 19381, train_loss: 0.898596465587616, val_loss: 0.8720253705978394\n",
            "Saving model, epoch: 19382, train_loss: 0.8985950350761414, val_loss: 0.8720247745513916\n",
            "Saving model, epoch: 19383, train_loss: 0.8985938429832458, val_loss: 0.8720235228538513\n",
            "Saving model, epoch: 19384, train_loss: 0.8985924124717712, val_loss: 0.8720228672027588\n",
            "Saving model, epoch: 19385, train_loss: 0.8985912203788757, val_loss: 0.8720219135284424\n",
            "Saving model, epoch: 19386, train_loss: 0.8985897898674011, val_loss: 0.8720210790634155\n",
            "Saving model, epoch: 19387, train_loss: 0.8985885977745056, val_loss: 0.8720201849937439\n",
            "Saving model, epoch: 19388, train_loss: 0.8985872268676758, val_loss: 0.8720190525054932\n",
            "Saving model, epoch: 19389, train_loss: 0.8985859751701355, val_loss: 0.8720182776451111\n",
            "Saving model, epoch: 19390, train_loss: 0.8985846042633057, val_loss: 0.8720173835754395\n",
            "Saving model, epoch: 19391, train_loss: 0.8985832929611206, val_loss: 0.8720162510871887\n",
            "Saving model, epoch: 19392, train_loss: 0.8985819816589355, val_loss: 0.8720158934593201\n",
            "Saving model, epoch: 19393, train_loss: 0.89858078956604, val_loss: 0.8720144033432007\n",
            "Saving model, epoch: 19394, train_loss: 0.8985793590545654, val_loss: 0.8720135688781738\n",
            "Saving model, epoch: 19395, train_loss: 0.8985780477523804, val_loss: 0.8720128536224365\n",
            "Saving model, epoch: 19396, train_loss: 0.8985767364501953, val_loss: 0.8720117807388306\n",
            "Saving model, epoch: 19397, train_loss: 0.8985755443572998, val_loss: 0.8720111846923828\n",
            "Saving model, epoch: 19398, train_loss: 0.8985741138458252, val_loss: 0.8720098733901978\n",
            "Saving model, epoch: 19399, train_loss: 0.8985728025436401, val_loss: 0.8720092177391052\n",
            "Saving model, epoch: 19400, train_loss: 0.8985715508460999, val_loss: 0.8720085620880127\n",
            "epoch: 19401, train_loss: 0.8985702991485596, val_loss: 0.8720073699951172\n",
            "Saving model, epoch: 19401, train_loss: 0.8985702991485596, val_loss: 0.8720073699951172\n",
            "Saving model, epoch: 19402, train_loss: 0.8985689282417297, val_loss: 0.8720065355300903\n",
            "Saving model, epoch: 19403, train_loss: 0.8985677361488342, val_loss: 0.8720055222511292\n",
            "Saving model, epoch: 19404, train_loss: 0.8985664248466492, val_loss: 0.8720045685768127\n",
            "Saving model, epoch: 19405, train_loss: 0.8985649943351746, val_loss: 0.8720038533210754\n",
            "Saving model, epoch: 19406, train_loss: 0.898563802242279, val_loss: 0.8720026016235352\n",
            "Saving model, epoch: 19407, train_loss: 0.898562490940094, val_loss: 0.8720018863677979\n",
            "Saving model, epoch: 19408, train_loss: 0.8985612988471985, val_loss: 0.872001051902771\n",
            "Saving model, epoch: 19409, train_loss: 0.8985598683357239, val_loss: 0.8719997406005859\n",
            "Saving model, epoch: 19410, train_loss: 0.8985585570335388, val_loss: 0.871999204158783\n",
            "Saving model, epoch: 19411, train_loss: 0.8985572457313538, val_loss: 0.8719982504844666\n",
            "Saving model, epoch: 19412, train_loss: 0.8985559344291687, val_loss: 0.871997058391571\n",
            "Saving model, epoch: 19413, train_loss: 0.8985546231269836, val_loss: 0.8719964623451233\n",
            "Saving model, epoch: 19414, train_loss: 0.8985534310340881, val_loss: 0.8719953298568726\n",
            "Saving model, epoch: 19415, train_loss: 0.8985520601272583, val_loss: 0.8719944357872009\n",
            "Saving model, epoch: 19416, train_loss: 0.8985507488250732, val_loss: 0.8719936609268188\n",
            "Saving model, epoch: 19417, train_loss: 0.898549497127533, val_loss: 0.8719926476478577\n",
            "Saving model, epoch: 19418, train_loss: 0.8985481858253479, val_loss: 0.871991753578186\n",
            "Saving model, epoch: 19419, train_loss: 0.8985469341278076, val_loss: 0.8719907402992249\n",
            "Saving model, epoch: 19420, train_loss: 0.8985456228256226, val_loss: 0.871989905834198\n",
            "Saving model, epoch: 19421, train_loss: 0.898544192314148, val_loss: 0.8719891309738159\n",
            "Saving model, epoch: 19422, train_loss: 0.8985430002212524, val_loss: 0.87198805809021\n",
            "Saving model, epoch: 19423, train_loss: 0.8985416889190674, val_loss: 0.8719874620437622\n",
            "Saving model, epoch: 19424, train_loss: 0.8985403776168823, val_loss: 0.8719860911369324\n",
            "Saving model, epoch: 19425, train_loss: 0.8985390663146973, val_loss: 0.8719856142997742\n",
            "Saving model, epoch: 19426, train_loss: 0.8985377550125122, val_loss: 0.8719845414161682\n",
            "Saving model, epoch: 19427, train_loss: 0.8985365629196167, val_loss: 0.871983528137207\n",
            "Saving model, epoch: 19428, train_loss: 0.8985352516174316, val_loss: 0.8719826340675354\n",
            "Saving model, epoch: 19429, train_loss: 0.8985339403152466, val_loss: 0.8719816207885742\n",
            "Saving model, epoch: 19430, train_loss: 0.8985325694084167, val_loss: 0.871981143951416\n",
            "Saving model, epoch: 19431, train_loss: 0.8985313177108765, val_loss: 0.8719796538352966\n",
            "Saving model, epoch: 19433, train_loss: 0.8985286951065063, val_loss: 0.871977686882019\n",
            "Saving model, epoch: 19434, train_loss: 0.8985274434089661, val_loss: 0.8719775080680847\n",
            "Saving model, epoch: 19435, train_loss: 0.898526132106781, val_loss: 0.8719760179519653\n",
            "Saving model, epoch: 19436, train_loss: 0.898524820804596, val_loss: 0.8719755411148071\n",
            "Saving model, epoch: 19437, train_loss: 0.8985235095024109, val_loss: 0.8719745874404907\n",
            "Saving model, epoch: 19438, train_loss: 0.8985221982002258, val_loss: 0.8719733357429504\n",
            "Saving model, epoch: 19439, train_loss: 0.8985210061073303, val_loss: 0.871972918510437\n",
            "Saving model, epoch: 19440, train_loss: 0.8985196948051453, val_loss: 0.871971607208252\n",
            "Saving model, epoch: 19441, train_loss: 0.8985182642936707, val_loss: 0.8719708919525146\n",
            "Saving model, epoch: 19442, train_loss: 0.8985169529914856, val_loss: 0.8719701766967773\n",
            "Saving model, epoch: 19443, train_loss: 0.8985157608985901, val_loss: 0.8719688057899475\n",
            "Saving model, epoch: 19444, train_loss: 0.898514449596405, val_loss: 0.8719683885574341\n",
            "Saving model, epoch: 19445, train_loss: 0.89851313829422, val_loss: 0.871967077255249\n",
            "Saving model, epoch: 19446, train_loss: 0.8985118269920349, val_loss: 0.8719663619995117\n",
            "Saving model, epoch: 19447, train_loss: 0.8985106348991394, val_loss: 0.8719655275344849\n",
            "Saving model, epoch: 19448, train_loss: 0.8985094428062439, val_loss: 0.8719643354415894\n",
            "Saving model, epoch: 19449, train_loss: 0.8985079526901245, val_loss: 0.8719637989997864\n",
            "Saving model, epoch: 19450, train_loss: 0.8985066413879395, val_loss: 0.8719626069068909\n",
            "Saving model, epoch: 19451, train_loss: 0.8985053896903992, val_loss: 0.8719615936279297\n",
            "Saving model, epoch: 19452, train_loss: 0.8985040187835693, val_loss: 0.8719611167907715\n",
            "Saving model, epoch: 19453, train_loss: 0.8985028266906738, val_loss: 0.8719600439071655\n",
            "Saving model, epoch: 19454, train_loss: 0.8985015153884888, val_loss: 0.8719592094421387\n",
            "Saving model, epoch: 19455, train_loss: 0.8985002040863037, val_loss: 0.8719581961631775\n",
            "Saving model, epoch: 19456, train_loss: 0.8984988927841187, val_loss: 0.8719572424888611\n",
            "Saving model, epoch: 19457, train_loss: 0.8984975814819336, val_loss: 0.871956467628479\n",
            "Saving model, epoch: 19458, train_loss: 0.8984962701797485, val_loss: 0.8719553351402283\n",
            "Saving model, epoch: 19459, train_loss: 0.8984949588775635, val_loss: 0.8719546794891357\n",
            "Saving model, epoch: 19460, train_loss: 0.898493766784668, val_loss: 0.8719537854194641\n",
            "Saving model, epoch: 19461, train_loss: 0.8984923362731934, val_loss: 0.8719524145126343\n",
            "Saving model, epoch: 19462, train_loss: 0.8984911441802979, val_loss: 0.8719520568847656\n",
            "Saving model, epoch: 19463, train_loss: 0.8984897136688232, val_loss: 0.8719507455825806\n",
            "Saving model, epoch: 19464, train_loss: 0.8984885215759277, val_loss: 0.8719500303268433\n",
            "Saving model, epoch: 19465, train_loss: 0.8984873294830322, val_loss: 0.8719491362571716\n",
            "Saving model, epoch: 19466, train_loss: 0.8984858989715576, val_loss: 0.8719481229782104\n",
            "Saving model, epoch: 19467, train_loss: 0.8984847068786621, val_loss: 0.8719474673271179\n",
            "Saving model, epoch: 19468, train_loss: 0.8984833359718323, val_loss: 0.8719465136528015\n",
            "Saving model, epoch: 19469, train_loss: 0.898482084274292, val_loss: 0.8719456195831299\n",
            "Saving model, epoch: 19470, train_loss: 0.8984807729721069, val_loss: 0.8719447255134583\n",
            "Saving model, epoch: 19471, train_loss: 0.8984795212745667, val_loss: 0.8719437122344971\n",
            "Saving model, epoch: 19472, train_loss: 0.898478090763092, val_loss: 0.8719429969787598\n",
            "Saving model, epoch: 19473, train_loss: 0.8984768986701965, val_loss: 0.8719418048858643\n",
            "Saving model, epoch: 19474, train_loss: 0.8984755873680115, val_loss: 0.8719409704208374\n",
            "Saving model, epoch: 19475, train_loss: 0.8984742760658264, val_loss: 0.8719401955604553\n",
            "Saving model, epoch: 19476, train_loss: 0.8984730839729309, val_loss: 0.8719391226768494\n",
            "Saving model, epoch: 19477, train_loss: 0.8984716534614563, val_loss: 0.8719383478164673\n",
            "Saving model, epoch: 19478, train_loss: 0.8984704613685608, val_loss: 0.8719372749328613\n",
            "Saving model, epoch: 19479, train_loss: 0.8984692692756653, val_loss: 0.8719365000724792\n",
            "Saving model, epoch: 19480, train_loss: 0.8984679579734802, val_loss: 0.8719356656074524\n",
            "Saving model, epoch: 19481, train_loss: 0.8984665274620056, val_loss: 0.8719346523284912\n",
            "Saving model, epoch: 19482, train_loss: 0.8984652161598206, val_loss: 0.8719339370727539\n",
            "Saving model, epoch: 19483, train_loss: 0.898464024066925, val_loss: 0.8719326853752136\n",
            "Saving model, epoch: 19484, train_loss: 0.89846271276474, val_loss: 0.8719319701194763\n",
            "Saving model, epoch: 19485, train_loss: 0.8984614014625549, val_loss: 0.8719310760498047\n",
            "Saving model, epoch: 19486, train_loss: 0.8984602093696594, val_loss: 0.8719301223754883\n",
            "Saving model, epoch: 19487, train_loss: 0.8984587788581848, val_loss: 0.8719292879104614\n",
            "Saving model, epoch: 19488, train_loss: 0.8984575867652893, val_loss: 0.8719280958175659\n",
            "Saving model, epoch: 19489, train_loss: 0.8984562754631042, val_loss: 0.8719274401664734\n",
            "Saving model, epoch: 19490, train_loss: 0.8984549641609192, val_loss: 0.8719264268875122\n",
            "Saving model, epoch: 19491, train_loss: 0.8984535932540894, val_loss: 0.8719255924224854\n",
            "Saving model, epoch: 19492, train_loss: 0.8984523415565491, val_loss: 0.871924877166748\n",
            "Saving model, epoch: 19493, train_loss: 0.8984511494636536, val_loss: 0.8719238042831421\n",
            "Saving model, epoch: 19494, train_loss: 0.8984497785568237, val_loss: 0.8719231486320496\n",
            "Saving model, epoch: 19495, train_loss: 0.8984483480453491, val_loss: 0.8719222545623779\n",
            "Saving model, epoch: 19496, train_loss: 0.8984471559524536, val_loss: 0.8719211220741272\n",
            "Saving model, epoch: 19497, train_loss: 0.8984459638595581, val_loss: 0.8719204068183899\n",
            "Saving model, epoch: 19498, train_loss: 0.898444652557373, val_loss: 0.8719195127487183\n",
            "Saving model, epoch: 19499, train_loss: 0.898443341255188, val_loss: 0.8719187378883362\n",
            "Saving model, epoch: 19500, train_loss: 0.8984420299530029, val_loss: 0.8719173669815063\n",
            "epoch: 19501, train_loss: 0.8984407186508179, val_loss: 0.8719168901443481\n",
            "Saving model, epoch: 19501, train_loss: 0.8984407186508179, val_loss: 0.8719168901443481\n",
            "Saving model, epoch: 19502, train_loss: 0.8984395265579224, val_loss: 0.8719156980514526\n",
            "Saving model, epoch: 19503, train_loss: 0.8984382152557373, val_loss: 0.8719149231910706\n",
            "Saving model, epoch: 19504, train_loss: 0.8984369039535522, val_loss: 0.871913731098175\n",
            "Saving model, epoch: 19505, train_loss: 0.8984357118606567, val_loss: 0.871912956237793\n",
            "Saving model, epoch: 19506, train_loss: 0.8984342813491821, val_loss: 0.87191241979599\n",
            "Saving model, epoch: 19507, train_loss: 0.8984329700469971, val_loss: 0.8719111084938049\n",
            "Saving model, epoch: 19508, train_loss: 0.8984317779541016, val_loss: 0.871910572052002\n",
            "Saving model, epoch: 19509, train_loss: 0.8984304666519165, val_loss: 0.8719092607498169\n",
            "Saving model, epoch: 19510, train_loss: 0.898429274559021, val_loss: 0.8719086647033691\n",
            "Saving model, epoch: 19511, train_loss: 0.8984278440475464, val_loss: 0.871907651424408\n",
            "Saving model, epoch: 19512, train_loss: 0.8984266519546509, val_loss: 0.8719067573547363\n",
            "Saving model, epoch: 19513, train_loss: 0.8984252214431763, val_loss: 0.8719059824943542\n",
            "Saving model, epoch: 19514, train_loss: 0.8984240293502808, val_loss: 0.871904730796814\n",
            "Saving model, epoch: 19515, train_loss: 0.8984228372573853, val_loss: 0.8719040751457214\n",
            "Saving model, epoch: 19516, train_loss: 0.8984214067459106, val_loss: 0.8719028830528259\n",
            "Saving model, epoch: 19517, train_loss: 0.8984202146530151, val_loss: 0.8719025254249573\n",
            "Saving model, epoch: 19518, train_loss: 0.8984187841415405, val_loss: 0.8719013333320618\n",
            "Saving model, epoch: 19519, train_loss: 0.898417592048645, val_loss: 0.8719005584716797\n",
            "Saving model, epoch: 19520, train_loss: 0.89841628074646, val_loss: 0.871899425983429\n",
            "Saving model, epoch: 19521, train_loss: 0.8984149694442749, val_loss: 0.8718987107276917\n",
            "Saving model, epoch: 19522, train_loss: 0.8984137773513794, val_loss: 0.8718979358673096\n",
            "Saving model, epoch: 19523, train_loss: 0.8984124064445496, val_loss: 0.8718968629837036\n",
            "Saving model, epoch: 19524, train_loss: 0.8984111547470093, val_loss: 0.8718959093093872\n",
            "Saving model, epoch: 19525, train_loss: 0.8984099626541138, val_loss: 0.8718950748443604\n",
            "Saving model, epoch: 19526, train_loss: 0.8984085917472839, val_loss: 0.8718942403793335\n",
            "Saving model, epoch: 19527, train_loss: 0.8984073400497437, val_loss: 0.8718933463096619\n",
            "Saving model, epoch: 19528, train_loss: 0.8984059691429138, val_loss: 0.8718923926353455\n",
            "Saving model, epoch: 19529, train_loss: 0.8984047174453735, val_loss: 0.871891438961029\n",
            "Saving model, epoch: 19530, train_loss: 0.8984033465385437, val_loss: 0.871890664100647\n",
            "Saving model, epoch: 19531, train_loss: 0.8984021544456482, val_loss: 0.8718896508216858\n",
            "Saving model, epoch: 19532, train_loss: 0.8984009027481079, val_loss: 0.8718888759613037\n",
            "Saving model, epoch: 19533, train_loss: 0.8983995318412781, val_loss: 0.871887743473053\n",
            "Saving model, epoch: 19534, train_loss: 0.8983983397483826, val_loss: 0.8718872666358948\n",
            "Saving model, epoch: 19535, train_loss: 0.8983970880508423, val_loss: 0.871886134147644\n",
            "Saving model, epoch: 19536, train_loss: 0.8983957171440125, val_loss: 0.8718854188919067\n",
            "Saving model, epoch: 19537, train_loss: 0.8983945250511169, val_loss: 0.8718843460083008\n",
            "Saving model, epoch: 19538, train_loss: 0.8983932137489319, val_loss: 0.8718834519386292\n",
            "Saving model, epoch: 19539, train_loss: 0.8983919024467468, val_loss: 0.8718823790550232\n",
            "Saving model, epoch: 19540, train_loss: 0.8983905911445618, val_loss: 0.8718814253807068\n",
            "Saving model, epoch: 19541, train_loss: 0.8983892798423767, val_loss: 0.8718809485435486\n",
            "Saving model, epoch: 19542, train_loss: 0.8983880877494812, val_loss: 0.8718798160552979\n",
            "Saving model, epoch: 19543, train_loss: 0.8983867764472961, val_loss: 0.8718790411949158\n",
            "Saving model, epoch: 19544, train_loss: 0.8983854651451111, val_loss: 0.8718781471252441\n",
            "Saving model, epoch: 19545, train_loss: 0.898384153842926, val_loss: 0.871877133846283\n",
            "Saving model, epoch: 19546, train_loss: 0.898382842540741, val_loss: 0.8718761801719666\n",
            "Saving model, epoch: 19547, train_loss: 0.8983816504478455, val_loss: 0.8718752861022949\n",
            "Saving model, epoch: 19548, train_loss: 0.8983802199363708, val_loss: 0.8718744516372681\n",
            "Saving model, epoch: 19549, train_loss: 0.8983790278434753, val_loss: 0.8718735575675964\n",
            "Saving model, epoch: 19550, train_loss: 0.8983777165412903, val_loss: 0.8718725442886353\n",
            "Saving model, epoch: 19551, train_loss: 0.8983765244483948, val_loss: 0.871872067451477\n",
            "Saving model, epoch: 19552, train_loss: 0.8983752131462097, val_loss: 0.8718705773353577\n",
            "Saving model, epoch: 19553, train_loss: 0.8983739018440247, val_loss: 0.8718699216842651\n",
            "Saving model, epoch: 19554, train_loss: 0.8983725905418396, val_loss: 0.8718690276145935\n",
            "Saving model, epoch: 19555, train_loss: 0.8983713984489441, val_loss: 0.8718680739402771\n",
            "Saving model, epoch: 19556, train_loss: 0.898370087146759, val_loss: 0.871867299079895\n",
            "Saving model, epoch: 19557, train_loss: 0.898368775844574, val_loss: 0.8718662858009338\n",
            "Saving model, epoch: 19558, train_loss: 0.8983675837516785, val_loss: 0.8718656301498413\n",
            "Saving model, epoch: 19559, train_loss: 0.8983661532402039, val_loss: 0.8718645572662354\n",
            "Saving model, epoch: 19560, train_loss: 0.8983649611473083, val_loss: 0.8718639016151428\n",
            "Saving model, epoch: 19561, train_loss: 0.8983637690544128, val_loss: 0.8718627691268921\n",
            "Saving model, epoch: 19562, train_loss: 0.8983623385429382, val_loss: 0.8718621134757996\n",
            "Saving model, epoch: 19563, train_loss: 0.8983610272407532, val_loss: 0.8718612194061279\n",
            "Saving model, epoch: 19564, train_loss: 0.8983598351478577, val_loss: 0.8718600273132324\n",
            "Saving model, epoch: 19565, train_loss: 0.8983585238456726, val_loss: 0.8718593716621399\n",
            "Saving model, epoch: 19566, train_loss: 0.8983573317527771, val_loss: 0.8718585968017578\n",
            "Saving model, epoch: 19567, train_loss: 0.8983559012413025, val_loss: 0.8718574047088623\n",
            "Saving model, epoch: 19568, train_loss: 0.898354709148407, val_loss: 0.8718565702438354\n",
            "Saving model, epoch: 19569, train_loss: 0.8983535170555115, val_loss: 0.8718557953834534\n",
            "Saving model, epoch: 19570, train_loss: 0.8983520865440369, val_loss: 0.8718545436859131\n",
            "Saving model, epoch: 19571, train_loss: 0.8983508944511414, val_loss: 0.8718541860580444\n",
            "Saving model, epoch: 19572, train_loss: 0.8983497023582458, val_loss: 0.871852457523346\n",
            "Saving model, epoch: 19574, train_loss: 0.8983470797538757, val_loss: 0.8718502521514893\n",
            "Saving model, epoch: 19576, train_loss: 0.8983444571495056, val_loss: 0.8718487024307251\n",
            "Saving model, epoch: 19578, train_loss: 0.8983420729637146, val_loss: 0.8718471527099609\n",
            "Saving model, epoch: 19580, train_loss: 0.8983394503593445, val_loss: 0.8718459606170654\n",
            "Saving model, epoch: 19581, train_loss: 0.898338258266449, val_loss: 0.871845006942749\n",
            "Saving model, epoch: 19582, train_loss: 0.8983368277549744, val_loss: 0.8718443512916565\n",
            "Saving model, epoch: 19583, train_loss: 0.8983356356620789, val_loss: 0.8718428015708923\n",
            "Saving model, epoch: 19585, train_loss: 0.8983330130577087, val_loss: 0.8718409538269043\n",
            "Saving model, epoch: 19587, train_loss: 0.8983303904533386, val_loss: 0.8718395233154297\n",
            "Saving model, epoch: 19588, train_loss: 0.8983291983604431, val_loss: 0.8718389272689819\n",
            "Saving model, epoch: 19589, train_loss: 0.8983277678489685, val_loss: 0.8718382120132446\n",
            "Saving model, epoch: 19590, train_loss: 0.898326575756073, val_loss: 0.87183678150177\n",
            "Saving model, epoch: 19591, train_loss: 0.8983253836631775, val_loss: 0.8718366622924805\n",
            "Saving model, epoch: 19592, train_loss: 0.898324191570282, val_loss: 0.8718348741531372\n",
            "Saving model, epoch: 19593, train_loss: 0.8983226418495178, val_loss: 0.8718346953392029\n",
            "Saving model, epoch: 19594, train_loss: 0.8983215689659119, val_loss: 0.8718335032463074\n",
            "Saving model, epoch: 19595, train_loss: 0.8983201384544373, val_loss: 0.8718324303627014\n",
            "Saving model, epoch: 19596, train_loss: 0.8983189463615417, val_loss: 0.871832013130188\n",
            "Saving model, epoch: 19597, train_loss: 0.8983175158500671, val_loss: 0.8718303442001343\n",
            "Saving model, epoch: 19599, train_loss: 0.8983151316642761, val_loss: 0.8718284368515015\n",
            "Saving model, epoch: 19600, train_loss: 0.8983138203620911, val_loss: 0.8718283176422119\n",
            "epoch: 19601, train_loss: 0.898312509059906, val_loss: 0.8718271255493164\n",
            "Saving model, epoch: 19601, train_loss: 0.898312509059906, val_loss: 0.8718271255493164\n",
            "Saving model, epoch: 19602, train_loss: 0.8983113169670105, val_loss: 0.8718261122703552\n",
            "Saving model, epoch: 19603, train_loss: 0.8983100056648254, val_loss: 0.8718254566192627\n",
            "Saving model, epoch: 19604, train_loss: 0.8983086943626404, val_loss: 0.8718242645263672\n",
            "Saving model, epoch: 19605, train_loss: 0.8983075022697449, val_loss: 0.8718239068984985\n",
            "Saving model, epoch: 19606, train_loss: 0.8983061909675598, val_loss: 0.871822714805603\n",
            "Saving model, epoch: 19607, train_loss: 0.8983048796653748, val_loss: 0.8718219995498657\n",
            "Saving model, epoch: 19608, train_loss: 0.8983036875724792, val_loss: 0.8718211054801941\n",
            "Saving model, epoch: 19609, train_loss: 0.8983023762702942, val_loss: 0.8718198537826538\n",
            "Saving model, epoch: 19610, train_loss: 0.8983010649681091, val_loss: 0.8718194961547852\n",
            "Saving model, epoch: 19611, train_loss: 0.8982998728752136, val_loss: 0.8718181252479553\n",
            "Saving model, epoch: 19612, train_loss: 0.8982985615730286, val_loss: 0.8718177080154419\n",
            "Saving model, epoch: 19613, train_loss: 0.8982972502708435, val_loss: 0.8718164563179016\n",
            "Saving model, epoch: 19614, train_loss: 0.8982959389686584, val_loss: 0.871816098690033\n",
            "Saving model, epoch: 19615, train_loss: 0.8982946276664734, val_loss: 0.8718147277832031\n",
            "Saving model, epoch: 19616, train_loss: 0.8982934355735779, val_loss: 0.871813952922821\n",
            "Saving model, epoch: 19617, train_loss: 0.8982922434806824, val_loss: 0.8718130588531494\n",
            "Saving model, epoch: 19618, train_loss: 0.8982909321784973, val_loss: 0.8718118667602539\n",
            "Saving model, epoch: 19619, train_loss: 0.8982896208763123, val_loss: 0.87181156873703\n",
            "Saving model, epoch: 19620, train_loss: 0.8982884287834167, val_loss: 0.8718098998069763\n",
            "Saving model, epoch: 19622, train_loss: 0.8982858061790466, val_loss: 0.8718084096908569\n",
            "Saving model, epoch: 19623, train_loss: 0.8982845544815063, val_loss: 0.8718078136444092\n",
            "Saving model, epoch: 19624, train_loss: 0.8982831835746765, val_loss: 0.871806800365448\n",
            "Saving model, epoch: 19625, train_loss: 0.898281991481781, val_loss: 0.871805727481842\n",
            "Saving model, epoch: 19626, train_loss: 0.8982807397842407, val_loss: 0.8718053102493286\n",
            "Saving model, epoch: 19627, train_loss: 0.8982793688774109, val_loss: 0.8718037605285645\n",
            "Saving model, epoch: 19628, train_loss: 0.8982781767845154, val_loss: 0.8718037009239197\n",
            "Saving model, epoch: 19629, train_loss: 0.8982769250869751, val_loss: 0.8718020915985107\n",
            "Saving model, epoch: 19630, train_loss: 0.8982755541801453, val_loss: 0.8718018531799316\n",
            "Saving model, epoch: 19631, train_loss: 0.8982743620872498, val_loss: 0.8718004822731018\n",
            "Saving model, epoch: 19632, train_loss: 0.8982731103897095, val_loss: 0.8718000650405884\n",
            "Saving model, epoch: 19633, train_loss: 0.8982717990875244, val_loss: 0.8717989325523376\n",
            "Saving model, epoch: 19634, train_loss: 0.8982705473899841, val_loss: 0.8717978000640869\n",
            "Saving model, epoch: 19635, train_loss: 0.8982692956924438, val_loss: 0.8717973828315735\n",
            "Saving model, epoch: 19636, train_loss: 0.8982679843902588, val_loss: 0.8717961311340332\n",
            "Saving model, epoch: 19637, train_loss: 0.8982666730880737, val_loss: 0.871795654296875\n",
            "Saving model, epoch: 19638, train_loss: 0.8982654809951782, val_loss: 0.8717939257621765\n",
            "Saving model, epoch: 19640, train_loss: 0.8982628583908081, val_loss: 0.8717922568321228\n",
            "Saving model, epoch: 19641, train_loss: 0.8982616662979126, val_loss: 0.8717920184135437\n",
            "Saving model, epoch: 19642, train_loss: 0.8982604742050171, val_loss: 0.8717910051345825\n",
            "Saving model, epoch: 19643, train_loss: 0.898259162902832, val_loss: 0.8717901110649109\n",
            "Saving model, epoch: 19644, train_loss: 0.898257851600647, val_loss: 0.8717892169952393\n",
            "Saving model, epoch: 19645, train_loss: 0.8982566595077515, val_loss: 0.8717880249023438\n",
            "Saving model, epoch: 19646, train_loss: 0.8982553482055664, val_loss: 0.8717877268791199\n",
            "Saving model, epoch: 19647, train_loss: 0.8982541561126709, val_loss: 0.87178635597229\n",
            "Saving model, epoch: 19648, train_loss: 0.8982528448104858, val_loss: 0.8717858791351318\n",
            "Saving model, epoch: 19649, train_loss: 0.8982515335083008, val_loss: 0.8717845678329468\n",
            "Saving model, epoch: 19650, train_loss: 0.8982502222061157, val_loss: 0.8717838525772095\n",
            "Saving model, epoch: 19651, train_loss: 0.8982490301132202, val_loss: 0.8717829585075378\n",
            "Saving model, epoch: 19652, train_loss: 0.8982477188110352, val_loss: 0.8717818856239319\n",
            "Saving model, epoch: 19653, train_loss: 0.8982464075088501, val_loss: 0.8717814087867737\n",
            "Saving model, epoch: 19654, train_loss: 0.8982452154159546, val_loss: 0.8717801570892334\n",
            "Saving model, epoch: 19655, train_loss: 0.8982440233230591, val_loss: 0.8717796206474304\n",
            "Saving model, epoch: 19656, train_loss: 0.8982425928115845, val_loss: 0.8717787265777588\n",
            "Saving model, epoch: 19657, train_loss: 0.898241400718689, val_loss: 0.8717775344848633\n",
            "Saving model, epoch: 19658, train_loss: 0.8982400894165039, val_loss: 0.8717767596244812\n",
            "Saving model, epoch: 19659, train_loss: 0.8982388973236084, val_loss: 0.8717758655548096\n",
            "Saving model, epoch: 19660, train_loss: 0.8982375860214233, val_loss: 0.8717755079269409\n",
            "Saving model, epoch: 19661, train_loss: 0.8982363343238831, val_loss: 0.871773898601532\n",
            "Saving model, epoch: 19662, train_loss: 0.898235023021698, val_loss: 0.8717733025550842\n",
            "Saving model, epoch: 19663, train_loss: 0.8982337713241577, val_loss: 0.8717722296714783\n",
            "Saving model, epoch: 19664, train_loss: 0.8982325196266174, val_loss: 0.8717717528343201\n",
            "Saving model, epoch: 19665, train_loss: 0.8982312083244324, val_loss: 0.8717707991600037\n",
            "Saving model, epoch: 19666, train_loss: 0.8982299566268921, val_loss: 0.8717697262763977\n",
            "Saving model, epoch: 19667, train_loss: 0.8982287049293518, val_loss: 0.8717689514160156\n",
            "Saving model, epoch: 19668, train_loss: 0.8982275128364563, val_loss: 0.8717679977416992\n",
            "Saving model, epoch: 19669, train_loss: 0.8982260823249817, val_loss: 0.8717675805091858\n",
            "Saving model, epoch: 19670, train_loss: 0.8982248902320862, val_loss: 0.871765673160553\n",
            "Saving model, epoch: 19672, train_loss: 0.8982223868370056, val_loss: 0.8717632293701172\n",
            "Saving model, epoch: 19674, train_loss: 0.898219883441925, val_loss: 0.871760904788971\n",
            "Saving model, epoch: 19676, train_loss: 0.8982173800468445, val_loss: 0.871758759021759\n",
            "Saving model, epoch: 19678, train_loss: 0.8982148766517639, val_loss: 0.8717571496963501\n",
            "Saving model, epoch: 19680, train_loss: 0.8982123732566833, val_loss: 0.871755838394165\n",
            "Saving model, epoch: 19682, train_loss: 0.898209810256958, val_loss: 0.8717543482780457\n",
            "Saving model, epoch: 19684, train_loss: 0.8982072472572327, val_loss: 0.8717530965805054\n",
            "Saving model, epoch: 19686, train_loss: 0.8982048034667969, val_loss: 0.8717523217201233\n",
            "Saving model, epoch: 19687, train_loss: 0.8982034921646118, val_loss: 0.871751070022583\n",
            "Saving model, epoch: 19689, train_loss: 0.8982008695602417, val_loss: 0.8717489838600159\n",
            "Saving model, epoch: 19691, train_loss: 0.8981983661651611, val_loss: 0.8717471361160278\n",
            "Saving model, epoch: 19693, train_loss: 0.8981958627700806, val_loss: 0.8717454671859741\n",
            "Saving model, epoch: 19695, train_loss: 0.898193359375, val_loss: 0.8717437982559204\n",
            "Saving model, epoch: 19697, train_loss: 0.8981908559799194, val_loss: 0.8717425465583801\n",
            "Saving model, epoch: 19698, train_loss: 0.8981895446777344, val_loss: 0.8717418909072876\n",
            "Saving model, epoch: 19699, train_loss: 0.8981883525848389, val_loss: 0.8717411160469055\n",
            "Saving model, epoch: 19700, train_loss: 0.8981870412826538, val_loss: 0.87173992395401\n",
            "epoch: 19701, train_loss: 0.8981859087944031, val_loss: 0.8717395067214966\n",
            "Saving model, epoch: 19701, train_loss: 0.8981859087944031, val_loss: 0.8717395067214966\n",
            "Saving model, epoch: 19702, train_loss: 0.8981845378875732, val_loss: 0.8717378377914429\n",
            "Saving model, epoch: 19704, train_loss: 0.8981819748878479, val_loss: 0.8717361688613892\n",
            "Saving model, epoch: 19705, train_loss: 0.8981807231903076, val_loss: 0.8717360496520996\n",
            "Saving model, epoch: 19706, train_loss: 0.8981794714927673, val_loss: 0.871734619140625\n",
            "Saving model, epoch: 19707, train_loss: 0.8981782793998718, val_loss: 0.8717344999313354\n",
            "Saving model, epoch: 19708, train_loss: 0.8981769680976868, val_loss: 0.871732771396637\n",
            "Saving model, epoch: 19710, train_loss: 0.8981744647026062, val_loss: 0.8717311024665833\n",
            "Saving model, epoch: 19711, train_loss: 0.8981732726097107, val_loss: 0.8717308640480042\n",
            "Saving model, epoch: 19712, train_loss: 0.8981718420982361, val_loss: 0.8717296123504639\n",
            "Saving model, epoch: 19713, train_loss: 0.8981706500053406, val_loss: 0.8717285990715027\n",
            "Saving model, epoch: 19714, train_loss: 0.8981694579124451, val_loss: 0.8717280626296997\n",
            "Saving model, epoch: 19715, train_loss: 0.8981682658195496, val_loss: 0.8717266917228699\n",
            "Saving model, epoch: 19716, train_loss: 0.898166835308075, val_loss: 0.8717265129089355\n",
            "Saving model, epoch: 19717, train_loss: 0.8981656432151794, val_loss: 0.8717250227928162\n",
            "Saving model, epoch: 19718, train_loss: 0.8981644511222839, val_loss: 0.8717249035835266\n",
            "Saving model, epoch: 19719, train_loss: 0.8981630206108093, val_loss: 0.8717231154441833\n",
            "Saving model, epoch: 19721, train_loss: 0.8981606960296631, val_loss: 0.8717213869094849\n",
            "Saving model, epoch: 19723, train_loss: 0.898158073425293, val_loss: 0.8717197179794312\n",
            "Saving model, epoch: 19724, train_loss: 0.8981567621231079, val_loss: 0.8717196583747864\n",
            "Saving model, epoch: 19725, train_loss: 0.8981555700302124, val_loss: 0.8717178702354431\n",
            "Saving model, epoch: 19726, train_loss: 0.8981542587280273, val_loss: 0.8717178106307983\n",
            "Saving model, epoch: 19727, train_loss: 0.8981530666351318, val_loss: 0.871716320514679\n",
            "Saving model, epoch: 19728, train_loss: 0.8981519937515259, val_loss: 0.8717159628868103\n",
            "Saving model, epoch: 19729, train_loss: 0.8981505632400513, val_loss: 0.8717145919799805\n",
            "Saving model, epoch: 19730, train_loss: 0.8981493711471558, val_loss: 0.871714174747467\n",
            "Saving model, epoch: 19731, train_loss: 0.8981480598449707, val_loss: 0.8717130422592163\n",
            "Saving model, epoch: 19732, train_loss: 0.8981467485427856, val_loss: 0.8717125058174133\n",
            "Saving model, epoch: 19733, train_loss: 0.8981455564498901, val_loss: 0.8717113137245178\n",
            "Saving model, epoch: 19734, train_loss: 0.8981443047523499, val_loss: 0.8717105388641357\n",
            "Saving model, epoch: 19735, train_loss: 0.8981429934501648, val_loss: 0.8717095851898193\n",
            "Saving model, epoch: 19736, train_loss: 0.8981418013572693, val_loss: 0.8717089891433716\n",
            "Saving model, epoch: 19737, train_loss: 0.8981406092643738, val_loss: 0.8717077970504761\n",
            "Saving model, epoch: 19738, train_loss: 0.8981392979621887, val_loss: 0.8717070817947388\n",
            "Saving model, epoch: 19739, train_loss: 0.8981381058692932, val_loss: 0.8717058897018433\n",
            "Saving model, epoch: 19740, train_loss: 0.8981367945671082, val_loss: 0.8717054724693298\n",
            "Saving model, epoch: 19741, train_loss: 0.8981354832649231, val_loss: 0.8717041015625\n",
            "Saving model, epoch: 19743, train_loss: 0.8981330990791321, val_loss: 0.8717019557952881\n",
            "Saving model, epoch: 19745, train_loss: 0.898130476474762, val_loss: 0.8716994524002075\n",
            "Saving model, epoch: 19747, train_loss: 0.8981278538703918, val_loss: 0.8716967701911926\n",
            "Saving model, epoch: 19749, train_loss: 0.8981254696846008, val_loss: 0.8716935515403748\n",
            "Saving model, epoch: 19751, train_loss: 0.898123025894165, val_loss: 0.8716894388198853\n",
            "Saving model, epoch: 19753, train_loss: 0.8981204032897949, val_loss: 0.8716837167739868\n",
            "Saving model, epoch: 19755, train_loss: 0.8981180191040039, val_loss: 0.8716756105422974\n",
            "Saving model, epoch: 19757, train_loss: 0.8981155753135681, val_loss: 0.8716651201248169\n",
            "Saving model, epoch: 19759, train_loss: 0.8981131911277771, val_loss: 0.8716549277305603\n",
            "Saving model, epoch: 19761, train_loss: 0.8981107473373413, val_loss: 0.8716548085212708\n",
            "Saving model, epoch: 19797, train_loss: 0.8980664610862732, val_loss: 0.8716529011726379\n",
            "epoch: 19801, train_loss: 0.8980615139007568, val_loss: 0.8716561794281006\n",
            "Saving model, epoch: 19802, train_loss: 0.8980603218078613, val_loss: 0.8716505169868469\n",
            "Saving model, epoch: 19804, train_loss: 0.8980579376220703, val_loss: 0.8716492056846619\n",
            "Saving model, epoch: 19807, train_loss: 0.8980541825294495, val_loss: 0.8716480135917664\n",
            "Saving model, epoch: 19809, train_loss: 0.8980517983436584, val_loss: 0.871645450592041\n",
            "Saving model, epoch: 19811, train_loss: 0.8980494141578674, val_loss: 0.8716449737548828\n",
            "Saving model, epoch: 19814, train_loss: 0.8980456590652466, val_loss: 0.871641993522644\n",
            "Saving model, epoch: 19816, train_loss: 0.8980432748794556, val_loss: 0.8716404438018799\n",
            "Saving model, epoch: 19818, train_loss: 0.8980409502983093, val_loss: 0.8716399669647217\n",
            "Saving model, epoch: 19819, train_loss: 0.8980396389961243, val_loss: 0.8716390132904053\n",
            "Saving model, epoch: 19821, train_loss: 0.8980372548103333, val_loss: 0.8716366291046143\n",
            "Saving model, epoch: 19823, train_loss: 0.8980348110198975, val_loss: 0.871634840965271\n",
            "Saving model, epoch: 19825, train_loss: 0.8980323076248169, val_loss: 0.8716338872909546\n",
            "Saving model, epoch: 19826, train_loss: 0.8980311155319214, val_loss: 0.8716335296630859\n",
            "Saving model, epoch: 19827, train_loss: 0.8980299234390259, val_loss: 0.871632993221283\n",
            "Saving model, epoch: 19828, train_loss: 0.8980287909507751, val_loss: 0.8716313242912292\n",
            "Saving model, epoch: 19830, train_loss: 0.898026168346405, val_loss: 0.8716291785240173\n",
            "Saving model, epoch: 19832, train_loss: 0.898023784160614, val_loss: 0.8716275095939636\n",
            "Saving model, epoch: 19834, train_loss: 0.8980214595794678, val_loss: 0.8716263771057129\n",
            "Saving model, epoch: 19835, train_loss: 0.8980202674865723, val_loss: 0.8716260194778442\n",
            "Saving model, epoch: 19836, train_loss: 0.8980189561843872, val_loss: 0.8716251850128174\n",
            "Saving model, epoch: 19837, train_loss: 0.8980177640914917, val_loss: 0.871623694896698\n",
            "Saving model, epoch: 19839, train_loss: 0.8980154395103455, val_loss: 0.8716219663619995\n",
            "Saving model, epoch: 19840, train_loss: 0.8980141282081604, val_loss: 0.8716219067573547\n",
            "Saving model, epoch: 19841, train_loss: 0.8980129361152649, val_loss: 0.8716208338737488\n",
            "Saving model, epoch: 19842, train_loss: 0.8980116844177246, val_loss: 0.8716198801994324\n",
            "Saving model, epoch: 19843, train_loss: 0.8980104923248291, val_loss: 0.8716193437576294\n",
            "Saving model, epoch: 19844, train_loss: 0.8980093002319336, val_loss: 0.8716180920600891\n",
            "Saving model, epoch: 19845, train_loss: 0.8980081081390381, val_loss: 0.8716177344322205\n",
            "Saving model, epoch: 19846, train_loss: 0.8980069160461426, val_loss: 0.8716166615486145\n",
            "Saving model, epoch: 19847, train_loss: 0.8980056047439575, val_loss: 0.8716158270835876\n",
            "Saving model, epoch: 19848, train_loss: 0.8980044722557068, val_loss: 0.8716149926185608\n",
            "Saving model, epoch: 19849, train_loss: 0.8980032801628113, val_loss: 0.8716140389442444\n",
            "Saving model, epoch: 19850, train_loss: 0.8980020880699158, val_loss: 0.8716135621070862\n",
            "Saving model, epoch: 19851, train_loss: 0.8980007767677307, val_loss: 0.8716123104095459\n",
            "Saving model, epoch: 19852, train_loss: 0.8979995250701904, val_loss: 0.8716118335723877\n",
            "Saving model, epoch: 19853, train_loss: 0.8979984521865845, val_loss: 0.8716106414794922\n",
            "Saving model, epoch: 19854, train_loss: 0.8979971408843994, val_loss: 0.8716100454330444\n",
            "Saving model, epoch: 19855, train_loss: 0.8979960083961487, val_loss: 0.8716091513633728\n",
            "Saving model, epoch: 19856, train_loss: 0.8979947566986084, val_loss: 0.8716081380844116\n",
            "Saving model, epoch: 19857, train_loss: 0.8979936242103577, val_loss: 0.8716076612472534\n",
            "Saving model, epoch: 19858, train_loss: 0.8979923129081726, val_loss: 0.8716063499450684\n",
            "Saving model, epoch: 19859, train_loss: 0.8979912400245667, val_loss: 0.8716059923171997\n",
            "Saving model, epoch: 19860, train_loss: 0.8979899287223816, val_loss: 0.8716049790382385\n",
            "Saving model, epoch: 19861, train_loss: 0.8979886770248413, val_loss: 0.8716042041778564\n",
            "Saving model, epoch: 19862, train_loss: 0.8979874849319458, val_loss: 0.87160325050354\n",
            "Saving model, epoch: 19863, train_loss: 0.8979861736297607, val_loss: 0.8716022968292236\n",
            "Saving model, epoch: 19864, train_loss: 0.8979851603507996, val_loss: 0.8716017007827759\n",
            "Saving model, epoch: 19865, train_loss: 0.8979837894439697, val_loss: 0.8716008067131042\n",
            "Saving model, epoch: 19866, train_loss: 0.8979825973510742, val_loss: 0.8715999126434326\n",
            "Saving model, epoch: 19867, train_loss: 0.8979813456535339, val_loss: 0.8715994358062744\n",
            "Saving model, epoch: 19868, train_loss: 0.897980272769928, val_loss: 0.8715981245040894\n",
            "Saving model, epoch: 19869, train_loss: 0.8979789614677429, val_loss: 0.8715975880622864\n",
            "Saving model, epoch: 19870, train_loss: 0.8979777693748474, val_loss: 0.8715964555740356\n",
            "Saving model, epoch: 19871, train_loss: 0.8979765176773071, val_loss: 0.8715960383415222\n",
            "Saving model, epoch: 19872, train_loss: 0.8979753255844116, val_loss: 0.8715945482254028\n",
            "Saving model, epoch: 19873, train_loss: 0.8979740142822266, val_loss: 0.8715944886207581\n",
            "Saving model, epoch: 19874, train_loss: 0.8979729413986206, val_loss: 0.8715928792953491\n",
            "Saving model, epoch: 19876, train_loss: 0.8979704976081848, val_loss: 0.8715911507606506\n",
            "Saving model, epoch: 19878, train_loss: 0.8979681134223938, val_loss: 0.8715892434120178\n",
            "Saving model, epoch: 19880, train_loss: 0.897965669631958, val_loss: 0.8715876936912537\n",
            "Saving model, epoch: 19882, train_loss: 0.897963285446167, val_loss: 0.8715864419937134\n",
            "Saving model, epoch: 19883, train_loss: 0.8979620933532715, val_loss: 0.8715860247612\n",
            "Saving model, epoch: 19884, train_loss: 0.8979608416557312, val_loss: 0.8715851306915283\n",
            "Saving model, epoch: 19885, train_loss: 0.8979596495628357, val_loss: 0.8715837001800537\n",
            "Saving model, epoch: 19887, train_loss: 0.8979571461677551, val_loss: 0.8715819120407104\n",
            "Saving model, epoch: 19889, train_loss: 0.8979548215866089, val_loss: 0.8715804219245911\n",
            "Saving model, epoch: 19890, train_loss: 0.8979536294937134, val_loss: 0.8715801239013672\n",
            "Saving model, epoch: 19891, train_loss: 0.8979524374008179, val_loss: 0.8715794086456299\n",
            "Saving model, epoch: 19892, train_loss: 0.8979513049125671, val_loss: 0.8715779185295105\n",
            "Saving model, epoch: 19894, train_loss: 0.8979487419128418, val_loss: 0.8715762495994568\n",
            "Saving model, epoch: 19895, train_loss: 0.8979474902153015, val_loss: 0.8715760111808777\n",
            "Saving model, epoch: 19896, train_loss: 0.897946298122406, val_loss: 0.871574878692627\n",
            "Saving model, epoch: 19897, train_loss: 0.8979452252388, val_loss: 0.8715742230415344\n",
            "Saving model, epoch: 19898, train_loss: 0.8979439735412598, val_loss: 0.8715733885765076\n",
            "Saving model, epoch: 19899, train_loss: 0.8979427814483643, val_loss: 0.8715724349021912\n",
            "Saving model, epoch: 19900, train_loss: 0.8979415893554688, val_loss: 0.8715720176696777\n",
            "epoch: 19901, train_loss: 0.8979403972625732, val_loss: 0.8715704083442688\n",
            "Saving model, epoch: 19901, train_loss: 0.8979403972625732, val_loss: 0.8715704083442688\n",
            "Saving model, epoch: 19902, train_loss: 0.8979390859603882, val_loss: 0.8715701103210449\n",
            "Saving model, epoch: 19903, train_loss: 0.8979378342628479, val_loss: 0.871569037437439\n",
            "Saving model, epoch: 19904, train_loss: 0.8979366421699524, val_loss: 0.8715683221817017\n",
            "Saving model, epoch: 19905, train_loss: 0.8979354500770569, val_loss: 0.8715675473213196\n",
            "Saving model, epoch: 19906, train_loss: 0.8979343771934509, val_loss: 0.8715665340423584\n",
            "Saving model, epoch: 19907, train_loss: 0.8979331254959106, val_loss: 0.8715661764144897\n",
            "Saving model, epoch: 19908, train_loss: 0.8979319334030151, val_loss: 0.871564507484436\n",
            "Saving model, epoch: 19909, train_loss: 0.8979307413101196, val_loss: 0.8715643882751465\n",
            "Saving model, epoch: 19910, train_loss: 0.8979294300079346, val_loss: 0.8715632557868958\n",
            "Saving model, epoch: 19911, train_loss: 0.8979282975196838, val_loss: 0.8715627193450928\n",
            "Saving model, epoch: 19912, train_loss: 0.8979271054267883, val_loss: 0.8715616464614868\n",
            "Saving model, epoch: 19913, train_loss: 0.8979259133338928, val_loss: 0.8715608716011047\n",
            "Saving model, epoch: 19914, train_loss: 0.8979247212409973, val_loss: 0.8715603351593018\n",
            "Saving model, epoch: 19915, train_loss: 0.897923469543457, val_loss: 0.8715591430664062\n",
            "Saving model, epoch: 19916, train_loss: 0.8979222774505615, val_loss: 0.8715584874153137\n",
            "Saving model, epoch: 19917, train_loss: 0.897921085357666, val_loss: 0.8715574741363525\n",
            "Saving model, epoch: 19918, train_loss: 0.8979198932647705, val_loss: 0.8715568780899048\n",
            "Saving model, epoch: 19919, train_loss: 0.897918701171875, val_loss: 0.8715559244155884\n",
            "Saving model, epoch: 19920, train_loss: 0.8979174494743347, val_loss: 0.8715550899505615\n",
            "Saving model, epoch: 19921, train_loss: 0.8979162573814392, val_loss: 0.8715543150901794\n",
            "Saving model, epoch: 19922, train_loss: 0.8979149460792542, val_loss: 0.871553361415863\n",
            "Saving model, epoch: 19923, train_loss: 0.8979138731956482, val_loss: 0.8715526461601257\n",
            "Saving model, epoch: 19924, train_loss: 0.8979126214981079, val_loss: 0.8715518116950989\n",
            "Saving model, epoch: 19925, train_loss: 0.8979114294052124, val_loss: 0.8715507984161377\n",
            "Saving model, epoch: 19926, train_loss: 0.8979102373123169, val_loss: 0.8715502619743347\n",
            "Saving model, epoch: 19927, train_loss: 0.8979089260101318, val_loss: 0.8715488910675049\n",
            "Saving model, epoch: 19928, train_loss: 0.8979078531265259, val_loss: 0.871548593044281\n",
            "Saving model, epoch: 19929, train_loss: 0.8979066014289856, val_loss: 0.871547520160675\n",
            "Saving model, epoch: 19930, train_loss: 0.8979054093360901, val_loss: 0.8715471029281616\n",
            "Saving model, epoch: 19931, train_loss: 0.8979042172431946, val_loss: 0.8715458512306213\n",
            "Saving model, epoch: 19932, train_loss: 0.8979030251502991, val_loss: 0.8715452551841736\n",
            "Saving model, epoch: 19933, train_loss: 0.8979017734527588, val_loss: 0.8715445399284363\n",
            "Saving model, epoch: 19934, train_loss: 0.8979005813598633, val_loss: 0.8715433478355408\n",
            "Saving model, epoch: 19935, train_loss: 0.8978993892669678, val_loss: 0.8715430498123169\n",
            "Saving model, epoch: 19936, train_loss: 0.8978981971740723, val_loss: 0.8715416193008423\n",
            "Saving model, epoch: 19937, train_loss: 0.897896945476532, val_loss: 0.871541440486908\n",
            "Saving model, epoch: 19938, train_loss: 0.8978957533836365, val_loss: 0.8715400695800781\n",
            "Saving model, epoch: 19939, train_loss: 0.897894561290741, val_loss: 0.871539294719696\n",
            "Saving model, epoch: 19940, train_loss: 0.8978933691978455, val_loss: 0.8715385794639587\n",
            "Saving model, epoch: 19941, train_loss: 0.89789217710495, val_loss: 0.8715375661849976\n",
            "Saving model, epoch: 19942, train_loss: 0.8978910446166992, val_loss: 0.8715369701385498\n",
            "Saving model, epoch: 19943, train_loss: 0.8978897333145142, val_loss: 0.8715357780456543\n",
            "Saving model, epoch: 19944, train_loss: 0.8978885412216187, val_loss: 0.87153559923172\n",
            "Saving model, epoch: 19945, train_loss: 0.8978873491287231, val_loss: 0.8715342283248901\n",
            "Saving model, epoch: 19946, train_loss: 0.8978860974311829, val_loss: 0.8715338706970215\n",
            "Saving model, epoch: 19947, train_loss: 0.8978849053382874, val_loss: 0.8715323805809021\n",
            "Saving model, epoch: 19948, train_loss: 0.8978838324546814, val_loss: 0.871532142162323\n",
            "Saving model, epoch: 19949, train_loss: 0.8978825807571411, val_loss: 0.8715309500694275\n",
            "Saving model, epoch: 19950, train_loss: 0.8978813886642456, val_loss: 0.871530294418335\n",
            "Saving model, epoch: 19951, train_loss: 0.8978800773620605, val_loss: 0.871529757976532\n",
            "Saving model, epoch: 19952, train_loss: 0.8978790640830994, val_loss: 0.8715283274650574\n",
            "Saving model, epoch: 19953, train_loss: 0.8978778719902039, val_loss: 0.8715282678604126\n",
            "Saving model, epoch: 19954, train_loss: 0.8978765606880188, val_loss: 0.8715265393257141\n",
            "Saving model, epoch: 19956, train_loss: 0.8978741765022278, val_loss: 0.8715245127677917\n",
            "Saving model, epoch: 19958, train_loss: 0.8978718519210815, val_loss: 0.8715229034423828\n",
            "Saving model, epoch: 19960, train_loss: 0.8978694677352905, val_loss: 0.87152099609375\n",
            "Saving model, epoch: 19962, train_loss: 0.8978669047355652, val_loss: 0.8715195059776306\n",
            "Saving model, epoch: 19964, train_loss: 0.8978646397590637, val_loss: 0.8715177178382874\n",
            "Saving model, epoch: 19966, train_loss: 0.8978621959686279, val_loss: 0.8715159893035889\n",
            "Saving model, epoch: 19968, train_loss: 0.8978598117828369, val_loss: 0.8715143799781799\n",
            "Saving model, epoch: 19970, train_loss: 0.8978573679924011, val_loss: 0.871513307094574\n",
            "Saving model, epoch: 19971, train_loss: 0.8978561758995056, val_loss: 0.8715131878852844\n",
            "Saving model, epoch: 19972, train_loss: 0.8978549838066101, val_loss: 0.8715122938156128\n",
            "Saving model, epoch: 19973, train_loss: 0.8978538513183594, val_loss: 0.8715111613273621\n",
            "Saving model, epoch: 19974, train_loss: 0.8978525400161743, val_loss: 0.8715110421180725\n",
            "Saving model, epoch: 19975, train_loss: 0.8978514671325684, val_loss: 0.871509313583374\n",
            "Saving model, epoch: 19977, train_loss: 0.8978490233421326, val_loss: 0.8715077638626099\n",
            "Saving model, epoch: 19979, train_loss: 0.8978467583656311, val_loss: 0.8715064525604248\n",
            "Saving model, epoch: 19980, train_loss: 0.8978455066680908, val_loss: 0.8715059161186218\n",
            "Saving model, epoch: 19981, train_loss: 0.8978443145751953, val_loss: 0.8715049028396606\n",
            "Saving model, epoch: 19982, train_loss: 0.8978430032730103, val_loss: 0.8715041279792786\n",
            "Saving model, epoch: 19983, train_loss: 0.8978419303894043, val_loss: 0.8715034127235413\n",
            "Saving model, epoch: 19984, train_loss: 0.8978406190872192, val_loss: 0.871502161026001\n",
            "Saving model, epoch: 19985, train_loss: 0.8978394865989685, val_loss: 0.8715019822120667\n",
            "Saving model, epoch: 19986, train_loss: 0.897838294506073, val_loss: 0.8715007305145264\n",
            "Saving model, epoch: 19987, train_loss: 0.8978371024131775, val_loss: 0.8715001940727234\n",
            "Saving model, epoch: 19988, train_loss: 0.897835910320282, val_loss: 0.8714993000030518\n",
            "Saving model, epoch: 19989, train_loss: 0.8978346586227417, val_loss: 0.8714982271194458\n",
            "Saving model, epoch: 19990, train_loss: 0.8978334665298462, val_loss: 0.8714979887008667\n",
            "Saving model, epoch: 19991, train_loss: 0.8978322744369507, val_loss: 0.8714964389801025\n",
            "Saving model, epoch: 19992, train_loss: 0.8978311419487, val_loss: 0.8714960813522339\n",
            "Saving model, epoch: 19993, train_loss: 0.8978299498558044, val_loss: 0.8714947700500488\n",
            "Saving model, epoch: 19994, train_loss: 0.8978287577629089, val_loss: 0.8714944124221802\n",
            "Saving model, epoch: 19995, train_loss: 0.8978275656700134, val_loss: 0.8714935183525085\n",
            "Saving model, epoch: 19996, train_loss: 0.8978264331817627, val_loss: 0.8714924454689026\n",
            "Saving model, epoch: 19997, train_loss: 0.8978250622749329, val_loss: 0.8714919090270996\n",
            "Saving model, epoch: 19998, train_loss: 0.8978240489959717, val_loss: 0.8714906573295593\n",
            "Saving model, epoch: 19999, train_loss: 0.8978227972984314, val_loss: 0.8714903593063354\n",
            "Saving model, epoch: 20000, train_loss: 0.8978216052055359, val_loss: 0.8714891672134399\n",
            "epoch: 20001, train_loss: 0.8978204131126404, val_loss: 0.8714885711669922\n",
            "Saving model, epoch: 20001, train_loss: 0.8978204131126404, val_loss: 0.8714885711669922\n",
            "Saving model, epoch: 20002, train_loss: 0.8978191018104553, val_loss: 0.8714877367019653\n",
            "Saving model, epoch: 20003, train_loss: 0.8978180289268494, val_loss: 0.8714870810508728\n",
            "Saving model, epoch: 20004, train_loss: 0.8978167772293091, val_loss: 0.8714861869812012\n",
            "Saving model, epoch: 20005, train_loss: 0.8978155851364136, val_loss: 0.8714850544929504\n",
            "Saving model, epoch: 20006, train_loss: 0.8978143930435181, val_loss: 0.8714848756790161\n",
            "Saving model, epoch: 20007, train_loss: 0.8978132009506226, val_loss: 0.871483325958252\n",
            "Saving model, epoch: 20008, train_loss: 0.8978119492530823, val_loss: 0.8714831471443176\n",
            "Saving model, epoch: 20009, train_loss: 0.8978108763694763, val_loss: 0.8714815378189087\n",
            "Saving model, epoch: 20011, train_loss: 0.8978083729743958, val_loss: 0.8714801073074341\n",
            "Saving model, epoch: 20012, train_loss: 0.897807240486145, val_loss: 0.8714799880981445\n",
            "Saving model, epoch: 20013, train_loss: 0.89780592918396, val_loss: 0.8714785575866699\n",
            "Saving model, epoch: 20014, train_loss: 0.8978049159049988, val_loss: 0.8714783191680908\n",
            "Saving model, epoch: 20015, train_loss: 0.8978036642074585, val_loss: 0.8714766502380371\n",
            "Saving model, epoch: 20016, train_loss: 0.8978025317192078, val_loss: 0.8714765310287476\n",
            "Saving model, epoch: 20017, train_loss: 0.8978013396263123, val_loss: 0.8714752197265625\n",
            "Saving model, epoch: 20018, train_loss: 0.8978000283241272, val_loss: 0.8714749813079834\n",
            "Saving model, epoch: 20019, train_loss: 0.8977988958358765, val_loss: 0.8714736104011536\n",
            "Saving model, epoch: 20020, train_loss: 0.897797703742981, val_loss: 0.8714730143547058\n",
            "Saving model, epoch: 20021, train_loss: 0.8977965116500854, val_loss: 0.871472179889679\n",
            "Saving model, epoch: 20022, train_loss: 0.8977953195571899, val_loss: 0.8714715242385864\n",
            "Saving model, epoch: 20023, train_loss: 0.8977940678596497, val_loss: 0.8714709281921387\n",
            "Saving model, epoch: 20024, train_loss: 0.8977928757667542, val_loss: 0.8714697957038879\n",
            "Saving model, epoch: 20025, train_loss: 0.8977918028831482, val_loss: 0.8714691400527954\n",
            "Saving model, epoch: 20026, train_loss: 0.8977905511856079, val_loss: 0.8714682459831238\n",
            "Saving model, epoch: 20027, train_loss: 0.8977892994880676, val_loss: 0.8714672923088074\n",
            "Saving model, epoch: 20028, train_loss: 0.8977881669998169, val_loss: 0.8714666366577148\n",
            "Saving model, epoch: 20029, train_loss: 0.8977870345115662, val_loss: 0.8714656233787537\n",
            "Saving model, epoch: 20030, train_loss: 0.8977857828140259, val_loss: 0.8714651465415955\n",
            "Saving model, epoch: 20031, train_loss: 0.8977846503257751, val_loss: 0.8714638948440552\n",
            "Saving model, epoch: 20032, train_loss: 0.8977834582328796, val_loss: 0.8714635372161865\n",
            "Saving model, epoch: 20033, train_loss: 0.8977822661399841, val_loss: 0.8714621663093567\n",
            "Saving model, epoch: 20034, train_loss: 0.8977810144424438, val_loss: 0.8714620471000671\n",
            "Saving model, epoch: 20035, train_loss: 0.8977798223495483, val_loss: 0.8714604377746582\n",
            "Saving model, epoch: 20037, train_loss: 0.8977774977684021, val_loss: 0.8714587092399597\n",
            "Saving model, epoch: 20039, train_loss: 0.8977749943733215, val_loss: 0.8714572191238403\n",
            "Saving model, epoch: 20040, train_loss: 0.8977739214897156, val_loss: 0.871457040309906\n",
            "Saving model, epoch: 20041, train_loss: 0.8977727890014648, val_loss: 0.8714558482170105\n",
            "Saving model, epoch: 20042, train_loss: 0.8977715969085693, val_loss: 0.8714553117752075\n",
            "Saving model, epoch: 20043, train_loss: 0.8977704048156738, val_loss: 0.8714544773101807\n",
            "Saving model, epoch: 20044, train_loss: 0.8977691531181335, val_loss: 0.8714534640312195\n",
            "Saving model, epoch: 20045, train_loss: 0.897767961025238, val_loss: 0.8714527487754822\n",
            "Saving model, epoch: 20046, train_loss: 0.8977667689323425, val_loss: 0.871451735496521\n",
            "Saving model, epoch: 20047, train_loss: 0.897765576839447, val_loss: 0.8714512586593628\n",
            "Saving model, epoch: 20048, train_loss: 0.8977643251419067, val_loss: 0.8714499473571777\n",
            "Saving model, epoch: 20049, train_loss: 0.8977631330490112, val_loss: 0.871449887752533\n",
            "Saving model, epoch: 20050, train_loss: 0.8977620601654053, val_loss: 0.8714483976364136\n",
            "Saving model, epoch: 20052, train_loss: 0.897759735584259, val_loss: 0.8714461326599121\n",
            "Saving model, epoch: 20054, train_loss: 0.8977572917938232, val_loss: 0.8714435696601868\n",
            "Saving model, epoch: 20056, train_loss: 0.8977549076080322, val_loss: 0.8714408278465271\n",
            "Saving model, epoch: 20058, train_loss: 0.897752583026886, val_loss: 0.8714373111724854\n",
            "Saving model, epoch: 20060, train_loss: 0.897750198841095, val_loss: 0.8714329600334167\n",
            "Saving model, epoch: 20062, train_loss: 0.8977478742599487, val_loss: 0.8714267015457153\n",
            "Saving model, epoch: 20064, train_loss: 0.8977455496788025, val_loss: 0.8714183568954468\n",
            "Saving model, epoch: 20066, train_loss: 0.8977432250976562, val_loss: 0.8714079260826111\n",
            "Saving model, epoch: 20068, train_loss: 0.8977409601211548, val_loss: 0.8713993430137634\n",
            "epoch: 20101, train_loss: 0.8977024555206299, val_loss: 0.8714037537574768\n",
            "Saving model, epoch: 20111, train_loss: 0.897691011428833, val_loss: 0.8713982105255127\n",
            "Saving model, epoch: 20113, train_loss: 0.897688627243042, val_loss: 0.8713967800140381\n",
            "Saving model, epoch: 20116, train_loss: 0.8976852893829346, val_loss: 0.8713960647583008\n",
            "Saving model, epoch: 20118, train_loss: 0.8976829648017883, val_loss: 0.8713933229446411\n",
            "Saving model, epoch: 20123, train_loss: 0.8976771235466003, val_loss: 0.8713899254798889\n",
            "Saving model, epoch: 20125, train_loss: 0.8976749181747437, val_loss: 0.8713884949684143\n",
            "Saving model, epoch: 20127, train_loss: 0.8976725339889526, val_loss: 0.8713883757591248\n",
            "Saving model, epoch: 20128, train_loss: 0.8976715207099915, val_loss: 0.8713877201080322\n",
            "Saving model, epoch: 20130, train_loss: 0.8976691365242004, val_loss: 0.8713849782943726\n",
            "Saving model, epoch: 20132, train_loss: 0.897666871547699, val_loss: 0.8713842630386353\n",
            "Saving model, epoch: 20133, train_loss: 0.8976656794548035, val_loss: 0.8713842034339905\n",
            "Saving model, epoch: 20135, train_loss: 0.8976634740829468, val_loss: 0.8713816404342651\n",
            "Saving model, epoch: 20137, train_loss: 0.8976610898971558, val_loss: 0.8713801503181458\n",
            "Saving model, epoch: 20139, train_loss: 0.8976588845252991, val_loss: 0.8713797926902771\n",
            "Saving model, epoch: 20140, train_loss: 0.8976576328277588, val_loss: 0.8713783025741577\n",
            "Saving model, epoch: 20142, train_loss: 0.8976553678512573, val_loss: 0.8713762164115906\n",
            "Saving model, epoch: 20144, train_loss: 0.8976530432701111, val_loss: 0.8713752627372742\n",
            "Saving model, epoch: 20145, train_loss: 0.8976519107818604, val_loss: 0.8713748455047607\n",
            "Saving model, epoch: 20146, train_loss: 0.8976507186889648, val_loss: 0.8713746070861816\n",
            "Saving model, epoch: 20147, train_loss: 0.8976496458053589, val_loss: 0.8713726997375488\n",
            "Saving model, epoch: 20149, train_loss: 0.8976473808288574, val_loss: 0.871371328830719\n",
            "Saving model, epoch: 20151, train_loss: 0.8976449966430664, val_loss: 0.8713701367378235\n",
            "Saving model, epoch: 20152, train_loss: 0.8976439833641052, val_loss: 0.8713691234588623\n",
            "Saving model, epoch: 20154, train_loss: 0.897641658782959, val_loss: 0.8713672161102295\n",
            "Saving model, epoch: 20156, train_loss: 0.897639274597168, val_loss: 0.871366024017334\n",
            "Saving model, epoch: 20157, train_loss: 0.8976381421089172, val_loss: 0.8713657855987549\n",
            "Saving model, epoch: 20158, train_loss: 0.8976369500160217, val_loss: 0.8713647723197937\n",
            "Saving model, epoch: 20159, train_loss: 0.8976359367370605, val_loss: 0.871363639831543\n",
            "Saving model, epoch: 20161, train_loss: 0.8976335525512695, val_loss: 0.8713619709014893\n",
            "Saving model, epoch: 20163, train_loss: 0.8976312279701233, val_loss: 0.8713608980178833\n",
            "Saving model, epoch: 20164, train_loss: 0.8976301550865173, val_loss: 0.8713599443435669\n",
            "Saving model, epoch: 20165, train_loss: 0.8976290225982666, val_loss: 0.8713594675064087\n",
            "Saving model, epoch: 20166, train_loss: 0.8976277112960815, val_loss: 0.8713582158088684\n",
            "Saving model, epoch: 20167, train_loss: 0.8976266980171204, val_loss: 0.8713580965995789\n",
            "Saving model, epoch: 20168, train_loss: 0.8976255059242249, val_loss: 0.8713569641113281\n",
            "Saving model, epoch: 20169, train_loss: 0.8976243734359741, val_loss: 0.871356189250946\n",
            "Saving model, epoch: 20170, train_loss: 0.8976233005523682, val_loss: 0.871355414390564\n",
            "Saving model, epoch: 20171, train_loss: 0.8976221680641174, val_loss: 0.8713544607162476\n",
            "Saving model, epoch: 20172, train_loss: 0.8976209759712219, val_loss: 0.8713541030883789\n",
            "Saving model, epoch: 20173, train_loss: 0.897619903087616, val_loss: 0.8713526725769043\n",
            "Saving model, epoch: 20174, train_loss: 0.8976186513900757, val_loss: 0.87135249376297\n",
            "Saving model, epoch: 20175, train_loss: 0.8976174592971802, val_loss: 0.8713513016700745\n",
            "Saving model, epoch: 20176, train_loss: 0.897616446018219, val_loss: 0.8713507652282715\n",
            "Saving model, epoch: 20177, train_loss: 0.8976152539253235, val_loss: 0.8713497519493103\n",
            "Saving model, epoch: 20178, train_loss: 0.897614061832428, val_loss: 0.8713488578796387\n",
            "Saving model, epoch: 20179, train_loss: 0.8976130485534668, val_loss: 0.8713484406471252\n",
            "Saving model, epoch: 20180, train_loss: 0.8976118564605713, val_loss: 0.8713476061820984\n",
            "Saving model, epoch: 20181, train_loss: 0.8976106643676758, val_loss: 0.8713468313217163\n",
            "Saving model, epoch: 20182, train_loss: 0.8976094126701355, val_loss: 0.8713459968566895\n",
            "Saving model, epoch: 20183, train_loss: 0.8976083993911743, val_loss: 0.8713451027870178\n",
            "Saving model, epoch: 20184, train_loss: 0.8976072072982788, val_loss: 0.8713445663452148\n",
            "Saving model, epoch: 20185, train_loss: 0.8976061344146729, val_loss: 0.8713434338569641\n",
            "Saving model, epoch: 20186, train_loss: 0.8976050019264221, val_loss: 0.8713433146476746\n",
            "Saving model, epoch: 20187, train_loss: 0.8976038098335266, val_loss: 0.8713420629501343\n",
            "Saving model, epoch: 20188, train_loss: 0.8976027965545654, val_loss: 0.8713414072990417\n",
            "Saving model, epoch: 20189, train_loss: 0.8976014852523804, val_loss: 0.8713407516479492\n",
            "Saving model, epoch: 20190, train_loss: 0.8976004123687744, val_loss: 0.8713393807411194\n",
            "Saving model, epoch: 20192, train_loss: 0.8975980877876282, val_loss: 0.8713380098342896\n",
            "Saving model, epoch: 20193, train_loss: 0.8975969552993774, val_loss: 0.8713374733924866\n",
            "Saving model, epoch: 20194, train_loss: 0.8975959420204163, val_loss: 0.8713368773460388\n",
            "Saving model, epoch: 20195, train_loss: 0.8975947499275208, val_loss: 0.8713357448577881\n",
            "Saving model, epoch: 20196, train_loss: 0.8975935578346252, val_loss: 0.8713353276252747\n",
            "Saving model, epoch: 20197, train_loss: 0.8975923657417297, val_loss: 0.8713343143463135\n",
            "Saving model, epoch: 20198, train_loss: 0.8975911736488342, val_loss: 0.8713337779045105\n",
            "Saving model, epoch: 20199, train_loss: 0.897590160369873, val_loss: 0.8713327050209045\n",
            "Saving model, epoch: 20200, train_loss: 0.8975890278816223, val_loss: 0.8713321089744568\n",
            "epoch: 20201, train_loss: 0.8975877165794373, val_loss: 0.871331512928009\n",
            "Saving model, epoch: 20201, train_loss: 0.8975877165794373, val_loss: 0.871331512928009\n",
            "Saving model, epoch: 20202, train_loss: 0.8975867033004761, val_loss: 0.8713303208351135\n",
            "Saving model, epoch: 20203, train_loss: 0.8975855112075806, val_loss: 0.8713297843933105\n",
            "Saving model, epoch: 20204, train_loss: 0.8975844383239746, val_loss: 0.871328592300415\n",
            "Saving model, epoch: 20205, train_loss: 0.8975833058357239, val_loss: 0.8713281750679016\n",
            "Saving model, epoch: 20206, train_loss: 0.8975821137428284, val_loss: 0.8713272213935852\n",
            "Saving model, epoch: 20207, train_loss: 0.8975809812545776, val_loss: 0.8713265061378479\n",
            "Saving model, epoch: 20208, train_loss: 0.8975797891616821, val_loss: 0.8713259100914001\n",
            "Saving model, epoch: 20209, train_loss: 0.8975786566734314, val_loss: 0.871324896812439\n",
            "Saving model, epoch: 20210, train_loss: 0.8975775837898254, val_loss: 0.8713244199752808\n",
            "Saving model, epoch: 20211, train_loss: 0.8975764513015747, val_loss: 0.8713233470916748\n",
            "Saving model, epoch: 20212, train_loss: 0.8975752592086792, val_loss: 0.8713228702545166\n",
            "Saving model, epoch: 20213, train_loss: 0.8975741863250732, val_loss: 0.8713220357894897\n",
            "Saving model, epoch: 20214, train_loss: 0.897572934627533, val_loss: 0.8713211417198181\n",
            "Saving model, epoch: 20215, train_loss: 0.897571861743927, val_loss: 0.8713206052780151\n",
            "Saving model, epoch: 20216, train_loss: 0.8975707292556763, val_loss: 0.8713196516036987\n",
            "Saving model, epoch: 20217, train_loss: 0.8975696563720703, val_loss: 0.8713188767433167\n",
            "Saving model, epoch: 20218, train_loss: 0.8975684642791748, val_loss: 0.8713181614875793\n",
            "Saving model, epoch: 20219, train_loss: 0.8975672125816345, val_loss: 0.8713172078132629\n",
            "Saving model, epoch: 20220, train_loss: 0.8975661993026733, val_loss: 0.8713167309761047\n",
            "Saving model, epoch: 20221, train_loss: 0.8975650072097778, val_loss: 0.871315598487854\n",
            "Saving model, epoch: 20222, train_loss: 0.8975639343261719, val_loss: 0.8713151216506958\n",
            "Saving model, epoch: 20223, train_loss: 0.8975628018379211, val_loss: 0.8713141679763794\n",
            "Saving model, epoch: 20224, train_loss: 0.8975616097450256, val_loss: 0.8713134527206421\n",
            "Saving model, epoch: 20225, train_loss: 0.8975604772567749, val_loss: 0.8713127970695496\n",
            "Saving model, epoch: 20226, train_loss: 0.897559404373169, val_loss: 0.8713119626045227\n",
            "Saving model, epoch: 20227, train_loss: 0.8975582122802734, val_loss: 0.8713110685348511\n",
            "Saving model, epoch: 20228, train_loss: 0.8975570797920227, val_loss: 0.8713101744651794\n",
            "Saving model, epoch: 20229, train_loss: 0.8975560069084167, val_loss: 0.8713099360466003\n",
            "Saving model, epoch: 20230, train_loss: 0.8975547552108765, val_loss: 0.8713083863258362\n",
            "Saving model, epoch: 20232, train_loss: 0.897552490234375, val_loss: 0.8713062405586243\n",
            "Saving model, epoch: 20234, train_loss: 0.8975501656532288, val_loss: 0.8713045716285706\n",
            "Saving model, epoch: 20236, train_loss: 0.8975479602813721, val_loss: 0.8713032603263855\n",
            "Saving model, epoch: 20238, train_loss: 0.8975456357002258, val_loss: 0.8713020086288452\n",
            "Saving model, epoch: 20240, train_loss: 0.8975434303283691, val_loss: 0.8713011741638184\n",
            "Saving model, epoch: 20241, train_loss: 0.8975422978401184, val_loss: 0.871300458908081\n",
            "Saving model, epoch: 20242, train_loss: 0.8975411057472229, val_loss: 0.871300220489502\n",
            "Saving model, epoch: 20243, train_loss: 0.8975399136543274, val_loss: 0.8712982535362244\n",
            "Saving model, epoch: 20245, train_loss: 0.8975377082824707, val_loss: 0.8712967038154602\n",
            "Saving model, epoch: 20247, train_loss: 0.8975353837013245, val_loss: 0.8712955713272095\n",
            "Saving model, epoch: 20248, train_loss: 0.8975342512130737, val_loss: 0.871295154094696\n",
            "Saving model, epoch: 20249, train_loss: 0.8975331783294678, val_loss: 0.8712945580482483\n",
            "Saving model, epoch: 20250, train_loss: 0.897532045841217, val_loss: 0.871293306350708\n",
            "Saving model, epoch: 20251, train_loss: 0.8975309729576111, val_loss: 0.8712930679321289\n",
            "Saving model, epoch: 20252, train_loss: 0.8975298404693604, val_loss: 0.8712918162345886\n",
            "Saving model, epoch: 20253, train_loss: 0.8975286483764648, val_loss: 0.8712917566299438\n",
            "Saving model, epoch: 20254, train_loss: 0.8975274562835693, val_loss: 0.8712902069091797\n",
            "Saving model, epoch: 20255, train_loss: 0.8975264430046082, val_loss: 0.871289849281311\n",
            "Saving model, epoch: 20256, train_loss: 0.8975252509117126, val_loss: 0.871289074420929\n",
            "Saving model, epoch: 20257, train_loss: 0.8975240588188171, val_loss: 0.8712881803512573\n",
            "Saving model, epoch: 20258, train_loss: 0.897523045539856, val_loss: 0.8712875247001648\n",
            "Saving model, epoch: 20259, train_loss: 0.8975217938423157, val_loss: 0.8712865114212036\n",
            "Saving model, epoch: 20260, train_loss: 0.8975207209587097, val_loss: 0.8712859749794006\n",
            "Saving model, epoch: 20261, train_loss: 0.8975195288658142, val_loss: 0.8712848424911499\n",
            "Saving model, epoch: 20262, train_loss: 0.8975183963775635, val_loss: 0.8712844252586365\n",
            "Saving model, epoch: 20263, train_loss: 0.897517204284668, val_loss: 0.8712835907936096\n",
            "Saving model, epoch: 20264, train_loss: 0.8975161910057068, val_loss: 0.8712829947471619\n",
            "Saving model, epoch: 20265, train_loss: 0.8975151181221008, val_loss: 0.8712819218635559\n",
            "Saving model, epoch: 20266, train_loss: 0.8975138664245605, val_loss: 0.8712815046310425\n",
            "Saving model, epoch: 20267, train_loss: 0.8975127935409546, val_loss: 0.8712803721427917\n",
            "Saving model, epoch: 20268, train_loss: 0.8975116610527039, val_loss: 0.8712798953056335\n",
            "Saving model, epoch: 20269, train_loss: 0.8975105881690979, val_loss: 0.8712790012359619\n",
            "Saving model, epoch: 20270, train_loss: 0.8975093364715576, val_loss: 0.8712781667709351\n",
            "Saving model, epoch: 20271, train_loss: 0.8975081443786621, val_loss: 0.8712775111198425\n",
            "Saving model, epoch: 20272, train_loss: 0.8975070714950562, val_loss: 0.8712766170501709\n",
            "Saving model, epoch: 20273, train_loss: 0.8975059390068054, val_loss: 0.8712760210037231\n",
            "Saving model, epoch: 20274, train_loss: 0.8975047469139099, val_loss: 0.8712751269340515\n",
            "Saving model, epoch: 20275, train_loss: 0.8975037336349487, val_loss: 0.8712747097015381\n",
            "Saving model, epoch: 20276, train_loss: 0.8975025415420532, val_loss: 0.871273398399353\n",
            "Saving model, epoch: 20277, train_loss: 0.8975014090538025, val_loss: 0.8712731599807739\n",
            "Saving model, epoch: 20278, train_loss: 0.8975003361701965, val_loss: 0.8712719082832336\n",
            "Saving model, epoch: 20279, train_loss: 0.8974990844726562, val_loss: 0.871271550655365\n",
            "Saving model, epoch: 20280, train_loss: 0.8974981307983398, val_loss: 0.8712705373764038\n",
            "Saving model, epoch: 20281, train_loss: 0.8974968791007996, val_loss: 0.871269702911377\n",
            "Saving model, epoch: 20282, train_loss: 0.897495687007904, val_loss: 0.8712691068649292\n",
            "Saving model, epoch: 20283, train_loss: 0.8974946141242981, val_loss: 0.8712681531906128\n",
            "Saving model, epoch: 20284, train_loss: 0.8974934816360474, val_loss: 0.871267557144165\n",
            "Saving model, epoch: 20285, train_loss: 0.8974924683570862, val_loss: 0.8712666630744934\n",
            "Saving model, epoch: 20286, train_loss: 0.8974911570549011, val_loss: 0.8712661266326904\n",
            "Saving model, epoch: 20287, train_loss: 0.8974900841712952, val_loss: 0.8712654113769531\n",
            "Saving model, epoch: 20288, train_loss: 0.8974889516830444, val_loss: 0.8712642192840576\n",
            "Saving model, epoch: 20289, train_loss: 0.8974878787994385, val_loss: 0.871263861656189\n",
            "Saving model, epoch: 20290, train_loss: 0.8974867463111877, val_loss: 0.8712628483772278\n",
            "Saving model, epoch: 20291, train_loss: 0.8974856734275818, val_loss: 0.8712626099586487\n",
            "Saving model, epoch: 20292, train_loss: 0.8974844217300415, val_loss: 0.8712611198425293\n",
            "Saving model, epoch: 20293, train_loss: 0.8974833488464355, val_loss: 0.8712608218193054\n",
            "Saving model, epoch: 20294, train_loss: 0.8974822163581848, val_loss: 0.8712596297264099\n",
            "Saving model, epoch: 20295, train_loss: 0.8974810242652893, val_loss: 0.8712592720985413\n",
            "Saving model, epoch: 20296, train_loss: 0.8974799513816833, val_loss: 0.871258020401001\n",
            "Saving model, epoch: 20297, train_loss: 0.8974788188934326, val_loss: 0.8712578415870667\n",
            "Saving model, epoch: 20298, train_loss: 0.8974776864051819, val_loss: 0.8712567090988159\n",
            "Saving model, epoch: 20299, train_loss: 0.8974764943122864, val_loss: 0.8712562918663025\n",
            "Saving model, epoch: 20300, train_loss: 0.8974754214286804, val_loss: 0.8712552189826965\n",
            "epoch: 20301, train_loss: 0.8974742889404297, val_loss: 0.8712549209594727\n",
            "Saving model, epoch: 20301, train_loss: 0.8974742889404297, val_loss: 0.8712549209594727\n",
            "Saving model, epoch: 20302, train_loss: 0.8974732160568237, val_loss: 0.8712535500526428\n",
            "Saving model, epoch: 20303, train_loss: 0.8974719643592834, val_loss: 0.8712531328201294\n",
            "Saving model, epoch: 20304, train_loss: 0.8974708914756775, val_loss: 0.8712520599365234\n",
            "Saving model, epoch: 20305, train_loss: 0.8974697589874268, val_loss: 0.87125164270401\n",
            "Saving model, epoch: 20306, train_loss: 0.8974686861038208, val_loss: 0.8712506294250488\n",
            "Saving model, epoch: 20307, train_loss: 0.8974675536155701, val_loss: 0.8712499141693115\n",
            "Saving model, epoch: 20308, train_loss: 0.8974663615226746, val_loss: 0.8712493777275085\n",
            "Saving model, epoch: 20309, train_loss: 0.897465169429779, val_loss: 0.8712482452392578\n",
            "Saving model, epoch: 20310, train_loss: 0.8974641561508179, val_loss: 0.8712478876113892\n",
            "Saving model, epoch: 20311, train_loss: 0.8974629640579224, val_loss: 0.8712466359138489\n",
            "Saving model, epoch: 20312, train_loss: 0.8974619507789612, val_loss: 0.8712465763092041\n",
            "Saving model, epoch: 20313, train_loss: 0.8974607586860657, val_loss: 0.8712446093559265\n",
            "Saving model, epoch: 20315, train_loss: 0.897458553314209, val_loss: 0.871242344379425\n",
            "Saving model, epoch: 20317, train_loss: 0.8974562883377075, val_loss: 0.8712400197982788\n",
            "Saving model, epoch: 20319, train_loss: 0.897454023361206, val_loss: 0.871237576007843\n",
            "Saving model, epoch: 20321, train_loss: 0.8974516987800598, val_loss: 0.8712353706359863\n",
            "Saving model, epoch: 20323, train_loss: 0.8974494934082031, val_loss: 0.8712332248687744\n",
            "Saving model, epoch: 20325, train_loss: 0.8974471688270569, val_loss: 0.8712303638458252\n",
            "Saving model, epoch: 20327, train_loss: 0.8974449634552002, val_loss: 0.8712272644042969\n",
            "Saving model, epoch: 20329, train_loss: 0.897442638874054, val_loss: 0.8712237477302551\n",
            "Saving model, epoch: 20331, train_loss: 0.8974404335021973, val_loss: 0.8712198138237\n",
            "Saving model, epoch: 20333, train_loss: 0.8974382281303406, val_loss: 0.8712155818939209\n",
            "Saving model, epoch: 20335, train_loss: 0.8974360227584839, val_loss: 0.8712119460105896\n",
            "Saving model, epoch: 20337, train_loss: 0.8974339365959167, val_loss: 0.8712098002433777\n",
            "Saving model, epoch: 20359, train_loss: 0.897409200668335, val_loss: 0.8712069392204285\n",
            "Saving model, epoch: 20361, train_loss: 0.8974069952964783, val_loss: 0.8712047934532166\n",
            "Saving model, epoch: 20363, train_loss: 0.897404670715332, val_loss: 0.871204137802124\n",
            "Saving model, epoch: 20370, train_loss: 0.8973970413208008, val_loss: 0.8712008595466614\n",
            "Saving model, epoch: 20372, train_loss: 0.8973947167396545, val_loss: 0.8711987733840942\n",
            "Saving model, epoch: 20374, train_loss: 0.8973925113677979, val_loss: 0.871197521686554\n",
            "Saving model, epoch: 20376, train_loss: 0.8973901867866516, val_loss: 0.8711963891983032\n",
            "Saving model, epoch: 20378, train_loss: 0.8973879814147949, val_loss: 0.8711958527565002\n",
            "Saving model, epoch: 20380, train_loss: 0.8973857760429382, val_loss: 0.8711954951286316\n",
            "Saving model, epoch: 20381, train_loss: 0.8973847031593323, val_loss: 0.8711942434310913\n",
            "Saving model, epoch: 20383, train_loss: 0.8973825573921204, val_loss: 0.8711921572685242\n",
            "Saving model, epoch: 20385, train_loss: 0.8973803520202637, val_loss: 0.8711905479431152\n",
            "Saving model, epoch: 20387, train_loss: 0.8973780274391174, val_loss: 0.8711893558502197\n",
            "Saving model, epoch: 20389, train_loss: 0.897375762462616, val_loss: 0.8711885213851929\n",
            "Saving model, epoch: 20390, train_loss: 0.8973747491836548, val_loss: 0.8711880445480347\n",
            "Saving model, epoch: 20391, train_loss: 0.897373616695404, val_loss: 0.8711873888969421\n",
            "Saving model, epoch: 20392, train_loss: 0.8973725438117981, val_loss: 0.8711861968040466\n",
            "Saving model, epoch: 20394, train_loss: 0.8973703384399414, val_loss: 0.8711844682693481\n",
            "Saving model, epoch: 20396, train_loss: 0.8973680138587952, val_loss: 0.8711829781532288\n",
            "Saving model, epoch: 20398, train_loss: 0.8973658680915833, val_loss: 0.8711816668510437\n",
            "Saving model, epoch: 20400, train_loss: 0.8973636031150818, val_loss: 0.8711802363395691\n",
            "epoch: 20401, train_loss: 0.897362470626831, val_loss: 0.8711801767349243\n",
            "Saving model, epoch: 20401, train_loss: 0.897362470626831, val_loss: 0.8711801767349243\n",
            "Saving model, epoch: 20402, train_loss: 0.8973614573478699, val_loss: 0.8711790442466736\n",
            "Saving model, epoch: 20403, train_loss: 0.8973602652549744, val_loss: 0.8711786866188049\n",
            "Saving model, epoch: 20404, train_loss: 0.8973592519760132, val_loss: 0.8711773753166199\n",
            "Saving model, epoch: 20405, train_loss: 0.8973580598831177, val_loss: 0.8711770176887512\n",
            "Saving model, epoch: 20406, train_loss: 0.8973569273948669, val_loss: 0.8711759448051453\n",
            "Saving model, epoch: 20407, train_loss: 0.897355854511261, val_loss: 0.8711756467819214\n",
            "Saving model, epoch: 20408, train_loss: 0.8973548412322998, val_loss: 0.8711743354797363\n",
            "Saving model, epoch: 20410, train_loss: 0.8973526358604431, val_loss: 0.8711729049682617\n",
            "Saving model, epoch: 20411, train_loss: 0.8973515033721924, val_loss: 0.8711726069450378\n",
            "Saving model, epoch: 20412, train_loss: 0.8973501920700073, val_loss: 0.8711713552474976\n",
            "Saving model, epoch: 20413, train_loss: 0.8973492980003357, val_loss: 0.8711711764335632\n",
            "Saving model, epoch: 20414, train_loss: 0.8973481059074402, val_loss: 0.871169924736023\n",
            "Saving model, epoch: 20415, train_loss: 0.8973470330238342, val_loss: 0.8711696267127991\n",
            "Saving model, epoch: 20416, train_loss: 0.897346019744873, val_loss: 0.8711684346199036\n",
            "Saving model, epoch: 20417, train_loss: 0.897344708442688, val_loss: 0.8711681962013245\n",
            "Saving model, epoch: 20418, train_loss: 0.8973436951637268, val_loss: 0.871167004108429\n",
            "Saving model, epoch: 20419, train_loss: 0.8973425626754761, val_loss: 0.8711668848991394\n",
            "Saving model, epoch: 20420, train_loss: 0.8973414897918701, val_loss: 0.8711655139923096\n",
            "Saving model, epoch: 20422, train_loss: 0.8973391652107239, val_loss: 0.8711640238761902\n",
            "Saving model, epoch: 20423, train_loss: 0.8973381519317627, val_loss: 0.8711636662483215\n",
            "Saving model, epoch: 20424, train_loss: 0.8973370790481567, val_loss: 0.8711626529693604\n",
            "Saving model, epoch: 20425, train_loss: 0.897335946559906, val_loss: 0.8711619973182678\n",
            "Saving model, epoch: 20426, train_loss: 0.8973349332809448, val_loss: 0.8711613416671753\n",
            "Saving model, epoch: 20427, train_loss: 0.8973337411880493, val_loss: 0.8711603879928589\n",
            "Saving model, epoch: 20428, train_loss: 0.8973326086997986, val_loss: 0.8711597919464111\n",
            "Saving model, epoch: 20429, train_loss: 0.8973315358161926, val_loss: 0.871159017086029\n",
            "Saving model, epoch: 20430, train_loss: 0.8973303437232971, val_loss: 0.8711585402488708\n",
            "Saving model, epoch: 20431, train_loss: 0.8973293304443359, val_loss: 0.8711574077606201\n",
            "Saving model, epoch: 20432, train_loss: 0.8973281979560852, val_loss: 0.8711569309234619\n",
            "Saving model, epoch: 20433, train_loss: 0.8973271250724792, val_loss: 0.8711561560630798\n",
            "Saving model, epoch: 20434, train_loss: 0.8973259925842285, val_loss: 0.8711555004119873\n",
            "Saving model, epoch: 20435, train_loss: 0.8973249197006226, val_loss: 0.8711544871330261\n",
            "Saving model, epoch: 20436, train_loss: 0.8973236680030823, val_loss: 0.8711539506912231\n",
            "Saving model, epoch: 20437, train_loss: 0.8973227143287659, val_loss: 0.8711532950401306\n",
            "Saving model, epoch: 20438, train_loss: 0.8973215818405151, val_loss: 0.8711525201797485\n",
            "Saving model, epoch: 20439, train_loss: 0.8973205089569092, val_loss: 0.8711518049240112\n",
            "Saving model, epoch: 20440, train_loss: 0.8973192572593689, val_loss: 0.8711509108543396\n",
            "Saving model, epoch: 20441, train_loss: 0.8973182439804077, val_loss: 0.8711503148078918\n",
            "Saving model, epoch: 20442, train_loss: 0.8973172903060913, val_loss: 0.8711493611335754\n",
            "Saving model, epoch: 20443, train_loss: 0.897316038608551, val_loss: 0.8711490035057068\n",
            "Saving model, epoch: 20444, train_loss: 0.8973149657249451, val_loss: 0.871147871017456\n",
            "Saving model, epoch: 20445, train_loss: 0.8973138332366943, val_loss: 0.8711473345756531\n",
            "Saving model, epoch: 20446, train_loss: 0.8973126411437988, val_loss: 0.8711465001106262\n",
            "Saving model, epoch: 20447, train_loss: 0.8973116278648376, val_loss: 0.8711457848548889\n",
            "Saving model, epoch: 20448, train_loss: 0.8973106145858765, val_loss: 0.8711451888084412\n",
            "Saving model, epoch: 20449, train_loss: 0.8973095417022705, val_loss: 0.8711441159248352\n",
            "Saving model, epoch: 20450, train_loss: 0.8973084092140198, val_loss: 0.8711439967155457\n",
            "Saving model, epoch: 20451, train_loss: 0.8973073363304138, val_loss: 0.8711421489715576\n",
            "Saving model, epoch: 20453, train_loss: 0.8973050117492676, val_loss: 0.8711397051811218\n",
            "Saving model, epoch: 20455, train_loss: 0.8973028063774109, val_loss: 0.8711370229721069\n",
            "Saving model, epoch: 20457, train_loss: 0.8973006010055542, val_loss: 0.8711342215538025\n",
            "Saving model, epoch: 20459, train_loss: 0.8972984552383423, val_loss: 0.8711303472518921\n",
            "Saving model, epoch: 20461, train_loss: 0.8972962498664856, val_loss: 0.8711252212524414\n",
            "Saving model, epoch: 20463, train_loss: 0.8972941637039185, val_loss: 0.8711184859275818\n",
            "Saving model, epoch: 20465, train_loss: 0.8972919583320618, val_loss: 0.8711101412773132\n",
            "Saving model, epoch: 20467, train_loss: 0.8972899317741394, val_loss: 0.8711018562316895\n",
            "Saving model, epoch: 20469, train_loss: 0.8972877264022827, val_loss: 0.8710989952087402\n",
            "epoch: 20501, train_loss: 0.8972529768943787, val_loss: 0.8711090683937073\n",
            "Saving model, epoch: 20511, train_loss: 0.8972421884536743, val_loss: 0.8710968494415283\n",
            "Saving model, epoch: 20516, train_loss: 0.8972368836402893, val_loss: 0.8710955381393433\n",
            "Saving model, epoch: 20518, train_loss: 0.8972348570823669, val_loss: 0.8710930943489075\n",
            "Saving model, epoch: 20520, train_loss: 0.8972325921058655, val_loss: 0.8710924983024597\n",
            "Saving model, epoch: 20523, train_loss: 0.8972293734550476, val_loss: 0.8710913062095642\n",
            "Saving model, epoch: 20525, train_loss: 0.8972272276878357, val_loss: 0.8710892200469971\n",
            "Saving model, epoch: 20527, train_loss: 0.897225022315979, val_loss: 0.8710883855819702\n",
            "Saving model, epoch: 20529, train_loss: 0.8972228169441223, val_loss: 0.8710880279541016\n",
            "Saving model, epoch: 20530, train_loss: 0.8972218036651611, val_loss: 0.8710862398147583\n",
            "Saving model, epoch: 20532, train_loss: 0.897219717502594, val_loss: 0.8710846304893494\n",
            "Saving model, epoch: 20534, train_loss: 0.8972175717353821, val_loss: 0.8710839152336121\n",
            "Saving model, epoch: 20535, train_loss: 0.8972166180610657, val_loss: 0.8710830807685852\n",
            "Saving model, epoch: 20537, train_loss: 0.897214412689209, val_loss: 0.8710812330245972\n",
            "Saving model, epoch: 20539, train_loss: 0.8972122669219971, val_loss: 0.8710799217224121\n",
            "Saving model, epoch: 20541, train_loss: 0.8972100615501404, val_loss: 0.8710793852806091\n",
            "Saving model, epoch: 20542, train_loss: 0.8972090482711792, val_loss: 0.8710780143737793\n",
            "Saving model, epoch: 20544, train_loss: 0.8972069621086121, val_loss: 0.8710765242576599\n",
            "Saving model, epoch: 20546, train_loss: 0.8972047567367554, val_loss: 0.8710753321647644\n",
            "Saving model, epoch: 20547, train_loss: 0.8972037434577942, val_loss: 0.8710749745368958\n",
            "Saving model, epoch: 20548, train_loss: 0.8972025513648987, val_loss: 0.8710743188858032\n",
            "Saving model, epoch: 20549, train_loss: 0.8972015380859375, val_loss: 0.8710731863975525\n",
            "Saving model, epoch: 20551, train_loss: 0.8971993923187256, val_loss: 0.8710716962814331\n",
            "Saving model, epoch: 20552, train_loss: 0.8971983194351196, val_loss: 0.8710715174674988\n",
            "Saving model, epoch: 20553, train_loss: 0.8971973061561584, val_loss: 0.8710706233978271\n",
            "Saving model, epoch: 20554, train_loss: 0.8971961736679077, val_loss: 0.8710697889328003\n",
            "Saving model, epoch: 20555, train_loss: 0.8971951007843018, val_loss: 0.8710693120956421\n",
            "Saving model, epoch: 20556, train_loss: 0.8971940875053406, val_loss: 0.8710684180259705\n",
            "Saving model, epoch: 20557, train_loss: 0.8971930742263794, val_loss: 0.8710678815841675\n",
            "Saving model, epoch: 20558, train_loss: 0.8971918821334839, val_loss: 0.8710669279098511\n",
            "Saving model, epoch: 20559, train_loss: 0.8971908688545227, val_loss: 0.8710664510726929\n",
            "Saving model, epoch: 20560, train_loss: 0.8971897959709167, val_loss: 0.8710660338401794\n",
            "Saving model, epoch: 20561, train_loss: 0.897188663482666, val_loss: 0.8710649013519287\n",
            "Saving model, epoch: 20562, train_loss: 0.8971876502037048, val_loss: 0.8710644841194153\n",
            "Saving model, epoch: 20563, train_loss: 0.8971865773200989, val_loss: 0.8710633516311646\n",
            "Saving model, epoch: 20564, train_loss: 0.8971854448318481, val_loss: 0.8710627555847168\n",
            "Saving model, epoch: 20565, train_loss: 0.8971845507621765, val_loss: 0.8710620999336243\n",
            "Saving model, epoch: 20566, train_loss: 0.8971834182739258, val_loss: 0.8710613250732422\n",
            "Saving model, epoch: 20567, train_loss: 0.8971822261810303, val_loss: 0.8710607290267944\n",
            "Saving model, epoch: 20568, train_loss: 0.8971812129020691, val_loss: 0.8710598945617676\n",
            "Saving model, epoch: 20569, train_loss: 0.8971801996231079, val_loss: 0.8710592985153198\n",
            "Saving model, epoch: 20570, train_loss: 0.8971790075302124, val_loss: 0.871058464050293\n",
            "Saving model, epoch: 20571, train_loss: 0.8971779942512512, val_loss: 0.8710578083992004\n",
            "Saving model, epoch: 20572, train_loss: 0.89717698097229, val_loss: 0.871057391166687\n",
            "Saving model, epoch: 20573, train_loss: 0.8971759080886841, val_loss: 0.8710565567016602\n",
            "Saving model, epoch: 20574, train_loss: 0.8971747756004333, val_loss: 0.8710559606552124\n",
            "Saving model, epoch: 20575, train_loss: 0.8971738219261169, val_loss: 0.8710550665855408\n",
            "Saving model, epoch: 20576, train_loss: 0.8971728086471558, val_loss: 0.8710546493530273\n",
            "Saving model, epoch: 20577, train_loss: 0.8971715569496155, val_loss: 0.8710536360740662\n",
            "Saving model, epoch: 20578, train_loss: 0.8971706032752991, val_loss: 0.8710527420043945\n",
            "Saving model, epoch: 20579, train_loss: 0.8971695899963379, val_loss: 0.8710523843765259\n",
            "Saving model, epoch: 20580, train_loss: 0.8971684575080872, val_loss: 0.8710514903068542\n",
            "Saving model, epoch: 20581, train_loss: 0.8971673846244812, val_loss: 0.8710511326789856\n",
            "Saving model, epoch: 20582, train_loss: 0.89716637134552, val_loss: 0.8710499405860901\n",
            "Saving model, epoch: 20583, train_loss: 0.8971652388572693, val_loss: 0.8710495829582214\n",
            "Saving model, epoch: 20584, train_loss: 0.8971641659736633, val_loss: 0.8710483908653259\n",
            "Saving model, epoch: 20585, train_loss: 0.8971631526947021, val_loss: 0.8710482120513916\n",
            "Saving model, epoch: 20586, train_loss: 0.897162139415741, val_loss: 0.8710472583770752\n",
            "Saving model, epoch: 20587, train_loss: 0.8971611261367798, val_loss: 0.8710467219352722\n",
            "Saving model, epoch: 20588, train_loss: 0.8971599340438843, val_loss: 0.8710457682609558\n",
            "Saving model, epoch: 20589, train_loss: 0.8971589207649231, val_loss: 0.8710449934005737\n",
            "Saving model, epoch: 20590, train_loss: 0.8971578478813171, val_loss: 0.8710444569587708\n",
            "Saving model, epoch: 20591, train_loss: 0.8971567153930664, val_loss: 0.871043860912323\n",
            "Saving model, epoch: 20592, train_loss: 0.8971557021141052, val_loss: 0.871042788028717\n",
            "Saving model, epoch: 20594, train_loss: 0.8971536159515381, val_loss: 0.8710411787033081\n",
            "Saving model, epoch: 20596, train_loss: 0.8971514701843262, val_loss: 0.8710399866104126\n",
            "Saving model, epoch: 20597, train_loss: 0.8971502780914307, val_loss: 0.8710398077964783\n",
            "Saving model, epoch: 20598, train_loss: 0.8971492648124695, val_loss: 0.8710389137268066\n",
            "Saving model, epoch: 20599, train_loss: 0.8971482515335083, val_loss: 0.8710382580757141\n",
            "Saving model, epoch: 20600, train_loss: 0.8971470594406128, val_loss: 0.871037483215332\n",
            "epoch: 20601, train_loss: 0.8971460461616516, val_loss: 0.8710364103317261\n",
            "Saving model, epoch: 20601, train_loss: 0.8971460461616516, val_loss: 0.8710364103317261\n",
            "Saving model, epoch: 20602, train_loss: 0.8971449732780457, val_loss: 0.8710363507270813\n",
            "Saving model, epoch: 20603, train_loss: 0.897144079208374, val_loss: 0.8710348606109619\n",
            "Saving model, epoch: 20605, train_loss: 0.8971418738365173, val_loss: 0.8710337281227112\n",
            "Saving model, epoch: 20606, train_loss: 0.8971408605575562, val_loss: 0.8710331916809082\n",
            "Saving model, epoch: 20607, train_loss: 0.8971396088600159, val_loss: 0.8710324764251709\n",
            "Saving model, epoch: 20608, train_loss: 0.8971386551856995, val_loss: 0.8710315823554993\n",
            "Saving model, epoch: 20609, train_loss: 0.8971376419067383, val_loss: 0.8710314631462097\n",
            "Saving model, epoch: 20610, train_loss: 0.8971365094184875, val_loss: 0.8710298538208008\n",
            "Saving model, epoch: 20612, train_loss: 0.8971344232559204, val_loss: 0.87102872133255\n",
            "Saving model, epoch: 20613, train_loss: 0.8971334099769592, val_loss: 0.8710283637046814\n",
            "Saving model, epoch: 20614, train_loss: 0.8971322774887085, val_loss: 0.8710275888442993\n",
            "Saving model, epoch: 20615, train_loss: 0.8971312046051025, val_loss: 0.8710265755653381\n",
            "Saving model, epoch: 20617, train_loss: 0.8971290588378906, val_loss: 0.8710249662399292\n",
            "Saving model, epoch: 20619, train_loss: 0.8971269726753235, val_loss: 0.8710231781005859\n",
            "Saving model, epoch: 20621, train_loss: 0.8971247673034668, val_loss: 0.8710212707519531\n",
            "Saving model, epoch: 20623, train_loss: 0.8971227407455444, val_loss: 0.8710193634033203\n",
            "Saving model, epoch: 20625, train_loss: 0.8971205353736877, val_loss: 0.8710178732872009\n",
            "Saving model, epoch: 20627, train_loss: 0.8971185088157654, val_loss: 0.8710167407989502\n",
            "Saving model, epoch: 20629, train_loss: 0.8971163034439087, val_loss: 0.8710156679153442\n",
            "Saving model, epoch: 20631, train_loss: 0.8971142172813416, val_loss: 0.8710144758224487\n",
            "Saving model, epoch: 20633, train_loss: 0.8971121311187744, val_loss: 0.8710135817527771\n",
            "Saving model, epoch: 20634, train_loss: 0.8971109986305237, val_loss: 0.8710134625434875\n",
            "Saving model, epoch: 20635, train_loss: 0.897109866142273, val_loss: 0.8710132241249084\n",
            "Saving model, epoch: 20636, train_loss: 0.8971089124679565, val_loss: 0.8710116147994995\n",
            "Saving model, epoch: 20638, train_loss: 0.8971068859100342, val_loss: 0.8710096478462219\n",
            "Saving model, epoch: 20640, train_loss: 0.8971046805381775, val_loss: 0.871008038520813\n",
            "Saving model, epoch: 20642, train_loss: 0.8971026539802551, val_loss: 0.8710069060325623\n",
            "Saving model, epoch: 20644, train_loss: 0.8971004486083984, val_loss: 0.8710058927536011\n",
            "Saving model, epoch: 20646, train_loss: 0.8970983624458313, val_loss: 0.8710048794746399\n",
            "Saving model, epoch: 20647, train_loss: 0.8970972299575806, val_loss: 0.871004581451416\n",
            "Saving model, epoch: 20648, train_loss: 0.8970962166786194, val_loss: 0.8710035085678101\n",
            "Saving model, epoch: 20649, train_loss: 0.8970951437950134, val_loss: 0.871002733707428\n",
            "Saving model, epoch: 20650, train_loss: 0.8970941305160522, val_loss: 0.8710024952888489\n",
            "Saving model, epoch: 20651, train_loss: 0.8970929980278015, val_loss: 0.8710014820098877\n",
            "Saving model, epoch: 20652, train_loss: 0.8970919847488403, val_loss: 0.8710013031959534\n",
            "Saving model, epoch: 20653, train_loss: 0.8970907926559448, val_loss: 0.870999813079834\n",
            "Saving model, epoch: 20655, train_loss: 0.8970888257026672, val_loss: 0.8709983229637146\n",
            "Saving model, epoch: 20657, train_loss: 0.8970865607261658, val_loss: 0.8709967732429504\n",
            "Saving model, epoch: 20659, train_loss: 0.8970845937728882, val_loss: 0.8709953427314758\n",
            "Saving model, epoch: 20661, train_loss: 0.8970824480056763, val_loss: 0.8709940314292908\n",
            "Saving model, epoch: 20663, train_loss: 0.8970803618431091, val_loss: 0.8709924221038818\n",
            "Saving model, epoch: 20665, train_loss: 0.897078275680542, val_loss: 0.8709909915924072\n",
            "Saving model, epoch: 20667, train_loss: 0.8970760107040405, val_loss: 0.8709899187088013\n",
            "Saving model, epoch: 20668, train_loss: 0.8970749378204346, val_loss: 0.8709898591041565\n",
            "Saving model, epoch: 20669, train_loss: 0.8970739245414734, val_loss: 0.8709888458251953\n",
            "Saving model, epoch: 20670, train_loss: 0.8970729112625122, val_loss: 0.8709882497787476\n",
            "Saving model, epoch: 20671, train_loss: 0.8970718383789062, val_loss: 0.8709877133369446\n",
            "Saving model, epoch: 20672, train_loss: 0.8970707058906555, val_loss: 0.8709867000579834\n",
            "Saving model, epoch: 20674, train_loss: 0.8970686793327332, val_loss: 0.8709850907325745\n",
            "Saving model, epoch: 20676, train_loss: 0.897066593170166, val_loss: 0.8709833025932312\n",
            "Saving model, epoch: 20678, train_loss: 0.8970643877983093, val_loss: 0.8709813356399536\n",
            "Saving model, epoch: 20680, train_loss: 0.897062361240387, val_loss: 0.870979368686676\n",
            "Saving model, epoch: 20682, train_loss: 0.8970602750778198, val_loss: 0.8709771037101746\n",
            "Saving model, epoch: 20684, train_loss: 0.8970582485198975, val_loss: 0.8709746599197388\n",
            "Saving model, epoch: 20686, train_loss: 0.8970560431480408, val_loss: 0.8709714412689209\n",
            "Saving model, epoch: 20688, train_loss: 0.8970540165901184, val_loss: 0.8709678053855896\n",
            "Saving model, epoch: 20690, train_loss: 0.8970518112182617, val_loss: 0.8709629774093628\n",
            "Saving model, epoch: 20692, train_loss: 0.8970498442649841, val_loss: 0.8709571957588196\n",
            "Saving model, epoch: 20694, train_loss: 0.8970476984977722, val_loss: 0.8709510564804077\n",
            "Saving model, epoch: 20696, train_loss: 0.8970456719398499, val_loss: 0.8709463477134705\n",
            "Saving model, epoch: 20698, train_loss: 0.8970435857772827, val_loss: 0.8709458708763123\n",
            "epoch: 20701, train_loss: 0.8970403671264648, val_loss: 0.870977520942688\n",
            "Saving model, epoch: 20725, train_loss: 0.8970154523849487, val_loss: 0.8709455728530884\n",
            "Saving model, epoch: 20732, train_loss: 0.8970081210136414, val_loss: 0.870943009853363\n",
            "Saving model, epoch: 20734, train_loss: 0.8970060348510742, val_loss: 0.8709412813186646\n",
            "Saving model, epoch: 20736, train_loss: 0.8970040082931519, val_loss: 0.870941162109375\n",
            "Saving model, epoch: 20739, train_loss: 0.8970009088516235, val_loss: 0.8709410429000854\n",
            "Saving model, epoch: 20741, train_loss: 0.8969987630844116, val_loss: 0.8709381818771362\n",
            "Saving model, epoch: 20743, train_loss: 0.896996796131134, val_loss: 0.870936393737793\n",
            "Saving model, epoch: 20745, train_loss: 0.8969947695732117, val_loss: 0.8709353804588318\n",
            "Saving model, epoch: 20747, train_loss: 0.8969926238059998, val_loss: 0.8709352612495422\n",
            "Saving model, epoch: 20750, train_loss: 0.8969895243644714, val_loss: 0.8709336519241333\n",
            "Saving model, epoch: 20752, train_loss: 0.8969874978065491, val_loss: 0.8709313869476318\n",
            "Saving model, epoch: 20754, train_loss: 0.8969852924346924, val_loss: 0.8709294199943542\n",
            "Saving model, epoch: 20756, train_loss: 0.8969833850860596, val_loss: 0.8709284067153931\n",
            "Saving model, epoch: 20758, train_loss: 0.8969812989234924, val_loss: 0.8709279298782349\n",
            "Saving model, epoch: 20760, train_loss: 0.8969792127609253, val_loss: 0.8709269762039185\n",
            "Saving model, epoch: 20762, train_loss: 0.8969771862030029, val_loss: 0.8709262013435364\n",
            "Saving model, epoch: 20763, train_loss: 0.8969760537147522, val_loss: 0.8709254860877991\n",
            "Saving model, epoch: 20764, train_loss: 0.896975040435791, val_loss: 0.8709251880645752\n",
            "Saving model, epoch: 20765, train_loss: 0.8969740867614746, val_loss: 0.8709238171577454\n",
            "Saving model, epoch: 20767, train_loss: 0.8969720602035522, val_loss: 0.8709225058555603\n",
            "Saving model, epoch: 20769, train_loss: 0.8969698548316956, val_loss: 0.87092125415802\n",
            "Saving model, epoch: 20770, train_loss: 0.8969689607620239, val_loss: 0.8709208965301514\n",
            "Saving model, epoch: 20771, train_loss: 0.8969679474830627, val_loss: 0.8709202408790588\n",
            "Saving model, epoch: 20772, train_loss: 0.896966814994812, val_loss: 0.8709192276000977\n",
            "Saving model, epoch: 20773, train_loss: 0.8969658017158508, val_loss: 0.8709190487861633\n",
            "Saving model, epoch: 20774, train_loss: 0.8969647884368896, val_loss: 0.8709176778793335\n",
            "Saving model, epoch: 20776, train_loss: 0.8969627022743225, val_loss: 0.8709163069725037\n",
            "Saving model, epoch: 20778, train_loss: 0.8969606757164001, val_loss: 0.8709149360656738\n",
            "Saving model, epoch: 20780, train_loss: 0.896958589553833, val_loss: 0.8709135055541992\n",
            "Saving model, epoch: 20782, train_loss: 0.8969565629959106, val_loss: 0.8709124326705933\n",
            "Saving model, epoch: 20783, train_loss: 0.8969554901123047, val_loss: 0.8709120750427246\n",
            "Saving model, epoch: 20784, train_loss: 0.8969544768333435, val_loss: 0.8709116578102112\n",
            "Saving model, epoch: 20785, train_loss: 0.8969534635543823, val_loss: 0.8709103465080261\n",
            "Saving model, epoch: 20787, train_loss: 0.8969513773918152, val_loss: 0.8709090352058411\n",
            "Saving model, epoch: 20789, train_loss: 0.8969493508338928, val_loss: 0.8709079027175903\n",
            "Saving model, epoch: 20790, train_loss: 0.8969483375549316, val_loss: 0.8709077835083008\n",
            "Saving model, epoch: 20791, train_loss: 0.8969473242759705, val_loss: 0.8709065318107605\n",
            "Saving model, epoch: 20792, train_loss: 0.8969462513923645, val_loss: 0.8709062337875366\n",
            "Saving model, epoch: 20793, train_loss: 0.8969452977180481, val_loss: 0.8709052801132202\n",
            "Saving model, epoch: 20794, train_loss: 0.8969443440437317, val_loss: 0.8709044456481934\n",
            "Saving model, epoch: 20795, train_loss: 0.896943211555481, val_loss: 0.8709043264389038\n",
            "Saving model, epoch: 20796, train_loss: 0.896942138671875, val_loss: 0.8709028959274292\n",
            "Saving model, epoch: 20798, train_loss: 0.8969401121139526, val_loss: 0.8709019422531128\n",
            "Saving model, epoch: 20799, train_loss: 0.8969390988349915, val_loss: 0.8709016442298889\n",
            "Saving model, epoch: 20800, train_loss: 0.8969380855560303, val_loss: 0.8709007501602173\n",
            "epoch: 20801, train_loss: 0.8969368934631348, val_loss: 0.8708999752998352\n",
            "Saving model, epoch: 20801, train_loss: 0.8969368934631348, val_loss: 0.8708999752998352\n",
            "Saving model, epoch: 20802, train_loss: 0.8969359993934631, val_loss: 0.8708993792533875\n",
            "Saving model, epoch: 20803, train_loss: 0.8969348669052124, val_loss: 0.8708986639976501\n",
            "Saving model, epoch: 20804, train_loss: 0.8969338536262512, val_loss: 0.8708977103233337\n",
            "Saving model, epoch: 20805, train_loss: 0.89693284034729, val_loss: 0.8708972930908203\n",
            "Saving model, epoch: 20806, train_loss: 0.8969318866729736, val_loss: 0.8708965182304382\n",
            "Saving model, epoch: 20807, train_loss: 0.8969308733940125, val_loss: 0.8708959817886353\n",
            "Saving model, epoch: 20808, train_loss: 0.8969298601150513, val_loss: 0.8708952069282532\n",
            "Saving model, epoch: 20809, train_loss: 0.8969288468360901, val_loss: 0.8708947896957397\n",
            "Saving model, epoch: 20810, train_loss: 0.8969277739524841, val_loss: 0.8708937764167786\n",
            "Saving model, epoch: 20811, train_loss: 0.896926760673523, val_loss: 0.8708934783935547\n",
            "Saving model, epoch: 20812, train_loss: 0.8969256281852722, val_loss: 0.8708925247192383\n",
            "Saving model, epoch: 20813, train_loss: 0.8969247341156006, val_loss: 0.8708922266960144\n",
            "Saving model, epoch: 20814, train_loss: 0.8969236016273499, val_loss: 0.8708910346031189\n",
            "Saving model, epoch: 20815, train_loss: 0.8969226479530334, val_loss: 0.8708904981613159\n",
            "Saving model, epoch: 20816, train_loss: 0.8969216346740723, val_loss: 0.8708899617195129\n",
            "Saving model, epoch: 20817, train_loss: 0.8969206213951111, val_loss: 0.8708890676498413\n",
            "Saving model, epoch: 20818, train_loss: 0.8969196081161499, val_loss: 0.8708886504173279\n",
            "Saving model, epoch: 20819, train_loss: 0.8969185948371887, val_loss: 0.8708876967430115\n",
            "Saving model, epoch: 20820, train_loss: 0.8969174027442932, val_loss: 0.8708874583244324\n",
            "Saving model, epoch: 20821, train_loss: 0.8969165086746216, val_loss: 0.8708862066268921\n",
            "Saving model, epoch: 20822, train_loss: 0.8969153761863708, val_loss: 0.8708860278129578\n",
            "Saving model, epoch: 20823, train_loss: 0.8969143629074097, val_loss: 0.8708850145339966\n",
            "Saving model, epoch: 20824, train_loss: 0.8969133496284485, val_loss: 0.8708849549293518\n",
            "Saving model, epoch: 20825, train_loss: 0.8969123959541321, val_loss: 0.8708836436271667\n",
            "Saving model, epoch: 20826, train_loss: 0.8969113826751709, val_loss: 0.8708835244178772\n",
            "Saving model, epoch: 20827, train_loss: 0.8969102501869202, val_loss: 0.8708822727203369\n",
            "Saving model, epoch: 20828, train_loss: 0.8969093561172485, val_loss: 0.870881974697113\n",
            "Saving model, epoch: 20829, train_loss: 0.8969083428382874, val_loss: 0.8708810806274414\n",
            "Saving model, epoch: 20830, train_loss: 0.8969072699546814, val_loss: 0.8708805441856384\n",
            "Saving model, epoch: 20831, train_loss: 0.8969061374664307, val_loss: 0.8708796501159668\n",
            "Saving model, epoch: 20832, train_loss: 0.8969051241874695, val_loss: 0.8708791136741638\n",
            "Saving model, epoch: 20833, train_loss: 0.8969042301177979, val_loss: 0.8708786368370056\n",
            "Saving model, epoch: 20834, train_loss: 0.8969031572341919, val_loss: 0.8708776831626892\n",
            "Saving model, epoch: 20835, train_loss: 0.8969020247459412, val_loss: 0.8708772659301758\n",
            "Saving model, epoch: 20836, train_loss: 0.8969011306762695, val_loss: 0.8708763122558594\n",
            "Saving model, epoch: 20837, train_loss: 0.8969001173973083, val_loss: 0.870875895023346\n",
            "Saving model, epoch: 20838, train_loss: 0.8968991041183472, val_loss: 0.8708749413490295\n",
            "Saving model, epoch: 20839, train_loss: 0.8968980312347412, val_loss: 0.8708743453025818\n",
            "Saving model, epoch: 20840, train_loss: 0.8968968987464905, val_loss: 0.8708738684654236\n",
            "Saving model, epoch: 20841, train_loss: 0.8968961238861084, val_loss: 0.8708730936050415\n",
            "Saving model, epoch: 20842, train_loss: 0.8968948721885681, val_loss: 0.8708726763725281\n",
            "Saving model, epoch: 20843, train_loss: 0.8968938589096069, val_loss: 0.8708717823028564\n",
            "Saving model, epoch: 20844, train_loss: 0.8968929052352905, val_loss: 0.8708711862564087\n",
            "Saving model, epoch: 20845, train_loss: 0.8968918919563293, val_loss: 0.8708704113960266\n",
            "Saving model, epoch: 20846, train_loss: 0.8968908786773682, val_loss: 0.8708696365356445\n",
            "Saving model, epoch: 20847, train_loss: 0.8968897461891174, val_loss: 0.8708690404891968\n",
            "Saving model, epoch: 20848, train_loss: 0.8968888521194458, val_loss: 0.8708683848381042\n",
            "Saving model, epoch: 20849, train_loss: 0.8968877792358398, val_loss: 0.870867908000946\n",
            "Saving model, epoch: 20850, train_loss: 0.8968867659568787, val_loss: 0.8708670139312744\n",
            "Saving model, epoch: 20851, train_loss: 0.8968857526779175, val_loss: 0.8708666563034058\n",
            "Saving model, epoch: 20852, train_loss: 0.8968846201896667, val_loss: 0.8708658218383789\n",
            "Saving model, epoch: 20853, train_loss: 0.8968836665153503, val_loss: 0.8708652257919312\n",
            "Saving model, epoch: 20854, train_loss: 0.8968826532363892, val_loss: 0.8708645105361938\n",
            "Saving model, epoch: 20855, train_loss: 0.896881639957428, val_loss: 0.8708638548851013\n",
            "Saving model, epoch: 20856, train_loss: 0.8968806266784668, val_loss: 0.8708629608154297\n",
            "Saving model, epoch: 20858, train_loss: 0.8968786001205444, val_loss: 0.8708611726760864\n",
            "Saving model, epoch: 20860, train_loss: 0.8968766331672668, val_loss: 0.8708590865135193\n",
            "Saving model, epoch: 20862, train_loss: 0.8968746066093445, val_loss: 0.8708567023277283\n",
            "Saving model, epoch: 20864, train_loss: 0.8968724608421326, val_loss: 0.8708540797233582\n",
            "Saving model, epoch: 20866, train_loss: 0.8968703746795654, val_loss: 0.8708508610725403\n",
            "Saving model, epoch: 20868, train_loss: 0.8968683481216431, val_loss: 0.8708468675613403\n",
            "Saving model, epoch: 20870, train_loss: 0.8968663811683655, val_loss: 0.8708414435386658\n",
            "Saving model, epoch: 20872, train_loss: 0.8968643546104431, val_loss: 0.8708344101905823\n",
            "Saving model, epoch: 20874, train_loss: 0.8968624472618103, val_loss: 0.8708258867263794\n",
            "Saving model, epoch: 20876, train_loss: 0.8968605399131775, val_loss: 0.870819091796875\n",
            "epoch: 20901, train_loss: 0.896835207939148, val_loss: 0.8708292245864868\n",
            "Saving model, epoch: 20922, train_loss: 0.8968144059181213, val_loss: 0.8708189725875854\n",
            "Saving model, epoch: 20925, train_loss: 0.8968114256858826, val_loss: 0.8708184957504272\n",
            "Saving model, epoch: 20927, train_loss: 0.8968095183372498, val_loss: 0.8708160519599915\n",
            "Saving model, epoch: 20929, train_loss: 0.8968074917793274, val_loss: 0.8708152174949646\n",
            "Saving model, epoch: 20931, train_loss: 0.896805465221405, val_loss: 0.870815098285675\n",
            "Saving model, epoch: 20932, train_loss: 0.8968044519424438, val_loss: 0.8708148002624512\n",
            "Saving model, epoch: 20934, train_loss: 0.8968024849891663, val_loss: 0.8708122968673706\n",
            "Saving model, epoch: 20936, train_loss: 0.8968004584312439, val_loss: 0.8708108067512512\n",
            "Saving model, epoch: 20938, train_loss: 0.8967984318733215, val_loss: 0.8708105683326721\n",
            "Saving model, epoch: 20939, train_loss: 0.8967974185943604, val_loss: 0.8708103895187378\n",
            "Saving model, epoch: 20941, train_loss: 0.8967954516410828, val_loss: 0.8708080649375916\n",
            "Saving model, epoch: 20943, train_loss: 0.8967936038970947, val_loss: 0.8708066344261169\n",
            "Saving model, epoch: 20945, train_loss: 0.8967915177345276, val_loss: 0.8708059787750244\n",
            "Saving model, epoch: 20947, train_loss: 0.8967896103858948, val_loss: 0.8708055019378662\n",
            "Saving model, epoch: 20948, train_loss: 0.8967885971069336, val_loss: 0.8708038926124573\n",
            "Saving model, epoch: 20950, train_loss: 0.8967866897583008, val_loss: 0.8708027601242065\n",
            "Saving model, epoch: 20952, train_loss: 0.8967846035957336, val_loss: 0.8708019256591797\n",
            "Saving model, epoch: 20953, train_loss: 0.8967835903167725, val_loss: 0.8708013296127319\n",
            "Saving model, epoch: 20955, train_loss: 0.8967816829681396, val_loss: 0.8707994818687439\n",
            "Saving model, epoch: 20957, train_loss: 0.8967797756195068, val_loss: 0.8707982897758484\n",
            "Saving model, epoch: 20959, train_loss: 0.8967777490615845, val_loss: 0.870797336101532\n",
            "Saving model, epoch: 20960, train_loss: 0.8967767357826233, val_loss: 0.8707969784736633\n",
            "Saving model, epoch: 20961, train_loss: 0.8967757225036621, val_loss: 0.8707965612411499\n",
            "Saving model, epoch: 20962, train_loss: 0.8967747092247009, val_loss: 0.8707954287528992\n",
            "Saving model, epoch: 20964, train_loss: 0.8967727422714233, val_loss: 0.8707942962646484\n",
            "Saving model, epoch: 20965, train_loss: 0.8967717885971069, val_loss: 0.870793879032135\n",
            "Saving model, epoch: 20966, train_loss: 0.8967708349227905, val_loss: 0.8707931041717529\n",
            "Saving model, epoch: 20967, train_loss: 0.8967698216438293, val_loss: 0.8707923889160156\n",
            "Saving model, epoch: 20968, train_loss: 0.8967688083648682, val_loss: 0.8707920908927917\n",
            "Saving model, epoch: 20969, train_loss: 0.896767795085907, val_loss: 0.8707910180091858\n",
            "Saving model, epoch: 20970, train_loss: 0.8967669010162354, val_loss: 0.870790958404541\n",
            "Saving model, epoch: 20971, train_loss: 0.8967658877372742, val_loss: 0.8707898259162903\n",
            "Saving model, epoch: 20972, train_loss: 0.896764874458313, val_loss: 0.8707895874977112\n",
            "Saving model, epoch: 20973, train_loss: 0.8967638611793518, val_loss: 0.8707886934280396\n",
            "Saving model, epoch: 20974, train_loss: 0.8967628479003906, val_loss: 0.870788037776947\n",
            "Saving model, epoch: 20975, train_loss: 0.8967618942260742, val_loss: 0.8707877397537231\n",
            "Saving model, epoch: 20976, train_loss: 0.896760880947113, val_loss: 0.8707866072654724\n",
            "Saving model, epoch: 20977, train_loss: 0.8967598676681519, val_loss: 0.8707864284515381\n",
            "Saving model, epoch: 20978, train_loss: 0.8967588543891907, val_loss: 0.870785117149353\n",
            "Saving model, epoch: 20980, train_loss: 0.8967569470405579, val_loss: 0.8707839846611023\n",
            "Saving model, epoch: 20981, train_loss: 0.8967558145523071, val_loss: 0.8707837462425232\n",
            "Saving model, epoch: 20982, train_loss: 0.8967549204826355, val_loss: 0.8707828521728516\n",
            "Saving model, epoch: 20983, train_loss: 0.8967540264129639, val_loss: 0.870782196521759\n",
            "Saving model, epoch: 20984, train_loss: 0.8967530131340027, val_loss: 0.8707819581031799\n",
            "Saving model, epoch: 20985, train_loss: 0.8967519998550415, val_loss: 0.8707806468009949\n",
            "Saving model, epoch: 20986, train_loss: 0.8967510461807251, val_loss: 0.8707804679870605\n",
            "Saving model, epoch: 20987, train_loss: 0.8967500329017639, val_loss: 0.8707795143127441\n",
            "Saving model, epoch: 20988, train_loss: 0.8967490196228027, val_loss: 0.8707793951034546\n",
            "Saving model, epoch: 20989, train_loss: 0.8967480063438416, val_loss: 0.8707783222198486\n",
            "Saving model, epoch: 20990, train_loss: 0.8967471122741699, val_loss: 0.8707779049873352\n",
            "Saving model, epoch: 20991, train_loss: 0.8967459797859192, val_loss: 0.8707771301269531\n",
            "Saving model, epoch: 20992, train_loss: 0.896744966506958, val_loss: 0.8707767724990845\n",
            "Saving model, epoch: 20993, train_loss: 0.8967440724372864, val_loss: 0.8707762360572815\n",
            "Saving model, epoch: 20994, train_loss: 0.8967431783676147, val_loss: 0.8707752227783203\n",
            "Saving model, epoch: 20995, train_loss: 0.8967421054840088, val_loss: 0.8707747459411621\n",
            "Saving model, epoch: 20996, train_loss: 0.8967411518096924, val_loss: 0.8707736730575562\n",
            "Saving model, epoch: 20998, train_loss: 0.8967391848564148, val_loss: 0.8707723021507263\n",
            "Saving model, epoch: 21000, train_loss: 0.8967371582984924, val_loss: 0.8707714080810547\n",
            "epoch: 21001, train_loss: 0.8967362642288208, val_loss: 0.8707706928253174\n",
            "Saving model, epoch: 21001, train_loss: 0.8967362642288208, val_loss: 0.8707706928253174\n",
            "Saving model, epoch: 21003, train_loss: 0.896734356880188, val_loss: 0.870769202709198\n",
            "Saving model, epoch: 21005, train_loss: 0.8967323303222656, val_loss: 0.8707677125930786\n",
            "Saving model, epoch: 21007, train_loss: 0.8967303037643433, val_loss: 0.8707666397094727\n",
            "Saving model, epoch: 21009, train_loss: 0.8967283368110657, val_loss: 0.8707654476165771\n",
            "Saving model, epoch: 21010, train_loss: 0.8967273235321045, val_loss: 0.8707652688026428\n",
            "Saving model, epoch: 21011, train_loss: 0.8967263102531433, val_loss: 0.8707645535469055\n",
            "Saving model, epoch: 21012, train_loss: 0.8967254161834717, val_loss: 0.8707637786865234\n",
            "Saving model, epoch: 21013, train_loss: 0.8967244029045105, val_loss: 0.8707634210586548\n",
            "Saving model, epoch: 21014, train_loss: 0.8967235088348389, val_loss: 0.8707625269889832\n",
            "Saving model, epoch: 21015, train_loss: 0.8967224955558777, val_loss: 0.8707621693611145\n",
            "Saving model, epoch: 21016, train_loss: 0.896721363067627, val_loss: 0.8707610964775085\n",
            "Saving model, epoch: 21017, train_loss: 0.8967204689979553, val_loss: 0.870760977268219\n",
            "Saving model, epoch: 21018, train_loss: 0.8967194557189941, val_loss: 0.8707598447799683\n",
            "Saving model, epoch: 21019, train_loss: 0.896718442440033, val_loss: 0.8707594871520996\n",
            "Saving model, epoch: 21020, train_loss: 0.8967175483703613, val_loss: 0.870758593082428\n",
            "Saving model, epoch: 21021, train_loss: 0.8967164754867554, val_loss: 0.8707581758499146\n",
            "Saving model, epoch: 21022, train_loss: 0.8967154622077942, val_loss: 0.8707574605941772\n",
            "Saving model, epoch: 21023, train_loss: 0.8967145681381226, val_loss: 0.8707569241523743\n",
            "Saving model, epoch: 21024, train_loss: 0.8967135548591614, val_loss: 0.8707562685012817\n",
            "Saving model, epoch: 21025, train_loss: 0.8967126607894897, val_loss: 0.8707553148269653\n",
            "Saving model, epoch: 21027, train_loss: 0.8967106342315674, val_loss: 0.8707535862922668\n",
            "Saving model, epoch: 21029, train_loss: 0.896708607673645, val_loss: 0.8707517385482788\n",
            "Saving model, epoch: 21031, train_loss: 0.8967066407203674, val_loss: 0.8707500696182251\n",
            "Saving model, epoch: 21033, train_loss: 0.8967046737670898, val_loss: 0.87074875831604\n",
            "Saving model, epoch: 21035, train_loss: 0.8967027068138123, val_loss: 0.8707476258277893\n",
            "Saving model, epoch: 21037, train_loss: 0.8967007994651794, val_loss: 0.8707467317581177\n",
            "Saving model, epoch: 21039, train_loss: 0.8966987729072571, val_loss: 0.8707459568977356\n",
            "Saving model, epoch: 21041, train_loss: 0.8966968655586243, val_loss: 0.8707456588745117\n",
            "Saving model, epoch: 21042, train_loss: 0.8966958522796631, val_loss: 0.8707444667816162\n",
            "Saving model, epoch: 21044, train_loss: 0.8966939449310303, val_loss: 0.8707430362701416\n",
            "Saving model, epoch: 21046, train_loss: 0.8966919183731079, val_loss: 0.8707419037818909\n",
            "Saving model, epoch: 21048, train_loss: 0.8966899514198303, val_loss: 0.870741069316864\n",
            "Saving model, epoch: 21049, train_loss: 0.8966889381408691, val_loss: 0.8707406520843506\n",
            "Saving model, epoch: 21050, train_loss: 0.896687924861908, val_loss: 0.8707399368286133\n",
            "Saving model, epoch: 21051, train_loss: 0.8966870307922363, val_loss: 0.8707391619682312\n",
            "Saving model, epoch: 21052, train_loss: 0.8966861367225647, val_loss: 0.8707391023635864\n",
            "Saving model, epoch: 21053, train_loss: 0.8966851234436035, val_loss: 0.8707376718521118\n",
            "Saving model, epoch: 21055, train_loss: 0.8966832160949707, val_loss: 0.8707363605499268\n",
            "Saving model, epoch: 21057, train_loss: 0.8966810703277588, val_loss: 0.8707353472709656\n",
            "Saving model, epoch: 21058, train_loss: 0.8966801166534424, val_loss: 0.8707349896430969\n",
            "Saving model, epoch: 21059, train_loss: 0.8966792821884155, val_loss: 0.8707340955734253\n",
            "Saving model, epoch: 21060, train_loss: 0.8966781497001648, val_loss: 0.8707337975502014\n",
            "Saving model, epoch: 21061, train_loss: 0.8966772556304932, val_loss: 0.8707330822944641\n",
            "Saving model, epoch: 21062, train_loss: 0.896676242351532, val_loss: 0.8707324862480164\n",
            "Saving model, epoch: 21063, train_loss: 0.8966752886772156, val_loss: 0.8707317113876343\n",
            "Saving model, epoch: 21064, train_loss: 0.8966742753982544, val_loss: 0.8707311153411865\n",
            "Saving model, epoch: 21065, train_loss: 0.8966733813285828, val_loss: 0.8707306981086731\n",
            "Saving model, epoch: 21066, train_loss: 0.896672248840332, val_loss: 0.8707299828529358\n",
            "Saving model, epoch: 21067, train_loss: 0.8966713547706604, val_loss: 0.8707292675971985\n",
            "Saving model, epoch: 21068, train_loss: 0.8966703414916992, val_loss: 0.8707288503646851\n",
            "Saving model, epoch: 21069, train_loss: 0.8966694474220276, val_loss: 0.870728075504303\n",
            "Saving model, epoch: 21070, train_loss: 0.8966684341430664, val_loss: 0.8707273602485657\n",
            "Saving model, epoch: 21071, train_loss: 0.8966674208641052, val_loss: 0.8707267642021179\n",
            "Saving model, epoch: 21072, train_loss: 0.896666407585144, val_loss: 0.8707260489463806\n",
            "Saving model, epoch: 21073, train_loss: 0.8966653943061829, val_loss: 0.8707256317138672\n",
            "Saving model, epoch: 21074, train_loss: 0.8966644406318665, val_loss: 0.8707249760627747\n",
            "Saving model, epoch: 21075, train_loss: 0.8966634273529053, val_loss: 0.8707242608070374\n",
            "Saving model, epoch: 21076, train_loss: 0.8966625332832336, val_loss: 0.8707237839698792\n",
            "Saving model, epoch: 21077, train_loss: 0.8966615796089172, val_loss: 0.8707228899002075\n",
            "Saving model, epoch: 21078, train_loss: 0.8966606259346008, val_loss: 0.870722770690918\n",
            "Saving model, epoch: 21079, train_loss: 0.8966596126556396, val_loss: 0.8707214593887329\n",
            "Saving model, epoch: 21081, train_loss: 0.8966575860977173, val_loss: 0.8707201480865479\n",
            "Saving model, epoch: 21083, train_loss: 0.8966555595397949, val_loss: 0.870718777179718\n",
            "Saving model, epoch: 21085, train_loss: 0.8966537714004517, val_loss: 0.8707175850868225\n",
            "Saving model, epoch: 21087, train_loss: 0.8966517448425293, val_loss: 0.8707159757614136\n",
            "Saving model, epoch: 21089, train_loss: 0.8966497182846069, val_loss: 0.8707149624824524\n",
            "Saving model, epoch: 21091, train_loss: 0.8966478109359741, val_loss: 0.8707135915756226\n",
            "Saving model, epoch: 21093, train_loss: 0.8966459035873413, val_loss: 0.8707123398780823\n",
            "Saving model, epoch: 21095, train_loss: 0.8966439962387085, val_loss: 0.8707107901573181\n",
            "Saving model, epoch: 21097, train_loss: 0.8966420292854309, val_loss: 0.8707089424133301\n",
            "Saving model, epoch: 21099, train_loss: 0.8966400027275085, val_loss: 0.8707063794136047\n",
            "epoch: 21101, train_loss: 0.8966380953788757, val_loss: 0.8707038760185242\n",
            "Saving model, epoch: 21101, train_loss: 0.8966380953788757, val_loss: 0.8707038760185242\n",
            "Saving model, epoch: 21103, train_loss: 0.8966361880302429, val_loss: 0.8707005977630615\n",
            "Saving model, epoch: 21105, train_loss: 0.8966341018676758, val_loss: 0.8706963658332825\n",
            "Saving model, epoch: 21107, train_loss: 0.8966322541236877, val_loss: 0.8706902265548706\n",
            "Saving model, epoch: 21109, train_loss: 0.8966304659843445, val_loss: 0.8706826567649841\n",
            "Saving model, epoch: 21111, train_loss: 0.8966284394264221, val_loss: 0.8706737160682678\n",
            "Saving model, epoch: 21113, train_loss: 0.8966267108917236, val_loss: 0.8706681728363037\n",
            "Saving model, epoch: 21166, train_loss: 0.8965759873390198, val_loss: 0.8706678748130798\n",
            "Saving model, epoch: 21169, train_loss: 0.8965730667114258, val_loss: 0.8706673979759216\n",
            "Saving model, epoch: 21171, train_loss: 0.896571159362793, val_loss: 0.8706653118133545\n",
            "Saving model, epoch: 21173, train_loss: 0.8965693712234497, val_loss: 0.8706642389297485\n",
            "Saving model, epoch: 21176, train_loss: 0.8965663313865662, val_loss: 0.8706632256507874\n",
            "Saving model, epoch: 21178, train_loss: 0.8965646624565125, val_loss: 0.8706616163253784\n",
            "Saving model, epoch: 21180, train_loss: 0.8965627551078796, val_loss: 0.8706607222557068\n",
            "Saving model, epoch: 21181, train_loss: 0.8965617418289185, val_loss: 0.8706604838371277\n",
            "Saving model, epoch: 21183, train_loss: 0.8965598344802856, val_loss: 0.8706589341163635\n",
            "Saving model, epoch: 21185, train_loss: 0.8965580463409424, val_loss: 0.8706575036048889\n",
            "Saving model, epoch: 21187, train_loss: 0.8965560793876648, val_loss: 0.8706569671630859\n",
            "Saving model, epoch: 21188, train_loss: 0.8965551257133484, val_loss: 0.8706564903259277\n",
            "Saving model, epoch: 21189, train_loss: 0.8965542912483215, val_loss: 0.8706559538841248\n",
            "Saving model, epoch: 21190, train_loss: 0.8965532779693604, val_loss: 0.870654821395874\n",
            "Saving model, epoch: 21192, train_loss: 0.8965513706207275, val_loss: 0.8706537485122681\n",
            "Saving model, epoch: 21193, train_loss: 0.8965504765510559, val_loss: 0.8706535696983337\n",
            "Saving model, epoch: 21194, train_loss: 0.8965494632720947, val_loss: 0.8706530332565308\n",
            "Saving model, epoch: 21195, train_loss: 0.8965485692024231, val_loss: 0.87065190076828\n",
            "Saving model, epoch: 21197, train_loss: 0.8965466618537903, val_loss: 0.8706506490707397\n",
            "Saving model, epoch: 21199, train_loss: 0.8965447545051575, val_loss: 0.8706498742103577\n",
            "Saving model, epoch: 21200, train_loss: 0.8965438604354858, val_loss: 0.8706494569778442\n",
            "epoch: 21201, train_loss: 0.8965429663658142, val_loss: 0.8706487417221069\n",
            "Saving model, epoch: 21201, train_loss: 0.8965429663658142, val_loss: 0.8706487417221069\n",
            "Saving model, epoch: 21202, train_loss: 0.896541953086853, val_loss: 0.8706479072570801\n",
            "Saving model, epoch: 21203, train_loss: 0.8965410590171814, val_loss: 0.8706478476524353\n",
            "Saving model, epoch: 21204, train_loss: 0.896540105342865, val_loss: 0.870646595954895\n",
            "Saving model, epoch: 21205, train_loss: 0.896539032459259, val_loss: 0.8706464767456055\n",
            "Saving model, epoch: 21206, train_loss: 0.8965381979942322, val_loss: 0.8706454038619995\n",
            "Saving model, epoch: 21207, train_loss: 0.8965372443199158, val_loss: 0.8706451058387756\n",
            "Saving model, epoch: 21208, train_loss: 0.8965362310409546, val_loss: 0.8706445097923279\n",
            "Saving model, epoch: 21209, train_loss: 0.896535336971283, val_loss: 0.870643675327301\n",
            "Saving model, epoch: 21210, train_loss: 0.8965343832969666, val_loss: 0.8706434965133667\n",
            "Saving model, epoch: 21211, train_loss: 0.8965334892272949, val_loss: 0.8706426620483398\n",
            "Saving model, epoch: 21212, train_loss: 0.8965325951576233, val_loss: 0.8706422448158264\n",
            "Saving model, epoch: 21213, train_loss: 0.8965315818786621, val_loss: 0.8706415295600891\n",
            "Saving model, epoch: 21214, train_loss: 0.8965305685997009, val_loss: 0.8706408143043518\n",
            "Saving model, epoch: 21215, train_loss: 0.8965297937393188, val_loss: 0.8706404566764832\n",
            "Saving model, epoch: 21216, train_loss: 0.8965287804603577, val_loss: 0.8706395030021667\n",
            "Saving model, epoch: 21217, train_loss: 0.8965277671813965, val_loss: 0.8706391453742981\n",
            "Saving model, epoch: 21218, train_loss: 0.8965267539024353, val_loss: 0.8706386089324951\n",
            "Saving model, epoch: 21219, train_loss: 0.8965259790420532, val_loss: 0.8706380724906921\n",
            "Saving model, epoch: 21220, train_loss: 0.896524965763092, val_loss: 0.8706373572349548\n",
            "Saving model, epoch: 21221, train_loss: 0.8965241312980652, val_loss: 0.8706365823745728\n",
            "Saving model, epoch: 21222, train_loss: 0.8965230584144592, val_loss: 0.8706361055374146\n",
            "Saving model, epoch: 21223, train_loss: 0.8965221643447876, val_loss: 0.8706357479095459\n",
            "Saving model, epoch: 21224, train_loss: 0.8965211510658264, val_loss: 0.8706348538398743\n",
            "Saving model, epoch: 21225, train_loss: 0.8965202569961548, val_loss: 0.8706346154212952\n",
            "Saving model, epoch: 21226, train_loss: 0.8965193629264832, val_loss: 0.8706336617469788\n",
            "Saving model, epoch: 21227, train_loss: 0.896518349647522, val_loss: 0.8706332445144653\n",
            "Saving model, epoch: 21228, train_loss: 0.8965175151824951, val_loss: 0.8706324696540833\n",
            "Saving model, epoch: 21229, train_loss: 0.8965165019035339, val_loss: 0.8706322312355042\n",
            "Saving model, epoch: 21230, train_loss: 0.8965155482292175, val_loss: 0.8706313371658325\n",
            "Saving model, epoch: 21231, train_loss: 0.8965145945549011, val_loss: 0.8706310987472534\n",
            "Saving model, epoch: 21232, train_loss: 0.8965135812759399, val_loss: 0.8706303238868713\n",
            "Saving model, epoch: 21233, train_loss: 0.8965126872062683, val_loss: 0.8706297278404236\n",
            "Saving model, epoch: 21234, train_loss: 0.8965117931365967, val_loss: 0.8706288933753967\n",
            "Saving model, epoch: 21235, train_loss: 0.896510899066925, val_loss: 0.8706285953521729\n",
            "Saving model, epoch: 21236, train_loss: 0.8965098857879639, val_loss: 0.8706278204917908\n",
            "Saving model, epoch: 21237, train_loss: 0.8965088725090027, val_loss: 0.8706274628639221\n",
            "Saving model, epoch: 21238, train_loss: 0.896507978439331, val_loss: 0.8706268072128296\n",
            "Saving model, epoch: 21239, train_loss: 0.8965070843696594, val_loss: 0.8706261515617371\n",
            "Saving model, epoch: 21240, train_loss: 0.8965061902999878, val_loss: 0.8706254363059998\n",
            "Saving model, epoch: 21241, train_loss: 0.8965051770210266, val_loss: 0.8706247210502625\n",
            "Saving model, epoch: 21242, train_loss: 0.896504282951355, val_loss: 0.870624303817749\n",
            "Saving model, epoch: 21243, train_loss: 0.8965032696723938, val_loss: 0.8706236481666565\n",
            "Saving model, epoch: 21244, train_loss: 0.8965024352073669, val_loss: 0.8706232309341431\n",
            "Saving model, epoch: 21245, train_loss: 0.8965014815330505, val_loss: 0.8706221580505371\n",
            "Saving model, epoch: 21247, train_loss: 0.896499514579773, val_loss: 0.8706210255622864\n",
            "Saving model, epoch: 21248, train_loss: 0.8964986205101013, val_loss: 0.8706209659576416\n",
            "Saving model, epoch: 21249, train_loss: 0.8964976072311401, val_loss: 0.8706198930740356\n",
            "Saving model, epoch: 21250, train_loss: 0.8964966535568237, val_loss: 0.8706197738647461\n",
            "Saving model, epoch: 21251, train_loss: 0.8964958190917969, val_loss: 0.8706188201904297\n",
            "Saving model, epoch: 21252, train_loss: 0.8964948058128357, val_loss: 0.8706185221672058\n",
            "Saving model, epoch: 21253, train_loss: 0.8964940309524536, val_loss: 0.870617687702179\n",
            "Saving model, epoch: 21254, train_loss: 0.8964930176734924, val_loss: 0.8706173300743103\n",
            "Saving model, epoch: 21255, train_loss: 0.8964920043945312, val_loss: 0.870616614818573\n",
            "Saving model, epoch: 21256, train_loss: 0.8964909911155701, val_loss: 0.8706157803535461\n",
            "Saving model, epoch: 21257, train_loss: 0.896490216255188, val_loss: 0.870615541934967\n",
            "Saving model, epoch: 21258, train_loss: 0.8964892029762268, val_loss: 0.870614767074585\n",
            "Saving model, epoch: 21259, train_loss: 0.8964883089065552, val_loss: 0.8706144094467163\n",
            "Saving model, epoch: 21260, train_loss: 0.8964874148368835, val_loss: 0.8706136345863342\n",
            "Saving model, epoch: 21261, train_loss: 0.8964864015579224, val_loss: 0.8706132769584656\n",
            "Saving model, epoch: 21262, train_loss: 0.8964855074882507, val_loss: 0.8706125617027283\n",
            "Saving model, epoch: 21263, train_loss: 0.8964845538139343, val_loss: 0.8706116080284119\n",
            "Saving model, epoch: 21264, train_loss: 0.8964835405349731, val_loss: 0.8706114292144775\n",
            "Saving model, epoch: 21265, train_loss: 0.8964826464653015, val_loss: 0.870610237121582\n",
            "Saving model, epoch: 21267, train_loss: 0.8964807391166687, val_loss: 0.8706087470054626\n",
            "Saving model, epoch: 21269, train_loss: 0.8964788317680359, val_loss: 0.8706073760986328\n",
            "Saving model, epoch: 21271, train_loss: 0.8964769244194031, val_loss: 0.8706057071685791\n",
            "Saving model, epoch: 21273, train_loss: 0.8964750170707703, val_loss: 0.8706038594245911\n",
            "Saving model, epoch: 21275, train_loss: 0.8964733481407166, val_loss: 0.8706024289131165\n",
            "Saving model, epoch: 21277, train_loss: 0.8964713215827942, val_loss: 0.8706014752388\n",
            "Saving model, epoch: 21279, train_loss: 0.8964695334434509, val_loss: 0.8706004023551941\n",
            "Saving model, epoch: 21281, train_loss: 0.8964675664901733, val_loss: 0.8705995082855225\n",
            "Saving model, epoch: 21283, train_loss: 0.8964656591415405, val_loss: 0.8705988526344299\n",
            "Saving model, epoch: 21285, train_loss: 0.8964638710021973, val_loss: 0.8705981373786926\n",
            "Saving model, epoch: 21287, train_loss: 0.8964619636535645, val_loss: 0.8705980777740479\n",
            "Saving model, epoch: 21288, train_loss: 0.8964610695838928, val_loss: 0.8705965280532837\n",
            "Saving model, epoch: 21290, train_loss: 0.8964590430259705, val_loss: 0.8705950379371643\n",
            "Saving model, epoch: 21292, train_loss: 0.8964572548866272, val_loss: 0.8705937266349792\n",
            "Saving model, epoch: 21294, train_loss: 0.8964554667472839, val_loss: 0.8705925941467285\n",
            "Saving model, epoch: 21296, train_loss: 0.8964534997940063, val_loss: 0.8705913424491882\n",
            "Saving model, epoch: 21298, train_loss: 0.8964515924453735, val_loss: 0.8705905079841614\n",
            "Saving model, epoch: 21300, train_loss: 0.8964498043060303, val_loss: 0.8705898523330688\n",
            "epoch: 21301, train_loss: 0.8964489102363586, val_loss: 0.870589554309845\n",
            "Saving model, epoch: 21301, train_loss: 0.8964489102363586, val_loss: 0.870589554309845\n",
            "Saving model, epoch: 21302, train_loss: 0.8964478969573975, val_loss: 0.8705889582633972\n",
            "Saving model, epoch: 21303, train_loss: 0.8964470028877258, val_loss: 0.8705880641937256\n",
            "Saving model, epoch: 21305, train_loss: 0.8964452147483826, val_loss: 0.8705864548683167\n",
            "Saving model, epoch: 21307, train_loss: 0.8964431881904602, val_loss: 0.8705853223800659\n",
            "Saving model, epoch: 21309, train_loss: 0.8964414000511169, val_loss: 0.8705839514732361\n",
            "Saving model, epoch: 21311, train_loss: 0.8964395523071289, val_loss: 0.8705827593803406\n",
            "Saving model, epoch: 21313, train_loss: 0.8964375257492065, val_loss: 0.8705816268920898\n",
            "Saving model, epoch: 21315, train_loss: 0.8964357376098633, val_loss: 0.8705804944038391\n",
            "Saving model, epoch: 21317, train_loss: 0.8964338302612305, val_loss: 0.8705791234970093\n",
            "Saving model, epoch: 21319, train_loss: 0.8964320421218872, val_loss: 0.8705779910087585\n",
            "Saving model, epoch: 21321, train_loss: 0.8964301347732544, val_loss: 0.8705766797065735\n",
            "Saving model, epoch: 21323, train_loss: 0.8964282274246216, val_loss: 0.8705757260322571\n",
            "Saving model, epoch: 21325, train_loss: 0.8964263796806335, val_loss: 0.870574414730072\n",
            "Saving model, epoch: 21327, train_loss: 0.8964244723320007, val_loss: 0.8705735206604004\n",
            "Saving model, epoch: 21329, train_loss: 0.8964226841926575, val_loss: 0.8705726265907288\n",
            "Saving model, epoch: 21331, train_loss: 0.8964206576347351, val_loss: 0.8705716729164124\n",
            "Saving model, epoch: 21332, train_loss: 0.896419882774353, val_loss: 0.8705711364746094\n",
            "Saving model, epoch: 21333, train_loss: 0.8964188694953918, val_loss: 0.8705707788467407\n",
            "Saving model, epoch: 21334, train_loss: 0.8964179754257202, val_loss: 0.8705699443817139\n",
            "Saving model, epoch: 21335, train_loss: 0.8964170813560486, val_loss: 0.8705698251724243\n",
            "Saving model, epoch: 21336, train_loss: 0.8964161276817322, val_loss: 0.8705686330795288\n",
            "Saving model, epoch: 21338, train_loss: 0.8964142203330994, val_loss: 0.8705672025680542\n",
            "Saving model, epoch: 21340, train_loss: 0.8964124321937561, val_loss: 0.8705654740333557\n",
            "Saving model, epoch: 21342, train_loss: 0.8964105248451233, val_loss: 0.8705636262893677\n",
            "Saving model, epoch: 21344, train_loss: 0.8964086174964905, val_loss: 0.8705615401268005\n",
            "Saving model, epoch: 21346, train_loss: 0.8964068293571472, val_loss: 0.8705588579177856\n",
            "Saving model, epoch: 21348, train_loss: 0.8964049220085144, val_loss: 0.8705549240112305\n",
            "Saving model, epoch: 21350, train_loss: 0.8964031934738159, val_loss: 0.8705495595932007\n",
            "Saving model, epoch: 21352, train_loss: 0.8964011669158936, val_loss: 0.8705428838729858\n",
            "Saving model, epoch: 21354, train_loss: 0.8963994979858398, val_loss: 0.8705340027809143\n",
            "Saving model, epoch: 21356, train_loss: 0.8963978886604309, val_loss: 0.8705247640609741\n",
            "Saving model, epoch: 21358, train_loss: 0.8963959813117981, val_loss: 0.8705214858055115\n",
            "epoch: 21401, train_loss: 0.8963567614555359, val_loss: 0.8705343008041382\n",
            "Saving model, epoch: 21422, train_loss: 0.8963378667831421, val_loss: 0.8705213665962219\n",
            "Saving model, epoch: 21424, train_loss: 0.8963361978530884, val_loss: 0.8705195784568787\n",
            "Saving model, epoch: 21426, train_loss: 0.8963342905044556, val_loss: 0.8705185651779175\n",
            "Saving model, epoch: 21428, train_loss: 0.8963324427604675, val_loss: 0.8705184459686279\n",
            "Saving model, epoch: 21429, train_loss: 0.8963316679000854, val_loss: 0.8705174922943115\n",
            "Saving model, epoch: 21431, train_loss: 0.8963298797607422, val_loss: 0.8705159425735474\n",
            "Saving model, epoch: 21433, train_loss: 0.8963280320167542, val_loss: 0.8705154657363892\n",
            "Saving model, epoch: 21434, train_loss: 0.8963271379470825, val_loss: 0.8705147504806519\n",
            "Saving model, epoch: 21436, train_loss: 0.896325409412384, val_loss: 0.870513379573822\n",
            "Saving model, epoch: 21438, train_loss: 0.8963236212730408, val_loss: 0.8705124855041504\n",
            "Saving model, epoch: 21439, train_loss: 0.8963226079940796, val_loss: 0.8705121874809265\n",
            "Saving model, epoch: 21440, train_loss: 0.896321713924408, val_loss: 0.8705115914344788\n",
            "Saving model, epoch: 21441, train_loss: 0.8963208198547363, val_loss: 0.8705110549926758\n",
            "Saving model, epoch: 21442, train_loss: 0.8963199853897095, val_loss: 0.870510995388031\n",
            "Saving model, epoch: 21443, train_loss: 0.8963189721107483, val_loss: 0.8705095052719116\n",
            "Saving model, epoch: 21445, train_loss: 0.896317183971405, val_loss: 0.8705088496208191\n",
            "Saving model, epoch: 21446, train_loss: 0.896316409111023, val_loss: 0.8705081343650818\n",
            "Saving model, epoch: 21447, train_loss: 0.8963153958320618, val_loss: 0.8705078959465027\n",
            "Saving model, epoch: 21448, train_loss: 0.8963145613670349, val_loss: 0.870506763458252\n",
            "Saving model, epoch: 21450, train_loss: 0.8963127732276917, val_loss: 0.8705057501792908\n",
            "Saving model, epoch: 21451, train_loss: 0.8963117599487305, val_loss: 0.8705055713653564\n",
            "Saving model, epoch: 21452, train_loss: 0.8963109850883484, val_loss: 0.8705050945281982\n",
            "Saving model, epoch: 21453, train_loss: 0.896310031414032, val_loss: 0.8705042600631714\n",
            "Saving model, epoch: 21454, train_loss: 0.8963091373443604, val_loss: 0.8705041408538818\n",
            "Saving model, epoch: 21455, train_loss: 0.8963083624839783, val_loss: 0.8705030679702759\n",
            "Saving model, epoch: 21456, train_loss: 0.8963073492050171, val_loss: 0.8705029487609863\n",
            "Saving model, epoch: 21457, train_loss: 0.8963064551353455, val_loss: 0.870502233505249\n",
            "Saving model, epoch: 21458, train_loss: 0.8963056206703186, val_loss: 0.8705015778541565\n",
            "Saving model, epoch: 21459, train_loss: 0.896304726600647, val_loss: 0.8705013394355774\n",
            "Saving model, epoch: 21460, train_loss: 0.8963037133216858, val_loss: 0.8705001473426819\n",
            "Saving model, epoch: 21462, train_loss: 0.8963019251823425, val_loss: 0.8704995512962341\n",
            "Saving model, epoch: 21463, train_loss: 0.8963010311126709, val_loss: 0.8704989552497864\n",
            "Saving model, epoch: 21464, train_loss: 0.896300196647644, val_loss: 0.8704986572265625\n",
            "Saving model, epoch: 21465, train_loss: 0.8962993025779724, val_loss: 0.8704975247383118\n",
            "Saving model, epoch: 21467, train_loss: 0.8962975144386292, val_loss: 0.8704965710639954\n",
            "Saving model, epoch: 21468, train_loss: 0.8962966799736023, val_loss: 0.8704963326454163\n",
            "Saving model, epoch: 21469, train_loss: 0.8962957859039307, val_loss: 0.8704953193664551\n",
            "Saving model, epoch: 21470, train_loss: 0.896294891834259, val_loss: 0.8704950213432312\n",
            "Saving model, epoch: 21471, train_loss: 0.8962938785552979, val_loss: 0.870494544506073\n",
            "Saving model, epoch: 21472, train_loss: 0.8962931036949158, val_loss: 0.8704941272735596\n",
            "Saving model, epoch: 21473, train_loss: 0.8962921500205994, val_loss: 0.8704933524131775\n",
            "Saving model, epoch: 21474, train_loss: 0.8962912559509277, val_loss: 0.8704927563667297\n",
            "Saving model, epoch: 21475, train_loss: 0.8962902426719666, val_loss: 0.8704923987388611\n",
            "Saving model, epoch: 21476, train_loss: 0.8962894678115845, val_loss: 0.870491623878479\n",
            "Saving model, epoch: 21477, train_loss: 0.8962886333465576, val_loss: 0.8704912662506104\n",
            "Saving model, epoch: 21478, train_loss: 0.8962876796722412, val_loss: 0.870490550994873\n",
            "Saving model, epoch: 21479, train_loss: 0.8962867259979248, val_loss: 0.870490312576294\n",
            "Saving model, epoch: 21480, train_loss: 0.8962858319282532, val_loss: 0.8704891204833984\n",
            "Saving model, epoch: 21482, train_loss: 0.8962840437889099, val_loss: 0.8704882264137268\n",
            "Saving model, epoch: 21483, train_loss: 0.8962832093238831, val_loss: 0.8704879879951477\n",
            "Saving model, epoch: 21484, train_loss: 0.8962823152542114, val_loss: 0.8704872727394104\n",
            "Saving model, epoch: 21485, train_loss: 0.8962814211845398, val_loss: 0.8704869151115417\n",
            "Saving model, epoch: 21486, train_loss: 0.8962806463241577, val_loss: 0.8704861402511597\n",
            "Saving model, epoch: 21487, train_loss: 0.8962796330451965, val_loss: 0.8704855442047119\n",
            "Saving model, epoch: 21488, train_loss: 0.8962787985801697, val_loss: 0.8704853653907776\n",
            "Saving model, epoch: 21489, train_loss: 0.8962777853012085, val_loss: 0.8704844117164612\n",
            "Saving model, epoch: 21490, train_loss: 0.8962768912315369, val_loss: 0.8704842329025269\n",
            "Saving model, epoch: 21491, train_loss: 0.8962759971618652, val_loss: 0.8704831600189209\n",
            "Saving model, epoch: 21492, train_loss: 0.8962752223014832, val_loss: 0.8704831004142761\n",
            "Saving model, epoch: 21493, train_loss: 0.896274209022522, val_loss: 0.8704821467399597\n",
            "Saving model, epoch: 21494, train_loss: 0.8962733745574951, val_loss: 0.870481550693512\n",
            "Saving model, epoch: 21495, train_loss: 0.8962723612785339, val_loss: 0.8704814910888672\n",
            "Saving model, epoch: 21496, train_loss: 0.8962715864181519, val_loss: 0.8704804182052612\n",
            "Saving model, epoch: 21498, train_loss: 0.8962697982788086, val_loss: 0.8704791069030762\n",
            "Saving model, epoch: 21500, train_loss: 0.8962679505348206, val_loss: 0.8704782724380493\n",
            "epoch: 21501, train_loss: 0.8962669372558594, val_loss: 0.870478093624115\n",
            "Saving model, epoch: 21501, train_loss: 0.8962669372558594, val_loss: 0.870478093624115\n",
            "Saving model, epoch: 21502, train_loss: 0.8962661623954773, val_loss: 0.8704776167869568\n",
            "Saving model, epoch: 21503, train_loss: 0.8962652683258057, val_loss: 0.8704767823219299\n",
            "Saving model, epoch: 21504, train_loss: 0.896264374256134, val_loss: 0.8704763650894165\n",
            "Saving model, epoch: 21505, train_loss: 0.8962635397911072, val_loss: 0.870475709438324\n",
            "Saving model, epoch: 21506, train_loss: 0.8962626457214355, val_loss: 0.8704753518104553\n",
            "Saving model, epoch: 21507, train_loss: 0.8962617516517639, val_loss: 0.8704747557640076\n",
            "Saving model, epoch: 21508, train_loss: 0.8962607383728027, val_loss: 0.8704741597175598\n",
            "Saving model, epoch: 21509, train_loss: 0.8962599039077759, val_loss: 0.8704736828804016\n",
            "Saving model, epoch: 21510, train_loss: 0.8962588906288147, val_loss: 0.8704728484153748\n",
            "Saving model, epoch: 21511, train_loss: 0.8962581157684326, val_loss: 0.8704724311828613\n",
            "Saving model, epoch: 21512, train_loss: 0.8962573409080505, val_loss: 0.8704717755317688\n",
            "Saving model, epoch: 21513, train_loss: 0.8962563276290894, val_loss: 0.8704714179039001\n",
            "Saving model, epoch: 21514, train_loss: 0.8962554931640625, val_loss: 0.8704705834388733\n",
            "Saving model, epoch: 21515, train_loss: 0.8962545990943909, val_loss: 0.8704702854156494\n",
            "Saving model, epoch: 21516, train_loss: 0.8962537050247192, val_loss: 0.8704696297645569\n",
            "Saving model, epoch: 21517, train_loss: 0.8962528109550476, val_loss: 0.8704689741134644\n",
            "Saving model, epoch: 21518, train_loss: 0.8962518572807312, val_loss: 0.8704686760902405\n",
            "Saving model, epoch: 21519, train_loss: 0.8962510824203491, val_loss: 0.8704679608345032\n",
            "Saving model, epoch: 21520, train_loss: 0.8962501883506775, val_loss: 0.8704676032066345\n",
            "Saving model, epoch: 21521, train_loss: 0.8962492942810059, val_loss: 0.8704668879508972\n",
            "Saving model, epoch: 21522, train_loss: 0.896248459815979, val_loss: 0.870466411113739\n",
            "Saving model, epoch: 21523, train_loss: 0.8962474465370178, val_loss: 0.8704661130905151\n",
            "Saving model, epoch: 21524, train_loss: 0.8962465524673462, val_loss: 0.8704651594161987\n",
            "Saving model, epoch: 21526, train_loss: 0.8962448835372925, val_loss: 0.8704637289047241\n",
            "Saving model, epoch: 21528, train_loss: 0.8962429165840149, val_loss: 0.8704620003700256\n",
            "Saving model, epoch: 21530, train_loss: 0.8962411284446716, val_loss: 0.870460569858551\n",
            "Saving model, epoch: 21532, train_loss: 0.8962393999099731, val_loss: 0.8704589009284973\n",
            "Saving model, epoch: 21534, train_loss: 0.8962376117706299, val_loss: 0.8704579472541809\n",
            "Saving model, epoch: 21536, train_loss: 0.8962358832359314, val_loss: 0.8704571723937988\n",
            "Saving model, epoch: 21538, train_loss: 0.8962340950965881, val_loss: 0.8704563975334167\n",
            "Saving model, epoch: 21540, train_loss: 0.8962323665618896, val_loss: 0.8704555034637451\n",
            "Saving model, epoch: 21542, train_loss: 0.8962305784225464, val_loss: 0.8704552054405212\n",
            "Saving model, epoch: 21543, train_loss: 0.8962296843528748, val_loss: 0.8704550266265869\n",
            "Saving model, epoch: 21544, train_loss: 0.8962287902832031, val_loss: 0.8704546689987183\n",
            "Saving model, epoch: 21545, train_loss: 0.8962278962135315, val_loss: 0.8704535365104675\n",
            "Saving model, epoch: 21547, train_loss: 0.8962259888648987, val_loss: 0.8704522252082825\n",
            "Saving model, epoch: 21549, train_loss: 0.8962243795394897, val_loss: 0.8704509735107422\n",
            "Saving model, epoch: 21551, train_loss: 0.8962225317955017, val_loss: 0.8704498410224915\n",
            "Saving model, epoch: 21553, train_loss: 0.8962207436561584, val_loss: 0.8704491853713989\n",
            "Saving model, epoch: 21554, train_loss: 0.8962199091911316, val_loss: 0.8704490065574646\n",
            "Saving model, epoch: 21555, train_loss: 0.8962189555168152, val_loss: 0.8704485297203064\n",
            "Saving model, epoch: 21556, train_loss: 0.8962181210517883, val_loss: 0.87044757604599\n",
            "Saving model, epoch: 21558, train_loss: 0.8962162137031555, val_loss: 0.8704464435577393\n",
            "Saving model, epoch: 21560, train_loss: 0.896214485168457, val_loss: 0.8704448938369751\n",
            "Saving model, epoch: 21562, train_loss: 0.8962126970291138, val_loss: 0.8704438209533691\n",
            "Saving model, epoch: 21564, train_loss: 0.8962109088897705, val_loss: 0.8704430460929871\n",
            "Saving model, epoch: 21566, train_loss: 0.8962092995643616, val_loss: 0.8704419136047363\n",
            "Saving model, epoch: 21568, train_loss: 0.8962073922157288, val_loss: 0.8704411387443542\n",
            "Saving model, epoch: 21570, train_loss: 0.8962056636810303, val_loss: 0.8704400658607483\n",
            "Saving model, epoch: 21571, train_loss: 0.8962046504020691, val_loss: 0.8704397678375244\n",
            "Saving model, epoch: 21572, train_loss: 0.896203875541687, val_loss: 0.8704391717910767\n",
            "Saving model, epoch: 21573, train_loss: 0.8962029814720154, val_loss: 0.8704384565353394\n",
            "Saving model, epoch: 21574, train_loss: 0.896202027797699, val_loss: 0.870438277721405\n",
            "Saving model, epoch: 21575, train_loss: 0.8962012529373169, val_loss: 0.8704370260238647\n",
            "Saving model, epoch: 21577, train_loss: 0.8961992263793945, val_loss: 0.8704361319541931\n",
            "Saving model, epoch: 21579, train_loss: 0.8961976170539856, val_loss: 0.8704349398612976\n",
            "Saving model, epoch: 21581, train_loss: 0.8961958289146423, val_loss: 0.8704337477684021\n",
            "Saving model, epoch: 21583, train_loss: 0.8961940407752991, val_loss: 0.8704326152801514\n",
            "Saving model, epoch: 21585, train_loss: 0.896192193031311, val_loss: 0.870431661605835\n",
            "Saving model, epoch: 21587, train_loss: 0.8961905837059021, val_loss: 0.8704307079315186\n",
            "Saving model, epoch: 21589, train_loss: 0.8961887955665588, val_loss: 0.8704299926757812\n",
            "Saving model, epoch: 21590, train_loss: 0.8961879014968872, val_loss: 0.8704294562339783\n",
            "Saving model, epoch: 21591, train_loss: 0.8961870074272156, val_loss: 0.8704291582107544\n",
            "Saving model, epoch: 21592, train_loss: 0.8961861729621887, val_loss: 0.8704279661178589\n",
            "Saving model, epoch: 21594, train_loss: 0.8961843848228455, val_loss: 0.8704264163970947\n",
            "Saving model, epoch: 21596, train_loss: 0.8961825370788574, val_loss: 0.8704248070716858\n",
            "Saving model, epoch: 21598, train_loss: 0.8961807489395142, val_loss: 0.8704225420951843\n",
            "Saving model, epoch: 21600, train_loss: 0.8961789608001709, val_loss: 0.8704202175140381\n",
            "epoch: 21601, train_loss: 0.896178126335144, val_loss: 0.8704284429550171\n",
            "Saving model, epoch: 21602, train_loss: 0.896177351474762, val_loss: 0.8704173564910889\n",
            "Saving model, epoch: 21604, train_loss: 0.8961755037307739, val_loss: 0.870413064956665\n",
            "Saving model, epoch: 21606, train_loss: 0.8961737155914307, val_loss: 0.8704075813293457\n",
            "Saving model, epoch: 21608, train_loss: 0.8961719274520874, val_loss: 0.8703999519348145\n",
            "Saving model, epoch: 21610, train_loss: 0.8961703181266785, val_loss: 0.8703919649124146\n",
            "Saving model, epoch: 21612, train_loss: 0.89616858959198, val_loss: 0.870386004447937\n",
            "Saving model, epoch: 21674, train_loss: 0.8961149454116821, val_loss: 0.8703852295875549\n",
            "Saving model, epoch: 21676, train_loss: 0.8961132168769836, val_loss: 0.870384156703949\n",
            "Saving model, epoch: 21678, train_loss: 0.8961116075515747, val_loss: 0.8703836798667908\n",
            "Saving model, epoch: 21680, train_loss: 0.896109938621521, val_loss: 0.8703830242156982\n",
            "Saving model, epoch: 21681, train_loss: 0.8961090445518494, val_loss: 0.8703827857971191\n",
            "Saving model, epoch: 21682, train_loss: 0.8961082100868225, val_loss: 0.8703817129135132\n",
            "Saving model, epoch: 21684, train_loss: 0.8961064219474792, val_loss: 0.8703805208206177\n",
            "Saving model, epoch: 21686, train_loss: 0.8961048126220703, val_loss: 0.8703790903091431\n",
            "Saving model, epoch: 21688, train_loss: 0.8961029648780823, val_loss: 0.8703782558441162\n",
            "Saving model, epoch: 21690, train_loss: 0.8961012959480286, val_loss: 0.8703773021697998\n",
            "Saving model, epoch: 21692, train_loss: 0.8960995674133301, val_loss: 0.8703765869140625\n",
            "Saving model, epoch: 21694, train_loss: 0.8960978388786316, val_loss: 0.8703756928443909\n",
            "Saving model, epoch: 21696, train_loss: 0.8960961699485779, val_loss: 0.8703744411468506\n",
            "Saving model, epoch: 21698, train_loss: 0.8960943222045898, val_loss: 0.8703733682632446\n",
            "Saving model, epoch: 21700, train_loss: 0.8960927724838257, val_loss: 0.8703721761703491\n",
            "epoch: 21701, train_loss: 0.8960918188095093, val_loss: 0.8703733086585999\n",
            "Saving model, epoch: 21702, train_loss: 0.8960910439491272, val_loss: 0.8703708052635193\n",
            "Saving model, epoch: 21704, train_loss: 0.8960891366004944, val_loss: 0.8703696727752686\n",
            "Saving model, epoch: 21706, train_loss: 0.8960877060890198, val_loss: 0.8703688383102417\n",
            "Saving model, epoch: 21708, train_loss: 0.8960859179496765, val_loss: 0.8703677654266357\n",
            "Saving model, epoch: 21710, train_loss: 0.8960841298103333, val_loss: 0.8703670501708984\n",
            "Saving model, epoch: 21712, train_loss: 0.8960825204849243, val_loss: 0.870366096496582\n",
            "Saving model, epoch: 21714, train_loss: 0.8960807919502258, val_loss: 0.8703654408454895\n",
            "Saving model, epoch: 21715, train_loss: 0.8960797786712646, val_loss: 0.8703653216362\n",
            "Saving model, epoch: 21716, train_loss: 0.8960790634155273, val_loss: 0.870364785194397\n",
            "Saving model, epoch: 21717, train_loss: 0.8960782885551453, val_loss: 0.8703637719154358\n",
            "Saving model, epoch: 21719, train_loss: 0.8960764408111572, val_loss: 0.8703625798225403\n",
            "Saving model, epoch: 21721, train_loss: 0.896074652671814, val_loss: 0.8703615665435791\n",
            "Saving model, epoch: 21723, train_loss: 0.896073043346405, val_loss: 0.8703606128692627\n",
            "Saving model, epoch: 21725, train_loss: 0.8960712552070618, val_loss: 0.8703603148460388\n",
            "Saving model, epoch: 21726, train_loss: 0.8960704207420349, val_loss: 0.8703594207763672\n",
            "Saving model, epoch: 21728, train_loss: 0.896068811416626, val_loss: 0.8703577518463135\n",
            "Saving model, epoch: 21730, train_loss: 0.8960670232772827, val_loss: 0.8703566789627075\n",
            "Saving model, epoch: 21732, train_loss: 0.8960652947425842, val_loss: 0.870356023311615\n",
            "Saving model, epoch: 21734, train_loss: 0.8960636258125305, val_loss: 0.8703554272651672\n",
            "Saving model, epoch: 21735, train_loss: 0.8960627913475037, val_loss: 0.8703547716140747\n",
            "Saving model, epoch: 21737, train_loss: 0.89606112241745, val_loss: 0.8703534007072449\n",
            "Saving model, epoch: 21739, train_loss: 0.8960593938827515, val_loss: 0.8703523278236389\n",
            "Saving model, epoch: 21741, train_loss: 0.8960576057434082, val_loss: 0.8703514933586121\n",
            "Saving model, epoch: 21743, train_loss: 0.8960559964179993, val_loss: 0.8703503608703613\n",
            "Saving model, epoch: 21745, train_loss: 0.8960541486740112, val_loss: 0.8703495860099792\n",
            "Saving model, epoch: 21746, train_loss: 0.8960533738136292, val_loss: 0.8703493475914001\n",
            "Saving model, epoch: 21747, train_loss: 0.8960524797439575, val_loss: 0.8703488707542419\n",
            "Saving model, epoch: 21748, train_loss: 0.8960517644882202, val_loss: 0.8703478574752808\n",
            "Saving model, epoch: 21750, train_loss: 0.8960499167442322, val_loss: 0.8703467845916748\n",
            "Saving model, epoch: 21752, train_loss: 0.8960483074188232, val_loss: 0.8703457117080688\n",
            "Saving model, epoch: 21754, train_loss: 0.89604651927948, val_loss: 0.8703450560569763\n",
            "Saving model, epoch: 21755, train_loss: 0.8960457444190979, val_loss: 0.8703448176383972\n",
            "Saving model, epoch: 21756, train_loss: 0.8960447907447815, val_loss: 0.8703441023826599\n",
            "Saving model, epoch: 21757, train_loss: 0.8960438966751099, val_loss: 0.8703435659408569\n",
            "Saving model, epoch: 21758, train_loss: 0.8960431218147278, val_loss: 0.8703433275222778\n",
            "Saving model, epoch: 21759, train_loss: 0.8960422873497009, val_loss: 0.8703424334526062\n",
            "Saving model, epoch: 21760, train_loss: 0.8960413932800293, val_loss: 0.8703421950340271\n",
            "Saving model, epoch: 21761, train_loss: 0.8960404992103577, val_loss: 0.8703410625457764\n",
            "Saving model, epoch: 21763, train_loss: 0.8960388898849487, val_loss: 0.8703400492668152\n",
            "Saving model, epoch: 21765, train_loss: 0.8960371613502502, val_loss: 0.870339035987854\n",
            "Saving model, epoch: 21767, train_loss: 0.8960354924201965, val_loss: 0.8703380227088928\n",
            "Saving model, epoch: 21769, train_loss: 0.896033763885498, val_loss: 0.870337188243866\n",
            "Saving model, epoch: 21770, train_loss: 0.8960328698158264, val_loss: 0.8703370690345764\n",
            "Saving model, epoch: 21771, train_loss: 0.8960320353507996, val_loss: 0.87033611536026\n",
            "Saving model, epoch: 21773, train_loss: 0.8960303664207458, val_loss: 0.8703351020812988\n",
            "Saving model, epoch: 21775, train_loss: 0.8960286378860474, val_loss: 0.8703340291976929\n",
            "Saving model, epoch: 21777, train_loss: 0.8960268497467041, val_loss: 0.8703330755233765\n",
            "Saving model, epoch: 21779, train_loss: 0.8960252404212952, val_loss: 0.8703321218490601\n",
            "Saving model, epoch: 21780, train_loss: 0.8960244059562683, val_loss: 0.8703320026397705\n",
            "Saving model, epoch: 21781, train_loss: 0.8960233926773071, val_loss: 0.8703312873840332\n",
            "Saving model, epoch: 21782, train_loss: 0.8960227370262146, val_loss: 0.8703309297561646\n",
            "Saving model, epoch: 21783, train_loss: 0.8960217833518982, val_loss: 0.870330274105072\n",
            "Saving model, epoch: 21784, train_loss: 0.8960210084915161, val_loss: 0.8703298568725586\n",
            "Saving model, epoch: 21785, train_loss: 0.8960201144218445, val_loss: 0.8703295588493347\n",
            "Saving model, epoch: 21786, train_loss: 0.8960193991661072, val_loss: 0.8703285455703735\n",
            "Saving model, epoch: 21787, train_loss: 0.896018385887146, val_loss: 0.8703283667564392\n",
            "Saving model, epoch: 21788, train_loss: 0.8960176110267639, val_loss: 0.8703277707099915\n",
            "Saving model, epoch: 21789, train_loss: 0.8960167169570923, val_loss: 0.8703276515007019\n",
            "Saving model, epoch: 21790, train_loss: 0.8960157632827759, val_loss: 0.8703266382217407\n",
            "Saving model, epoch: 21791, train_loss: 0.8960151076316833, val_loss: 0.8703263401985168\n",
            "Saving model, epoch: 21792, train_loss: 0.8960141539573669, val_loss: 0.8703257441520691\n",
            "Saving model, epoch: 21793, train_loss: 0.8960132598876953, val_loss: 0.8703250885009766\n",
            "Saving model, epoch: 21794, train_loss: 0.8960123658180237, val_loss: 0.8703246712684631\n",
            "Saving model, epoch: 21795, train_loss: 0.8960115909576416, val_loss: 0.8703240752220154\n",
            "Saving model, epoch: 21796, train_loss: 0.8960107564926147, val_loss: 0.8703237175941467\n",
            "Saving model, epoch: 21797, train_loss: 0.8960098624229431, val_loss: 0.8703230619430542\n",
            "Saving model, epoch: 21799, train_loss: 0.8960081338882446, val_loss: 0.8703222274780273\n",
            "Saving model, epoch: 21800, train_loss: 0.8960073590278625, val_loss: 0.8703217506408691\n",
            "epoch: 21801, train_loss: 0.8960065245628357, val_loss: 0.8703209757804871\n",
            "Saving model, epoch: 21801, train_loss: 0.8960065245628357, val_loss: 0.8703209757804871\n",
            "Saving model, epoch: 21802, train_loss: 0.8960057497024536, val_loss: 0.8703207969665527\n",
            "Saving model, epoch: 21803, train_loss: 0.8960047364234924, val_loss: 0.8703201413154602\n",
            "Saving model, epoch: 21804, train_loss: 0.8960039019584656, val_loss: 0.8703196048736572\n",
            "Saving model, epoch: 21805, train_loss: 0.8960031270980835, val_loss: 0.8703193664550781\n",
            "Saving model, epoch: 21806, train_loss: 0.8960022926330566, val_loss: 0.870318591594696\n",
            "Saving model, epoch: 21807, train_loss: 0.896001398563385, val_loss: 0.8703185319900513\n",
            "Saving model, epoch: 21808, train_loss: 0.8960005044937134, val_loss: 0.870316743850708\n",
            "Saving model, epoch: 21810, train_loss: 0.8959987759590149, val_loss: 0.8703149557113647\n",
            "Saving model, epoch: 21812, train_loss: 0.8959972262382507, val_loss: 0.8703128099441528\n",
            "Saving model, epoch: 21814, train_loss: 0.8959954977035522, val_loss: 0.8703105449676514\n",
            "Saving model, epoch: 21816, train_loss: 0.8959937691688538, val_loss: 0.8703076839447021\n",
            "Saving model, epoch: 21818, train_loss: 0.8959921002388, val_loss: 0.8703040480613708\n",
            "Saving model, epoch: 21820, train_loss: 0.8959904909133911, val_loss: 0.8702991604804993\n",
            "Saving model, epoch: 21822, train_loss: 0.8959888815879822, val_loss: 0.8702927827835083\n",
            "Saving model, epoch: 21824, train_loss: 0.8959872722625732, val_loss: 0.8702854514122009\n",
            "Saving model, epoch: 21826, train_loss: 0.8959855437278748, val_loss: 0.8702791929244995\n",
            "Saving model, epoch: 21828, train_loss: 0.8959838151931763, val_loss: 0.8702783584594727\n",
            "Saving model, epoch: 21890, train_loss: 0.8959324955940247, val_loss: 0.8702772259712219\n",
            "Saving model, epoch: 21892, train_loss: 0.8959307670593262, val_loss: 0.8702766299247742\n",
            "Saving model, epoch: 21894, train_loss: 0.8959290981292725, val_loss: 0.8702763319015503\n",
            "Saving model, epoch: 21895, train_loss: 0.8959283828735352, val_loss: 0.8702756762504578\n",
            "Saving model, epoch: 21897, train_loss: 0.8959266543388367, val_loss: 0.8702743053436279\n",
            "Saving model, epoch: 21899, train_loss: 0.8959251642227173, val_loss: 0.8702731132507324\n",
            "epoch: 21901, train_loss: 0.895923376083374, val_loss: 0.8702725768089294\n",
            "Saving model, epoch: 21901, train_loss: 0.895923376083374, val_loss: 0.8702725768089294\n",
            "Saving model, epoch: 21903, train_loss: 0.8959218263626099, val_loss: 0.8702720403671265\n",
            "Saving model, epoch: 21904, train_loss: 0.8959210515022278, val_loss: 0.8702714443206787\n",
            "Saving model, epoch: 21905, train_loss: 0.8959201574325562, val_loss: 0.8702713847160339\n",
            "Saving model, epoch: 21906, train_loss: 0.8959193229675293, val_loss: 0.8702701926231384\n",
            "Saving model, epoch: 21908, train_loss: 0.8959176540374756, val_loss: 0.8702691793441772\n",
            "Saving model, epoch: 21910, train_loss: 0.8959159255027771, val_loss: 0.8702683448791504\n",
            "Saving model, epoch: 21911, train_loss: 0.8959153294563293, val_loss: 0.8702682852745056\n",
            "Saving model, epoch: 21912, train_loss: 0.8959144949913025, val_loss: 0.8702676892280579\n",
            "Saving model, epoch: 21913, train_loss: 0.8959134817123413, val_loss: 0.8702672719955444\n",
            "Saving model, epoch: 21914, train_loss: 0.8959128260612488, val_loss: 0.8702667355537415\n",
            "Saving model, epoch: 21915, train_loss: 0.8959118723869324, val_loss: 0.8702662587165833\n",
            "Saving model, epoch: 21916, train_loss: 0.8959110975265503, val_loss: 0.8702659010887146\n",
            "Saving model, epoch: 21917, train_loss: 0.8959102630615234, val_loss: 0.8702649474143982\n",
            "Saving model, epoch: 21919, train_loss: 0.8959087133407593, val_loss: 0.870263934135437\n",
            "Saving model, epoch: 21921, train_loss: 0.8959071040153503, val_loss: 0.8702632188796997\n",
            "Saving model, epoch: 21922, train_loss: 0.8959062695503235, val_loss: 0.8702630996704102\n",
            "Saving model, epoch: 21923, train_loss: 0.8959053754806519, val_loss: 0.8702623844146729\n",
            "Saving model, epoch: 21924, train_loss: 0.8959044814109802, val_loss: 0.8702621459960938\n",
            "Saving model, epoch: 21925, train_loss: 0.8959036469459534, val_loss: 0.8702613115310669\n",
            "Saving model, epoch: 21926, train_loss: 0.8959028720855713, val_loss: 0.8702607750892639\n",
            "Saving model, epoch: 21928, train_loss: 0.8959012627601624, val_loss: 0.8702598810195923\n",
            "Saving model, epoch: 21929, train_loss: 0.8959004282951355, val_loss: 0.8702595829963684\n",
            "Saving model, epoch: 21930, train_loss: 0.8958996534347534, val_loss: 0.8702588677406311\n",
            "Saving model, epoch: 21931, train_loss: 0.8958988189697266, val_loss: 0.8702588081359863\n",
            "Saving model, epoch: 21932, train_loss: 0.895898163318634, val_loss: 0.8702580332756042\n",
            "Saving model, epoch: 21933, train_loss: 0.8958972096443176, val_loss: 0.8702576756477356\n",
            "Saving model, epoch: 21934, train_loss: 0.895896315574646, val_loss: 0.8702569603919983\n",
            "Saving model, epoch: 21935, train_loss: 0.8958955407142639, val_loss: 0.8702566623687744\n",
            "Saving model, epoch: 21936, train_loss: 0.8958946466445923, val_loss: 0.8702560663223267\n",
            "Saving model, epoch: 21937, train_loss: 0.8958938121795654, val_loss: 0.8702555298805237\n",
            "Saving model, epoch: 21938, train_loss: 0.8958930373191833, val_loss: 0.870255172252655\n",
            "Saving model, epoch: 21939, train_loss: 0.8958922028541565, val_loss: 0.8702545166015625\n",
            "Saving model, epoch: 21940, train_loss: 0.8958914279937744, val_loss: 0.8702542781829834\n",
            "Saving model, epoch: 21941, train_loss: 0.8958905935287476, val_loss: 0.8702537417411804\n",
            "Saving model, epoch: 21942, train_loss: 0.8958898186683655, val_loss: 0.8702530860900879\n",
            "Saving model, epoch: 21943, train_loss: 0.8958889842033386, val_loss: 0.8702528476715088\n",
            "Saving model, epoch: 21944, train_loss: 0.8958882093429565, val_loss: 0.8702520132064819\n",
            "Saving model, epoch: 21946, train_loss: 0.8958864808082581, val_loss: 0.8702512979507446\n",
            "Saving model, epoch: 21947, train_loss: 0.8958856463432312, val_loss: 0.8702508807182312\n",
            "Saving model, epoch: 21948, train_loss: 0.8958848714828491, val_loss: 0.870250403881073\n",
            "Saving model, epoch: 21949, train_loss: 0.895884096622467, val_loss: 0.8702498078346252\n",
            "Saving model, epoch: 21950, train_loss: 0.8958832621574402, val_loss: 0.8702496290206909\n",
            "Saving model, epoch: 21951, train_loss: 0.8958823680877686, val_loss: 0.8702486753463745\n",
            "Saving model, epoch: 21953, train_loss: 0.8958807587623596, val_loss: 0.870247483253479\n",
            "Saving model, epoch: 21955, train_loss: 0.8958791494369507, val_loss: 0.8702463507652283\n",
            "Saving model, epoch: 21957, train_loss: 0.8958775401115417, val_loss: 0.8702448606491089\n",
            "Saving model, epoch: 21959, train_loss: 0.8958759307861328, val_loss: 0.8702432513237\n",
            "Saving model, epoch: 21961, train_loss: 0.8958741426467896, val_loss: 0.8702418208122253\n",
            "Saving model, epoch: 21963, train_loss: 0.8958725333213806, val_loss: 0.8702404499053955\n",
            "Saving model, epoch: 21965, train_loss: 0.8958709239959717, val_loss: 0.8702394962310791\n",
            "Saving model, epoch: 21967, train_loss: 0.8958693146705627, val_loss: 0.8702384233474731\n",
            "Saving model, epoch: 21969, train_loss: 0.8958677053451538, val_loss: 0.8702377080917358\n",
            "Saving model, epoch: 21971, train_loss: 0.8958660960197449, val_loss: 0.8702370524406433\n",
            "Saving model, epoch: 21973, train_loss: 0.8958644866943359, val_loss: 0.8702362179756165\n",
            "Saving model, epoch: 21975, train_loss: 0.8958626389503479, val_loss: 0.8702356815338135\n",
            "Saving model, epoch: 21977, train_loss: 0.8958610892295837, val_loss: 0.8702353835105896\n",
            "Saving model, epoch: 21979, train_loss: 0.8958594799041748, val_loss: 0.8702349662780762\n",
            "Saving model, epoch: 21982, train_loss: 0.895857036113739, val_loss: 0.8702331185340881\n",
            "Saving model, epoch: 21984, train_loss: 0.8958554267883301, val_loss: 0.8702319860458374\n",
            "Saving model, epoch: 21986, train_loss: 0.8958538174629211, val_loss: 0.8702308535575867\n",
            "Saving model, epoch: 21988, train_loss: 0.8958521485328674, val_loss: 0.8702298998832703\n",
            "Saving model, epoch: 21990, train_loss: 0.8958505392074585, val_loss: 0.870229184627533\n",
            "Saving model, epoch: 21992, train_loss: 0.8958489298820496, val_loss: 0.8702285885810852\n",
            "Saving model, epoch: 21994, train_loss: 0.8958472013473511, val_loss: 0.8702278137207031\n",
            "Saving model, epoch: 21996, train_loss: 0.8958455920219421, val_loss: 0.8702271580696106\n",
            "Saving model, epoch: 21997, train_loss: 0.8958447575569153, val_loss: 0.8702268004417419\n",
            "Saving model, epoch: 21998, train_loss: 0.8958439826965332, val_loss: 0.8702267408370972\n",
            "Saving model, epoch: 21999, train_loss: 0.8958431482315063, val_loss: 0.8702256679534912\n",
            "Saving model, epoch: 22000, train_loss: 0.8958423733711243, val_loss: 0.8702256083488464\n",
            "epoch: 22001, train_loss: 0.8958414793014526, val_loss: 0.8702245950698853\n",
            "Saving model, epoch: 22001, train_loss: 0.8958414793014526, val_loss: 0.8702245950698853\n",
            "Saving model, epoch: 22003, train_loss: 0.8958398699760437, val_loss: 0.8702232837677002\n",
            "Saving model, epoch: 22005, train_loss: 0.8958383798599243, val_loss: 0.8702223896980286\n",
            "Saving model, epoch: 22007, train_loss: 0.8958365321159363, val_loss: 0.8702213764190674\n",
            "Saving model, epoch: 22009, train_loss: 0.8958350419998169, val_loss: 0.870219886302948\n",
            "Saving model, epoch: 22011, train_loss: 0.8958333134651184, val_loss: 0.8702182173728943\n",
            "Saving model, epoch: 22013, train_loss: 0.895831823348999, val_loss: 0.8702160120010376\n",
            "Saving model, epoch: 22015, train_loss: 0.8958300948143005, val_loss: 0.8702142834663391\n",
            "Saving model, epoch: 22017, train_loss: 0.8958284854888916, val_loss: 0.8702117204666138\n",
            "Saving model, epoch: 22019, train_loss: 0.8958268165588379, val_loss: 0.8702080249786377\n",
            "Saving model, epoch: 22021, train_loss: 0.8958252668380737, val_loss: 0.87020343542099\n",
            "Saving model, epoch: 22023, train_loss: 0.8958236575126648, val_loss: 0.8701977133750916\n",
            "Saving model, epoch: 22025, train_loss: 0.8958221077919006, val_loss: 0.8701903820037842\n",
            "Saving model, epoch: 22027, train_loss: 0.895820677280426, val_loss: 0.8701836466789246\n",
            "Saving model, epoch: 22029, train_loss: 0.8958190679550171, val_loss: 0.8701816201210022\n",
            "Saving model, epoch: 22098, train_loss: 0.8957642316818237, val_loss: 0.8701804280281067\n",
            "Saving model, epoch: 22100, train_loss: 0.8957626223564148, val_loss: 0.8701797127723694\n",
            "epoch: 22101, train_loss: 0.8957617878913879, val_loss: 0.8701807260513306\n",
            "Saving model, epoch: 22102, train_loss: 0.8957610726356506, val_loss: 0.8701792359352112\n",
            "Saving model, epoch: 22103, train_loss: 0.8957602977752686, val_loss: 0.8701789975166321\n",
            "Saving model, epoch: 22105, train_loss: 0.8957586884498596, val_loss: 0.8701774477958679\n",
            "Saving model, epoch: 22107, train_loss: 0.8957570791244507, val_loss: 0.8701766133308411\n",
            "Saving model, epoch: 22109, train_loss: 0.8957555890083313, val_loss: 0.8701761364936829\n",
            "Saving model, epoch: 22110, train_loss: 0.8957547545433044, val_loss: 0.8701757192611694\n",
            "Saving model, epoch: 22112, train_loss: 0.8957532644271851, val_loss: 0.8701743483543396\n",
            "Saving model, epoch: 22114, train_loss: 0.8957516551017761, val_loss: 0.870173454284668\n",
            "Saving model, epoch: 22116, train_loss: 0.8957500457763672, val_loss: 0.8701730370521545\n",
            "Saving model, epoch: 22117, train_loss: 0.8957493305206299, val_loss: 0.8701725602149963\n",
            "Saving model, epoch: 22118, train_loss: 0.8957484364509583, val_loss: 0.8701723217964172\n",
            "Saving model, epoch: 22119, train_loss: 0.8957476019859314, val_loss: 0.8701717853546143\n",
            "Saving model, epoch: 22120, train_loss: 0.8957469463348389, val_loss: 0.8701714873313904\n",
            "Saving model, epoch: 22121, train_loss: 0.895746111869812, val_loss: 0.8701708316802979\n",
            "Saving model, epoch: 22122, train_loss: 0.8957453370094299, val_loss: 0.8701706528663635\n",
            "Saving model, epoch: 22123, train_loss: 0.8957445025444031, val_loss: 0.8701697587966919\n",
            "Saving model, epoch: 22124, train_loss: 0.895743727684021, val_loss: 0.8701695203781128\n",
            "Saving model, epoch: 22125, train_loss: 0.8957428932189941, val_loss: 0.8701690435409546\n",
            "Saving model, epoch: 22126, train_loss: 0.8957421183586121, val_loss: 0.8701687455177307\n",
            "Saving model, epoch: 22127, train_loss: 0.8957415223121643, val_loss: 0.8701683282852173\n",
            "Saving model, epoch: 22128, train_loss: 0.8957405090332031, val_loss: 0.8701676726341248\n",
            "Saving model, epoch: 22129, train_loss: 0.8957399129867554, val_loss: 0.8701675534248352\n",
            "Saving model, epoch: 22130, train_loss: 0.8957390189170837, val_loss: 0.8701665997505188\n",
            "Saving model, epoch: 22132, train_loss: 0.8957374691963196, val_loss: 0.8701657056808472\n",
            "Saving model, epoch: 22134, train_loss: 0.8957358598709106, val_loss: 0.8701648712158203\n",
            "Saving model, epoch: 22135, train_loss: 0.8957350850105286, val_loss: 0.870164692401886\n",
            "Saving model, epoch: 22136, train_loss: 0.8957343697547913, val_loss: 0.8701642155647278\n",
            "Saving model, epoch: 22137, train_loss: 0.8957334756851196, val_loss: 0.8701638579368591\n",
            "Saving model, epoch: 22138, train_loss: 0.8957327604293823, val_loss: 0.8701635003089905\n",
            "Saving model, epoch: 22139, train_loss: 0.8957319855690002, val_loss: 0.8701623678207397\n",
            "Saving model, epoch: 22141, train_loss: 0.8957304358482361, val_loss: 0.8701612949371338\n",
            "Saving model, epoch: 22143, train_loss: 0.8957288265228271, val_loss: 0.8701605200767517\n",
            "Saving model, epoch: 22145, train_loss: 0.8957272171974182, val_loss: 0.870159924030304\n",
            "Saving model, epoch: 22146, train_loss: 0.8957264423370361, val_loss: 0.8701597452163696\n",
            "Saving model, epoch: 22147, train_loss: 0.8957256078720093, val_loss: 0.8701592683792114\n",
            "Saving model, epoch: 22148, train_loss: 0.8957249522209167, val_loss: 0.8701587915420532\n",
            "Saving model, epoch: 22149, train_loss: 0.8957241177558899, val_loss: 0.870158314704895\n",
            "Saving model, epoch: 22150, train_loss: 0.8957234025001526, val_loss: 0.8701575398445129\n",
            "Saving model, epoch: 22152, train_loss: 0.8957217335700989, val_loss: 0.8701565265655518\n",
            "Saving model, epoch: 22154, train_loss: 0.8957201838493347, val_loss: 0.870155394077301\n",
            "Saving model, epoch: 22156, train_loss: 0.8957185745239258, val_loss: 0.8701546788215637\n",
            "Saving model, epoch: 22158, train_loss: 0.8957169651985168, val_loss: 0.8701540231704712\n",
            "Saving model, epoch: 22160, train_loss: 0.8957154750823975, val_loss: 0.8701533079147339\n",
            "Saving model, epoch: 22161, train_loss: 0.8957145810127258, val_loss: 0.8701527118682861\n",
            "Saving model, epoch: 22163, train_loss: 0.8957130908966064, val_loss: 0.8701515197753906\n",
            "Saving model, epoch: 22165, train_loss: 0.8957115411758423, val_loss: 0.8701508045196533\n",
            "Saving model, epoch: 22167, train_loss: 0.8957099318504333, val_loss: 0.8701501488685608\n",
            "Saving model, epoch: 22168, train_loss: 0.8957091569900513, val_loss: 0.8701497912406921\n",
            "Saving model, epoch: 22169, train_loss: 0.8957083225250244, val_loss: 0.8701492547988892\n",
            "Saving model, epoch: 22170, train_loss: 0.8957075476646423, val_loss: 0.8701490163803101\n",
            "Saving model, epoch: 22171, train_loss: 0.8957067131996155, val_loss: 0.8701484203338623\n",
            "Saving model, epoch: 22172, train_loss: 0.895706057548523, val_loss: 0.8701479434967041\n",
            "Saving model, epoch: 22174, train_loss: 0.8957045078277588, val_loss: 0.8701471090316772\n",
            "Saving model, epoch: 22175, train_loss: 0.8957037329673767, val_loss: 0.8701466917991638\n",
            "Saving model, epoch: 22176, train_loss: 0.8957028985023499, val_loss: 0.8701462745666504\n",
            "Saving model, epoch: 22177, train_loss: 0.8957021236419678, val_loss: 0.8701457977294922\n",
            "Saving model, epoch: 22178, train_loss: 0.8957012891769409, val_loss: 0.870145320892334\n",
            "Saving model, epoch: 22179, train_loss: 0.8957005143165588, val_loss: 0.8701447248458862\n",
            "Saving model, epoch: 22180, train_loss: 0.8956997990608215, val_loss: 0.8701443672180176\n",
            "Saving model, epoch: 22181, train_loss: 0.8956990242004395, val_loss: 0.8701441287994385\n",
            "Saving model, epoch: 22182, train_loss: 0.895698070526123, val_loss: 0.8701435327529907\n",
            "Saving model, epoch: 22183, train_loss: 0.8956974148750305, val_loss: 0.8701434135437012\n",
            "Saving model, epoch: 22184, train_loss: 0.8956965804100037, val_loss: 0.8701421618461609\n",
            "Saving model, epoch: 22186, train_loss: 0.8956950902938843, val_loss: 0.8701411485671997\n",
            "Saving model, epoch: 22188, train_loss: 0.8956934809684753, val_loss: 0.8701397776603699\n",
            "Saving model, epoch: 22190, train_loss: 0.895691990852356, val_loss: 0.8701382875442505\n",
            "Saving model, epoch: 22192, train_loss: 0.8956902623176575, val_loss: 0.8701366186141968\n",
            "Saving model, epoch: 22194, train_loss: 0.8956888318061829, val_loss: 0.8701348304748535\n",
            "Saving model, epoch: 22196, train_loss: 0.8956872224807739, val_loss: 0.870133101940155\n",
            "Saving model, epoch: 22198, train_loss: 0.8956857323646545, val_loss: 0.8701310157775879\n",
            "Saving model, epoch: 22200, train_loss: 0.8956841230392456, val_loss: 0.8701289296150208\n",
            "epoch: 22201, train_loss: 0.8956834077835083, val_loss: 0.87014240026474\n",
            "Saving model, epoch: 22202, train_loss: 0.8956826329231262, val_loss: 0.8701266646385193\n",
            "Saving model, epoch: 22204, train_loss: 0.8956809043884277, val_loss: 0.8701236248016357\n",
            "Saving model, epoch: 22206, train_loss: 0.8956794142723083, val_loss: 0.8701203465461731\n",
            "Saving model, epoch: 22208, train_loss: 0.895677924156189, val_loss: 0.8701168894767761\n",
            "Saving model, epoch: 22210, train_loss: 0.8956763744354248, val_loss: 0.8701130747795105\n",
            "Saving model, epoch: 22212, train_loss: 0.8956747651100159, val_loss: 0.8701107501983643\n",
            "Saving model, epoch: 22214, train_loss: 0.895673394203186, val_loss: 0.8701103925704956\n",
            "Saving model, epoch: 22255, train_loss: 0.8956416249275208, val_loss: 0.8701103329658508\n",
            "Saving model, epoch: 22257, train_loss: 0.8956402540206909, val_loss: 0.870110034942627\n",
            "Saving model, epoch: 22259, train_loss: 0.895638644695282, val_loss: 0.8701096773147583\n",
            "Saving model, epoch: 22261, train_loss: 0.895637035369873, val_loss: 0.8701089024543762\n",
            "Saving model, epoch: 22263, train_loss: 0.8956356048583984, val_loss: 0.8701081871986389\n",
            "Saving model, epoch: 22265, train_loss: 0.8956339955329895, val_loss: 0.870107114315033\n",
            "Saving model, epoch: 22267, train_loss: 0.8956325054168701, val_loss: 0.8701057434082031\n",
            "Saving model, epoch: 22269, train_loss: 0.8956307768821716, val_loss: 0.8701046705245972\n",
            "Saving model, epoch: 22271, train_loss: 0.8956294059753418, val_loss: 0.8701032400131226\n",
            "Saving model, epoch: 22273, train_loss: 0.8956279754638672, val_loss: 0.870102047920227\n",
            "Saving model, epoch: 22275, train_loss: 0.8956263661384583, val_loss: 0.8701011538505554\n",
            "Saving model, epoch: 22277, train_loss: 0.8956247568130493, val_loss: 0.8701004385948181\n",
            "Saving model, epoch: 22279, train_loss: 0.8956232666969299, val_loss: 0.8700999617576599\n",
            "Saving model, epoch: 22281, train_loss: 0.8956217169761658, val_loss: 0.8700993657112122\n",
            "Saving model, epoch: 22283, train_loss: 0.8956202268600464, val_loss: 0.870098888874054\n",
            "Saving model, epoch: 22285, train_loss: 0.8956186175346375, val_loss: 0.8700980544090271\n",
            "Saving model, epoch: 22287, train_loss: 0.8956171274185181, val_loss: 0.8700971007347107\n",
            "Saving model, epoch: 22289, train_loss: 0.8956156373023987, val_loss: 0.8700956106185913\n",
            "Saving model, epoch: 22291, train_loss: 0.8956140279769897, val_loss: 0.8700944781303406\n",
            "Saving model, epoch: 22293, train_loss: 0.8956125974655151, val_loss: 0.8700932860374451\n",
            "Saving model, epoch: 22295, train_loss: 0.8956109881401062, val_loss: 0.8700922131538391\n",
            "Saving model, epoch: 22297, train_loss: 0.8956094980239868, val_loss: 0.8700911402702332\n",
            "Saving model, epoch: 22299, train_loss: 0.8956080079078674, val_loss: 0.8700901865959167\n",
            "epoch: 22301, train_loss: 0.8956064581871033, val_loss: 0.8700891137123108\n",
            "Saving model, epoch: 22301, train_loss: 0.8956064581871033, val_loss: 0.8700891137123108\n",
            "Saving model, epoch: 22303, train_loss: 0.8956048488616943, val_loss: 0.870087206363678\n",
            "Saving model, epoch: 22305, train_loss: 0.895603358745575, val_loss: 0.8700858354568481\n",
            "Saving model, epoch: 22307, train_loss: 0.8956018686294556, val_loss: 0.8700839877128601\n",
            "Saving model, epoch: 22309, train_loss: 0.8956002593040466, val_loss: 0.8700817823410034\n",
            "Saving model, epoch: 22311, train_loss: 0.895598828792572, val_loss: 0.8700793981552124\n",
            "Saving model, epoch: 22313, train_loss: 0.8955973386764526, val_loss: 0.8700767755508423\n",
            "Saving model, epoch: 22315, train_loss: 0.8955958485603333, val_loss: 0.8700733780860901\n",
            "Saving model, epoch: 22317, train_loss: 0.8955942392349243, val_loss: 0.8700702786445618\n",
            "Saving model, epoch: 22319, train_loss: 0.8955928087234497, val_loss: 0.8700671195983887\n",
            "Saving model, epoch: 22321, train_loss: 0.8955913186073303, val_loss: 0.8700653910636902\n",
            "Saving model, epoch: 22360, train_loss: 0.8955618739128113, val_loss: 0.8700647950172424\n",
            "Saving model, epoch: 22362, train_loss: 0.8955602645874023, val_loss: 0.8700633645057678\n",
            "Saving model, epoch: 22364, train_loss: 0.8955588340759277, val_loss: 0.8700624704360962\n",
            "Saving model, epoch: 22366, train_loss: 0.8955572247505188, val_loss: 0.8700618743896484\n",
            "Saving model, epoch: 22368, train_loss: 0.8955557346343994, val_loss: 0.8700611591339111\n",
            "Saving model, epoch: 22370, train_loss: 0.8955543041229248, val_loss: 0.8700608015060425\n",
            "Saving model, epoch: 22372, train_loss: 0.8955526947975159, val_loss: 0.8700604438781738\n",
            "Saving model, epoch: 22374, train_loss: 0.8955514430999756, val_loss: 0.870060384273529\n",
            "Saving model, epoch: 22376, train_loss: 0.8955498337745667, val_loss: 0.8700600266456604\n",
            "Saving model, epoch: 22378, train_loss: 0.8955482840538025, val_loss: 0.8700594305992126\n",
            "Saving model, epoch: 22380, train_loss: 0.8955467939376831, val_loss: 0.8700588941574097\n",
            "Saving model, epoch: 22382, train_loss: 0.8955451846122742, val_loss: 0.8700578212738037\n",
            "Saving model, epoch: 22384, train_loss: 0.8955437541007996, val_loss: 0.870056688785553\n",
            "Saving model, epoch: 22386, train_loss: 0.8955422639846802, val_loss: 0.870055079460144\n",
            "Saving model, epoch: 22388, train_loss: 0.8955407738685608, val_loss: 0.8700535297393799\n",
            "Saving model, epoch: 22390, train_loss: 0.8955392837524414, val_loss: 0.8700523972511292\n",
            "Saving model, epoch: 22392, train_loss: 0.8955377340316772, val_loss: 0.8700511455535889\n",
            "Saving model, epoch: 22394, train_loss: 0.8955363631248474, val_loss: 0.8700501918792725\n",
            "Saving model, epoch: 22396, train_loss: 0.8955347537994385, val_loss: 0.8700495362281799\n",
            "Saving model, epoch: 22398, train_loss: 0.8955332040786743, val_loss: 0.8700485825538635\n",
            "Saving model, epoch: 22400, train_loss: 0.8955317139625549, val_loss: 0.8700478672981262\n",
            "epoch: 22401, train_loss: 0.8955311179161072, val_loss: 0.8700541853904724\n",
            "Saving model, epoch: 22402, train_loss: 0.8955303430557251, val_loss: 0.8700472712516785\n",
            "Saving model, epoch: 22404, train_loss: 0.8955287933349609, val_loss: 0.8700464963912964\n",
            "Saving model, epoch: 22406, train_loss: 0.8955273032188416, val_loss: 0.8700457215309143\n",
            "Saving model, epoch: 22408, train_loss: 0.8955258131027222, val_loss: 0.8700450658798218\n",
            "Saving model, epoch: 22410, train_loss: 0.895524263381958, val_loss: 0.8700438141822815\n",
            "Saving model, epoch: 22412, train_loss: 0.8955228924751282, val_loss: 0.8700426816940308\n",
            "Saving model, epoch: 22414, train_loss: 0.8955214023590088, val_loss: 0.8700406551361084\n",
            "Saving model, epoch: 22416, train_loss: 0.8955198526382446, val_loss: 0.8700387477874756\n",
            "Saving model, epoch: 22418, train_loss: 0.8955182433128357, val_loss: 0.8700359463691711\n",
            "Saving model, epoch: 22420, train_loss: 0.8955168724060059, val_loss: 0.8700326085090637\n",
            "Saving model, epoch: 22422, train_loss: 0.8955154418945312, val_loss: 0.8700283765792847\n",
            "Saving model, epoch: 22424, train_loss: 0.8955139517784119, val_loss: 0.8700236082077026\n",
            "Saving model, epoch: 22426, train_loss: 0.8955124616622925, val_loss: 0.8700193762779236\n",
            "Saving model, epoch: 22428, train_loss: 0.8955110311508179, val_loss: 0.8700172901153564\n",
            "Saving model, epoch: 22485, train_loss: 0.8954690098762512, val_loss: 0.8700163960456848\n",
            "Saving model, epoch: 22487, train_loss: 0.8954676389694214, val_loss: 0.8700156807899475\n",
            "Saving model, epoch: 22489, train_loss: 0.8954662084579468, val_loss: 0.8700149655342102\n",
            "Saving model, epoch: 22491, train_loss: 0.8954648375511169, val_loss: 0.8700147867202759\n",
            "Saving model, epoch: 22492, train_loss: 0.8954640030860901, val_loss: 0.870014488697052\n",
            "Saving model, epoch: 22494, train_loss: 0.8954625725746155, val_loss: 0.8700133562088013\n",
            "Saving model, epoch: 22496, train_loss: 0.8954610824584961, val_loss: 0.8700123429298401\n",
            "Saving model, epoch: 22498, train_loss: 0.8954595923423767, val_loss: 0.8700112104415894\n",
            "Saving model, epoch: 22500, train_loss: 0.8954581618309021, val_loss: 0.8700103759765625\n",
            "epoch: 22501, train_loss: 0.8954574465751648, val_loss: 0.8700119256973267\n",
            "Saving model, epoch: 22502, train_loss: 0.8954567909240723, val_loss: 0.8700098395347595\n",
            "Saving model, epoch: 22504, train_loss: 0.8954551815986633, val_loss: 0.8700095415115356\n",
            "Saving model, epoch: 22506, train_loss: 0.8954537510871887, val_loss: 0.8700092434883118\n",
            "Saving model, epoch: 22507, train_loss: 0.8954530358314514, val_loss: 0.8700084090232849\n",
            "Saving model, epoch: 22509, train_loss: 0.895451545715332, val_loss: 0.870007336139679\n",
            "Saving model, epoch: 22511, train_loss: 0.8954501152038574, val_loss: 0.8700065016746521\n",
            "Saving model, epoch: 22513, train_loss: 0.895448625087738, val_loss: 0.8700056076049805\n",
            "Saving model, epoch: 22515, train_loss: 0.8954472541809082, val_loss: 0.870005190372467\n",
            "Saving model, epoch: 22517, train_loss: 0.8954458236694336, val_loss: 0.8700043559074402\n",
            "Saving model, epoch: 22519, train_loss: 0.8954443335533142, val_loss: 0.8700039386749268\n",
            "Saving model, epoch: 22520, train_loss: 0.8954436182975769, val_loss: 0.8700036406517029\n",
            "Saving model, epoch: 22521, train_loss: 0.89544278383255, val_loss: 0.870003342628479\n",
            "Saving model, epoch: 22522, train_loss: 0.8954421877861023, val_loss: 0.8700028657913208\n",
            "Saving model, epoch: 22523, train_loss: 0.895441472530365, val_loss: 0.8700023293495178\n",
            "Saving model, epoch: 22524, train_loss: 0.8954406976699829, val_loss: 0.8700019121170044\n",
            "Saving model, epoch: 22526, train_loss: 0.8954392075538635, val_loss: 0.8700008392333984\n",
            "Saving model, epoch: 22528, train_loss: 0.8954376578330994, val_loss: 0.8699999451637268\n",
            "Saving model, epoch: 22530, train_loss: 0.8954362869262695, val_loss: 0.8699989318847656\n",
            "Saving model, epoch: 22532, train_loss: 0.8954347968101501, val_loss: 0.8699979186058044\n",
            "Saving model, epoch: 22534, train_loss: 0.8954334855079651, val_loss: 0.8699970841407776\n",
            "Saving model, epoch: 22536, train_loss: 0.8954319357872009, val_loss: 0.8699966669082642\n",
            "Saving model, epoch: 22538, train_loss: 0.8954304456710815, val_loss: 0.8699960708618164\n",
            "Saving model, epoch: 22540, train_loss: 0.8954290151596069, val_loss: 0.8699954152107239\n",
            "Saving model, epoch: 22541, train_loss: 0.8954282402992249, val_loss: 0.8699952363967896\n",
            "Saving model, epoch: 22542, train_loss: 0.8954276442527771, val_loss: 0.8699950575828552\n",
            "Saving model, epoch: 22543, train_loss: 0.8954268097877502, val_loss: 0.8699941635131836\n",
            "Saving model, epoch: 22545, train_loss: 0.8954253196716309, val_loss: 0.869993269443512\n",
            "Saving model, epoch: 22547, train_loss: 0.8954240083694458, val_loss: 0.8699926733970642\n",
            "Saving model, epoch: 22548, train_loss: 0.8954232335090637, val_loss: 0.8699925541877747\n",
            "Saving model, epoch: 22549, train_loss: 0.8954223990440369, val_loss: 0.869992196559906\n",
            "Saving model, epoch: 22550, train_loss: 0.8954218029975891, val_loss: 0.869991660118103\n",
            "Saving model, epoch: 22551, train_loss: 0.895421028137207, val_loss: 0.8699916005134583\n",
            "Saving model, epoch: 22552, train_loss: 0.8954203128814697, val_loss: 0.8699907064437866\n",
            "Saving model, epoch: 22554, train_loss: 0.8954188227653503, val_loss: 0.869989812374115\n",
            "Saving model, epoch: 22556, train_loss: 0.8954173922538757, val_loss: 0.8699888586997986\n",
            "Saving model, epoch: 22558, train_loss: 0.8954159617424011, val_loss: 0.8699880242347717\n",
            "Saving model, epoch: 22560, train_loss: 0.8954144716262817, val_loss: 0.8699871301651001\n",
            "Saving model, epoch: 22562, train_loss: 0.8954129815101624, val_loss: 0.8699856400489807\n",
            "Saving model, epoch: 22564, train_loss: 0.8954115509986877, val_loss: 0.8699841499328613\n",
            "Saving model, epoch: 22566, train_loss: 0.8954101800918579, val_loss: 0.8699820041656494\n",
            "Saving model, epoch: 22568, train_loss: 0.8954086303710938, val_loss: 0.8699791431427002\n",
            "Saving model, epoch: 22570, train_loss: 0.8954071402549744, val_loss: 0.8699753284454346\n",
            "Saving model, epoch: 22572, train_loss: 0.8954058289527893, val_loss: 0.8699701428413391\n",
            "Saving model, epoch: 22574, train_loss: 0.8954043388366699, val_loss: 0.8699617385864258\n",
            "Saving model, epoch: 22576, train_loss: 0.8954031467437744, val_loss: 0.8699513077735901\n",
            "Saving model, epoch: 22578, train_loss: 0.8954018354415894, val_loss: 0.8699418902397156\n",
            "epoch: 22601, train_loss: 0.8953854441642761, val_loss: 0.869966447353363\n",
            "Saving model, epoch: 22688, train_loss: 0.8953250646591187, val_loss: 0.8699411153793335\n",
            "Saving model, epoch: 22690, train_loss: 0.8953237533569336, val_loss: 0.8699401617050171\n",
            "Saving model, epoch: 22692, train_loss: 0.895322322845459, val_loss: 0.8699392676353455\n",
            "Saving model, epoch: 22694, train_loss: 0.8953209519386292, val_loss: 0.8699386715888977\n",
            "Saving model, epoch: 22697, train_loss: 0.8953189253807068, val_loss: 0.86993807554245\n",
            "Saving model, epoch: 22699, train_loss: 0.8953174948692322, val_loss: 0.8699373006820679\n",
            "epoch: 22701, train_loss: 0.8953161239624023, val_loss: 0.8699365258216858\n",
            "Saving model, epoch: 22701, train_loss: 0.8953161239624023, val_loss: 0.8699365258216858\n",
            "Saving model, epoch: 22703, train_loss: 0.8953146934509277, val_loss: 0.8699359893798828\n",
            "Saving model, epoch: 22704, train_loss: 0.89531409740448, val_loss: 0.8699357509613037\n",
            "Saving model, epoch: 22705, train_loss: 0.8953133225440979, val_loss: 0.8699353933334351\n",
            "Saving model, epoch: 22706, train_loss: 0.8953127264976501, val_loss: 0.8699349164962769\n",
            "Saving model, epoch: 22708, train_loss: 0.8953112959861755, val_loss: 0.8699338436126709\n",
            "Saving model, epoch: 22710, train_loss: 0.8953098058700562, val_loss: 0.8699332475662231\n",
            "Saving model, epoch: 22712, train_loss: 0.8953085541725159, val_loss: 0.8699327707290649\n",
            "Saving model, epoch: 22713, train_loss: 0.8953078985214233, val_loss: 0.8699323534965515\n",
            "Saving model, epoch: 22714, train_loss: 0.895307183265686, val_loss: 0.8699322938919067\n",
            "Saving model, epoch: 22715, train_loss: 0.8953064680099487, val_loss: 0.869931697845459\n",
            "Saving model, epoch: 22716, train_loss: 0.895305871963501, val_loss: 0.8699312806129456\n",
            "Saving model, epoch: 22717, train_loss: 0.8953051567077637, val_loss: 0.8699311017990112\n",
            "Saving model, epoch: 22718, train_loss: 0.8953043818473816, val_loss: 0.8699304461479187\n",
            "Saving model, epoch: 22720, train_loss: 0.8953030705451965, val_loss: 0.869929850101471\n",
            "Saving model, epoch: 22721, train_loss: 0.8953024744987488, val_loss: 0.8699297308921814\n",
            "Saving model, epoch: 22722, train_loss: 0.8953016400337219, val_loss: 0.8699289560317993\n",
            "Saving model, epoch: 22724, train_loss: 0.8953002691268921, val_loss: 0.869928240776062\n",
            "Saving model, epoch: 22726, train_loss: 0.895298957824707, val_loss: 0.8699275255203247\n",
            "Saving model, epoch: 22728, train_loss: 0.8952975273132324, val_loss: 0.869926929473877\n",
            "Saving model, epoch: 22729, train_loss: 0.8952968120574951, val_loss: 0.8699266314506531\n",
            "Saving model, epoch: 22730, train_loss: 0.8952960968017578, val_loss: 0.869926393032074\n",
            "Saving model, epoch: 22731, train_loss: 0.8952954411506653, val_loss: 0.8699257969856262\n",
            "Saving model, epoch: 22732, train_loss: 0.8952948451042175, val_loss: 0.8699256181716919\n",
            "Saving model, epoch: 22733, train_loss: 0.8952940106391907, val_loss: 0.8699251413345337\n",
            "Saving model, epoch: 22734, train_loss: 0.8952932953834534, val_loss: 0.8699250817298889\n",
            "Saving model, epoch: 22735, train_loss: 0.8952928185462952, val_loss: 0.8699244260787964\n",
            "Saving model, epoch: 22736, train_loss: 0.8952919840812683, val_loss: 0.8699238896369934\n",
            "Saving model, epoch: 22738, train_loss: 0.8952906131744385, val_loss: 0.8699231743812561\n",
            "Saving model, epoch: 22740, train_loss: 0.8952891826629639, val_loss: 0.8699224591255188\n",
            "Saving model, epoch: 22741, train_loss: 0.8952885866165161, val_loss: 0.8699223399162292\n",
            "Saving model, epoch: 22742, train_loss: 0.895287811756134, val_loss: 0.869921863079071\n",
            "Saving model, epoch: 22743, train_loss: 0.8952871561050415, val_loss: 0.8699213862419128\n",
            "Saving model, epoch: 22744, train_loss: 0.8952865600585938, val_loss: 0.8699210286140442\n",
            "Saving model, epoch: 22746, train_loss: 0.8952851891517639, val_loss: 0.8699202537536621\n",
            "Saving model, epoch: 22748, train_loss: 0.8952837586402893, val_loss: 0.8699195981025696\n",
            "Saving model, epoch: 22750, train_loss: 0.8952823281288147, val_loss: 0.8699187636375427\n",
            "Saving model, epoch: 22751, train_loss: 0.8952815532684326, val_loss: 0.869918704032898\n",
            "Saving model, epoch: 22752, train_loss: 0.8952809572219849, val_loss: 0.8699184656143188\n",
            "Saving model, epoch: 22753, train_loss: 0.8952802419662476, val_loss: 0.8699179887771606\n",
            "Saving model, epoch: 22754, train_loss: 0.8952795267105103, val_loss: 0.8699176907539368\n",
            "Saving model, epoch: 22755, train_loss: 0.8952789306640625, val_loss: 0.8699172735214233\n",
            "Saving model, epoch: 22756, train_loss: 0.8952781558036804, val_loss: 0.8699168562889099\n",
            "Saving model, epoch: 22757, train_loss: 0.8952775597572327, val_loss: 0.8699166178703308\n",
            "Saving model, epoch: 22758, train_loss: 0.8952768445014954, val_loss: 0.8699159622192383\n",
            "Saving model, epoch: 22759, train_loss: 0.8952761292457581, val_loss: 0.869915783405304\n",
            "Saving model, epoch: 22760, train_loss: 0.8952755331993103, val_loss: 0.869915246963501\n",
            "Saving model, epoch: 22761, train_loss: 0.8952746987342834, val_loss: 0.8699151873588562\n",
            "Saving model, epoch: 22762, train_loss: 0.8952741026878357, val_loss: 0.869914710521698\n",
            "Saving model, epoch: 22763, train_loss: 0.8952733278274536, val_loss: 0.8699142932891846\n",
            "Saving model, epoch: 22764, train_loss: 0.8952726125717163, val_loss: 0.8699140548706055\n",
            "Saving model, epoch: 22765, train_loss: 0.8952720165252686, val_loss: 0.8699133992195129\n",
            "Saving model, epoch: 22767, train_loss: 0.8952707052230835, val_loss: 0.8699123859405518\n",
            "Saving model, epoch: 22769, train_loss: 0.8952692747116089, val_loss: 0.8699116706848145\n",
            "Saving model, epoch: 22771, train_loss: 0.8952679634094238, val_loss: 0.8699105978012085\n",
            "Saving model, epoch: 22773, train_loss: 0.8952664732933044, val_loss: 0.8699101209640503\n",
            "Saving model, epoch: 22775, train_loss: 0.8952651023864746, val_loss: 0.8699094653129578\n",
            "Saving model, epoch: 22777, train_loss: 0.8952637910842896, val_loss: 0.86990886926651\n",
            "Saving model, epoch: 22779, train_loss: 0.8952623605728149, val_loss: 0.8699085116386414\n",
            "Saving model, epoch: 22780, train_loss: 0.8952617645263672, val_loss: 0.8699082732200623\n",
            "Saving model, epoch: 22781, train_loss: 0.8952610492706299, val_loss: 0.8699080348014832\n",
            "Saving model, epoch: 22782, train_loss: 0.8952602744102478, val_loss: 0.8699074387550354\n",
            "Saving model, epoch: 22783, train_loss: 0.8952595591545105, val_loss: 0.8699073791503906\n",
            "Saving model, epoch: 22784, train_loss: 0.8952589631080627, val_loss: 0.8699063658714294\n",
            "Saving model, epoch: 22786, train_loss: 0.8952575325965881, val_loss: 0.8699051737785339\n",
            "Saving model, epoch: 22788, train_loss: 0.8952562212944031, val_loss: 0.8699043989181519\n",
            "Saving model, epoch: 22790, train_loss: 0.8952548503875732, val_loss: 0.8699036836624146\n",
            "Saving model, epoch: 22792, train_loss: 0.8952534198760986, val_loss: 0.8699024319648743\n",
            "Saving model, epoch: 22794, train_loss: 0.8952521085739136, val_loss: 0.8699016571044922\n",
            "Saving model, epoch: 22796, train_loss: 0.8952507376670837, val_loss: 0.8699003458023071\n",
            "Saving model, epoch: 22798, train_loss: 0.8952493071556091, val_loss: 0.8698991537094116\n",
            "Saving model, epoch: 22800, train_loss: 0.8952479958534241, val_loss: 0.8698983788490295\n",
            "epoch: 22801, train_loss: 0.895247220993042, val_loss: 0.869903564453125\n",
            "Saving model, epoch: 22802, train_loss: 0.895246684551239, val_loss: 0.8698973059654236\n",
            "Saving model, epoch: 22804, train_loss: 0.8952451944351196, val_loss: 0.869896650314331\n",
            "Saving model, epoch: 22806, train_loss: 0.895243763923645, val_loss: 0.8698958158493042\n",
            "Saving model, epoch: 22808, train_loss: 0.89524245262146, val_loss: 0.8698949217796326\n",
            "Saving model, epoch: 22810, train_loss: 0.8952411413192749, val_loss: 0.8698936104774475\n",
            "Saving model, epoch: 22812, train_loss: 0.8952397704124451, val_loss: 0.8698924779891968\n",
            "Saving model, epoch: 22814, train_loss: 0.8952383399009705, val_loss: 0.8698915839195251\n",
            "Saving model, epoch: 22816, train_loss: 0.8952369689941406, val_loss: 0.8698901534080505\n",
            "Saving model, epoch: 22818, train_loss: 0.8952356576919556, val_loss: 0.8698886036872864\n",
            "Saving model, epoch: 22820, train_loss: 0.8952343463897705, val_loss: 0.8698862791061401\n",
            "Saving model, epoch: 22822, train_loss: 0.8952329158782959, val_loss: 0.8698829412460327\n",
            "Saving model, epoch: 22824, train_loss: 0.8952315449714661, val_loss: 0.8698792457580566\n",
            "Saving model, epoch: 22826, train_loss: 0.895230233669281, val_loss: 0.8698753118515015\n",
            "Saving model, epoch: 22828, train_loss: 0.8952286839485168, val_loss: 0.8698714375495911\n",
            "Saving model, epoch: 22830, train_loss: 0.8952274918556213, val_loss: 0.8698689341545105\n",
            "Saving model, epoch: 22889, train_loss: 0.8951877355575562, val_loss: 0.869868278503418\n",
            "Saving model, epoch: 22891, train_loss: 0.8951864242553711, val_loss: 0.8698667883872986\n",
            "Saving model, epoch: 22893, train_loss: 0.895185112953186, val_loss: 0.869866132736206\n",
            "Saving model, epoch: 22895, train_loss: 0.8951836824417114, val_loss: 0.8698657751083374\n",
            "Saving model, epoch: 22897, train_loss: 0.8951824903488159, val_loss: 0.8698656558990479\n",
            "epoch: 22901, train_loss: 0.8951796889305115, val_loss: 0.8698651194572449\n",
            "Saving model, epoch: 22901, train_loss: 0.8951796889305115, val_loss: 0.8698651194572449\n",
            "Saving model, epoch: 22903, train_loss: 0.8951783776283264, val_loss: 0.8698647618293762\n",
            "Saving model, epoch: 22905, train_loss: 0.8951771855354309, val_loss: 0.8698639869689941\n",
            "Saving model, epoch: 22907, train_loss: 0.8951756358146667, val_loss: 0.8698630332946777\n",
            "Saving model, epoch: 22909, train_loss: 0.8951744437217712, val_loss: 0.8698620796203613\n",
            "Saving model, epoch: 22911, train_loss: 0.8951729536056519, val_loss: 0.8698607683181763\n",
            "Saving model, epoch: 22913, train_loss: 0.8951717615127563, val_loss: 0.8698596954345703\n",
            "Saving model, epoch: 22915, train_loss: 0.8951704502105713, val_loss: 0.8698586821556091\n",
            "Saving model, epoch: 22917, train_loss: 0.8951690196990967, val_loss: 0.8698580861091614\n",
            "Saving model, epoch: 22919, train_loss: 0.8951676487922668, val_loss: 0.8698574900627136\n",
            "Saving model, epoch: 22921, train_loss: 0.8951662182807922, val_loss: 0.8698569536209106\n",
            "Saving model, epoch: 22923, train_loss: 0.8951650261878967, val_loss: 0.8698565363883972\n",
            "Saving model, epoch: 22925, train_loss: 0.8951637744903564, val_loss: 0.869856059551239\n",
            "Saving model, epoch: 22927, train_loss: 0.8951624035835266, val_loss: 0.8698555827140808\n",
            "Saving model, epoch: 22929, train_loss: 0.895160973072052, val_loss: 0.8698549866676331\n",
            "Saving model, epoch: 22931, train_loss: 0.8951596021652222, val_loss: 0.8698546290397644\n",
            "Saving model, epoch: 22933, train_loss: 0.8951583504676819, val_loss: 0.8698536157608032\n",
            "Saving model, epoch: 22935, train_loss: 0.895156979560852, val_loss: 0.8698523044586182\n",
            "Saving model, epoch: 22937, train_loss: 0.895155668258667, val_loss: 0.8698506355285645\n",
            "Saving model, epoch: 22939, train_loss: 0.8951543569564819, val_loss: 0.8698487877845764\n",
            "Saving model, epoch: 22941, train_loss: 0.8951530456542969, val_loss: 0.8698460459709167\n",
            "Saving model, epoch: 22943, train_loss: 0.8951517343521118, val_loss: 0.8698432445526123\n",
            "Saving model, epoch: 22945, train_loss: 0.895150363445282, val_loss: 0.8698396682739258\n",
            "Saving model, epoch: 22947, train_loss: 0.8951490521430969, val_loss: 0.869835376739502\n",
            "Saving model, epoch: 22949, train_loss: 0.8951477408409119, val_loss: 0.869830846786499\n",
            "Saving model, epoch: 22951, train_loss: 0.8951464295387268, val_loss: 0.8698270320892334\n",
            "Saving model, epoch: 22953, train_loss: 0.8951451182365417, val_loss: 0.8698253631591797\n",
            "epoch: 23001, train_loss: 0.8951137661933899, val_loss: 0.8698318600654602\n",
            "Saving model, epoch: 23026, train_loss: 0.8950973749160767, val_loss: 0.8698252439498901\n",
            "Saving model, epoch: 23028, train_loss: 0.8950960636138916, val_loss: 0.869824230670929\n",
            "Saving model, epoch: 23030, train_loss: 0.8950947523117065, val_loss: 0.8698237538337708\n",
            "Saving model, epoch: 23032, train_loss: 0.8950935006141663, val_loss: 0.8698232173919678\n",
            "Saving model, epoch: 23034, train_loss: 0.895092248916626, val_loss: 0.8698228001594543\n",
            "Saving model, epoch: 23035, train_loss: 0.8950915932655334, val_loss: 0.8698226809501648\n",
            "Saving model, epoch: 23036, train_loss: 0.8950908780097961, val_loss: 0.8698221445083618\n",
            "Saving model, epoch: 23037, train_loss: 0.8950902819633484, val_loss: 0.8698217272758484\n",
            "Saving model, epoch: 23039, train_loss: 0.8950890302658081, val_loss: 0.8698208332061768\n",
            "Saving model, epoch: 23041, train_loss: 0.8950876593589783, val_loss: 0.8698203563690186\n",
            "Saving model, epoch: 23043, train_loss: 0.8950864672660828, val_loss: 0.869819700717926\n",
            "Saving model, epoch: 23045, train_loss: 0.8950850367546082, val_loss: 0.8698188066482544\n",
            "Saving model, epoch: 23047, train_loss: 0.8950838446617126, val_loss: 0.8698181509971619\n",
            "Saving model, epoch: 23049, train_loss: 0.8950826525688171, val_loss: 0.8698177337646484\n",
            "Saving model, epoch: 23051, train_loss: 0.895081102848053, val_loss: 0.8698173761367798\n",
            "Saving model, epoch: 23052, train_loss: 0.8950803875923157, val_loss: 0.8698170781135559\n",
            "Saving model, epoch: 23054, train_loss: 0.8950791954994202, val_loss: 0.8698159456253052\n",
            "Saving model, epoch: 23056, train_loss: 0.8950778841972351, val_loss: 0.8698149919509888\n",
            "Saving model, epoch: 23058, train_loss: 0.8950766921043396, val_loss: 0.8698140382766724\n",
            "Saving model, epoch: 23060, train_loss: 0.8950753808021545, val_loss: 0.8698132038116455\n",
            "Saving model, epoch: 23062, train_loss: 0.8950740098953247, val_loss: 0.8698124289512634\n",
            "Saving model, epoch: 23064, train_loss: 0.8950727581977844, val_loss: 0.86981201171875\n",
            "Saving model, epoch: 23066, train_loss: 0.8950713872909546, val_loss: 0.8698117733001709\n",
            "Saving model, epoch: 23068, train_loss: 0.8950700759887695, val_loss: 0.8698112964630127\n",
            "Saving model, epoch: 23070, train_loss: 0.895068883895874, val_loss: 0.8698112368583679\n",
            "Saving model, epoch: 23071, train_loss: 0.8950681686401367, val_loss: 0.8698107600212097\n",
            "Saving model, epoch: 23073, train_loss: 0.8950669765472412, val_loss: 0.8698098659515381\n",
            "Saving model, epoch: 23075, train_loss: 0.8950655460357666, val_loss: 0.8698089122772217\n",
            "Saving model, epoch: 23077, train_loss: 0.8950643539428711, val_loss: 0.8698080778121948\n",
            "Saving model, epoch: 23079, train_loss: 0.895063042640686, val_loss: 0.8698070645332336\n",
            "Saving model, epoch: 23081, train_loss: 0.895061731338501, val_loss: 0.8698058128356934\n",
            "Saving model, epoch: 23083, train_loss: 0.8950604200363159, val_loss: 0.869803786277771\n",
            "Saving model, epoch: 23085, train_loss: 0.8950591087341309, val_loss: 0.8698017001152039\n",
            "Saving model, epoch: 23087, train_loss: 0.8950577974319458, val_loss: 0.8697986602783203\n",
            "Saving model, epoch: 23089, train_loss: 0.8950564861297607, val_loss: 0.869794487953186\n",
            "Saving model, epoch: 23091, train_loss: 0.8950552940368652, val_loss: 0.8697882890701294\n",
            "Saving model, epoch: 23093, train_loss: 0.8950541019439697, val_loss: 0.8697798848152161\n",
            "Saving model, epoch: 23095, train_loss: 0.8950529098510742, val_loss: 0.8697707056999207\n",
            "Saving model, epoch: 23097, train_loss: 0.8950516581535339, val_loss: 0.8697654008865356\n",
            "epoch: 23101, train_loss: 0.8950488567352295, val_loss: 0.8697932362556458\n",
            "epoch: 23201, train_loss: 0.8949868679046631, val_loss: 0.86977219581604\n",
            "Saving model, epoch: 23225, train_loss: 0.894972026348114, val_loss: 0.8697652816772461\n",
            "Saving model, epoch: 23226, train_loss: 0.894971489906311, val_loss: 0.8697647452354431\n",
            "Saving model, epoch: 23227, train_loss: 0.8949708938598633, val_loss: 0.8697646856307983\n",
            "Saving model, epoch: 23228, train_loss: 0.894970178604126, val_loss: 0.869763970375061\n",
            "Saving model, epoch: 23230, train_loss: 0.8949689865112305, val_loss: 0.8697636723518372\n",
            "Saving model, epoch: 23231, train_loss: 0.8949683904647827, val_loss: 0.8697636127471924\n",
            "Saving model, epoch: 23232, train_loss: 0.8949679136276245, val_loss: 0.8697631359100342\n",
            "Saving model, epoch: 23233, train_loss: 0.8949670791625977, val_loss: 0.8697630167007446\n",
            "Saving model, epoch: 23234, train_loss: 0.8949664831161499, val_loss: 0.869762122631073\n",
            "Saving model, epoch: 23236, train_loss: 0.8949652910232544, val_loss: 0.8697617053985596\n",
            "Saving model, epoch: 23238, train_loss: 0.8949640989303589, val_loss: 0.869761049747467\n",
            "Saving model, epoch: 23240, train_loss: 0.8949628472328186, val_loss: 0.8697608113288879\n",
            "Saving model, epoch: 23241, train_loss: 0.8949621319770813, val_loss: 0.8697602152824402\n",
            "Saving model, epoch: 23243, train_loss: 0.8949609398841858, val_loss: 0.8697597980499268\n",
            "Saving model, epoch: 23244, train_loss: 0.894960343837738, val_loss: 0.8697593212127686\n",
            "Saving model, epoch: 23246, train_loss: 0.8949592709541321, val_loss: 0.8697589635848999\n",
            "Saving model, epoch: 23247, train_loss: 0.8949585556983948, val_loss: 0.8697588443756104\n",
            "Saving model, epoch: 23248, train_loss: 0.8949578404426575, val_loss: 0.8697583079338074\n",
            "Saving model, epoch: 23250, train_loss: 0.8949565291404724, val_loss: 0.8697575330734253\n",
            "Saving model, epoch: 23252, train_loss: 0.8949554562568665, val_loss: 0.8697569370269775\n",
            "Saving model, epoch: 23254, train_loss: 0.8949542045593262, val_loss: 0.869756281375885\n",
            "Saving model, epoch: 23256, train_loss: 0.8949528336524963, val_loss: 0.869755744934082\n",
            "Saving model, epoch: 23258, train_loss: 0.8949516415596008, val_loss: 0.8697553873062134\n",
            "Saving model, epoch: 23259, train_loss: 0.8949511051177979, val_loss: 0.8697550892829895\n",
            "Saving model, epoch: 23260, train_loss: 0.8949505090713501, val_loss: 0.8697548508644104\n",
            "Saving model, epoch: 23261, train_loss: 0.8949497938156128, val_loss: 0.8697545528411865\n",
            "Saving model, epoch: 23262, train_loss: 0.8949493169784546, val_loss: 0.8697542548179626\n",
            "Saving model, epoch: 23263, train_loss: 0.8949486017227173, val_loss: 0.869753897190094\n",
            "Saving model, epoch: 23264, train_loss: 0.8949480056762695, val_loss: 0.8697537183761597\n",
            "Saving model, epoch: 23265, train_loss: 0.8949472904205322, val_loss: 0.8697532415390015\n",
            "Saving model, epoch: 23266, train_loss: 0.894946813583374, val_loss: 0.8697531819343567\n",
            "Saving model, epoch: 23267, train_loss: 0.8949461579322815, val_loss: 0.8697527647018433\n",
            "Saving model, epoch: 23268, train_loss: 0.8949455618858337, val_loss: 0.8697525858879089\n",
            "Saving model, epoch: 23269, train_loss: 0.894944965839386, val_loss: 0.8697519898414612\n",
            "Saving model, epoch: 23271, train_loss: 0.8949437737464905, val_loss: 0.8697510957717896\n",
            "Saving model, epoch: 23273, train_loss: 0.8949423432350159, val_loss: 0.8697503209114075\n",
            "Saving model, epoch: 23275, train_loss: 0.8949412703514099, val_loss: 0.8697498440742493\n",
            "Saving model, epoch: 23277, train_loss: 0.8949400782585144, val_loss: 0.8697492480278015\n",
            "Saving model, epoch: 23279, train_loss: 0.8949387669563293, val_loss: 0.8697488307952881\n",
            "Saving model, epoch: 23281, train_loss: 0.8949375748634338, val_loss: 0.8697484135627747\n",
            "Saving model, epoch: 23283, train_loss: 0.8949363231658936, val_loss: 0.8697481155395508\n",
            "Saving model, epoch: 23284, train_loss: 0.8949357271194458, val_loss: 0.8697475790977478\n",
            "Saving model, epoch: 23286, train_loss: 0.8949344158172607, val_loss: 0.8697468042373657\n",
            "Saving model, epoch: 23288, train_loss: 0.8949332237243652, val_loss: 0.8697459697723389\n",
            "Saving model, epoch: 23290, train_loss: 0.8949320316314697, val_loss: 0.8697453141212463\n",
            "Saving model, epoch: 23292, train_loss: 0.8949307203292847, val_loss: 0.8697444796562195\n",
            "Saving model, epoch: 23294, train_loss: 0.8949295282363892, val_loss: 0.8697434067726135\n",
            "Saving model, epoch: 23296, train_loss: 0.8949283957481384, val_loss: 0.8697419762611389\n",
            "Saving model, epoch: 23298, train_loss: 0.8949270844459534, val_loss: 0.86974036693573\n",
            "Saving model, epoch: 23300, train_loss: 0.8949258923530579, val_loss: 0.8697389960289001\n",
            "epoch: 23301, train_loss: 0.8949252963066101, val_loss: 0.8697472214698792\n",
            "Saving model, epoch: 23302, train_loss: 0.8949247002601624, val_loss: 0.8697373867034912\n",
            "Saving model, epoch: 23304, train_loss: 0.8949235081672668, val_loss: 0.869735598564148\n",
            "Saving model, epoch: 23306, train_loss: 0.8949220776557922, val_loss: 0.8697336316108704\n",
            "Saving model, epoch: 23308, train_loss: 0.894921064376831, val_loss: 0.869730532169342\n",
            "Saving model, epoch: 23310, train_loss: 0.894919753074646, val_loss: 0.8697268962860107\n",
            "Saving model, epoch: 23312, train_loss: 0.8949185609817505, val_loss: 0.869722306728363\n",
            "Saving model, epoch: 23314, train_loss: 0.894917368888855, val_loss: 0.8697178959846497\n",
            "Saving model, epoch: 23316, train_loss: 0.8949161171913147, val_loss: 0.8697150945663452\n",
            "epoch: 23401, train_loss: 0.8948650360107422, val_loss: 0.8697152733802795\n",
            "Saving model, epoch: 23402, train_loss: 0.8948643803596497, val_loss: 0.8697146773338318\n",
            "Saving model, epoch: 23404, train_loss: 0.8948631882667542, val_loss: 0.869714081287384\n",
            "Saving model, epoch: 23406, train_loss: 0.8948619961738586, val_loss: 0.8697139024734497\n",
            "Saving model, epoch: 23407, train_loss: 0.8948614001274109, val_loss: 0.8697134256362915\n",
            "Saving model, epoch: 23408, train_loss: 0.8948608040809631, val_loss: 0.8697131872177124\n",
            "Saving model, epoch: 23409, train_loss: 0.8948602676391602, val_loss: 0.8697126507759094\n",
            "Saving model, epoch: 23411, train_loss: 0.8948589563369751, val_loss: 0.8697120547294617\n",
            "Saving model, epoch: 23413, train_loss: 0.8948578834533691, val_loss: 0.8697113990783691\n",
            "Saving model, epoch: 23415, train_loss: 0.8948565721511841, val_loss: 0.869710385799408\n",
            "Saving model, epoch: 23417, train_loss: 0.8948554396629333, val_loss: 0.869709849357605\n",
            "Saving model, epoch: 23419, train_loss: 0.8948542475700378, val_loss: 0.8697088956832886\n",
            "Saving model, epoch: 23421, train_loss: 0.8948531746864319, val_loss: 0.8697080016136169\n",
            "Saving model, epoch: 23423, train_loss: 0.8948517441749573, val_loss: 0.8697066307067871\n",
            "Saving model, epoch: 23425, train_loss: 0.8948507308959961, val_loss: 0.8697054386138916\n",
            "Saving model, epoch: 23427, train_loss: 0.894849419593811, val_loss: 0.8697037100791931\n",
            "Saving model, epoch: 23429, train_loss: 0.8948482275009155, val_loss: 0.8697018623352051\n",
            "Saving model, epoch: 23431, train_loss: 0.8948470950126648, val_loss: 0.8696991801261902\n",
            "Saving model, epoch: 23433, train_loss: 0.8948459029197693, val_loss: 0.8696961998939514\n",
            "Saving model, epoch: 23435, train_loss: 0.8948447108268738, val_loss: 0.8696913123130798\n",
            "Saving model, epoch: 23437, train_loss: 0.8948435187339783, val_loss: 0.8696858286857605\n",
            "Saving model, epoch: 23439, train_loss: 0.8948425054550171, val_loss: 0.8696799278259277\n",
            "Saving model, epoch: 23441, train_loss: 0.8948413133621216, val_loss: 0.8696754574775696\n",
            "epoch: 23501, train_loss: 0.8948063254356384, val_loss: 0.8696891069412231\n",
            "Saving model, epoch: 23545, train_loss: 0.8947808146476746, val_loss: 0.8696752190589905\n",
            "Saving model, epoch: 23547, train_loss: 0.8947796821594238, val_loss: 0.869674563407898\n",
            "Saving model, epoch: 23549, train_loss: 0.8947786092758179, val_loss: 0.8696740865707397\n",
            "Saving model, epoch: 23551, train_loss: 0.8947774171829224, val_loss: 0.869673490524292\n",
            "Saving model, epoch: 23553, train_loss: 0.8947762846946716, val_loss: 0.8696731925010681\n",
            "Saving model, epoch: 23555, train_loss: 0.8947752118110657, val_loss: 0.869672954082489\n",
            "Saving model, epoch: 23559, train_loss: 0.8947727680206299, val_loss: 0.8696728348731995\n",
            "Saving model, epoch: 23561, train_loss: 0.8947715759277344, val_loss: 0.8696727752685547\n",
            "Saving model, epoch: 23563, train_loss: 0.8947705626487732, val_loss: 0.8696727156639099\n",
            "Saving model, epoch: 23564, train_loss: 0.8947699666023254, val_loss: 0.8696720600128174\n",
            "Saving model, epoch: 23566, train_loss: 0.8947687745094299, val_loss: 0.8696711659431458\n",
            "Saving model, epoch: 23568, train_loss: 0.8947675228118896, val_loss: 0.8696703910827637\n",
            "Saving model, epoch: 23570, train_loss: 0.8947665691375732, val_loss: 0.8696693778038025\n",
            "Saving model, epoch: 23572, train_loss: 0.894765317440033, val_loss: 0.8696688413619995\n",
            "Saving model, epoch: 23574, train_loss: 0.8947641253471375, val_loss: 0.8696686625480652\n",
            "Saving model, epoch: 23576, train_loss: 0.8947629332542419, val_loss: 0.8696683645248413\n",
            "Saving model, epoch: 23578, train_loss: 0.8947619199752808, val_loss: 0.8696678876876831\n",
            "Saving model, epoch: 23582, train_loss: 0.8947595357894897, val_loss: 0.8696675300598145\n",
            "Saving model, epoch: 23584, train_loss: 0.8947582840919495, val_loss: 0.8696669936180115\n",
            "Saving model, epoch: 23586, train_loss: 0.8947573304176331, val_loss: 0.8696666955947876\n",
            "Saving model, epoch: 23587, train_loss: 0.8947564959526062, val_loss: 0.8696664571762085\n",
            "Saving model, epoch: 23588, train_loss: 0.894756019115448, val_loss: 0.869666337966919\n",
            "Saving model, epoch: 23589, train_loss: 0.894755482673645, val_loss: 0.8696660995483398\n",
            "Saving model, epoch: 23590, train_loss: 0.8947548866271973, val_loss: 0.8696658611297607\n",
            "Saving model, epoch: 23591, train_loss: 0.8947542905807495, val_loss: 0.869665265083313\n",
            "Saving model, epoch: 23593, train_loss: 0.8947531580924988, val_loss: 0.86966472864151\n",
            "Saving model, epoch: 23595, train_loss: 0.8947520852088928, val_loss: 0.8696640133857727\n",
            "Saving model, epoch: 23597, train_loss: 0.8947508931159973, val_loss: 0.869663655757904\n",
            "Saving model, epoch: 23599, train_loss: 0.894749641418457, val_loss: 0.8696631193161011\n",
            "epoch: 23601, train_loss: 0.8947484493255615, val_loss: 0.8696624040603638\n",
            "Saving model, epoch: 23601, train_loss: 0.8947484493255615, val_loss: 0.8696624040603638\n",
            "Saving model, epoch: 23603, train_loss: 0.8947474360466003, val_loss: 0.869661808013916\n",
            "Saving model, epoch: 23605, train_loss: 0.8947462439537048, val_loss: 0.8696611523628235\n",
            "Saving model, epoch: 23607, train_loss: 0.8947450518608093, val_loss: 0.8696606159210205\n",
            "Saving model, epoch: 23609, train_loss: 0.8947440385818481, val_loss: 0.8696598410606384\n",
            "Saving model, epoch: 23611, train_loss: 0.8947428464889526, val_loss: 0.8696595430374146\n",
            "Saving model, epoch: 23613, train_loss: 0.8947416543960571, val_loss: 0.8696588277816772\n",
            "Saving model, epoch: 23615, train_loss: 0.894740641117096, val_loss: 0.8696584105491638\n",
            "Saving model, epoch: 23617, train_loss: 0.8947394490242004, val_loss: 0.8696579337120056\n",
            "Saving model, epoch: 23619, train_loss: 0.8947381377220154, val_loss: 0.8696572780609131\n",
            "Saving model, epoch: 23621, train_loss: 0.8947371244430542, val_loss: 0.8696565628051758\n",
            "Saving model, epoch: 23623, train_loss: 0.8947359919548035, val_loss: 0.869655966758728\n",
            "Saving model, epoch: 23625, train_loss: 0.894734799861908, val_loss: 0.8696550130844116\n",
            "Saving model, epoch: 23627, train_loss: 0.8947336077690125, val_loss: 0.8696534037590027\n",
            "Saving model, epoch: 23629, train_loss: 0.8947324752807617, val_loss: 0.869651198387146\n",
            "Saving model, epoch: 23631, train_loss: 0.8947314023971558, val_loss: 0.8696469664573669\n",
            "Saving model, epoch: 23633, train_loss: 0.894730269908905, val_loss: 0.8696403503417969\n",
            "Saving model, epoch: 23635, train_loss: 0.8947291970252991, val_loss: 0.8696293234825134\n",
            "Saving model, epoch: 23637, train_loss: 0.8947282433509827, val_loss: 0.8696136474609375\n",
            "Saving model, epoch: 23639, train_loss: 0.8947275876998901, val_loss: 0.8695995211601257\n",
            "epoch: 23701, train_loss: 0.8946937322616577, val_loss: 0.8696388006210327\n",
            "epoch: 23801, train_loss: 0.8946407437324524, val_loss: 0.8696178197860718\n",
            "Saving model, epoch: 23880, train_loss: 0.8945989608764648, val_loss: 0.8695992827415466\n",
            "Saving model, epoch: 23882, train_loss: 0.8945979475975037, val_loss: 0.8695986270904541\n",
            "Saving model, epoch: 23884, train_loss: 0.8945968747138977, val_loss: 0.8695983290672302\n",
            "Saving model, epoch: 23886, train_loss: 0.8945958614349365, val_loss: 0.8695979118347168\n",
            "Saving model, epoch: 23888, train_loss: 0.8945947289466858, val_loss: 0.8695976734161377\n",
            "Saving model, epoch: 23890, train_loss: 0.8945937156677246, val_loss: 0.8695973753929138\n",
            "Saving model, epoch: 23893, train_loss: 0.8945921063423157, val_loss: 0.8695971965789795\n",
            "Saving model, epoch: 23895, train_loss: 0.8945910334587097, val_loss: 0.869596540927887\n",
            "Saving model, epoch: 23897, train_loss: 0.894589900970459, val_loss: 0.869596004486084\n",
            "epoch: 23901, train_loss: 0.8945879340171814, val_loss: 0.869596004486084\n",
            "Saving model, epoch: 23902, train_loss: 0.8945873379707336, val_loss: 0.8695953488349915\n",
            "Saving model, epoch: 23904, train_loss: 0.8945863246917725, val_loss: 0.8695945143699646\n",
            "Saving model, epoch: 23906, train_loss: 0.8945853114128113, val_loss: 0.8695942759513855\n",
            "Saving model, epoch: 23908, train_loss: 0.8945841193199158, val_loss: 0.8695940971374512\n",
            "Saving model, epoch: 23909, train_loss: 0.8945837020874023, val_loss: 0.869593620300293\n",
            "Saving model, epoch: 23911, train_loss: 0.8945825695991516, val_loss: 0.8695932030677795\n",
            "Saving model, epoch: 23913, train_loss: 0.8945815563201904, val_loss: 0.8695923686027527\n",
            "Saving model, epoch: 23915, train_loss: 0.8945804834365845, val_loss: 0.8695918321609497\n",
            "Saving model, epoch: 23917, train_loss: 0.8945794701576233, val_loss: 0.8695917129516602\n",
            "Saving model, epoch: 23919, train_loss: 0.8945783972740173, val_loss: 0.8695913553237915\n",
            "Saving model, epoch: 23921, train_loss: 0.8945773839950562, val_loss: 0.8695910573005676\n",
            "Saving model, epoch: 23923, train_loss: 0.8945762515068054, val_loss: 0.8695909976959229\n",
            "Saving model, epoch: 23924, train_loss: 0.8945756554603577, val_loss: 0.8695905804634094\n",
            "Saving model, epoch: 23925, train_loss: 0.8945752382278442, val_loss: 0.8695904612541199\n",
            "Saving model, epoch: 23926, train_loss: 0.8945746421813965, val_loss: 0.8695901036262512\n",
            "Saving model, epoch: 23928, train_loss: 0.8945736289024353, val_loss: 0.8695896863937378\n",
            "Saving model, epoch: 23930, train_loss: 0.8945725560188293, val_loss: 0.8695892095565796\n",
            "Saving model, epoch: 23932, train_loss: 0.8945714235305786, val_loss: 0.8695889115333557\n",
            "Saving model, epoch: 23933, train_loss: 0.8945709466934204, val_loss: 0.8695886731147766\n",
            "Saving model, epoch: 23934, train_loss: 0.894570529460907, val_loss: 0.8695884943008423\n",
            "Saving model, epoch: 23935, train_loss: 0.8945699334144592, val_loss: 0.8695880770683289\n",
            "Saving model, epoch: 23937, train_loss: 0.894568920135498, val_loss: 0.8695876002311707\n",
            "Saving model, epoch: 23939, train_loss: 0.8945678472518921, val_loss: 0.8695870041847229\n",
            "Saving model, epoch: 23941, train_loss: 0.8945668339729309, val_loss: 0.8695866465568542\n",
            "Saving model, epoch: 23943, train_loss: 0.8945655822753906, val_loss: 0.869586169719696\n",
            "Saving model, epoch: 23945, train_loss: 0.8945646286010742, val_loss: 0.8695857524871826\n",
            "Saving model, epoch: 23947, train_loss: 0.894563615322113, val_loss: 0.8695854544639587\n",
            "Saving model, epoch: 23949, train_loss: 0.8945624828338623, val_loss: 0.8695850968360901\n",
            "Saving model, epoch: 23951, train_loss: 0.8945614099502563, val_loss: 0.8695847988128662\n",
            "Saving model, epoch: 23952, train_loss: 0.8945609927177429, val_loss: 0.8695846199989319\n",
            "Saving model, epoch: 23953, train_loss: 0.8945604562759399, val_loss: 0.8695845603942871\n",
            "Saving model, epoch: 23954, train_loss: 0.8945599794387817, val_loss: 0.8695839047431946\n",
            "Saving model, epoch: 23956, train_loss: 0.8945589065551758, val_loss: 0.8695833683013916\n",
            "Saving model, epoch: 23958, train_loss: 0.894557774066925, val_loss: 0.8695830702781677\n",
            "Saving model, epoch: 23960, train_loss: 0.8945567607879639, val_loss: 0.8695826530456543\n",
            "Saving model, epoch: 23962, train_loss: 0.8945556879043579, val_loss: 0.8695821762084961\n",
            "Saving model, epoch: 23964, train_loss: 0.8945546746253967, val_loss: 0.8695821166038513\n",
            "Saving model, epoch: 23965, train_loss: 0.8945541381835938, val_loss: 0.8695818781852722\n",
            "Saving model, epoch: 23966, train_loss: 0.894553542137146, val_loss: 0.869581401348114\n",
            "Saving model, epoch: 23969, train_loss: 0.8945519328117371, val_loss: 0.8695807456970215\n",
            "Saving model, epoch: 23971, train_loss: 0.8945509195327759, val_loss: 0.8695801496505737\n",
            "Saving model, epoch: 23973, train_loss: 0.8945498466491699, val_loss: 0.8695796132087708\n",
            "Saving model, epoch: 23975, train_loss: 0.8945488333702087, val_loss: 0.8695789575576782\n",
            "Saving model, epoch: 23977, train_loss: 0.8945478200912476, val_loss: 0.8695785999298096\n",
            "Saving model, epoch: 23979, train_loss: 0.8945467472076416, val_loss: 0.8695780038833618\n",
            "Saving model, epoch: 23981, train_loss: 0.8945457339286804, val_loss: 0.8695774674415588\n",
            "Saving model, epoch: 23983, train_loss: 0.8945444822311401, val_loss: 0.8695771098136902\n",
            "Saving model, epoch: 23985, train_loss: 0.8945435285568237, val_loss: 0.8695767521858215\n",
            "Saving model, epoch: 23987, train_loss: 0.8945425152778625, val_loss: 0.8695763349533081\n",
            "Saving model, epoch: 23989, train_loss: 0.8945413827896118, val_loss: 0.8695758581161499\n",
            "Saving model, epoch: 23993, train_loss: 0.8945394158363342, val_loss: 0.8695754408836365\n",
            "Saving model, epoch: 23995, train_loss: 0.8945382833480835, val_loss: 0.8695751428604126\n",
            "Saving model, epoch: 23997, train_loss: 0.8945371508598328, val_loss: 0.8695749640464783\n",
            "Saving model, epoch: 23998, train_loss: 0.8945367932319641, val_loss: 0.8695747256278992\n",
            "Saving model, epoch: 23999, train_loss: 0.8945360779762268, val_loss: 0.8695745468139648\n",
            "Saving model, epoch: 24000, train_loss: 0.8945356607437134, val_loss: 0.8695740699768066\n",
            "epoch: 24001, train_loss: 0.8945350646972656, val_loss: 0.8695744872093201\n",
            "Saving model, epoch: 24002, train_loss: 0.8945345878601074, val_loss: 0.869573712348938\n",
            "Saving model, epoch: 24004, train_loss: 0.8945335745811462, val_loss: 0.8695732951164246\n",
            "Saving model, epoch: 24006, train_loss: 0.8945324420928955, val_loss: 0.8695725798606873\n",
            "Saving model, epoch: 24008, train_loss: 0.8945314288139343, val_loss: 0.8695717453956604\n",
            "Saving model, epoch: 24010, train_loss: 0.8945304155349731, val_loss: 0.8695709109306335\n",
            "Saving model, epoch: 24012, train_loss: 0.8945293426513672, val_loss: 0.869569718837738\n",
            "Saving model, epoch: 24014, train_loss: 0.894528329372406, val_loss: 0.86956787109375\n",
            "Saving model, epoch: 24016, train_loss: 0.8945272564888, val_loss: 0.8695647120475769\n",
            "Saving model, epoch: 24018, train_loss: 0.8945262432098389, val_loss: 0.8695595860481262\n",
            "Saving model, epoch: 24020, train_loss: 0.8945252299308777, val_loss: 0.8695505857467651\n",
            "Saving model, epoch: 24022, train_loss: 0.894524335861206, val_loss: 0.8695362210273743\n",
            "Saving model, epoch: 24024, train_loss: 0.8945237994194031, val_loss: 0.8695181012153625\n",
            "Saving model, epoch: 24026, train_loss: 0.8945227861404419, val_loss: 0.8695124983787537\n",
            "epoch: 24101, train_loss: 0.8944859504699707, val_loss: 0.869556188583374\n",
            "epoch: 24201, train_loss: 0.8944380879402161, val_loss: 0.8695383071899414\n",
            "epoch: 24301, train_loss: 0.8943901062011719, val_loss: 0.8695198893547058\n",
            "finished training after 24327 epochs\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                    [-1, 1]               4\n",
            "================================================================\n",
            "Total params: 4\n",
            "Trainable params: 4\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "predicting...\n",
            "getting test data...\n",
            "config: {'INPUT_DIM': 14, 'TRAIN_PATH': 'covid.train.csv', 'TEST_PATH': 'covid.test.csv', 'MODEL_PATH': 'models/model.pth', 'PRED_PATH': 'pred.csv', 'EPOCH_NUM': 30000, 'BATCH_SIZE': 4096, 'VAL_RATIO': 0.1, 'OPTIMIZER': 'Adam', 'OPTIM_PARAMS': {'lr': 0.05}, 'DECAY_RATE': 0.999, 'MIN_LR': 0.0001, 'EARLY_STOP': 300, 'MODEL_NUM': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsFxYmKkF3eN",
        "outputId": "ed540144-3ce6-4777-eee6-af623fd93622"
      },
      "source": [
        "model = emssembler.model\n",
        "for param in model.parameters():\n",
        "  print(param.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.6077, 0.0980, 0.2946]], device='cuda:0')\n",
            "tensor([-0.0056], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}